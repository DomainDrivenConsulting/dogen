#+title: Sprint Backlog 71
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) spike(p) }

* Mission Statement

- Perform the main refactors;
- Simplify templates;
- Integrate Clang.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75
#+CAPTION: Clock summary at [2015-07-20 Mon 15:01]
| <75>                                                                        |        |      |      |
| Headline                                                                    | Time   |      |      |
|-----------------------------------------------------------------------------+--------+------+------|
| *Total time*                                                                | *0:26* |      |      |
|-----------------------------------------------------------------------------+--------+------+------|
| Stories                                                                     | 0:26   |      |      |
| Active                                                                      |        | 0:26 |      |
| STARTED Sprint and product backlog grooming                                 |        |      | 0:26 |
#+end:

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2015-07-20 Mon 14:30]--[2015-07-20 Mon 14:56] =>  0:26

Updates to sprint and product backlog.

*** STARTED Add support for pulling dependencies from biicode         :story:

[[https://www.biicode.com/][Biicode]] is a nuget-like repo for c++. We should look into both
consuming dependencies from it and pushing dogen into it. In addition
there are associated emblems:

https://github.com/Manu343726/snail

We should also look into [[https://www.biicode.com/biicode-open-source-challenge][the challenge]].

We should push both the C++ libraries as well as the dogen binary.

We should take the least intrusive possible approach to start with, by
creating a split setup for biicode.

*** Create a blog post on biicode                                     :story:

Investigate adding biicode support since we need to add a RapidJson
dependency. Create a blog post about it.

Post has [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/blog/biicode.org][already been started]].

*** Rename SML to =tack=                                              :story:

This will now be the name to reflect its "intermediate" state.

#+begin_quote
In sewing, to tack or baste is to make quick, temporary stitching
intended to be removed.
#+end_quote

*** Merge frontend with =tack=                                        :story:

Combine the two models performing the necessary renames:

- merge frontend with baste. Rename frontend interface to something
  like model source. Remove the dia frontend class, moving the code
  into the dia transformer.
- rename the types in baste to make them a bit more inline with
  MOF/eCore. As much as possible but without going overboard. Ensure
  we do not pick up meta-meta-model concepts by mistake. Rename nested
  qname to something more sensible from MOF/eCore. Review all concept
  names in this light.
- consider creating a top-level workflow that unites the frontend
  workflow with the "merging" workflow. Find good names for all
  workflows. A good name for the current SML workflow is =assembler=
  because it assembles a complete model from all the parts.
- consider creating a "file opener" that takes an input descriptor and
  returns a stream. This way the source interface can just be an
  ostream. This probably makes no sense for certain sources like dia
  though.

*** Rename types in =tack= using MOF/eCore terms                      :story:

Rename the types in =tack= to make them a bit more inline with
MOF/eCore. As much as possible but without going overboard. Ensure we
do not pick up meta-meta-model concepts by mistake. Rename nested
qname to something more sensible from MOF/eCore. Review all concept
names in this light.

*** Refactor code around model origination                            :story:

- remove origin types and generation types, replacing it with just a
  boolean for is target.

*Previous Understanding*

In the past we added a number of knobs around generation, all with
their own problems:

- =origin_types=: was the model/type created by the user or the
  system. in reality this means did the model come from Dia or
  JSON. this is confusing as the user can also add JSON files (their
  own model library) and in the future the user can use JSON
  exclusively without needed Dia at all.

- =generation_types=: if the model is target, all types are to be
  generated /unless/ they are not properly supported, in which case
  they are to be "partially" generated (as is the case with
  services). This is a formatter decision and SML should not know
  anything about it.

These can be replaced by a single enumeration that indicates if the
type/model is target or not.

This work should be integrated with the model types story.

*** Models should have an associated language                          :epic:

#+begin_quote
*Story*: As a dogen user, I want to make sure I only use valid system
models so that I don't generate models that code generate but do not
compile.
#+end_quote

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as dynamic extensions in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in SML
which is a property of the model, and one entry of which is "language
agnostic".

*** Set enumeration underlying type in SML                            :story:

In cpp transformer we have hacked the underlying type of the
enumeration. Remove this hack and set it in SML. Still a hack, but
a tad better.

Actually this could be the first case where LAM/PIM is used: we could
call this something like integer.

*** Add support for Language Agnostic Models (LAM)                    :story:

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N SML merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

FIXME: we believed this story was already backloged but could not find
it on a quick search. Do a more thorough search.

*** Thoughts on simplifying the formattables generation               :story:

We have a problem in the way which we are doing the formattables:
because we are doing model traversals for each of the factories, we
cannot easily introduce a set of manually generated qnames such as the
registrar and includers. However, if we started off the main workflow
by creating a structure like so:

- qname
- optional entity (new base class in SML); if null we need to create
  extensions as an empty object.

We then need a list of these that get passed in to all repository
factories. These use a visitor of entity to resolve to a type (where
required).

We can inject types to this list that have a qname but no entity. For
these we generate some parts of the formatter properties. Actually, we
still need to generate inclusion lists even when there is no
entity. Perhaps we need to create a new method in the provider that
does not take an SML entity but still generates the inclusion list.

Actually this should all be done in SML. We should have zero qname
look-ups coming out of SML, just follow references. This story is a
variation of the split between "partial" models and "full" models.

Well not everything should be done in SML. We still need to create a
structure with the properties above, but that is done by iterating
through a list in the SML model.

One slight problem with this approach: sometimes we need to preserve
some relationships in the newly generated objects. For registrar we
need to preserve the model leaves. For the includers / master headers
we need to express somehow the inclusion relationship at the formatter
level. The latter is definitely a special case because it is a pure
C++ concept: include files cannot be modeled in SML. However,
registrar is slightly different because we still need to compute the
includes based on the leaves. This means that the above approach will
not provide a clean solution, unless we synthesise an SML object when
providing the includes. And of course we need to be careful taking
that route or else we will end up generating the object across all
facets.

*** Consider reducing the number of qname lookups in cpp model        :story:

At present we are using qnames all over the place in CPP. Nothing
stops us from using strings instead of qnames if that is more
efficient.

What is worse is that we seem to be doing a ridiculous amount of qname
lookups. It would be much nicer if we could somehow have all the data
in the right shape to avoid doing so many lookups.

*** Handling of managed directories is incorrect                      :story:

At present we are querying the dia to sml transformer to figure out
what the managed directories are. These are basically the top-level
directories from where we want the housekeeper to operate. In reality
this is (or can be placed) in the meta-data. We should be able to
extract the managed directories from the meta-data as a step in one of
the workflows.

This can be done by the backend. It does mean that we should be
returning a composite type from generation:

- list of files;
- list of managed directories.

Alternatively we could have a =managed_directories= method that takes
in an SML model and then internally reads in the meta-data for a given
model to produce the list.

*Merged with previous story*

Compute managed directories from knitting options

At present the backend is returning empty managed directories. This
means housekeeping will fail in the new world. We need to change the
interface of this method to take in the knitting options and return
the managed directories.

This is not entirely trivial. At present the managed directories are
computed in the locator. It takes into account split project, etc to
come up with all the directories used by the backend. We need to make
these decisions during path expansion, expect we only need manged
directories for the root object. However we do not know which object
is the root object at present, during the expansion. We could identify
it via the QName and the SML model in context thought. We could then
populate the managed directories as a text collection. We then need
some settings and a factory to pull out the managed directories from
the root object. This could be done in =managed_directories=, by
having an SML model as input.

*** Add include providers for all types                               :story:

We need to implement the provider container support for primitives,
modules and concepts.

Update:

- inclusion dependencies factory
- provider container

*** Implement all formatter interfaces                                :story:

We still have a couple of skeleton interfaces:

- primitve
- concepts

*** Factor all =housekeeping_required= methods into one               :story:

In knit model we seem to have several of these: =housekeeping_required=.

*** Do not compute inclusion directives for system models             :story:

It seems we are computing inclusion directives and other path
derivatives for system models:

: {
:   "__type__": "dogen::cpp::expansion::path_derivatives",
:   "file_path": "/home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../test_data/all_primitives/actual/std/include/std/serialization/unique_ptr_fwd_ser.hpp",
:   "header_guard": "STD_SERIALIZATION_UNIQUE_PTR_FWD_SER_HPP",
:   "inclusion_directive": "<quote>std/serialization/unique_ptr_fwd_ser.hpp<quote>"
: }

This comes out of the workflow, so we possibly are then ignoring it
for the non-target types. So:

- can we avoid computing these altogether?
- are we ignoring it?

Actually this is the usual problem with the "origin" of the type. We
need a way to determine if this type needs computations or not. We
need to create a story to clean up the =origin_type= and
=generation_type= and then we can make use of it to determine if we
need to compute inclusion, path etc or not.

*** Header guard in formatters should be optional                     :story:

At present we are relying on empty header guards to determine what to
do in boilerplate. We should use boost optional.

*** Remove complete name and use qualified name                       :story:

At present we have both complete name and qualified name in
formatables. Qualified name is blank. We should remove complete name
and populate qualified name.

This is in nested type info.

*** Consider renaming registrar in boost serialisation                :story:

At present we have a registrar formatter that does the boost
serialisation work. However, the name =registrar= is a bit too
generic; we may for example add formatters for static registrars. We
should rename this formatter to something more meaningful. Also the
name registrar is already well understood to mean static registrar.

This is a big problem now that we cannot add a type with the name
registrar to the main model as it clashes with the serialisation
registrar.

** Deprecated
