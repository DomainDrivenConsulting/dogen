#+title: Sprint Backlog 94
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Finish adding C# support

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2016-12-19 Mon 17:44]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *41:56* |       |       | 100.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 41:56   |       |       | 100.0 |
| Active                                                                      |         | 41:56 |       | 100.0 |
| STARTED Sprint and product backlog grooming                                 |         |       |  0:56 |   2.2 |
| COMPLETED Edit release notes for previous sprint                            |         |       |  0:31 |   1.2 |
| COMPLETED Add helper infrastructure to C#                                   |         |       |  5:55 |  14.1 |
| COMPLETED Add support for Equals in C#                                      |         |       |  1:24 |   3.3 |
| COMPLETED Create a test model solution to validate C# code                  |         |       |  3:47 |   9.0 |
| COMPLETED Add basic io support to C#                                        |         |       | 11:56 |  28.5 |
| COMPLETED Add basic test data generation support to C#                      |         |       |  0:41 |   1.6 |
| COMPLETED Add io and equality canned unit tests to C# model                 |         |       |  3:55 |   9.3 |
| COMPLETED Update C# stitch templates to match code                          |         |       |  7:28 |  17.8 |
| COMPLETED Move enumeration population to yarn                               |         |       |  0:43 |   1.7 |
| COMPLETED Add support for types in packages in C#                           |         |       |  0:18 |   0.7 |
| COMPLETED Add support for enumerations in C#                                |         |       |  1:48 |   4.3 |
| COMPLETED Add support for exceptions in C#                                  |         |       |  0:14 |   0.6 |
| STARTED Add support for associations in C#                                  |         |       |  2:20 |   5.6 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2016-12-15 Thu 14:11]--[2016-12-15 Thu 14:44] =>  0:33
    CLOCK: [2016-12-12 Mon 15:07]--[2016-12-12 Mon 15:13] =>  0:06
    CLOCK: [2016-12-12 Mon 13:15]--[2016-12-12 Mon 13:32] =>  0:17

Updates to sprint and product backlog.

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2016-12-12 Mon 13:41]
    CLOCK: [2016-12-13 Tue 08:25]--[2016-12-13 Tue 08:40] =>  0:15
    CLOCK: [2016-12-12 Mon 13:42]--[2016-12-12 Mon 13:50] =>  0:08
    CLOCK: [2016-12-12 Mon 13:33]--[2016-12-12 Mon 13:41] =>  0:08

Add github release notes for previous sprint.

Title: Dogen v0.93.0, "Tombwa"

#+begin_src markdown
Overview
=======
The sprint's headline feature is the start of the C# kernel. It is only an experimental feature, but Dogen can already generate the basic infrastructure such as projects, solutions and simple classes. Only primitives are supported at present. For an example of a C# model see the test data sets:

- input: [Dia](https://github.com/DomainDrivenConsulting/dogen/blob/master/test_data/yarn.dia/input/CSharpModel.dia), [JSON](https://github.com/DomainDrivenConsulting/dogen/blob/master/test_data/yarn.json/input/CSharpModel.json)
- output: [CSharpModel](https://github.com/DomainDrivenConsulting/dogen/tree/master/projects/test_models/CSharpModel)

Most of the work carried out this sprint was related to adding multiple kernels to Dogen, so it is not user visible. Similarly, most of the remaining work was related to the C# kernel.

User visible changes
===============

- when outputting more than one kernel, Dogen now adds a kernel specific directory (e.g. ```cpp```. ```csharp```).
- the ```--project-dir``` command line option has been renamed to ```--output-dir```
- the ```hardware``` model has been split into language-specific "builtin" models

For more details see the [sprint log](https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_93.org).

Binaries
======
You can download experimental binaries from [Bintray](https://bintray.com/domaindrivenconsulting/Dogen) for OSX and Linux:

- [dogen_0.93.0_amd64-applications.deb](https://dl.bintray.com/domaindrivenconsulting/Dogen/0.93.0/dogen_0.93.0_amd64-applications.deb)
- [dogen-0.93.0-Darwin-x86_64.dmg](https://dl.bintray.com/domaindrivenconsulting/Dogen/0.93.0/dogen-0.93.0-Darwin-x86_64.dmg)

For other operative systems you need to build it from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/808591399855734784][Tweet]]

*** COMPLETED Add travis support for bintray                          :story:
    CLOSED: [2016-12-12 Mon 15:12]

*Rationale*: as of sprint 93 we are now generating packages in bintray
for OSX and Debian.

There is a new web application that interfaces with GitHub:
[[https://bintray.com/][bintray]].

- add JSON validation to bintray deployment descriptor.

Notes:

- an account was created [[https://bintray.com/mcraveiro][linked to GitHub]].
- it supports the uploads of [[https://bintray.com/mcraveiro/deb][debs]].
- [[https://bintray.com/docs/usermanual/uploads/uploads_howdoiuploadmystufftobintray.html][How Do I Upload My Stuff to Bintray?]]
- [[https://github.com/tim-janik/beast][Beast]] project [[https://raw.githubusercontent.com/tim-janik/beast/master/README.md][README]] with emblems
- Beast [[https://github.com/tim-janik/beast/blob/master/.travis.yml][travis.yml]]
- [[https://github.com/tim-janik/rapicorn/blob/master/citool.sh.][citool.sh]] script from [[https://github.com/tim-janik/rapicorn][Rapicorn]] project and their [[https://raw.githubusercontent.com/tim-janik/rapicorn/master/README.md][README]].

At present we are getting the following error:

: Skipping a deployment with the bintray provider because this branch is not permitted

This was fixed. We are now getting the following errors:

: Installing deploy dependencies
: [Bintray Upload] Reading descriptor file: build/scripts/bintray.json
: [Bintray Upload] Creating package 'auto-upload'...
: [Bintray Upload] Bintray response: 400 Bad Request. License 'GPL3' does not exist
: [Bintray Upload] Creating version '0.88.0'...
: [Bintray Upload] Bintray response: 404 Not Found. Package 'auto-upload' was not found
: [Bintray Upload] Warning: Path: build/output/Release/stage/pkg/*.deb does not exist.
: [Bintray Upload] Publishing version '0.88.0' of package 'auto-upload'...
: [Bintray Upload] Bintray response: 404 Not Found. Package 'auto-upload' was not found

Actually now we can't even find the bintray file:

: Installing deploy dependencies
: [Bintray Upload] Reading descriptor file: build/output/gcc/Release/stage/pkg/bintray.json
: /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/lib/dpl/provider/bintray.rb:54:in `read': No such file or directory @ rb_sysopen - build/output/gcc/Release/stage/pkg/bintray.json (Errno::ENOENT)
:   from /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/lib/dpl/provider/bintray.rb:54:in `read_descriptor'
:   from /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/lib/dpl/provider/bintray.rb:443:in `deploy'
:   from /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/lib/dpl/cli.rb:32:in `run'
:   from /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/lib/dpl/cli.rb:7:in `run'
:   from /home/travis/.rvm/gems/ruby-2.2.5/gems/dpl-1.8.21/bin/dpl:5:in `<top (required)>'
:   from /home/travis/.rvm/gems/ruby-2.2.5/bin/dpl:23:in `load'
:   from /home/travis/.rvm/gems/ruby-2.2.5/bin/dpl:23:in `<main>'
: failed to deploy

Ok so now we can find the bintray file but the upload fails with a
strange error:

: [Bintray Upload] Reading descriptor file: /home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/bintray.json
: [Bintray Upload] Warning: Path: /home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/dogen-0.91.0-Darwin-x86_64.dmg does not exist.
: [Bintray Upload] Uploading file '/home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/dogen_0.91.0_amd64-applications.deb' to
: [Bintray Upload] Bintray response: 400 Bad Request. Invalid file path and name
: [Bintray Upload] Uploading file '/home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/dogen_0.91.0_amd64-headers.deb' to
: [Bintray Upload] Bintray response: 400 Bad Request. Invalid file path and name
: [Bintray Upload] Uploading file '/home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/dogen_0.91.0_amd64-libraries.deb' to
: [Bintray Upload] Bintray response: 400 Bad Request. Invalid file path and name
: [Bintray Upload] Uploading file '/home/travis/build/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/pkg/dogen_0.91.0_amd64-tests.deb' to
: [Bintray Upload] Bintray response: 400 Bad Request. Invalid file path and name
: [Bintray Upload] Publishing version '0.91.0' of package 'dogen'...
: [Bintray Upload] Bintray response: 200 OK.

Same on OSX:

: [Bintray Upload] Uploading file '/Users/travis/build/DomainDrivenConsulting/dogen/build/output/clang/Release/stage/pkg/dogen-0.91.0-Darwin-x86_64.dmg' to
: [Bintray Upload] Bintray response: 400 Bad Request. Invalid file path and name
: [Bintray Upload] Publishing version '0.91.0' of package 'dogen'...
: [Bintray Upload] Bintray response: 200 OK.

Emailed bintray for help.

*** COMPLETED Add helper infrastructure to C#                         :story:
    CLOSED: [2016-12-13 Tue 12:47]
    CLOCK: [2016-12-13 Tue 12:21]--[2016-12-13 Tue 12:45] =>  0:24
    CLOCK: [2016-12-13 Tue 10:51]--[2016-12-13 Tue 11:42] =>  0:51
    CLOCK: [2016-12-13 Tue 09:55]--[2016-12-13 Tue 10:50] =>  0:55
    CLOCK: [2016-12-13 Tue 09:15]--[2016-12-13 Tue 09:54] =>  0:39
    CLOCK: [2016-12-12 Mon 16:31]--[2016-12-12 Mon 17:06] =>  0:35
    CLOCK: [2016-12-12 Mon 15:14]--[2016-12-12 Mon 16:30] =>  1:16
    CLOCK: [2016-12-12 Mon 13:51]--[2016-12-12 Mon 15:06] =>  1:15

It is now clear we will need helpers:

- types: floating point comparison.
- test data: for most proxy types.
- io: for most proxy types.

Add all of the required infrastructure to C# so we can make use of
helpers in these facets.

Notes:

- problem: we are transforming only those types for which we have
  formatters. This worked ok in C++ because we pretty much have
  formatters for all types. However, in C# we've only hooked
  =object=. This means we are filtering out all primitives, which then
  means helpers cannot bind. This also illustrates that helper
  families are really a yarn level concept - or at least should be
  extracted from the model prior to transformation. It is also a bit
  surprising that the transformer is performing some kind of
  reduction.
- we need to add "is enabled" to suppress number helpers for
  non-floating point elements in types. We also need to split the wale
  template for the helpers that need a different "is enabled".

Families:

- boolean
- object
- string
- character
- number

Patch:

#+begin_src
@@ -323,7 +324,7 @@ void helper_expander::populate_helper_properties(
          * We only want to process the master segment; the extensions
          * can be ignored.
          */
-        auto& e(*formattable.element());
+        const auto& e(*formattable.element());
#+end_src

*** COMPLETED Add support for Equals in C#                            :story:
    CLOSED: [2016-12-13 Tue 14:13]
    CLOCK: [2016-12-13 Tue 14:01]--[2016-12-13 Tue 14:13] =>  0:12
    CLOCK: [2016-12-13 Tue 13:40]--[2016-12-13 Tue 14:00] =>  0:20
    CLOCK: [2016-12-13 Tue 13:20]--[2016-12-13 Tue 13:39] =>  0:19
    CLOCK: [2016-12-13 Tue 12:47]--[2016-12-13 Tue 13:19] =>  0:32
    CLOCK: [2016-12-13 Tue 12:46]--[2016-12-13 Tue 12:47] =>  0:01

We need some basic support for Equals and GetHashCode.

Links:

- [[http://www.loganfranken.com/blog/687/overriding-equals-in-c-part-1/][Overriding Equals in C# (Part 1)]]
- [[http://www.loganfranken.com/blog/692/overriding-equals-in-c-part-2/][Overriding Equals in C# (Part 2)]]
- [[http://www.loganfranken.com/blog/698/overriding-equals-in-c-part-3/][Overriding Equals in C# (Part 3)]]

Problems with GetHashCode:

: Types/AllBuiltins.cs(109,38): error CS0176: Static member `object.ReferenceEquals(object, object)' cannot be accessed with an instance reference, qualify it with a type name instead
: Types/AllBuiltins.cs(111,38): error CS0176: Static member `object.ReferenceEquals(object, object)' cannot be accessed with an instance reference, qualify it with a type name instead

We need a way to know if a type has a static GetHashCode or not.

We also need a way to compare floating point numbers. In both cases
the easiest thing is to use helpers. See:

- [[http://stackoverflow.com/questions/3874627/floating-point-comparison-functions-for-c-sharp][Floating point comparison functions for C#]]

#+begin_src
public bool NearlyEqual(double a, double b, double epsilon)
{
    double absA = Math.Abs(a);
    double absB = Math.Abs(b);
    double diff = Math.Abs(a - b);

    if (a == b)
    { // shortcut, handles infinities
        return true;
    }
    else if (a == 0 || b == 0 || diff < Double.Epsilon)
    {
        // a or b is zero or both are extremely close to it
        // relative error is less meaningful here
        return diff < epsilon;
    }
    else
    { // use relative error
        return diff / (absA + absB) < epsilon;
    }
}
#+end_src

: NearlyEqual(FloatProperty, value.FloatProperty) &&
: NearlyEqual(DoubleProperty, value.DoubleProperty) &&

*** COMPLETED Create a test model solution to validate C# code        :story:
    CLOSED: [2016-12-14 Wed 16:04]
    CLOCK: [2016-12-14 Wed 15:05]--[2016-12-14 Wed 15:40] =>  0:35
    CLOCK: [2016-12-14 Wed 12:35]--[2016-12-14 Wed 12:53] =>  0:18
    CLOCK: [2016-12-14 Wed 11:23]--[2016-12-14 Wed 12:00] =>  0:37
    CLOCK: [2016-12-14 Wed 10:30]--[2016-12-14 Wed 11:22] =>  0:52
    CLOCK: [2016-12-14 Wed 10:25]--[2016-12-14 Wed 10:29] =>  0:04
    CLOCK: [2016-12-14 Wed 10:19]--[2016-12-14 Wed 10:24] =>  0:05
    CLOCK: [2016-12-14 Wed 09:02]--[2016-12-14 Wed 10:18] =>  1:16

We need to first implement the main C# features in MonoDevelop, and
then use that as a starting point for the templates.

Nuget:

: nuget restore Dogen.TestModels.sln

Build:

: xbuild Dogen.TestModels.sln

Run tests:

: mono packages/NUnit.ConsoleRunner.3.5.0/tools/nunit3-console.exe CSharpModel.Tests/bin/Debug/CSharpModel.Tests.dll

We have a problem with this approach: when we build from msbuild we
generate obj and bin directories; these then cause errors in the
codegen tests. We solved this in the Travis build by doing the C#
build at the end, but this is not ideal for local development.

The requirements are:

- we must be able to build from monodevelop without breaking code gen
  tests;
- we must be able to diff and rebase the monodevelop code.

One way to achieve this is to delete these directories after we copy
the data set across.

*** COMPLETED Add basic io support to C#                              :story:
    CLOSED: [2016-12-16 Fri 09:14]
    CLOCK: [2016-12-15 Thu 18:10]--[2016-12-15 Thu 18:32] =>  0:22
    CLOCK: [2016-12-15 Thu 14:45]--[2016-12-15 Thu 17:48] =>  3:03
    CLOCK: [2016-12-15 Thu 13:42]--[2016-12-15 Thu 14:10] =>  1:02
    CLOCK: [2016-12-15 Thu 10:05]--[2016-12-15 Thu 12:39] =>  2:34
    CLOCK: [2016-12-15 Thu 08:45]--[2016-12-15 Thu 09:38] =>  0:53
    CLOCK: [2016-12-14 Wed 16:05]--[2016-12-14 Wed 17:24] =>  1:19
    CLOCK: [2016-12-13 Tue 15:53]--[2016-12-13 Tue 18:24] =>  2:31
    CLOCK: [2016-12-13 Tue 15:15]--[2016-12-13 Tue 15:52] =>  0:37
    CLOCK: [2016-12-13 Tue 14:14]--[2016-12-13 Tue 14:23] =>  0:09

We need to implement the Dogen JSON debug output format in C#.

- namespaces for dumpers on other models
- use helper or dumper
- is simple type or not
- needs quotes or not
- needs tidy-up or not

Notes:

- create a dumper registrar per model
- add a static constructor that registers the dumper
- at the meta-model level we need to be able to distinguish between
  a) primitives that need quoting and those that do not b) proxy types
  with helpers c) regular model types with dumpers d) regular model
  types with dumpers and inheritance.

Tasks:

- Implement dumper registrar in MonoDevelop
- dumpers do not have the correct postfix
- change project to backslashes

Links:

- [[http://stackoverflow.com/questions/852181/c-printing-all-properties-of-an-object][C#: Printing all properties of an object]]
- [[https://github.com/mcshaz/BlowTrial/blob/master/GenericToDataFile/ObjectDumper.cs][GenericToDataFile/ObjectDumper.cs]]

Code:

#+begin_src
<#+
            for (const auto attr : o.local_attributes()) {
#>
            sb.Append("\"<#= attr.name().simple() #>\": ");
<#+
                if (!attr.parsed_type().is_current_simple_type())
#>
            sb.Append(<#= attr.parsed_type().current().simple() #>Dumper.Dump(target.<#= attr.name().simple() #>));
<#+
                else
#>
            sb.Append("\"<#= attr.name().simple() #>\": ");
<#+
            }
#>
#+end_src

*** COMPLETED Add basic test data generation support to C#            :story:
    CLOSED: [2016-12-16 Fri 09:37]
    CLOCK: [2016-12-16 Fri 08:55]--[2016-12-16 Fri 09:36] =>  0:41

We need to create a facet that generates domain types. It should be
idiomatic in C# - =IEnumerable=, etc.

Links:

- [[http://geekswithblogs.net/BlackRabbitCoder/archive/2010/04/21/more-fun-with-c-iterators-and-generators.aspx][More Fun with C# Iterators and Generators]]
- [[https://gist.github.com/DForshner/5533088][NaiveFibonacciSequenceGenerator.cs]]
- [[https://coding.abel.nu/2011/12/return-ienumerable-with-yield-return/][Return IEnumerable with yield return]]

*** COMPLETED Add io and equality canned unit tests to C# model       :story:
    CLOSED: [2016-12-16 Fri 15:39]
    CLOCK: [2016-12-16 Fri 15:21]--[2016-12-16 Fri 15:38] =>  0:17
    CLOCK: [2016-12-16 Fri 14:15]--[2016-12-16 Fri 15:20] =>  1:05
    CLOCK: [2016-12-16 Fri 11:07]--[2016-12-16 Fri 12:13] =>  1:06
    CLOCK: [2016-12-16 Fri 10:54]--[2016-12-16 Fri 11:07] =>  0:13
    CLOCK: [2016-12-16 Fri 10:42]--[2016-12-16 Fri 10:53] =>  0:11
    CLOCK: [2016-12-16 Fri 09:38]--[2016-12-16 Fri 10:41] =>  1:03

We developed a number of C++ "canned tests" for all model elements. We
need to port them across to C#.

*** COMPLETED Update C# stitch templates to match code                :story:
    CLOSED: [2016-12-19 Mon 10:49]
    CLOCK: [2016-12-19 Mon 10:43]--[2016-12-19 Mon 10:49] =>  0:06
    CLOCK: [2016-12-19 Mon 10:30]--[2016-12-19 Mon 10:42] =>  0:12
    CLOCK: [2016-12-19 Mon 10:18]--[2016-12-19 Mon 10:29] =>  0:11
    CLOCK: [2016-12-19 Mon 10:14]--[2016-12-19 Mon 10:17] =>  0:03
    CLOCK: [2016-12-19 Mon 09:02]--[2016-12-19 Mon 10:13] =>  1:11
    CLOCK: [2016-12-17 Sat 18:24]--[2016-12-17 Sat 18:42] =>  0:18
    CLOCK: [2016-12-17 Sat 18:09]--[2016-12-17 Sat 18:23] =>  0:14
    CLOCK: [2016-12-17 Sat 16:39]--[2016-12-17 Sat 16:58] =>  0:19
    CLOCK: [2016-12-17 Sat 16:35]--[2016-12-17 Sat 16:38] =>  0:03
    CLOCK: [2016-12-17 Sat 16:09]--[2016-12-17 Sat 16:34] =>  0:25
    CLOCK: [2016-12-17 Sat 15:31]--[2016-12-17 Sat 16:08] =>  0:37
    CLOCK: [2016-12-17 Sat 15:06]--[2016-12-17 Sat 15:30] =>  0:24
    CLOCK: [2016-12-17 Sat 14:52]--[2016-12-17 Sat 15:05] =>  0:13
    CLOCK: [2016-12-17 Sat 14:50]--[2016-12-17 Sat 14:51] =>  0:01
    CLOCK: [2016-12-17 Sat 14:24]--[2016-12-17 Sat 14:46] =>  0:22
    CLOCK: [2016-12-17 Sat 14:15]--[2016-12-17 Sat 14:23] =>  0:08
    CLOCK: [2016-12-17 Sat 14:02]--[2016-12-17 Sat 14:14] =>  0:12
    CLOCK: [2016-12-17 Sat 13:41]--[2016-12-17 Sat 14:01] =>  0:20
    CLOCK: [2016-12-17 Sat 12:11]--[2016-12-17 Sat 12:19] =>  0:08
    CLOCK: [2016-12-17 Sat 11:52]--[2016-12-17 Sat 12:10] =>  0:18
    CLOCK: [2016-12-17 Sat 11:45]--[2016-12-17 Sat 11:51] =>  0:06
    CLOCK: [2016-12-17 Sat 11:14]--[2016-12-17 Sat 11:44] =>  0:30
    CLOCK: [2016-12-17 Sat 10:39]--[2016-12-17 Sat 11:13] =>  0:34
    CLOCK: [2016-12-17 Sat 10:05]--[2016-12-17 Sat 10:38] =>  0:33

Now we've finished adding sequence generators and io support with
tests manually, we need to make the code generator match the
handcrafted code.

- qualified name support
- camel case support
- dumper does not have "Dumper" prefix
- no properties still needs methods
- helpers must end in "Dumper" since we're using that as the facet
  postfix. Also, its very confusing to have helpers which are not
  Dogen helpers. We need to rename them.

*** COMPLETED Move enumeration population to yarn                     :story:
    CLOSED: [2016-12-19 Mon 12:17]
    CLOCK: [2016-12-19 Mon 11:34]--[2016-12-19 Mon 12:17] =>  0:43

At present we are populating enumeration values, adding =invalid=
enumerator, etc in each of the frontends. This is not ideal; we should
only populate the basic information in the frontend and then expand it
in yarn.

*** COMPLETED Add support for types in packages in C#                 :story:
    CLOSED: [2016-12-19 Mon 14:02]
    CLOCK: [2016-12-19 Mon 13:47]--[2016-12-19 Mon 14:05] =>  0:18

We need to add elements in our test model inside a package and ensure
the generated code is correct.

*** COMPLETED Add support for enumerations in C#                      :story:
    CLOSED: [2016-12-19 Mon 13:49]
    CLOCK: [2016-12-19 Mon 14:06]--[2016-12-19 Mon 14:09] =>  0:03
    CLOCK: [2016-12-19 Mon 13:13]--[2016-12-19 Mon 13:46] =>  0:33
    CLOCK: [2016-12-19 Mon 12:41]--[2016-12-19 Mon 13:12] =>  0:31
    CLOCK: [2016-12-19 Mon 11:26]--[2016-12-19 Mon 11:33] =>  0:07
    CLOCK: [2016-12-19 Mon 10:51]--[2016-12-19 Mon 11:25] =>  0:34

Add all the machinery needed to generate enums in C#. We need to also
add the flag for default enum:

:         "is_default_enumeration_type": true,

This is causing errors at the moment:

: std::exception::what: Type not found: is_default_enumeration_type

Which is weird since it exists in C++. Actually this is not a
meta-data parameter, just a regular Json element.

Links:

- [[https://msdn.microsoft.com/en-us/library/sbbt4032.aspx][enum (C# Reference)]]

*** COMPLETED Add support for exceptions in C#                        :story:
    CLOSED: [2016-12-19 Mon 14:23]
    CLOCK: [2016-12-19 Mon 14:10]--[2016-12-19 Mon 14:24] =>  0:14

We don't need anything particularly elaborate, just a trivial
exception inheriting from =System.Exception=.

Links:

- [[https://msdn.microsoft.com/en-us/library/ms173163.aspx][Creating and Throwing Exceptions (C# Programming Guide)]]

*** STARTED Add support for associations in C#                        :story:
    CLOCK: [2016-12-19 Mon 17:01]--[2016-12-19 Mon 17:44] =>  0:43
    CLOCK: [2016-12-19 Mon 16:28]--[2016-12-19 Mon 17:00] =>  0:32
    CLOCK: [2016-12-19 Mon 16:01]--[2016-12-19 Mon 16:27] =>  0:26
    CLOCK: [2016-12-19 Mon 14:44]--[2016-12-19 Mon 15:05] =>  0:21
    CLOCK: [2016-12-19 Mon 14:25]--[2016-12-19 Mon 14:43] =>  0:18

Add support for association with other model types.

Problems:

- the key problem is helpers: we do not have a formatter for
  =Default=. The quick solution for this may just be to inject this
  family against an empty list of helpers.
- we have been using sequences for handling calls to add with member
  separator. However, this won't work in the case of model types which
  have a completely different API. We need to somehow have two
  different values in the sequence, depending of the type or somehow
  make the API consistent.

*** Rename formatting assistant                                       :story:

With the introduction of assistants in C#, we now have overloaded the
term. We need to find another name to refer to the formatting
assistant in C++ so avoid confusion.

*** Consider adding a clone method for C#                             :story:

It would be nice to have a way to clone a object graph. We probably
have an equivalent story for this for C++ in the backlog.

*** Consider making the output directory configurable in C#           :story:

At present we are outputting binaries into the =bin= directory,
locally on the project directory. However, it would make more sense to
output to =build/output= like C++ does. For this to work, we need to
be able to supply an output directory as meta-data.

*** Add depth detection to io in C++                                  :story:

In C# we added support for detecting the depth of the graph and
exiting after we've gone too deep. This is an effective way of
handling cycles in the graph until we have better solutions. We need
to adopt something similar for C++.

*** Move io code in types in C++ to io facet                          :story:

Originally we implemented io support for inheritance by making use of
virtual functions. This is still the easiest way to do type
dispatching; however, we then placed the io implementation in
types. This is a bit annoying because it clutters types with io
machinery. Another way of doing this is:

- create a class to do the streaming for each type, call it =dumper=;
- when there is no inheritance, =operator<<= simply calls the
  appropriate dumper.
- when there is inheritance, to_stream calls the appropriate dumper
  directly; =operator<<= calls =to_stream=. in an ideal world we could
  even make it private and =operator<<= a friend.

With this, we no longer need all the complications of supporting io
helpers in types (enabled in helpers, etc). We just need to determine
if io is enabled (and in inheritance), in which case we output
=to_stream= and for implementation, also include/use the dumper. Note
that we still need to declare the dumpers in the io headers - at least
for types involved in inheritance, but probably in all cases for
consistency.

*** Reducing the overhead of other facets in types                    :story:

Note: This story is a bit far-fetched at the moment, but it is a place
to collect ideas on this space.

There is a tricky problem with io and inheritance: when using a facet,
a user should only pay the cost of that facet and nothing else;
however, we could not find any efficient ways of type dispatching
across models for io. This meant that we ended up adding a
=to_stream= method to types that are part of an inheritance
relationship. The downside of this approach is that even if one does
not use io, one ends up paying the cost of carrying this method
around.

No good alternatives have been found:

- its not possible to use visitors because we now allow cross-model
  inheritance; thus we do not know what visitor to use.
- one could register types against a base streamer for an inheritance
  tree; the downside of this approach is efficiency. We'd have to do a
  map look-up to find the correct streamer. Its possible but not
  entirely trivial to use a vector as we only know the size of the
  inheritance tree at run-time and so we'd have to assign positions in
  the vector as types register. This means we'd have to have some kind
  of static member variable on each type to remember their index, and
  this would be populated as a result of registration. This also means
  we'd still be impacting types with the static index. This is akin to
  a vtable but with a twist. Whereas the vtable is associated with an
  object, we'd have a vtable per inheritance tree; the index for each
  object is in each class (but it must be populated at run time). The
  size of the vtable must also be determined once all types have
  registered (or we can continue to grow it during the registration
  phase; a one-off cost).

Actually this seems to be a common problem; we did the same for
visitors. It would be nice to only pay visitor costs when one intends
to use it. The current implementation menas we are carrying a vtable
just because of this (and of =to_stream=). In an ideal scenario,
visitor would itself carry the vtable.

Links:

- [[http://www.learncpp.com/cpp-tutorial/125-the-virtual-table/][12.5 — The virtual table]]

*** Add support for nuget                                             :story:

A proxy model may require obtaining a nuget package. Users should be
able to define a proxy model as requiring a nuget package and then
Dogen should generate packages.config and add all such models to it.

: +  <package id="NUnit" version="2.6.4" targetFramework="net45" />

*** Identifiable needs to use camel case in C#                        :story:

At present we are building identifiables with underscores.

*** Generate windows packages with CPack                              :story:

We tried to generate windows packages by using the NSIS tool, but
there are no binaries available for it at present. However, it seems
CPack can now generate MSIs directly:

- [[http://stackoverflow.com/questions/18437356/how-to-generate-msi-installer-with-cmake][How to generate .msi installer with cmake?]]
- [[https://cmake.org/cmake/help/v3.0/module/CPackWIX.html][CPackWIX]]

We need to investigate how to get the build to produce MSIs using WIX.

*** Language namespaces and modeling element locations                :story:

When we designed Dogen's meta-model yarn, we created a separation from
"physical space" and "modeling space". That is, a modeling element
living in modeling space does not know of any implementation specific
details such as serialisation or test data generation. Those are
concerns left to the kernels that implement "physical space" such as
the C++ kernel and are normally implemented as separate facets. Again,
facets are a "physical concept" and have no equivalent in modeling
space.

Facets normally tend to have a folder associated, originally
envisioned as a way keep the code a bit more manageable. If we take
the [[https://github.com/DomainDrivenConsulting/dogen/tree/master/projects/yarn/include/dogen/yarn][yarn model itself]] as an example:

- types: domain types
- hash: support for std::hash
- io: iostreams support
- serialization: boost serialisation support
- test_data: test data generators

Crucially, modeling space is not aware at all of these folders and
thus they are not related to the modeling space concept of modules. So
it is that the domain type, housed in the types folder, is [[https://github.com/DomainDrivenConsulting/dogen/blob/master/projects/yarn/include/dogen/yarn/types/enumeration.hpp][defined as]]:

#+begin_src
...
namespace dogen {
namespace yarn {

/**
 * @brief Defines a bounded set of logically related values for a primitive type
 * or a string.
 */
class enumeration final : public dogen::yarn::element {
...
#+end_src

And so forth (note the absence of "types" in the namespace
declaration). This worked well for C++. However, this approach may
cause problems for C# and will certainly cause problems for Java. This
is because in these languages, folders are supposed to correspond to
namespaces. In C# this is largely optional, but in Java it is
mandatory. Thus we need some way of injecting the facet directories as
internal modules before we code generate.

Actually this is non-trivial; all references to types will now have to
concern themselves with the facet. For example, say test data
generator is referring to the domain type; this now needs to be
qualified correctly, as they are in different namespaces. This
requires quite a bit of thinking in order to generate compilable
code.

On further thought, perhaps its not that bad. We just to be able to
distinguish proxy from non-proxy types (in order to know whether to
apply the "fake" facet namespace); then, we either apply the current
facet (say test data) or types. We don't refer to a third facet. In
addition, we can also use the facet folder as the fake namespace. So,
before we make use of a name, we need to call the assistant to inject
the fake internal module, either with the current facet or types; this
is done for all non-proxy names. The "is proxy" property needs to be
added to names.

Tasks:

- add a meta-data flag to enable/disable this feature.
- in assistant, during code generation, provide a function which
  injects the internal module.

*** Move enablement into quilt                                        :story:

We need to make use of the exact same logic as implemented in
=quilt.cpp= for enablement. Perhaps all of the enablement related
functionality can be lifted and grafted onto quilt without any major
changes.

*** Add support for Decimal numbers in C++                            :story:

- try using ICU DecNumber library.
- check compiler support (MSVC may have decimals; if so, use that instead)

*** Add feature to disable regions                                    :story:

We need a way to stop outputting regions if the user does not want
them.

*** Add parameters for using and imported assembly                    :story:

Assemblies imported via proxy models need to have the ability to
supply two parameters:

- assembly name: this is not always the same as the proxy model name;
- root namespace: similarly this may differ from the proxy model name.

These should be supplied as meta data and used when constructing
fabric types.

*** Add complete constructor for C# types                             :story:

We need a constructor that takes in all properties.

*** Add msbuild target for C# test model                              :story:

Once we are generating solutions, we should detect msbuild (or xbuild)
and build the solution. This should be a CMake target that runs on
Travis.

*** Use an unordered map in qualified name                            :story:

For some reason we are using a map, but its not clear that we need
sorting. Change it to unordered and see what breaks.

It seems we get errors in serialisation when using the map. Create a
patch and investigate this later.

*** Generate AssemblyInfo in C#                                       :story:

We need to inject a type for this in fabric. For now we can leave it
mainly blank but in the future we need to have meta-data in yarn for
all of its properties:

: [assembly: AssemblyTitle ("TestDogen")]
: [assembly: AssemblyDescription ("")]
: [assembly: AssemblyConfiguration ("")]
: [assembly: AssemblyCompany ("")]
: [assembly: AssemblyProduct ("")]
: [assembly: AssemblyCopyright ("marco")]
: [assembly: AssemblyTrademark ("")]
: [assembly: AssemblyCulture ("")]
: [assembly: AssemblyVersion ("1.0.*")]

These appear to just be properties at the model level.

*** Add visibility to yarn elements                                   :story:

We need to be able to mark yarn types as:

- public
- internal

This can then be used by C++ as well for visibility etc.

*** Add partial element support to yarn                               :story:

We need to be able to mark yarn elements as "partial". It is then up
to programming languages to map this to a language feature. At present
only [[https://msdn.microsoft.com/en-us/library/wa80x488.aspx][C# would do so]].

It would be nice to have a more meaningful name at yarn
level. However, seems like this is a fairly general programming
concept now: [[https://en.wikipedia.org/wiki/Class_(computer_programming)#Partial][wikipedia]].

*** Add visibility to yarn attributes                                 :story:

We need to be able to mark yarn attributes as:

- public
- private
- protected

*** Add final support in C#                                           :story:

Links:

- [[https://msdn.microsoft.com/en-us/library/88c54tsw.aspx][sealed (C# Reference)]]

*** Add aspects for C# serialisation support                          :story:

We need to add serialisation support:

- C# serialisation
- Data Contract serialisation
- Json serialisation

In C# these are done via attributes so we do not need additional
facets. We will need a lot of configuration knobs though:

- ability to switch a serialisation method on at model level or
  element level.
- support for serialisation specific arguments such as parameters for
  Json.Net.

Links:

- [[https://msdn.microsoft.com/en-us/library/ms731923(v%3Dvs.110).aspx][Types Supported by the Data Contract Serializer]]
- [[https://msdn.microsoft.com/en-us/library/ms731073(v%3Dvs.110).aspx][Serialization and Deserialization]]
- [[https://msdn.microsoft.com/en-us/library/ms733127(v%3Dvs.110).aspx][Using Data Contracts]]
- [[https://msdn.microsoft.com/en-us/library/ms731923(v%3Dvs.110).aspx][Types Supported by the Data Contract Serializer]]

*** Clean up comment formatter                                        :story:

Comment formatter is now a mess of ifs and boolean variables. We need
to create a proper state machine describing its internals and then
implement it.

*** Consider removing filtering ostream                               :story:

Originally we added a boost based stream to handle
indentation. However, since we moved over to stitch, there probably is
no need to use it any longer. We need to investigate if the formatters
model is making use of it (generating comments, namespaces, etc). If
not, remove it.

*** Knitting =quilt= does not work                                    :story:

When we invoke =knit_quilt= for some reason we seem to knit
=quilt.cpp=:

: $ ninja knit_quilt
: [1/1] Knitting Quilt C++ model

This seems to be some kind of ninja "feature".

For the moment we've put in a very ugly fix: we renamed the target
=knit_quiltx=.

*** Use templates for directory and prefix fields                     :story:

At present we have a lot of duplication on the annotations for certain
fields. This is because we need different defaults depending on the
facet etc. A different approach would be to use the appropriate
template (without default values) and then using profiles to default
those that need defaulting.

Other fields may also need a similar clean up:

- overwrite

In addition, we could add support for "default value variables". These
are useful for directories. They work as follows: the default value is
something like =${facet.simple_name}= or perhaps just
=${simple_name}=, in which case we assume the template kind determines
the target. Say the target is the kernel:

:      "family": "quilt",
:      "kernel": "quilt.cpp",

The simple name is then =kernel - family=, e.g. =cpp=. Unfortunately
this does not work for prefix.

Tasks:

- make prefix a recursive field at archetype level, adding default
  values to profiles.
- make directory a recursive field at facet level,  adding default
  values to profiles.

*** Add an example of redis and dogen                                 :story:

Building external project:

: cd /home/marco/Development/DomainDrivenConsulting/redis/build/output/gcc-6/Release &&
: CMAKE_PROGRAM_PATH=/home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/bin
: CMAKE_INCLUDE_PATH=/usr/local/personal/include CMAKE_LIB_PATH=/usr/local/personal/lib
: cmake ../../../.. -G Ninja && Ninja -j5

Redis client:

https://github.com/nekipelov/redisclient
git@github.com:nekipelov/redisclient.git

*** Add support for object caches                                      :epic:

It would be good to have meta-model knowledge of "cacheability". This
is done by marking objects with a stereotype of =Cacheable=. It then
could translate to:

- adding a serialisation like interface with gets, puts, etc. We need
  to bind this to a specific cache such as memcache, coherence, etc.
- create a type to string which converts a key made up of primitives
  into a underscore delimited string, used as a key in the cache.
- we should also consider external libraries like [[https://github.com/cripplet/cachepp][cachepp]].

*** Add support for Language Agnostic Models (LAM)                    :story:

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N yarn merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

Tasks:

- create the LAM model with a set of basic types.
- add a set of mapping fields into yarn: =yarn.mapping.csharp=, etc
  and populate the types with entries for each supported language.
- create a notion of mapping of intermediate models into
  languages. The input is the merged intermediate model and the output
  is N models one per language. We also need a way to associate
  backends with languages. Each model is sent down to its backend.
- note that reverse mapping is possible: we should be able to
  associate a type on a given language with it's lam type. This means
  that, given a model in say C#, we could reconstruct a yarn lam model
  (or tell the user about the list of failures to map). This should be
  logged as a separate story.

Links:

- [[http://stackoverflow.com/questions/741054/mapping-between-stl-c-and-c-sharp-containers][Mapping between stl C++ and C# containers]]
- [[http://stackoverflow.com/questions/3659044/comparison-of-c-stl-collections-and-c-sharp-collections][Comparison of C++ STL collections and C# collections?]]

*** Add C++-03 mode                                                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to create models in C++ 03 so that I
can interface with legacy code.
#+end_quote

It shouldn't be too hard to generate C++-03 code in addition to
C++-14. We could follow the gcc/odb convention and have a =-std=
option for this in meta-data. The only problem would be testing - at
present the language settings comes from cmake, and we'd have to make
sure the compiler is not in C++-14 mode when compiling test models
in 03. Also, the mixing and matching of 03 with 14 may not be
trivial. We should wait for a use case.

It may be possible to add different flags to different projects in CMake.

*** Add support for thrift and protocol buffers                        :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose dogen models to other
languages so that I can make use of them on these languages.
#+end_quote

Amongst other things, these technologies provide cross-language
support, allowing one to create c++ services and consume them from say
ruby, python, etc. At their heart they are simplified versions of
CORBA/DCOM, with IDL equivalents, IDL compilers, specification for
wire formats, etc. As they all share a number of commonalities, we
shall refer to these technologies in general as Distributed Services
Technologies (DST). We could integrate DST's with Dogen in two
ways. First approach A:

- generate the IDL for a model; we have enough information to produce
  something that is very close to it's Dogen representation,
  translated to the type system of the IDL; e.g. map =std::string=,
  =std::vector=, etc to their types. This IDL is then compiled by the
  DST's IDL to C++ compiler. Note: we could use LAM for this, but the
  problem is if one starts with a C++ model, one would have to convert
  it into LAM just to be able to do the mappings. A solution for this
  problem would be to "reverse map" LAM from C++ and get to the
  generic type this way.
- possibly generate the transformation code that takes a C++ object
  generated by Dogen and converts it into the C++ object generated by
  the DST's C++ compiler and vice-versa. We probably have enough
  information to generate these transformers automatically, after some
  analysis of the code generated by the DST's C++ compiler.

In order for this to work we need to have the ability to understand
function signatures for services so that we can generate the correct
service IDL for the DST. In fact, we should be able to mark certain
services as DST-only so that we do not generate a Dogen representation
for them. The DST service then internally uses the transformer to take
the DST's domain types and convert them into Dogen domain types, and
then uses the Dogen object model to implement the guts of the
service. When shipping data out, the reverse process takes place.

Approach A works really well when a service has a very narrow
interface, and performs most of it's work internally without exposing
it via the interface. Once the service requires the input (and/or
output) of a large number of domain types, we hit a cost limitation;
we may end up defining as many types in Dogen as there are in the IDL,
thus resulting in a large amount of transformations between the two
object models.

In these cases one may be tempted to ignore Dogen and implement the
service directly in terms of the DST's object model. This is not very
convenient as the type system is not as expressive as regular C++ -
there are a number of conventions that must be adopted, and
limitations imposed too due to the expressiveness of the IDL. We'd
also loose all the services provided by Dogen, which was the main
reason why we created it in the first place.

Approach B is more difficult. We could look into the wire format of
each DST and implement it as serialisation mechanism. For this to
work, the DST must:

- provide some kind of raw interface that allows one to plug in types
  serialisation manually. Ideally we wouldn't have to do this for
  services, just for domain types, but it depends on the low-level
  facilities available. A cursory look at both thrift and protocol
  buffers does not reveal easy access to such an interface.
- provide either a low-level wire format library (e.g. =std::string=
  to =string=, etc) or a well specified wire format that we could
  easily implement from scratch.

This approach is the cleaner technically, but its a lot of work, and
very hard to get right. We would have to have a lot of round-trip
tests. In addition, DST's such as thrift provide a wealth of wire
formats, so if there is no easy-access low-level wire format library,
it would be very difficult to get this right.

*** Add support for BSON serialisation                                :story:

It would be useful to support Mongo DB's BSON. There is a C++ stand
alone library for this:

https://github.com/jbenet/bson-cpp

For examples on how to use the C++ API see the tutorial:

https://github.com/mongodb/mongo-cxx-driver/wiki/Tutorial

*** Add support for deprecation                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to mark certain properties, classes
or methods as deprecated so that I can tell my users to stop using
them.
#+end_quote

We should be able to mark classes and properties as deprecated and
have that reflected in both doxygen and C++-11 deprecated attributes.

Note that at present nothing stops the users from adding the marker
themselves.

Perhaps we should add general support for attributes. This would be
useful for languages like C# and Java, to control serialisation, etc.

*** Add a frontend for visual studio models                           :story:

It should be "fairly straightforward" to add a frontend for visual
studio. A sample project has been added to test data:

: test_data/visual_studio_modeling

We should also extend tailor to output these projects so we can test
it with existing models.

*** Create a tool to generate product skeletons                       :story:

Now that dogen is evolving to a MDSD tool, it would be great to be
able to create a complete product skeleton from a tool. This would
entail:

- directory structure. We should document our standard product
  directory structure as part of this exercise. Initial document added
  to manual as "project_structure.org".
- licence: user can choose one.
- copyright: input by user, used in CMakeFiles, etc. added to the
  licence.
- CI support: travis, appveyor
- EDE support:
- CMake support: top-level CMakefiles, CPack. versioning
  templates, valgrind, doxygen. For CTest we should also generate a
  "setup cron" and "setup windows scheduler" scripts. User can just
  run these from the build machine and it will start running CTest.
- conan support: perhaps with just boost for now
- agile with first sprint
- README with emblems.

Name for the tool: dart.

Tool should have different "template sets" so that we could have a
"standard dogen product" but users can come up with other project
structures.

Tool should add FindODB if user wants ODB support. Similar for EOS
when we support it again. We should probably have HTTP links to the
sources of these packages and download them on the fly.

Tool should also create git repo and do first commit (optional).

For extra bonus points, we should create a project in GitHub, Travis
and AppVeyor from dart.

We should also generate a RPM/Deb installation script for at least
boost, doxygen, build essentials, clang.

We should also consider a "refresh" or "force" statement, perhaps on a
file-by-file basis, which would allow one to regenerate all of these
files. This would be useful to pick-up changes in travis files, etc.

One problem with travis files is that each project has its own
dependencies. We should move these over to a shell script and call
these. The script is not generated or perhaps we just generate a
skeleton. This also highlights the issue that we have different kinds
of files:

- files that we generate and expect the user to modify;
- files that we generate but don't expect user modifications;
- files that the user generates.

We need a way to classify these.

Dart should use stitch templates to generate files.

We may need some options such as "generate boost test ctest
integration", etc.

Notes:

- [[https://github.com/elbeno/skeleton][Skeleton]]: project to generate c++ project skeletons.
- split all of the configuration of CMake dependencies from main CMake
  file. Possible name: ConfigureX? ConfigureODB, etc. See how find_X
  is implemented.
- detect all projects by looping through directories.
- fix CMake generation so that most projects are generated by Dogen.
- add option to Dogen to generate test skeleton.
- detect all input models and generate targets by looping through
  them.
- add CMake file to find knitter etc and include those files in
  package. We probably should install dogen now and have dogen rely on
  installed dogen first, with an option to switch to "built" dogen.

*** Merge properties factory with stitching factory                   :story:

In stitch we still have a few classes that are light on
responsibilities. One case is the stitching properties factory, traits
etc. We should merge all of this into a single class, properties
factory.

*** Rename project directory path                                     :story:

The C++ options have an attribute called
=project_directory_path=. This is a bit misleading; it is actually the
top-level directory that will contain the project directory. In
addition, this is not really C++ specific at all; it would apply to
any kernel and sub-kernel. We should rename it and move it to output
options.

*** Add log-level to command line                                     :story:

We are now increasingly logging at trace levels. We need to allow
users to supply a more fine-grained log configuration. This could be
done by simply allowing users to set the log level via a command-line
flag: =log_level=. It would replace verbose.

*** Consider adding =artefact_set= to formatters' model               :story:

We are using collections of artefacts quite a bit, and it makes sense
to create an abstraction for it such as a =artefact_set=. However, for
this to work properly we need to add at least one basic behaviour: the
ability to merge two artefact sets. Or else we will end up having to
unpack the artefacts, then merging them, then creating a new artefact
set.

Problem is, we either create the artefact set as a non-generatable
type - not ideal - or we create it as generatable and need to add this
as a free function. We need to wait until dogen has support for
merging code generation.

*** Consider supplying element configuration as a parameter           :story:

Figure out if element configuration is context or if it is better
expressed as a stand alone formatting parameter.

*** Formatters' repository should be created in quilt                 :story:

At present we are creating the formatters' repository in
=quilt.cpp=. However it will be shared by all backends in the
kernel. Move it up to =quilt= level and supply it as a paramter to the
backends.

*** Initialise formatters in the formatter's translation unit         :story:

At present we are initialising the formatters in each of the facet
initialisers. However, it makes more sense to initialise them on the
translation unit for each formatter. This will also make life easier
when we move to a mustache world where there may not be a formatter
header file at all.

*** Add knobs to control output of constructors and operators         :story:

At present we are outputting all of the default constructors and the
operators in the handcrafted templates. Ideally it should just be the
class name. We need a way of controlling all of the default
constructors and all of the operators in one go so we can set it on
the handcrafted profile.

** Deprecated
