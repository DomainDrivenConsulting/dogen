#+title: Product Backlog
#+options: date:nil toc:nil author:nil num:nil
#+todo: ANALYSIS IMPLEMENTATION TESTING | COMPLETED CANCELLED
#+tags: story(s) epic(e) task(t) note(n) spike(p)

* Stories
** Development Stories

Stories that we intend to look at, at some point.

*** Allow placing types in the global module from Dia                 :story:

At present all types in a Dia diagram are placed in the model
module. However, there may be cases where one may wish to place types
in the global module. At present this is only done in the hardware
model, and that is supplied via JSON. However, we may need to do this
from Dia. Find example use cases for this first.

In terms of implementation, a trait could be added to dia
=dia.use_global_module=. This would force the type to be contained
directly in the global module rather than the model module. If the
trait is used in the model or a package, all types in the containing
scope will inherit it.

*** Make features optional at compile time                            :story:

One scenario we haven't accounted for is for compile time
optionality. For example, say we have several serialisation facets,
all of them useful to a general model; however, individual users of
that model may only be interested in one of the several
alternatives. In these cases, users should be able to opt out from
compiling some of the facets and only include those that they are
interested in. This is different from the current optionality we
support in that we allow the user to determine what to code
generate. In this case, the mainline project wants to code generate
all facets, but the users of the model may choose to compile only a
subset of the facets.

To implement this we need a trait - say =optional= - that when set
results in a set of macros that get defined to protect the facet. The
user can then pass in that macro to cmake to disable the facet. This
is not the same as the "feature" macros we use for ODB and EOS. These
are actually not Dogen macros, just hand-crafted macros we put in to
allow users to compile Dogen without support for EOS and ODB.

The macros should follow the standard notation of =MODEL.FACET= or
perhaps =MODEL.FACET.FEATURE=, e.g. =cpp.boost_serialization= to make
the whole of serialisation optional or
=cpp.boost_serialization.main_header= to make the header optional. Not
sure if the latter has any use.

*** Consider merging graphers into a single class                     :story:

After the meta-data work, we ended up with three separate graphers
doing similar things. We should look into the commonalities of these
classes to see if we can reuse code.

*** References to objects in package should assume package            :story:

At present if we define two objects in a package =p=, say =a= and =b=,
where =b= refers to =a= it must do so using a fully qualified path,
e.g.: =p::a=. Failure to do so results in an error:

: 2014-09-10 08:27:10.662113 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "simple_name": "registrar" }
: 2014-09-10 08:27:10.665861 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/resolver.cpp(178): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml16resolution_errorEEE
: std::exception::what: Object has property with undefined type: registrar
: [P12tag_workflow] = Code generation failure.

*** Handling of unsupported dia objects                               :story:

At present when we try to use a dia object that dogen knows nothing
about we get an error; for example using a standard line results in:

: 2014-09-10 08:09:43.480906 [ERROR] [dia_to_sml.processor] Invalid value for object type: Standard - Line
: 2014-09-10 08:09:43.487060 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/processor.cpp(124): Throw in function dogen::dia_to_sml::object_types dogen::dia_to_sml::processor::parse_object_type(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Invalid value for object type: Standard - Line

However, it may make more sense to just ignore these. To do so we
could relax the code in processor (object_types):

:    BOOST_LOG_SEV(lg, error) << invalid_object_type << ot;
:    BOOST_THROW_EXCEPTION(processing_error(invalid_object_type + ot));

We should also consider having a =strict= command line option to
enable/disable this behaviour.

*** Consider generation of validators                                 :story:

It would be nice if we could constraint the domain of a type via some
kind of rules; for example, provide a regular expression or an EBNF
definition with a string that tells dogen how to validate it. We could
then construct simple validators. As usual these expressions can be
supplied via the meta-data.

*** Providing view model hints                                        :story:

Once we start supporting view models, it would be nice to be able to
take an inheritance tree of objects and collapse it into a list view,
handling all of the use cases (reorder columns, remove columns, add
columns, updating rows, etc). All of this code can be inferred from
the type hierarchy.

*** Meta-data transformation in formatters                            :story:

Once we reach the formatters with the complete set of meta-data
(e.g. after population), we should be able to do a final
transformation before formatting. This would make the formatter code
much easier and type safe, instead of querying the ptree directly. And
since it would live in the C++ formatters model, we could share code
between formatters.

*** Merge C++ models                                                  :story:

Once all of the tidy-up for the meta-data is done, the C++ model will
become quite small. At that point we should merge it with the C++
formatters model. This is quite nice as we will end up with a cohesive
model - the separation between these two models was always arbitrary.

In addition, we should create namespaces:

- =formatters=: general formatting code used by several formatters
- facet-specific: e.g. =formatters::types= houses all the formatter
  code for the types facet and so on. This may be a bit messy in terms
  of the diagram but it will make the code a lot cleaner.

*** Make use of boilerplate                                           :story:

Remove all of the manual boilerplate and make use of the new
class. This will involve bring across some meta-data into C++ model.

*** Rename the include tags and add them to CPP model                 :story:

Update all the JSON files with names in the form
=cpp.include.types.header_file=. Add properties in =cpp= to capture
these.

While we're at it, add support for =family= too.

*** Create formatter interfaces for each concrete entity type         :story:

Formatters should have a single format method that takes a concrete
entity, formatting settings and knitting settings. It returns a file.

**** Remove =file_formatter_interface=                                 :task:

Update all C++ formatters to use specific types:

- remove factory; update workflow to call formatters directly
- remove file formatter interface
- formatters to have format() for each specific supported entity,
  overloaded as required

**** Update formatters to output =formatters::file=                    :task:

Instead of passing in a stream, we just want to return a file.

**** Update formatters to take settings and entity as input            :task:

We want the formatters to be stateless - well at least as far as
mutating state goes - so that we can run them in parallel. We also
want them to have no constructor arguments so we can register them. To
do so we need to pass in both knitting and formatting settings to
=format= as well as the entity.

**** Get formatters to register themselves with workflow               :task:

Copy the code from the deleted registration files. Formatters should
register against a content descriptor.

*** Create =includers_info= and =forward_declarations_info=           :story:

We should make sure the type system represents all the inputs to
formatters. Create these types and update their formatters,
transformer etc.

*** Update manual with detailed model descriptions                    :story:

We should add CRCs for the main classes, with an explanation of what
each class does; we should also explain the separation of the
transformation logic between the core model (e.g. =dia=) and the
transformation model (e.g. =dia_to_sml=). We should describe what the
workflow does in each model.

*** Refactor types header formatter                                   :story:
    CLOCK: [2014-08-25 Mon 12:01]--[2014-08-25 Mon 12:40] =>  0:39
    CLOCK: [2014-08-25 Mon 11:56]--[2014-08-25 Mon 11:57] =>  0:01
    CLOCK: [2014-08-25 Mon 11:35]--[2014-08-25 Mon 11:43] =>  0:08

Make the main types header formatters look like the =om= types
formatter. This model was deleted around commit 10157ad.

This is still not quite right. We need to drive the formatting from
two things:

- user options
- available formatters

**** Tidy-up =types_main_header_file_formatter=                        :task:
     CLOCK: [2014-08-25 Mon 11:57]--[2014-08-25 Mon 12:16] =>  0:19

Clean up internal functions in file and add documentation.

**** Make use of boilerplate                                           :task:

Generate the licence, etc using boilerplate formatter.

**** Copy across documentation from =om=                               :task:

We did a lot of doxygen comments that are readily applicable, copy
them across.

**** Make use of indenting stream                                      :task:

Remove uses of old indenter.

**** Copy across =om= types formatter tests                            :task:

Not sure how applicable this would be, but we may be able to scavenge
some tests.

*** Remove =cpp_formatters::formatting_error=                         :story:

Use the =formatters::formating_error= instead.

*** Delete content types                                              :story:

Now we have the type system representing the content, we can delete
this enumeration.

*** Delete aspect types                                               :story:

Now we have the type system representing the aspects, we can delete
this enumeration.

*** Delete key implementation formatter                               :story:

It doesn't seem like there is any good reason to treat the keys in a
special way so try to remove this.

*** Towards a more generic use of meta-data parameters                :epic:

We should do an inventory of all dogen features which can be
reimplemented as meta-data parameters. For example, immutability
should result in a generic parameter being added to the type at the
SML level:

: immutable = true

which then gets resolved into a set of language specific parameters:

: cpp.copy_constructor.status = disabled
: cpp.setters.status = disabled
: ...

The formatter then looks for these tags to decide whether to add a
method or not. If we had more languages, they would have equivalent
formatting commands.

The same would apply to facets. These would have a top-level generic
parameter such as =hashing=:

: hashing = true

Which then expands to implementation specific hashing:

: cpp.hashing.std_hashing = true

or

: cpp.hashing.boost_hashing = true

The facet is now just a short-hand for a set of implementation
specific parameters. There is some default mapping applied in this
grouping. The user can shortcut the process by disabling the mapping
and supplying implementation specific parameters:

: hashing = false
: cpp.hashing.boost_hashing = true

Assuming =std_hashing= as a default.

In addition, depending on the parameter, it may be propagatable /
expandable. For example, if hashing is set to false in a type at the
bottom of a graph relationship, we must propagate it to all members of
the graph. Similarly, if hashing is disabled in the model, we must
propagate it to all types in the model.

*** Refactor Dia to SML transformer                                   :story:

- remove all properties from context which are used only internally in
  the transformer.
- split context into inputs and outputs: =transformation_result= as a
  candidate for the outputs.
- inputs are passed in at construction time and remain constant.
- each transformation method returns a value which can be slotted into
  the model by the workflow, contained in a transformation result.
- this does mean a lot of concatenation at the workflow level though.

*** Consider adding facet specific types                              :story:

Types in dogen are somewhat "uni-dimensional"; that is, the main focus
of all work is types and the other facets are thought to either be
code generated in total (serialisation, hashing, etc) or manually
generated in total (test for mock factories). However, in some cases
it may make sense to add a type directly to a facet. For example, we
may want to add simple value objects to the mock factory. We don't
want to pollute =types= with these classes, but at the same time we'd
rather not have to manually generate them. It would be nice to be able
to associate a type with just a facet via the meta-data. Of course,
this does mean we would not be able to rely on all other facets such
as serialisation and even streaming or else things would get a bit
confusing. But it would still be useful.

Another possible (less clean) approach is this:

#+begin_quote
It would be great if we could use the meta-data to enable and disable
facets (there probably already is a story for this). But in addition
to this, it would also be great if one could override the default name
for an object in a facet; for instance: one could add an object called
=serialization_manager=, disable all facets bar serialisation,
disable the serialisation postfix of this file and disable code
generation. This way one could add manual code to any of the facets,
independently.

At present we support this, but only for types as it is hard-coded.
#+end_quote

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders. Instead, this story will be handled when we move
over to using =boost::property_tree::ptree=.

*** Add a configuration class to SML mock factory                     :story:

Every time we need to extend the mock factory we are finding we need
to modify every single function. This is particularly painful due to
the fact we rely on defaults. For example, we can't easily add an
external module path because we need to modify every single method. We
need to look into patterns for this. One option would be to create a
factory configuration class that has the super set of all parameters
required and pass that configuration to each function.

*** Update comments in C++ model                                      :story:

We have a very large blurb in this model that is rather old, and
reflects a legacy understanding of the role of the C++ model.

*** Dia to SML workflow should post-process model by leaves           :story:

At present in =dia_to_sml::workflow::post_process_model_activity= we
are post-processing by going through every single object; in reality
we only need to go through the leaves.

*** Refactor code around model origination                            :story:

In the past we added a number of knobs around generation, all with
their own problems:

- =origin_types=: was the model/type created by the user or the
  system. in reality this means did the model come from Dia or
  JSON. this is confusing as the user can also add JSON files (their
  own model library) and in the future the user can use JSON
  exclusively without needed Dia at all.

- =generation_types=: if the model is target, all types are to be
  generated /unless/ they are not properly supported, in which case
  they are to be "partially" generated (as is the case with
  services). This is a formatter decision and SML should not know
  anything about it.

These can be replaced by a single enumeration that indicates if the
type/model is target or not.

*** Tidy-up test models                                               :story:

We have a lot of fine grained test models for historic reasons. A lot
of these could be collapsed into a smaller number of models, focused
on testing a set of well defined features.

**** Models that need changing

Merge the following models into a =basic= or =trivial= model (no
aggregation, no association):

- classes_in_a_package
- classes_inout_package
- classes_without_package
- class_in_a_package
- class_without_attributes
- class_without_package
- stand_alone_class

We should also check the combined model has all the scenarios
described in [[*Cross%20package%20referencing%20tests][Cross package referencing tests]].

Merge the following models into stereotypes:

- enumeration
- exception

Consider deleting the comments model and make sure we have comments in
all models with the same features:

- top-level comment for the model
- package level comment
- notes

These models are at the right level of granularity but need renaming:

- all_primitives: primitives or primitives_model to line up with boost
  and std.
- trivial_association: association
- trivial_inheritance: inheritance

**** Models that do not need changing and why

These models test other models, and we cannot remove the postfix
=_model= to avoid clashes with namespaces:

- boost_model
- std_model

These models test command line options, which means they cannot be
merged:

- disable_cmakelists
- disable_facet_folders
- disable_full_ctor
- dmp
- enable_facet_domain
- enable_facet_hash
- enable_facet_io
- enable_facet_serialization
- split_project

These models test features which have enough scenarios to justify
keeping them in isolation:

- database

These models test dia features and must be kept isolated:

- compressed
- two_layers_with_objects

**** Add objects, enumerations and exceptions to comments model

At present we are only testing packages in comments.

*** Improve error message for blank types                             :story:

If the user does not supply a type at all in Dia, dogen spits out a
message that is not very informative:

: Error: Failed to parse string: .

The log file is not much better:

: 2014-09-06 16:11:54.143249 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: 
: 2014-09-06 16:11:54.150595 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: 
: [P12tag_workflow] = Code generation failure.

We should instead mention that the string was empty or blank. We also
need to provide the property and class that contained this string. To
reproduce this problem create an enumeration but remove the
=enumeration= stereotype.

*** Concepts cannot be placed inside of packages                      :story:

At present it is not possible to create a concept inside a
package. This is because the concept qualified name is assumed to be
at top-level. In the future it may be useful to use scoping for
concept names in the stereotype. We do not yet have a use case for
this.

*** Add support for deprecation                                       :story:

We should be able to mark classes and properties as deprecated and
have that reflected in both doxygen and C++-11 deprecated attributes.

*** Control JSON output via traits                                    :story:

Once we add support for JSON we will face the same sort of problems
that Json.net has already solved: we may want to have keys that do not
match the property names (for instance we may want to use human
readable names in the json), we may want to translate enumerations to
numbers or to human readable descriptions, we may want to collapse a
class into some less verbose JSON, etc. Some of these are describable
via traits, very much like Json.Net uses C# attributes. We should look
into the available attributes and see if they make sense as dogen
traits to control JSON. Some of these may have wider application and
be used to control other serialisation formats.

*** Raw JSON vs cooked JSON                                           :story:

If we do implement customisable JSON serialisation, we should still
use the raw format in streaming. We need a way to disable the cooked
JSON internally. We should also re-implement streaming in terms of
this JSON mode.

*** Add include files at the formatter level                          :story:

We need to remove all the include files from =includer= which are
related to formatter specific code. We need to inject these
dependencies inside of the formatters.

- implement includer in terms of json files
- get includer to work off of object relationships
- remove relationships from transformer
- remove helper models boost and std

*** Add warning =-Wunused-private-field=                              :story:

Seems like this warning is not part of =-Wall

*** Use formatting settings in formatters model                       :story:

We need to replace the references to the c++ settings directly with
formatter settings.

*** Move all the configuration options into tags                      :story:

Parameters as per analysis story.

**** Update all models and tests to use the new tags                   :task:

Once the config options are in we need to start making use of them to
ensure they work exactly as before.

**** Remove all C++ command line settings                              :task:

Once the overrides have been proven to work, we need to remove the
command line options and make sure nothing breaks.

*** Move code to C++ 14                                               :story:

Now that the standard is out, we should move to it. Both clang and gcc
have some kind of support at present, so it should be a matter of
compiling on this mode. However, as we have gcc 4.7 on OSX and
Windows, we would have to upgrade these compilers first.

We have already proven that the code builds out of the box in sprint 50.

*** Usage of external module path in cmakelists                       :story:

It seems like we are not populating the target names
properly. Originally the target name for test model all primitives was:

: dogen_all_primitives

When we moved the test models into =test_models= the target name did
not change. It should have changed to:

: dogen_test_models_all_primitives

*** Rename the =database= test model to =odb=                         :story:

This name is a bit misleading, this is not a generic database model
but its designed to specifically test odb.

*** Add support for BSON                                              :story:

It would be useful to support Mongo DB's BSON. There is a C++ stand
alone library for this:

https://github.com/jbenet/bson-cpp

For examples on how to use the C++ API see the tutorial:

https://github.com/mongodb/mongo-cxx-driver/wiki/Tutorial

*** Add dogen to travis                                               :story:

We're not doing a very good job of managing the CI support in
CDash. We are now down to a single build agent on OSX. We should add
the project to travis and stop managing the agents.

- https://travis-ci.org/
- [[http://computer-vision-talks.com/articles/2014-02-23-using-travis-ci/][Using Travis-CI for continuous testing your projects]]

We should also consider hosting the documentation here:

- https://readthedocs.org/

It is also a good time to add the "markers" to the readme page. For an
example see:

https://raw.githubusercontent.com/flycheck/flycheck/master/README.rst

*** Rename ODB parameters                                             :story:

At present we use the following form:

: #DOGEN ODB_PRAGMA=no_id

We need to use the new naming style =cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

*** Add targets to output manual in downloadable formats              :story:

We should build HTML and PDF representations of the manual.

*** Consider adding SWIG and Boost.Python support                     :story:

We could generate the code required to expose the C++ types into ruby
or python by creating a formatter for it. Boost.Python would be more
straightforward as it is plain C++ code; SWIG would require generating
an interface file (IDL-like) and as such is closer to [[*Add%20support%20for%20thrift%20and%20protocol%20buffers][this]] story.

*** Add yuml markup language support                                  :story:

It should be fairly straightforward to add a yuml front end that reads
a file using their markup language and generates an SML model from it.

*** Create a trivial Linux gcc script                                 :story:
    CLOCK: [2014-04-22 Tue 08:15]--[2014-04-22 Tue 08:35] =>  0:20

The previous attempts to clean up the build environment were too
elaborate given the available time. We need to go back to basics with
a trivial script that works for Linux 32-bit and 64-bit with gcc.

*** Create a trivial Linux clang script                               :story:

We need to be able to build Linux clang 32-bit and 64-bit again.

*** Fix windows nightly build                                         :story:

The nightly build for windows has been broken for quite a while. The
problem is we have hit a limit in COFF in the serialisation of context
(=sml_to_cpp=):

: cd /E/mingw/msys/1.0/home/ctest/build/Nightly/dogen/mingw-1.0.17-i686-gcc-4.7/build/projects/sml_to_cpp/src && /e/mingw/bin/g++.exe   -DBOOST_ALL_DYN_LINK -isystem /e/usr/local/pfh/include -std=gnu++0x -D_WIN32_WINNT=0x0501 -DBOOST_TEST_SOURCE  -g -O0 -Wall -Wextra -pedantic -Werror -Wno-system-headers -Woverloaded-virtual -Wwrite-strings -fprofile-arcs -ftest-coverage -std=gnu++11 -frtti -fvisibility-inlines-hidden -fvisibility=default @CMakeFiles/sml_to_cpp.dir/includes_CXX.rsp   -o CMakeFiles/sml_to_cpp.dir/io/std_types_io.cpp.obj -c /e/mingw/msys/1.0/home/ctest/build/Nightly/dogen/mingw-1.0.17-i686-gcc-4.7/source/projects/sml_to_cpp/src/io/std_types_io.cpp
: /e/cmake/bin/cmake.exe -E cmake_progress_report /E/mingw/msys/1.0/home/ctest/build/Nightly/dogen/mingw-1.0.17-i686-gcc-4.7/build/CMakeFiles 
: [ 77%] Building CXX object projects/sml_to_cpp/src/CMakeFiles/sml_to_cpp.dir/serialization/context_ser.cpp.obj
: cd /E/mingw/msys/1.0/home/ctest/build/Nightly/dogen/mingw-1.0.17-i686-gcc-4.7/build/projects/sml_to_cpp/src && /e/mingw/bin/g++.exe   -DBOOST_ALL_DYN_LINK -isystem /e/usr/local/pfh/include -std=gnu++0x -D_WIN32_WINNT=0x0501 -DBOOST_TEST_SOURCE  -g -O0 -Wall -Wextra -pedantic -Werror -Wno-system-headers -Woverloaded-virtual -Wwrite-strings -fprofile-arcs -ftest-coverage -std=gnu++11 -frtti -fvisibility-inlines-hidden -fvisibility=default @CMakeFiles/sml_to_cpp.dir/includes_CXX.rsp   -o CMakeFiles/sml_to_cpp.dir/serialization/context_ser.cpp.obj -c /e/mingw/msys/1.0/home/ctest/build/Nightly/dogen/mingw-1.0.17-i686-gcc-4.7/source/projects/sml_to_cpp/src/serialization/context_ser.cpp
: e:/mingw/bin/../lib/gcc/mingw32/4.7.2/../../../../mingw32/bin/as.exe: CMakeFiles/sml_to_cpp.dir/serialization/context_ser.cpp.obj: too many sections (32795)

This problem is solved with visual studio using the =/bigobj= flag,
but this is not available on mingw at the moment. Support has been
[[https://sourceware.org/ml/binutils/2014-03/msg00114.html][checked in]] to mingw64 trunk, so hopefully it will make its way to
32-bits too. See also: [[http://sourceforge.net/p/mingw-w64/bugs/341/][#341 Too many sections aka /bigobj replacement
flag]].

*** Re-enable schema updates in database model                        :story:

We are deleting the entire DB schema and re-applying it for every
invocation of the tests. This does not work on a concurrent world. We
commented it out for now, but we need a proper solution for this.

*** Implement flymake from the EDE project                            :story:

This move of directories highlighted the fragility of the current
flymake hack: every time the top-level directory changes we need to
update =cunene=. Ideally what we want is to have a top-level file -
most ideally =dogen.ede= with some lisp code that would setup the
dogen paths for flymake. Users would only need to load this up to use it.

*** Add support for automatic upload packages into GDrive             :story:

We need to upload the packages created by the build to a public Google
Drive (GDrive) location.

- Google drive folder created [[https://drive.google.com/folderview?id%3D0B4sIAJ9bC4XecFBOTE1LZEpINUE&usp%3Dsharing][here]].
- See [[https://developers.google.com/drive/quickstart-ruby][this article]].
- [[http://stackoverflow.com/questions/15798141/create-folder-in-google-drive-with-google-drive-ruby-gem][Create folders]] to represent the different types of uploads:
  =tag_x.y.z=, =last=, =previous=. maybe we should only have latest
  and tag as this would require no complex logic: if tag create new
  folder, if latest, delete then create.

*** Enable package sanity tests for Linux                             :story:

Now that we will be using docker, we could create a simple =systemd=
ctest script that runs as root in a docker container:

- it monitors the GDrive location for files that match a given regular
  expression (e.g. we need to make sure we match the bitness and the
  platform)
- if it finds one, it installs it and runs sanity scripts.
- it then uninstalls it and makes sure the docker image is identical
  to how we started (however that is done in docker)

*** Add support for thrift and protocol buffers                       :story:

Amongst other things, these technologies provide cross-language
support, allowing one to create c++ services and consume them from say
ruby, python, etc. At their heart they are simplified versions of
CORBA/DCOM, with IDL equivalents, IDL compilers, specification for
wire formats, etc. As they all share a number of commonalities, we
shall refer to these technologies in general as Distributed Services
Technologies (DST). We could integrate DST's with Dogen in two
ways. First approach A:

- generate the IDL for a model; we have enough information to produce
  something that is very close to it's Dogen representation,
  translated to the type system of the IDL; e.g. map =std::string=,
  =std::vector=, etc to their types. This IDL is then compiled by the
  DST's IDL to C++ compiler.
- possibly generate the transformation code that takes a C++ object
  generated by Dogen and converts it into the C++ object generated by
  the DST's C++ compiler and vice-versa. We probably have enough
  information to generate these transformers automatically, after some
  analysis of the code generated by the DST's C++ compiler.

In order for this to work we need to have the ability to understand
function signatures for services so that we can generate the correct
service IDL for the DST. In fact, we should be able to mark certain
services as DST-only so that we do not generate a Dogen representation
for them. The DST service then internally uses the transformer to take
the DST's domain types and convert them into Dogen domain types, and
then uses the Dogen object model to implement the guts of the
service. When shipping data out, the reverse process takes place.

Approach A works really well when a service has a very narrow
interface, and performs most of it's work internally without exposing
it via the interface. Once the service requires the input (and/or
output) of a large number of domain types, we hit a cost limitation;
we may end up defining as many types in Dogen as there are in the IDL,
thus resulting in a large amount of transformations between the two
object models.

In these cases one may be tempted in ignoring Dogen and implementing
the service directly in terms of the DST's object model. This is not
very convenient as the type system is not as expressive as regular
C++ - there are a number of conventions that must be adopted, and
limitations imposed too due to the expressiveness of the IDL. We'd
also loose all the services provided by Dogen, which was the main
reason why we created it in the first place.

Approach B is more difficult. We could look into the wire format of
each DST and implement it as serialisation mechanism. For this to
work, the DST must:

- provide some kind of raw interface that allows one to plug in types
  serialisation manually. Ideally we wouldn't have to do this for
  services, just for domain types, but it depends on the low-level
  facilities available. A cursory look at both thrift and protocol
  buffers does not reveal easy access to such an interface
- provide either a low-level wire format library (e.g. =std::string=
  to =string=, etc) or a well specified wire format that we could
  easily implement from scratch.

This approach is the cleaner technically, but its a lot of work, and
very hard to get right. We would have to have a lot of round-trip
tests. In addition, DST's such as thrift provide a wealth of wire
formats, so if there is no easy-access low-level wire format library,
it would be very difficult to get this right.

*** The =types= facet should always be on                             :story:

At present users are given the option to enable or disable the
=domain= facet; this is not very wise because all facets depend on
it. It must always be on. We should remove these options.

In addition the facet is incorrectly named: when we performed the
rename of =domain= to =types= we left the command-line facet. We
should rename it to =types= too.

*** Type with the same name as the project does not compile           :story:

It seems that if we create a type with exactly the same name as the
model, we get strange compilation errors:

: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.4/stage/bin/dogen_examples/source/hello_world/include/hello_world/test_data/hello_world_td.hpp:37:13: error: ‘hello_world::hello_world::hello_world’ names the constructor, not the type
:     typedef hello_world::hello_world result_type;
             ^
We should do a test case for this and fix the errors.

*** Diagrams used in manual should be in sanity and in docs           :story:

Users should be able to follow the examples in the manual by using a
set of diagrams supplied in the dogen package. However, to ensure
these samples are actually working we need to test them as part of
sanity. This means we need the same diagrams packaged twice.

*** Move the mock factories into the test_data directory              :story:

There is no good conceptual reason to split the mock factories from
the test_data generators. However, we did it because we don't have a
good way to give dogen visibility of the existence of these files: we
could add regexes but then its not very maintainable and not visible
from the project diagram.

The correct solution for this may be to have some tags that state that
an object only has representations in certain facets.

*** Move test model diagrams into main diagrams directory             :story:

For some reason - lost in the mists of time - we decided to split the
test model diagrams from the main models; the first is in the =diagrams=
directory, the latter is in the rather non-obvious location of
=test_data/dia_sml/input/=. All source code goes into =projects=
though, so this seems like a spurious split. Also, the test data
directory should really only have data that we generate as part of
testing (e.g. where there is a pairing of expected and actual) and
the test model diagrams are not of this kind - we never output dia
diagrams, at least at present.

The right thing to do is to move them into the =diagrams=
directory. This is not an easy undertaking because:

- there is hard-coding in the test model sets pointing to these
- the CMake scripts rely on the location of the diagrams to copy them
  across

*** Implement the file formatter interface in types formatter         :story:

Move across this formatter to the new way of doing things.

We need to cast the formatter entity to a C++ entity and dispatch it
using the type visitor. We also need to add registration support.

We also need to generate the following within the formatter:

- file name
- header guards
- formatter specific includes
- includes

*** Add tests for annotation factory                                  :story:

Zero coverage on this one for some reason. Some simple tests come to
mind:

- empty data files directory results in empty factory;
- valid data files directory results in non-empty factory;
- invalid data files directory results in exception;
- more than one data files directory results in expected load;
- creating annotation for test model types works as expected.

*** IOable services                                                   :story:

Even though we do not code generate services, it would be nice if we
could still setup their IO infrastructure - something basic just
outputting the type and taking inheritance into account. We end up
doing a lot of this manually anyway.

Also, if a service has a bunch of attributes that are IOable, we
should set them up too.

*** Remove references to PFH in makefiles                             :story:

Seems like the correct way of finding libraries is to use
=CMAKE_PREFIX_PATH= as explained [[https://blogs.kde.org/2008/12/12/how-get-cmake-find-what-you-want-it][in this article]]. We should stop using
any references to PFH and let the users provide a path to local
installs via this.

We need to add a note on the read me too.

*** Include forward declaration in visitable types                    :story:

There doesn't seem to be any good reason to include the full visitor
header in visitable types - we should be able to get away with
including only the forward declaration for the visitor.

*** Allow cross model inheritance                                     :story:

At present we can only inherit within the same model. This is a
limitation of how to express inheritance in a Dia diagram - either the
parent is part of that diagram or it is not, and if it's not we have
no way of connecting the generalisation relationship to it.

Having said that, it would actually be quite simple to allow cross
model inheritance by using meta-data:

- create a tag that forces a type to behave like a parent, regardless
  of whether there are any children or not;
- create a tag that contains a qualified name of a parent, regardless
  of whether it's in this model or not;
- change the transformer to convert these tags into SML inheritance
  relationships;

There may be some fallout in places where we assume that the
descendents are all in this model such as serialisation, visitors.

*** Consider creating a phoney target for header validation           :story:
    CLOCK: [2014-01-14 Tue 07:11]--[2014-01-14 Tue 07:26] =>  0:15

One way of solving the ninja problems may be to stop creating a static
library for header validation. We did it simply because it was easier
but there is no real reason for it. We may be able to save a bit on
archiving, and as an added bonus, it may fix the issues we're having
with ninja.

Links:

- [[http://cmake.3232098.n2.nabble.com/adding-extra-target-to-CMakeLists-txt-td4550492.html][Adding extra target to CMakeLists.txt]]
- See [[http://www.cmake.org/cmake/help/v3.0/release/3.0.0.html][CMake 3.0]] INTERFACES too

*** Add a CMake module for git                                        :story:

We are finding git manually at the moment, which means we are probably
not doing it right. It appears there is a CMake script for it:

- [[%20http://gnuradio.org/redmine/projects/gnuradio/repository/revisions/accb9f2fe8fd8f6a1e114adac5b15304b0e0012d/entry/cmake/Modules/FindGit.cmake][FindGIT.cmake]]

*** Add support for the =scan-build= static analyser                  :story:

scan-build is a command line utility that enables a user to run the
static analyzer over their codebase as part of performing a regular
build.

- [[%20http://clang-analyzer.llvm.org/scan-build.html][scan-build]] project page

*** Add support for iwyu                                              :story:

There is a clang based tool that checks if which includes are actually
used by the translation unit. We should have a build for this that
breaks whenever one includes something which is not required.

- [[http://code.google.com/p/include-what-you-use/][iwyu project page]]
- [[http://mpd.jinr.ru/svn/mpdroot/trunk/cmake/modules/FindIWYU.cmake][FindIWYU.cmake]]
- [[https://github.com/christophgysin/addp/blob/master/cmake/iwyu.cmake][iwyu.cmake]]

*** Improve error messages for unconnected objects                    :story:

At present when a Dia object is not connected we get the following
error message to std out:

: Error: Expected 2 connections but found: 1. See the log file for details.

The log file is a bit more verbose but still not particularly helpful:

: 2014-01-23 08:25:28.115363 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2014-01-23 08:25:28.118718 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Expected 2 connections but found: 1
: [P12tag_workflow] = Code generation failure.

We should try to at least name the object that has the one connection
to make the user's life easier.

*** Include groups                                                    :story:

One of my personal preferences has always been to group includes by
"library". Normally first come the C includes, then the standard
library ones, then boost, then utilities and finally types of the same
model. Each of these can be thought of as a group. Inside each group
the file names are normally ordered by size, smallest first. It would
be nice to have support for such a feature in Dogen.

Formatters would then push their includes into the correct
group. Group names could be the model name (=std=, etc).

A bit of a nitpick but nice nonetheless.

*** Naming of libraries and binaries                                  :epic:

We have a common problem with certain combination of utilities and
libraries; often we end up with a library which should be named after
the binary. For example, we should really call the "engine" library
"dogen" as all its missing is the command line interface. In addition
to this, it seems we are going to have more than one binary in the
code base. In order to solve this we should adopt a naming strategy
for both libraries and binaries. One approach is to choose verbs from
a "theme" such as "weaving" - a lot of these tools are doing things
that look a bit like weaving - and then use the noun for the library
and the verb for the binary. Dogen is then a suite of utilities for
domain generation.

 For example:

- stitch - library that generates formatters
- stitcher - command line tool for generating formatters
- knit - library that code generates [done]
- knitter - command line tool for code generation [done]
- tailor - tool to generate JSON / SML from Dia diagrams
- sew - command line tool to generate JSON / SML from Dia
  diagrams
- needle: the small library with support for boost and standard
  library (hashing, test data, etc.)
- patch: library with code to update a dia Diagram given a C++ code
  base. Doesn't create new classes - only adds or removes methods in
  existing classes.
- patcher: tool that uses the patch library.

All binaries are prefixed with dogen to avoid clashes,
e.g. =dogen_knitter=.

See the [[http://en.wikipedia.org/wiki/Glossary_of_sewing_terms][Wikipedia glossary of sewing terms]].

*** Code-generating formatters as text templates                      :story:

In the past we have used T4 and generated text templates mixed with
processing logic written in C#. We never quite considered this
approach for Dogen mainly because it's really hard to create a
templating engine. And as we used Dogen as an exercise to get up to
speed with C++ 11, it didn't make sense to implement significant parts
of it in C#.

Having said that, implementing file formatters as C++ code is not very
convenient - for the same reason that implementing file formatters in
any language is inconvenient: the boilerplate writing code obscures
the stuff one is writing, making it really difficult to make changes
without breaking things.

However, the more one thinks about it the more it seems that a simple
templating engine, designed specifically for the Dogen needs is not
actually that hard to write (famous last words). It would have to
support only a very limited subset of "commands" (specified as T4-like
syntax):

: <#= STRING_EXPRESSION #>
: <#+ CPP_BLOCK_OF_CODE #>

We could then make use of this in a text file like so:

: class <#= a.name() #> {
: ...
: <#+
:     for (const auto& p : a.properties()) {
: #>
:     void <#= p.name() #>() {
:         return <#= p.name() #>_;
:     }
: ...
: }

A tool would convert this template according to a very simple
heuristic:

- any line which has no "commands" will be simply streamed to an
  output stream followed by =<< std::endl;=
- any line with a =STRING_EXPRESSION= means the result of that
  expression will be streamed into the output stream (e.g. not
  quoted);
- any =CPP_BLOCK_OF_CODE= will be copied verbatim.

According to these rules we have:

: s << "class " << a.name() "{" << std::endl;
: ...
:     for (const auto& p : a.properties()) {
: s << "    void " << p.name() << "{" << std::endl;
: s << "        return " << p.name() << "_;" << std::endl;
: s << "    }" << std::endl;
: ...
: s << "}" << std::endl;

Indentation will be kept as simple as possible, with the expectation
that there will be a further pass using clang format.

Now of course there is the question of where to put this code in such
a way that it compiles. What one then needs is a formatter template:
something which provides a =hpp= and a =cpp= that would be the
containers of the code generated by expanding the template according
to the heuristic above. The formatter template would itself be a text
template, but with one special variable:

: <#= @insert_code #>

For example:

: #include <sstream>
: #include "A.hpp"
:
: void my_func() {
:     std::ostringstream s;
:     A a;
: <#= @insert_code #>
: }

This variable is then expanded by stitch itself.

Conceivably one could pair the formatter templates into =hpp= and
=cpp= files. The header file could be entirely static, and the cpp
file could contain the =insert_code= command.

The workflow would be as follows: first expand the original text
template into code; then expand the formatter templates using this
code. Finally, use clang formatter to indent the code. The end result
would look like one of our formaters, except with one _very_ long
format method. There are a few advantages to this approach:

- text templates would be maintained as more or less source code
  files, so editing them would be much easier;
- the expanded template would be just like any other source code so
  hopefully debugging / solving compiler errors should be
  straightforward - more so than with T4 where one ended up looking at
  complex generated code in temp directories quite a lot.
- users can add new templates with very little understanding of the
  internals of Dogen; we could have an agreed interface (a type from
  the C++ model) which is passed in to text templates of that "kind"
  and all they need to do is to use the type in the template. Unless
  they need something bespoke, nothing else is required.
- the templates could contain logic to setup the file name and include
  files but still make use of boilerplate formatter. We could even
  create some kind of static registration of formatters such that
  literally the user would have to change no code at all to add a new
  template other than the template itself.
- we could propagate the meta-data into the C++ model at the formatter
  level such that users could use it without any changes to the code
  generation engine.
- we could derive the location of the output file from the formatter
  itself, neatly separating groups of formatters. The Dogen formatters
  could be placed under "core" and then we could have others such as
  "qt", etc.
- new templates with use cases we havent foreseen could be done, but
  if they need more meta-data the user would have to add types to the
  C++ model.

We could create a totally separate tool for this that would make use
of existing infrastructure: =stitch=. The tool could be made up of a
library and a binary - it could be useful outside Dogen.

Inside Dogen, one can imagine a stereotype =Formatter= that requires a
few meta-data tags to locate the formatter template and the text
template, and then internally Dogen can instantiate the =stitch=
library to generate the files. The stitch invocation could also have
"variables" used by the formatter template such as say the class name,
etc.

Use of the variables could be controlled by the same command as for
inserting code, e.g.

: <#@ some_variable #>

One question is where the text templates and formatter templates
should be stored. The data directory is a natural fit, but it does not
make any sense to release the templates as users can't add their own.

Another interesting idea is to generate a model that contains
formatters, create a dynamic library and then have some kind of
loading mechanism in Dogen. The interesting thing is that with static
factories, dogen could make use of this without any code changes at
all (e.g. loading the library into the process is sufficient to
trigger registration, and then its up to the meta-data to decide
whether to use the formatters or not). So a user could create a model
with formatters, add its own text templates, compile and link it and
then add it to Dogen and then make use of the new formatters. The
usual constraints apply such as ABI (ensure one is using the same
compiler as used to compile Dogen, flags, etc).

In reality stitch has two modes of operation, which we can think of as
transformation types:

- variable substitution: take a source file, take a dictionary and
  replace variables with values from dictionary;
- streamer generation: take a text template and create the output
  streaming code that would generate it, potentially interspersed with
  additional C++ processing logic.

In a more general form, this is some kind of graph of transformations,
where the streamer generation's output becomes a variable in the
dictionary and the variable substitution transformation simply
replaces this variable with all of the generated code. Conceptually,
one can imagine there being other types of transformations, all of
which result in setting a variable and ultimately there being the
final variable substitution transformation. Of course, a command
interface would probably not be suitable to represent this graph of
transformations, so we will have to flatten it. However, internally
the stitch library should be implemented like this. We may find other
useful transformations in the future.

The Dogen interface with the stitch library will also be flat, hard
coded to these two types of transformations; this is just due to the
structure of the meta-data tags, which are not very suitable to
describe a structure like a graph.

Note: we should consider using [[https://github.com/jamboree/bustache][bustache]] for this.

*** Special purpose formatters                                        :story:

In the future, when the creation of formatters is made easier, we may
start designing formatters that are totally a application specific and
may not have any particular use for any other application. They should
be accepted in mainline Dogen:

- to make sure we don't break this code;
- to allow other people to copy and paste to generate their own
  formatters;
- because sometimes what one thinks is special purpose actually much
  more general.

However, we need to make sure we don't start cluttering the code base
with these formatters. We will also have to start to worry about
things like defining stable interfaces:

- at which point do we decide that some code has bitrot and
  deprecated, so will have to be removed?
- what happens when a formatter moves from version 1 to version 2 of
  some dependent library, must we create a version 1 and version 2
  formatter or just update the existing one? what if it breaks code
  for people using version 1 that do not wish to move to version 2?
- do we mandate compilation tests for all formatters? This would mean
  our build machine would be full of third-party libraries (some
  potentially not available in Debian), and quite hard to
  maintain. Alternatively we could mandate that if you have a
  formatter you must setup a CTest agent with a compilation for that
  formatter and publish the results of the build to dashboard; if your
  build becomes consistently red we are allowed to remove the
  formatter.
- for the diff tests, is it acceptable if someone refactors the code?
  Once "your" formatter is merged in it is now owned by the community
  and it is entirely possible that someone will improve it/extend it,
  etc. In order for this to work they need to be very sure they have
  not broken the original use case.

We probably just need to setup a very simple policy to start off with,
but its best to keep track of these potential pitfalls.

*** Error in log files when reading in Dia model                      :story:

For some reason the log file is full of errors like this:

: 2014-01-20 18:28:31.219549 [ERROR] [dia_to_sml.processor] Did not find expected attribute value type: composite

Presumably the errors are not fatal as code generation still
works. Investigate the errors and tidy-up the log.

*** Add tests for =flat_name_builder=                                 :story:

We refactored tagger code related to flattening names into this class
but added no tests. We need good coverage, hopefully available from
C++ formatters.

*** Add tests for =annotation_factory=                                :story:

We added this class without any tests initially because we wanted to
first prove =om= worked. Once this is achieved we need to revisit this
class and add tests.

- missing licence
- missing modeline
- empty marker
- different marker for two objects
- consider moving generate preamble into annotation

*** Add tests for main header file formatter with optionality         :story:

We should add a couple of tests that exercise the annotation
factory. As it will have its own tests, we just need to make sure it
works in general. For example, pass in an empty annotation.

*** Remove speculative facet layers for hash and serialization        :story:

For some unfathomable reason we decided to add a layer of indirection
for both hash and serialization. This is for speculative reasons as in
the future we may want to add boost hash and other forms of
serialization. However, in keeping with the (often violated) rule that
we never add code without a use case, we need to remove this.

*** Parameter to disable cpp file                                     :story:

It would be really useful to define a implementation specific
parameter which disables the generation of a cpp file for a
service. This would stop us from having to create noddy translation
units with dummy functions just to avoid having to define exclusion
regexes.

*** "Data driven" includer                                            :story:

We should simply go through all the types in the SML model and for
each type and each facet create the corresponding inclusion
path. locator can be used to generate standard paths, and a model
specific mapping is required for other models such as std.

Include then takes the relationships extracted by extractor, the
mappings generated by this mapper and simply appends to the inclusion
list the file names. it also appends the implementation specific
headers.

*** Includer generation should be done from meta-data                 :story:
    CLOCK: [2013-10-27 Sun 20:40]--[2013-10-27 Sun 20:58] =>  0:18

It would be nice if we could determine which includer files to create
by looking at the meta-data. For this we need a notion of an inclusion
group, defined at the model level:

- =cpp.types.includers.general=
- =cpp.types.includers.value_objects=
- ...

Under each of these one would configure the aspect:

- =cpp.types.includers.general.generate=: =true=
- =cpp.types.includers.general.file_name=: =a/b/c=
- =cpp.types.includers.general.is_system=: =false=

Then, each type, module etc would declare its membership (as a list):

- =cpp.includers.member=: =cpp.types.includers.general=
- =cpp.includers.member=: =cpp.types.includers.value_objects=
- ...

*** Fix Ninja build                                                   :story:

When compiling with [[http://martine.github.io/ninja/][Ninja]], we get a strange compilation error in
header validation. This is because there are too many object files in
the archiver command line:

: Argument list too long
: ninja: build stopped: subcommand failed.

We need to look at the CMake source to try to understand why this
happens, as it appears someone has already did a fix for large command
lines. See [[https://github.com/martine/ninja/issues/53][this]] issue and [[http://webcache.googleusercontent.com/search?q%3Dcache:_7Sj_3WKZIQJ:www.itk.org/Bug/bug_relationship_graph.php%253Fbug_id%253D13385%2526graph%253Ddependency%2B&cd%3D2&hl%3Den&ct%3Dclnk&gl%3Duk][this]] bug report.

For notes on how to use Ninja from CMake see [[http://www.kitware.com/blog/home/post/434][this]] article.

This is not related to the version of CMake we're using in the PFH
(2.8.8 instead of the debian unstable 2.8.12.1), as running with the
later did not fix the problem.

The correct solution for this appears to be to create targets that
check syntax only rather than generate object files, link static
libraries, etc. Google has made some changes to CMake to support
targets such as syntax only. We need to investigate how to use them
since this would most likely fix the Ninja build and would decrease
compile times for header validation.

See also [[http://public.kitware.com/pipermail/cmake-developers/2011-January/000998.html][this post]] for a more generic discussion on how to run these
sort of tools in CMake.

Note: this is still a problem even after splitting header validation
into two.

*** Add support for Address Sanitizer (ASan)                          :story:

This seems like another interest dynamic analysis tool:

[[https://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction][Address Sanitizer]]

*** Add support for CPPCheck                                          :story:

Seems like CPPCheck has a different take on dynamic analysis when
compared to Valgrind. We should look into how hard it is to integrate
it with CTest.

Links:

- [[http://cmake.3232098.n2.nabble.com/Static-code-analysis-with-CDash-td6079787.html][CMake and CPPCheck]]

*** Code generation as a service                                      :story:

One way of testing new functionality added to dogen is to try to
exercise it as part of the code generation itself. We have been doing
this with the bootstrapping, but there were limitations on
functionality such as ODB and EOS where we couldn't see any obvious
use for it in code generation. However, there is one way of exercising
this and a lot more of these sort of features: to create a Web-based code
generation service, along the lines of Web Sequence Diagrams or
YUML. We could create a simple bootstrap based website that forwards
requests to a set of end-points, all done within the dogen project.

We'd create a casablanca REST layer with a simple interface, with
functionality such:

- create workspace: returns a UUID and creates some kind of internal
  storage area.
- upload target: uploads a Dia or JSON model to be used as the code
  generation target.
- upload reference: uploads a Dia or JSON model to be used as a
  reference.
- set options: which facets to generate, which languages, etc.
- codegen: runs the code generation and returns a tarball with
  generated files and the log file; or returns a set of code
  generation errors.
- we could integrate with google drive to load the files from there.

As a further layer we could create an ASIO service that is queried by
the casablanca REST. This would exercise all of the messaging
infrastructure. Internally it would create the engine and run code
generation. It could also exercise ODB by writing session information
to a database and keeping track of the historical usage of the
service, log files etc.

This stack would allow us to continuously exercise pretty much every
feature we need out of dogen. As an added bonus, when we get to the UI
we could also exercise that (Wt, GTK).

Finally, this would also allow us to play with Docker, and place each
service in their own container, create load balancing etc.

Links:

- [[http://codeplanet.io/principles-good-restful-api-design/][REST API Design]]
- [[http://www.drdobbs.com/tools/json-and-the-microsoft-c-rest-sdk/240164821][Using Microsoft REST SDK]]

*** Investigate the integration of =boost::log= with throw exception  :story:

At present we write a lot of code like this:

: BOOST_LOG_SEV(lg, error) << object_not_found << qn;
: BOOST_THROW_EXCEPTION(indexing_error(object_not_found +
:     boost::lexical_cast<std::string>(qn)));

This is to ensure we log the fact that an exception occurred to make
debugging problems easier. However, it leads to a lot of duplicated
code. We need to figure out a way of simplifying this, most likely
through a macro.

*** Investigate integration of =boost::log= with =boost::test=        :story:

At present whenever there is a test failure, we get a compiler-style
error in the console, which is great for emacs integration - its easy
to go to the source code that generated the failure. However, we do
not write it to the log file of the test. Its very difficult to
understand the log file without the context of the =boost::test=
failures. Due to this we end up manually logging before doing boost
test assertions - a lot of duplicated effort. What would be ideal is
if =boost::test= logged to _both_ the console and to our log
file. There is a file output for boost log, but its not configurable
enough to accept a =boost::log= stream. We should send an email to
mailing list asking for help.

*** Feature models should always be tested by knit                    :story:

We recently implemented features into dogen; these work off of CMake
detection, where by if a library is not detected, all tests associated
with it are not built and executed. However, we should still try to
codegen these models to make sure that a change we did elsewhere did
not introduce bugs in features we're not interested in. We need to
check that knit has tests for both EOS and ODB that get executed
regardless of these features being on or off.

*** Automatic generation of C interfaces for C++ code                 :story:

Once we have proper C support, it should be doable to have a C++ facet
that automatically exposes a C interface.

*** Support for COM/CORBA                                             :story:

We should investigate how hard it is to add support for these IDLs.

*** Add a utility that converts a dia model into JSON                 :story:

It would be great if one could take a dia model and convert it into a
JSON representation. This would allow users to take models that are
not particularly useful in UML and convert them into JSON.

Name according to new convention: tailor.

*** Investigate support for automatic diagram updates                 :story:

For classes that are manually generated, it would be really nice if we
could update the properties of the class in the diagram from the
source code. This would work as follows:

- user creates a class =x= and marks it as non-generatable; executes
  dogen.
- dogen creates the initial file and adds as much boilerplate as
  possible. For instance if the user manually added properties or
  operations to the class, dogen generates skeletons for these.
- once the file exists, dogen will no longer touch it (see also the
  merging code generation story, for a different take on this).
- the user runs a second tool (=diup=? the Diagram Updator) which uses
  clang internally; it reads the diagram and looks for all of the
  non-generatable classes; for each of these, it updates the dia class
  with the properties found in the source file. Everything else is
  left untouched.

This feature would be extremely powerful when in presence of many
other features such as mocking, remote method invocation, etc - the
user would have no effort at all in generating the
code. Implementation-wise we'd have to:

- create an XML writer;
- add write support for the dia model and ensure we generate valid dia
  models;
- integrate clang libraries with dogen;
- create =diup= - or perhaps we should just have an "update diagram"
  mode in dogen?

*** Investigate the possibility of creating a mock facet              :story:

This is straight out of left-field, but may actually be a good
idea. One annoying thing with mocking frameworks such as [[http://turtle.sourceforge.net/index.html][turtle]] is
the amount of macros. However, =dogen= already has all the required
information needed to create an expectation based mock - the
meta-model. We could mimic the turtle API with a mock facet that is
made up of real C++ objects. When a class is marked as an interface,
we could automatically generate its mock in a mock facet.

This will require proper operations support.

*** Make test data generator more configurable                        :story:

One thing that would be useful is to have a way to attach lambdas to
test data generator. Let =a= be a class with a property =prop= of type
string. It would be nice to be able to do:

: a_generator g;
: g.prop([](const unsigned int seed) {
:     std::ostringstream s;
:     s << "my property " << seed * 10;
:     return s.str();
: });

And so on, for all member variables. The generators would have some
default behaviour, but it could be overridden at any point by the
user. With this, test data generator would be a great starting point
as a way of generating random data for test systems.

See also [[http://www.json-generator.com/][JSON generator]].

*** Indent stream can be made a bit less inefficient                  :story:

Out first attempt at creating a stream with indentation support was a
bit naive: we are intercepting every character and then deciding if we
need to change any states in the state machine. Its probably wiser to
just use manipulators to perform the state transitions and leave the
=put= undisturbed. We can leave this until we have a good way of
getting metrics out of the system.

*** Consider creating an iostreams filter for comments                :story:

Seems logic to follow the filtering idea and add a doxygen (or
generic) commenting filter; one inserts into the stream and it
automatically inserts all the comment markers such as =/**=, =@brief=
and so on. Basically takes on the work of =comment_formatter=. This
would mean we would no longer need the =contents= vector, and we could
stream directly to the stream, after pushing the comments formatter on
to it. However, it would probably mean we need to cast the stream to a
=filtering_ostream= in order to do the push.

*** Use consistently the American spelling for license                :story:

We have a mix of American and British spelling of license (e.g. data
file folder is called licence. For details on the subject see [[http://www.future-perfect.co.uk/grammar-tip/is-it-license-or-licence/][this
article]].

We are going to take the easy approach as we did for serialisation and
make all the code artefacts American. Documentation etc is not that
important.

*** Improve formatters code generation marker                         :story:

Things the marker can/should have:

- model level version;
- the dogen version too. However, this will make all our tests break
  every time there is a new commit so perhaps we need to have this
  switched off by default.

*** Create a new command line parameter for data files directories    :story:

Users should be able to provide directories for their own JSON
models. We just need to add a new parameter to the knitter and
transport it all the way to OM's workflow.

*** Check packaging code for non-distro dependencies                  :story:

We are manually copying a lot of shared objects from locally built
third party libraries when creating packages, this should be replaced
with appropriate dependencies (at least for Debian packages).

*** Fix cp error on cmake with local third-party packages             :story:

We are getting strange errors in cmake:

: cp: cannot stat ‘/usr/lib/i386-linux-gnu/libpthread.so.1.54.0’: No such file or directory

*** Add support for units                                             :story:

With user defined literals in C++11, defining one's own numeric types
became more convenient. We should look into adding support for this in
dogen.

See [[http://www.codeproject.com/Articles/447922/Application-of-Cplusplus11-User-Defined-Literals-t][Application of C++11 User-Defined Literals to Handling Scientific
Quantities, Number Representation and String Manipulation]]

*** Use xtime-like stopwatch in selected places to log timings        :story:

We should log the time it takes for certain operations in dogen so
that users can figure out if we are becoming slower (or faster) at
doing them and report regressions.

Boost used to provide a nifty little utility class called xtime. It
appears to have been deprecated by [[http://www.boost.org/doc/libs/1_55_0/doc/html/chrono/users_guide.html#chrono.users_guide.examples.duration.xtime_conversions][chrono]].

We should also provide a command line option that prints a timing
report. This would be useful so that users can compare timings between
releases.

*** Use clang to generate json system models                          :story:

We should be able to create a clang based utility that given a set of
libraries returns a basic JSON model for dogen for them, with as much
filled in as possible such as include directories, etc. This would
save us a lot of time instead of manually adding these.

*** Operations need to behave more like properties                    :story:

When we did the expansion and indexing work for properties, we omitted
operations altogether. This is fine for now, as we only have a
half-baked support for them anyway, but will need to be revisited as
we start to use it in anger. In particular:

- we need sets of operations: local, inherited, all
- we need an operations indexer

*** Consider using a graph in SML for indexing                        :story:

To keep things simple we created a number of specialised indexers,
each performing a complete loop, recursion, etc over the merged
model. A better way of doing things would be to do a DAG of the model
that includes both concepts and objects and then DFS it; at each
vertex we could plug in a set of indexers, each acting on the
vertex. We could also have dependencies between the indexers (for
example concept indexing must take place before property indexing and
so on).

*** Consider renaming SML                                             :story:

Originally we intended to rename SML - the Simplified Modeling
Language - to DDL - the Domain Driven Language. This was because we
had envisioned that SML was a model of the ideas in Domain Driven
Design, and not at all a cut down version of UML as the name seems to
imply. However, its becoming increasingly clear that, whilst we use a
lot of the Domain Driven Design ideas, we are also morphing them
considerably. Perhaps a more apt name would be SDML - the Simplified
Domain Modeling Language?

Or instead We could follow the compiler theme and call it the =ir= or
intermediate representation, or =im= for intermediate model.

*** Add POF serialisation support                                     :story:

If coherence has open source C++ libraries, we should add support for
serialisation to and from POF.

*** Types that share one file                                         :story:

At present we force all types etc to have their own file. However, in
cases it may be useful to have multiple types sharing the same
file. For instance, one may want to have all enumerations in one file,
or all exceptions, etc.

We could easily implement this using meta-data.

*** Canned tests rely on copy constructors rather than cloning        :story:

If an object has pointers, the canned tests will not perform a deep
copy of the object. We need to [[*Add%20support%20for%20object%20cloning][implement cloning]] and then use it in
canned tests.

*** Clean up SML resolver tests by extending mock factory             :story:

Now that the mock factory has the concept of "stages" of processing,
we need to create a "stage" for merged but unresolved models and
remove the merger from the resolver tests. The flag for this has been
added, we just need to go through the different scenarios and add
handling code for them.

*** Split library model loading from SML workflow                     :story:

We should really create a library model loader, with tests, instead of
doing all the work directly in the SML workflow.

We can then start thinking about adding features such as loading only
libraries the merged model depends on, etc.

*** Refactor SML mock factory method names                            :story:

We have a zoo of naming conventions, some starting with =build_=, some
starting with =object_= etc.

*** Validate SML mock factory on its own tests                        :story:

At present we have a lot of code that ensures that the output of mock
factory actually corresponds to expectations. However, this validation
is in the tests that use the mock factory, resulting in duplication
and possibly missing coverage. We should really just have a mock
factory test with this validation.

*** Add export macros support                                         :story:

We should add export macros for shared objects/DLLs:

: #ifdef ExportDeclaration
:    #undef ExportDeclaration
: #endif
:
: #ifdef
:    #define ExportMacro __declspec(dllexport)
: #else
:    #define ExportMacro __declspec(dllimport)
: #endif

There is also a GCC equivalent explained [[http://pic.dhe.ibm.com/infocenter/tpfhelp/current/index.jsp?topic%3D%252Fcom.ibm.ztpf-ztpfdf.doc_put.cur%252Fgtpl2%252Fexport.html][here]].

*** Generate Visual Studio solutions                                  :story:

At present we rely on CMake as the C++ meta-build system. There is
nothing stopping us from supporting more native build systems such as
Visual Studio. Consider adding direct support for Visual Studio.

*** Self-contained build files                                        :story:

It would be nice to be able to generate a complete application from a
given model, or a library. At present there is an expectation that the
user will slot in the generated CMake files into a larger, more
comprehensive CMake build. All we need is:

- some kind of binary type: e.g. executable or library. we should have
  this anyway. meta data at the model level could be used to convey
  this.
- if executable, we should automatically ignore a main.cpp in the
  source directory.
- generate a stand-alone CMake template.

The idea is that with this the user could immediately generate a
binary without any further configuration required.

*** Sanitizer: Add tests for empty objects                            :story:

This was mainly in the context of IO but could be useful for other
facets. Example:

: class empty_model_generator {
: public:
:     typedef dogen::sml::model result_type;
:
: public:
:     result_type operator()() {
:         dogen::sml::model r;
:         return r;
:     }
: };
: ...
: BOOST_AUTO_TEST_CASE(validate_io_for_empty_objects) {
:     SETUP_TEST_LOG("validate_io_for_empty_objects");
:
:     /* ensure we generate valid JSON for empty model. test was added
:      * because empty property trees were not correct, but its valid on
:      * its own right as we always use populated objects when testing
:      * JSON.
:      */
:     // test_io<empty_model_generator>();
: }

*** Create a visitor interface with multiple implementations          :story:

We decided to use a base class for visitor; it would have been better
to create an interface, with multiple implementations:

- negative visitor: any unimplemented methods throw
- default visitor: all methods do nothing
- [[*Visitor%20with%20%3Dstd::function%3D%20for%20each%20%3Dvisit%3D%20method][std::function visitor]]
- ...

Users can then inherit from these visitors where appropriate
(e.g. negative and default visitors).

*** Disable =invalid= value in enumerations                           :story:

At present all enumerations must have an invalid value. One can
conceive cases where that is not a useful thing. We should have a
meta-data flag that disables it.

*** Bitmask enumeration                                               :story:

We should have a meta-data flag that generates enumerators with values
that are powers of two. These can then be used for flags, as per the
[[*Add%20support%20for%20bitsets][bitset story]].

*** Replace Boolean attributes with flags in abstract object          :story:

We have a number of Boolean attributes in abstract object which could
easily be replaced by a single int and a flag enumeration. We would
also need a set of utility methods to access the values.

This story has a dependency on [[*Add%20support%20for%20bitsets][bitset support]].

*** Support for file level comments via meta-data                     :story:

We could easily have a tag for file level comments and transport that
all the way to the output. The only problem is that it would be a one
liner only so it may not be that useful.

Multi-line support could be simulated by concatenating multiple
entries - cumbersome but workable...

*** Caching qname lookups                                             :story:
    CLOCK: [2013-10-30 Wed 18:02]--[2013-10-30 Wed 18:03] =>  0:01
    CLOCK: [2013-10-30 Wed 08:38]--[2013-10-30 Wed 08:43] =>  0:05

Once the model has been merged and resolved, all qnames in the model
all known to resolve to a valid type, model or module. This means we
could cache in the qname itself a pointer to the object the qname
resolves into. There are two problems with this approach:

- we do not have a base class that covers types, models and
  module. one could be created (=modeling_entity=?) with an associated
  visitor. but then:
- formatters are not designed to think at the =modeling_entity= level;
  a formatter that does types may not necessarily be able to do
  modules or models. Thus we would need to convert from a
  =modeling_entity= to a type, model or module before we get to the
  formatter.

However one imagines that a great number of lookups would be avoided
if this was possible.

*** Models should have an associated language                         :story:
    CLOCK: [2013-10-30 Wed 08:07]--[2013-10-30 Wed 08:15] =>  0:08

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as meta-data in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in SML
which is a property of the model, and one entry of which is "language
agnostic".

*** Forward declaration is not always correct for services            :story:

In cases where we used a service as a way of declaring a stand alone
function (such as the traversals in SML), the forward declarations do
not match the header file at all. In this cases we should use
=nongeneratable= rather than =service= stereotypes, and perhaps when
that happens we should switch off forward declarations?

*** Add tests for tagging of modules, primitves and enumerations      :story:

We've tested abstract objects et al quite a lot but forgotten the
other aspects of the model.

*** Add tests for all permutations of the domain formatter            :story:

_All_ may be too strong a word as there quite a few. We need good
coverage around the combinations one can do within the domain
formatter.

*** Add getter and setter prefixes                                    :story:

External users may have getter and setter prefix conventions such as
=set_prop= or =SetProp=. It would be nice if we could pass in a
getter/setting prefix and then dogen would append them when converting
the diagram, e.g. =--getter-prefix=set_=.

We should check what ODB has done for this and implement the same
pattern.

*** Formatters should cache qname formatting                          :story:

We seem to re-format the same qname lots of times. We should just use
a =std::ostringstream= to format once and reuse the resulting
string. Probably worth doing this change after the performance tests
are in.

*** Split floating point stream settings from double                  :story:

We had a problem where the output of floating point numbers was being
truncated due to scientific notation being used. A quick fix was to
just update the properties of all streams which use either doubles,
floats or _bools_ with precision etc settings. The real fix is to
distinguish between the two such that we only enable =bool= related
settings when dealing with bools and floating point settings when
dealing with =double= or =float=.

*** Split is floating point like from int like in view model          :story:

At present we only have a single test data generator helper method for
any numeric type: =is_int_like=. This works ok, but it means we are not
generating useful test data for doubles, e.g: =1.0= instead of a
slightly more useful =1.2345= or some such number.

We need a =is_floating_point_like= method to be able to distinguish
between them, and then the associated changes in the generators to
create floating point numbers.

*** System models set meta-type to =invalid=                          :story:

Something is not quite right on the resolution logic

*** Improve the integration of dogen with dia                         :story:

It would be great if the model generation in dia was slightly more
interactive:

- dia could have a button to run/configure an external tool, where the
  setup for dogen would be kept
- pushing an execute button would code generate
- pushing a validate button would validate the current diagram, taking
  into account declared references. references to types that are not
  resolved could make the class or function go red.

The idea is to do the least intrusive changes in dia that would
provide us with this support. In order to access dogen, instead of
running the executable and parsing the command line output, it would
make more sense to create a C interface that supports these specific
use cases (and nothing else).

*** Add support for qualified class names in dia                      :story:

It has become apparent that creating large packages in dia and placing
all classes in a large package is cumbersome:

- there are issues with the large package implementation in dia,
  making copying and pasting a dark art; its not very obvious how one
  copies into a package (e.g. populating the child node id correctly).
- models do not always have a neat division between packages; in
  dogen, where packages would be useful, there are all sorts of
  connections (e.g. inheritance, association) between the package and
  the model "package" or other packages. Thus is very difficult to
  produce a representative diagram.

A solution to this problem would be to support qualified names in
class names; these would be interpreted as being part of the current
model. One would still have to define a large package, but it could be
empty, or contain only the types which only have connections inside
the package, plus comments for the package, etc.

*** Convert all files in library into JSON                            :story:

We started off by using the INI format, but then subsequently found it
too inexpressive to be able to carry SML representations and started
using JSON. However, modeline groups, etc are still in INI format.

*** Consider model as a container of types                            :story:

At present model is composed of objects, primitives, concepts,
modules, etc. We could bring together all descendants of types into a
single container (e.g. types). However, in places we do thinks like
looking at the primitive container to see if the container has any
primitive types - these would become slower as we'd now be looking at
the entire type collection. Need to look at all usages of these
containers in the code to see if this would be a win or not.

*** Consider adding YQL support                                       :story:

YQL offers a REST based API with lots of interesting information; an
example of the information provided is available [[https://github.com/yql/yql-tables/blob/master/yahoo/finance/yahoo.finance.quant.xml][here]]. There should be
somewhere a matching XML schema for each of these queries, at least
for the end points that return XML. It would be great if one could
take one of those schemas and generate an SML representation for them.

More generally, it would be great if dogen was able to create a domain
model off of an XML schema. However, we already have the Code
Synthesis [[http://www.codesynthesis.com/products/xsd/][XSD tool]] for that, so maybe this is just scope creep.

*** Consider adding merging code generation support                   :story:

At present it is not possible to manually add methods to a class that
was code generated; one must stop code generating the class and
maintain the whole class manually. However, in some cases it makes
sense to have a combination of both:

- value objects need helper methods such as for example boolean
  properties (e.g. =is_empty=) that make use of other properties, or
  simple methods such as population etc that really belong to the
  object rather than an external service
- services sometimes need state and it would be good if we could
  manage that via code generation.

For this we need a merging code generator: that is, a code generator
that is aware of code that was crafted manually and does not overwrite
it - but instead "intelligently" merges manual with code generated
code.

From the beginning we avoided this because we thought it would be too
complicated for dogen. However, its increasingly becoming apparent
that this is a needed feature for the real world - there are many
cases where we are working around this deficiency. A few solutions are
possible:

- let the code generator manage the header file and create two types
  of CPP files, one which includes the other: a manual and an
  "automatic" one. This would effectively separate the two types of
  code. For this dogen would have to be able to generate complex types
  in operations (e.g. we'd have to solve the lack of support for
  =const std::string&=).
- use clang to do the merging. this probably means adding some kind of
  attribute to every method - possibly using C++ attribute support
  (e.g. =[ [generated ] ]= and/or =[ [ manual ] ]= (spaces due to org
  mode). We could then say to clang: read current state of the file,
  grab every non-generated method and copy them across to the newly
  code generated file. Merging could be the final stage before
  writing. In addition, we should also have some meta-data to
  determine which files require merging. The meta-data could be
  populated automatically (e.g. grep for the manual attribute) or
  manually.

*** Create includers for value objects only                           :story:

At present we are using the facet includers in unit tests. This is not
ideal because it means that every time we do a change in a service
header, all tests recompile. In reality we should have two types of
inclusions:

- canned tests should include only value objects, etc - e.g. no
  services.
- service tests should include the header for the service and any
  additional dependencies the service may require.

Perhaps we could have a second type of includer that only has value
objects, etc.

*** Do not copy models in merger                                      :story:

At present we are adding the partial models into the merger by copying
them into an associative container. It would be nicer to avoid the
copying as it adds no value. This should wait until we have a way to
get performance numbers out.

*** Adding linking libraries is not handled                           :story:

At present whenever a model requires additional link library targets
we need to disable CMake generation and do it by hand. However:

- for well-known dependencies such as boost we could create a
  convention (e.g. assume/require that the CMake boost libraries flags
  are set via find boost)
- for user level dependencies we should add meta-data tags at the
  model level.

*** Test data generator does not detect cycles in object graph        :story:

At present we handle composition correctly, but not other forms of
cycles in the object graph.

Let model M be composed of class A with a member of type class B, and
class B with a member of type =shared_ptr= to class A. The test data
generated for such model will contain an infinite loop. We need a way
to detect such loops, potentially in SML, and then generate code which
breaks the loop.

This could be done by explicitly checking if the type of any member
variable loops back into the type itself. Of course one could conceive
cycles that involve many edges in the object graph, and for these we'd
still generate invalid code.

Another approach would be to have an unordered map of type
association; the map would have the IDs of every type as we go further
into the association graph. It would be pushed and popped as we go in
and out of branches; at the same time we need to have a look back
capacity to see the few elements in the stack. When a pattern emerges
that involved types of a certain ID, they would stop creating any
further associations.

*** Split a fully formed model from partial models                    :story:

We should really have two distinct types to represent the model that
is returned from the dia to sml transformer from the model returned by
the merger. Potentially this could be called =partial_model=.

*** Create a =key_extractor= service                                  :story:

We need a way to automatically extract a key for a =keyed_entity=.
The right solution is to create a service to represent this
concept.

Injector creates objects for these just like it does with keys; the
C++ transformer intercepts them and generates the correct view models.

*** Use explicit casting for versioned to unversioned conversions     :story:

At present we have to_versioned; in reality this would be dealt much
better using explicit casts:

: explicit operator std::string() { return "explicit"; }

Actually the real solution for this is to make the versioned key
contain the unversioned key; then dogen will generate all the
required code.

At this point in time we do not have enough use cases to make the
correct design decisions in this area. We need to wait until we start
using keys in anger in Creris and then design the API around the use
cases.

It is not possible to use global cast operators so we need to
introduce a dependency between versioned and unversioned keys in order
for this to work.

*** Consider not creating unversioned keys for single property        :story:

If a key is made up of a single property, its a bit nonsensical to
create an unversioned key. We should only generate the versioned
key. However, it does make life easier. Wait for real world use cases
to decide.

*** Detect invalid child nodes                                        :story:

When copying a set of classes from a diagram, where these classes
where contained in a package, dia seems to copy across the =childnode=
id. This is a problem because when pasted in a new diagram, if those
classes are not in a package there is now the potential for total
mismatching - for instance, they could be children of an
association. Dogen should validate that children belong to UML
elements which can have children, and if not issue good error
messages - perhaps even talking about the possible cause for the
error.

*** Add tests for SML workflow                                        :story:

We don't seem to have any. A few come to mind:

- model with no generatable types returns false
- model with generatable types returns true
- multiple models get merged
- system models get injected

*** Rename nested qname to composite qname                            :story:

*New understanding*:

This story requires further analysis. Blindly following the composite
pattern was tried but it resulted in a lot of inconsistencies because
we then had to follow MEC-33 and create =abstract_qname=; however, the
nested qname does not really behave like a composite qname; its more
like the difference between a type in isolation and a type
instantiated as an argument of a function. For example, whilst the
type in isolation may have unknown template parameters, presumably, as
an argument of a function these have been instantiated with real
types.

*Previous understanding*:

We should just follow the composite pattern in the naming.

*** Injection framework                                               :story:

We need a more generic way of handling system types injection into
models. This is because there is a number of things that can be
derived from the existing model types:

- keys
- diff support
- reflection
- cache code
- etc.

So we need to:

- make injector a composite of injectors that do the real work such as
  =key_injector=. internally =injector= just delegates the work to
  these classes.
- injector decides which internal injectors to use based on options
  passed in.
- in the IoC spirit, we should probably create a =injector_interface=.

*** Register types for multiple models is misbehaving                 :story:

It seems that somehow we're clobbering the type registration of one
model with another in register types. This is probably because we are
reusing type id's somehow. This wasn't a problem until now because we
were not using inheritance in anger but with the sml changes, it is a
problem as one cannot load dia and sml types off the same registration
(e.g. as in XML serialisation helper).

One solution for this problem would be to create serialisers which
hide the machinery of serialisation internally; one should be able to
just pass in a stream in and get a type out.

*** Comments seem to be trimmed                                       :story:

For some reason we seem to be munching any blank lines at the end of
comments. We should only remove the lines with the well known dogen
marker, all other lines should be left untouched.

*** Type resolution in referenced models                              :story:

We did a hack a while ago whereby if a type is of a referenced model,
we don't bother resolving it. As an optimisation this is probably
fine, but however, it hides a bug which is that we fail to resolve
properties of referenced models properly. The reason why is that these
properties have a blank model name. We could simply force it to be the
name of the referenced model but then it would fail to find
primitives. So we leave it blank during the dia to sml translation and
then if it gets to the resolver, it will not be able to resolve the
type. We could add yet another layer of try-logic (e.g. try every
model name in the references) but it seems that this is just another
hack to solve a more fundamental problem. The sort of errors one gets
due to this are like so:

: 2013-06-29 23:10:34.831009 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "type_name": "qname", "meta_type": { "__type__": "meta_types", "value": "invalid" } }
: 2013-06-29 23:10:34.831294 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/sml/src/types/resolver.cpp(202): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname&) const
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::sml::resolution_error>

*** Visitor adaptor for usage in ranges                               :story:

It would be great if we automatically generated an adaptor to visitors
which could be plugged into a range. Internally the adaptor would
perform the accept on its =operator()=. We could also have an adaptor
for a =std::pair= which would be templatised on the first member of
the pair. Or should one just use a keys or values range iterator.

*** Visitor with =std::function= for each =visit= method              :story:

It would be nice if the code generator created a visitor which has as
its properties a set of =std::function= which match the signature of
the visit functions; then the visit functions would just check that
the functions have been assigned and call them. If not, throw.

*** Check concept properties for identity                             :story:
    CLOCK: [2013-06-24 Mon 22:33]--[2013-06-24 Mon 22:36] =>  0:03

When we added concepts we didn't had a link to the processing of
identity attributes. This means that if we get a property via modeling
a concept it is not processed and added to the keys.

Update injector to follow concepts.

*** Sort model dependencies                                           :story:

It seems the order of registration of models has moved with recent
builds of dogen (1418). Investigate if we sort the dependencies and if
not, sort them.

*** Use pimpl for a few "one-shot" services                           :story:

We have quite a few services where it would be great to have
transactional semantics. For example, when building a graph in
=sml::grapher=, it would be great if one could have a list of objects
to graph as an input and some kind of =grapher_result= as the
output. From a potential =grapher_interface= it would look like a
simple method in the interface, almost static. The problem with this
approach of course is that it makes the =grapher_interface=
implementations cumbersome because one has to pass all parameters to
all internal methods instead of using class state. The present
approach is to make it a "prepare" and then "use" sort of service,
causing the usual nonsensical methods of "is it finished yet" and "are
you trying to use the service a second time" (e.g. =is_built=,
etc). Even if we pass in all the inputs in the constructor, its still
not ideal. There are two options:

- set member variables inside the "one-shot" function and then unset
  them at the end;
- have a =grapher= implementation which uses a =grapher_impl= that
  does provide a sensible implementation. We used to do this inside
  the =.cpp= files but then they became too big to manage.

*** Assignment operator should be protected in ABC                    :story:

As per MEC 33. We should probably do the same for the move and copy
constructors.

*** SML models could have a model classification                      :story:

Consider creating an enumeration for model classification (e.g. type
of the model):

- relational model
- core domain model
- generic sub-domain model
- segregated core model

This still requires a lot of analysis work around the DDD book.

*** Change transformation code to use a type visitor                  :story:

Now we have a base type, we could probably simplify some of the
transformation code:

- dia to sml
- sml to c++
- potentially merger

*** Test data generator with immutability looks wrong                 :story:

We are using the full constructor for immutability, but its not clear
how that would work on a inheritance tree. Ensure we have test cases
for this.

*** Add support for boost concept                                     :story:

Now dogen supports concepts, the natural thing to do is to express
them in C++ code. This could easily be done using boost concept, or
the C++-14 concepts light.

See [[http://www.boost.org/doc/libs/1_53_0/libs/concept_check/creating_concepts.htm][Creating Concepts]].

*** Add support for boost and/or std tuple                            :story:
    CLOCK: [2013-06-04 Tue 18:30]--[2013-06-04 Tue 18:32] =>  0:02
    CLOCK: [2013-06-04 Tue 18:18]--[2013-06-04 Tue 18:27] =>  0:09

It would be nice to be able to use =std::tuple= and/or =boost::tuple=
from dogen. The processing would be rather similar to containers. It
would be even nicer if one could associate an enumeration to a tuple
so that the gets would be more meaningful, e.g.:

: std::get<my_field>()

rather than

: std::get<0>()

Using =std::tuple= would mean we'd have to create our own serialisers
for it most likely.

*** Add support for posix_time_zone                                   :story:

At present we need to use std::string to convey time zone
information. We should be able to use the time zones available in
boost date time library.

See boost documentation: [[http://www.boost.org/doc/libs/1_53_0/doc/html/date_time/local_time.html#date_time.local_time.posix_time_zone][Posix Time Zone]]

*** Add support for GtkBuilder / Glade XML files                       :epic:

There is nothing stopping us from using a GtkBuilder / Glade XML file
as an input, create some SML from it and then generate code which
would do the boiler plate setup of the UI. With a bit more work one
could potentially even generate the bindings for a presentation model.

*** Consider renaming formatters                                      :story:

These are not really formatters; not sure what the right name should
be though; templates?

*** Add support for object cloning                                    :story:

We should have a clone method which copy constructs all non-pointer
types, and then creates new objects for pointer types.

*** Remote method invocation                                          :story:
    CLOCK: [2013-05-24 Fri 07:46]--[2013-05-24 Fri 08:13] =>  0:27

See [[*Type%20framing][type framing]], [[*Model%20and%20type%20enums][Model and type enums]],

It seems fairly straightforward to add remote method invocation to a
few select types. The following would have to be done:

- create a new stereotype like =dispatchable=, =remotable= or suchlike
- create a new stereotype: interface.
- add support for interface code generation.
- validation: model must have a model ID, thought to be unique across
  models.
- validation: types must be marked as both =remotable= and
  =interface= and have a unique type ID in the model.
- validation: types must have at least one public method
- injector: if at least one type is =remotable=, a new system
  package is created: =rmi=.
- injector: a system enumeration will be created with all the
  supported serialisation types. actually, we should create this
  anyway in serialisation or reflection.
- rmi will contain one class that represents a "frame". this
  frame will be composed as follows: model ID, type ID, serialisation
  type, raw buffer. we need to look at RMI terminology to come up with
  a good name for this frame.
- messages: for each method that exists in each dispatchable service,
  a message class will be created with a name following some well
  defined convention such as =CLASS_NAME_METHOD_NAME=. we need
  examples to make up a sensible convention. or perhaps an
  implementation specific parameter can override the class name. the
  message class is a data object and has as attributes all of the
  parameters of the method.
- a dispatcher class will be created in dispatching. it will have as
  constructor arguments references to all the dispatchable
  services. when passed in a frame, it will hydrate it and dispatch it
  to the correct service.
- a "framer" class will be created in dispatching. it will be
  configured for a given serialisation type. it will take a message
  object, serialise it and frame it.
- we could support the notion of callbacks. for this we need to be
  able to serialise stubs as references such that when the other end
  receives it, it calls a registrar to activate a client stub.

Now we just need a way of creating some generic interfaces that take a
wire client and a wire service and plug the framer and the dispatcher
into it.

*** Inserter for enumerations shouldn't throw                         :story:

We only use the inserter for debug dumping and it could happen that we
are about to write the message for an exception when we decide to
throw. Instead we should just print unexpected/invalid value and cast
it to a numeric value in brackets.

*** Generate state diagrams                                           :story:

There is nothing stopping us from reading the UML State Chart objects
in Dia and generating an FSM off of it, using one (or both) of boost's
state machines. We could make the state machine contain inheritable
methods which could be overridden by the user manually.

*** Generate service skeleton                                         :story:

Since we already have all of the boiler plate code for services such
as licence, header guards, etc - we could just create a service
skeleton to stop us from having to copy it from the forward
declarations.

In addition to the class definition, it should also define all of the
automatic constructors, and add a private section at the bottom.

*** Add versioning support                                            :story:
    CLOCK: [2013-05-13 Mon 08:28]--[2013-05-13 Mon 08:37] =>  0:09

New understanding:

- Add versioning support by adding versions at the object level and at
  the property level. Properties with 0 version will have no special
  handling. Properties with non-zero version (V) will have the
  following code added in serialisation:

: if (version > V)
:    // read or write property

- If a number of consecutive properties all share the same version,
  dogen will group them under the same version if. There will be no
  other special grouping or otherwise changing of order of properties.
- The object version will be max(version) of all properties for that
  class, excluding inherited properties.
- The object version will be stamped using boost serialisation class
  version macro, unless the object version is zero.
- Dogen will make no validation or otherwise dictate the management of
  version numbers; its up to the users to ensure they make sensible
  backwards compatibility decisions such as adding only new properties
  and always adding to the end.
- The model version is a human level concept and has no direct
  relation to class versioning. It will be implemented as an
  implementation specific parameter in the Dia model and as a string
  in the SML model class. See [[*Improve%20OM's%20code%20generation%20marker][this story]].
- Model version will be used for the following:
  - stamped on doxygen documentation for the model namespace;
  - stamped on DLLs, etc.
  - used by humans to convey the "type" of changes made to the
    diagram/model (e.g. a minor version bump is a small change, etc).

Previous understanding:

Versioning support is now available in SML, so we need to apply it to
SML itself. That is, we need a way of having two versions of an SML
model coexist, and allow Dogen to diff those two versions to make code
generation decisions so that we can add basic backwards compatibility
support.

Before we can do this, we need a way of stamping a model version into
models. This can easily be done via implementation specific
parameters. See [[*Improve%20OM's%20code%20generation%20marker][this story]].

We then need to create some kind of strategy for version number
management:

- minor bumps are backwards compatible; e.g. only adding new fields.
- major bumps are not backwards compatible: e.g. deleting fields,
  classes, etc.

However, at present we only support a single version number. Perhaps
we should just declare which versions are backwards compatible and
which ones are not.

Once all of these are in place we should add versioning support to
dogen:

- add a new command line argument: =--previous-version= or something
  of the kind.
- the model supplied by this argument must have the same name as the
  model supplied by =--target=.
- change all SML types to be versioned.
- dogen will load up both models, and stamp the versions in each
  type. Merger will then be responsible for stamping the versions on
  each property, taking previous and new as input.
- for every field which is in new model but not in previous, add boost
  serialisation code to handle that.
- add unit tests with v1, v2 models.
- in order for dia diagrams with multiple versions to coexist in the
  same directory we will probably need to add the version to the
  diagram name, e.g. =sml_1.dia= or =sml_v1.dia=. We probably need
  some parsing code that looks for the last few characters of the file
  name and if it obeys a simple convention such as =_v= followed by a
  number, it ignores these for the model name and uses it for the
  version.

With this in place, when rebasing we can now do a proper comparison
between expected and actual.

Potential future feature: to put the files of different versions in
separate folders. This would allow the creation of "conversion" apps
which take types for one version and transform them into the next
version.

*** Add support for boost parameter                                   :story:

It would be nice to have boost parameter support. [[http://www.boost.org/doc/libs/1_53_0/libs/parameter/doc/html/index.html#named-function-parameters][Documentation here]].

Ideally one would mark a type with a stereotype such as =named
parameter= and this would result in a full constructor with named
parameters. However since it seems one has to add a lot of boiler
plate code, perhaps its better to have a create function on a separate
header which internally calls the appropriate setters.

*** Build shared objects instead of dynamic libraries                 :story:

With the increase in tests build speeds have started to suffer,
especially on low hardware. One potential way to mitigate this is to
avoid unnecessary linking. The problem we have at present is that
every time something changes in any model we have to relink all the
binaries that use that model as it is consumed as a static library. If
all the static libraries were converted to shared objects this would
no longer be necessary.

We probably need a dogen command line option to determine what to
build so that users are not forced to always build static / shared
libraries. We should make sure one of the tests is using a static
library to make sure this scenario doesn't get borked.

*** Add comments to test model sanitizer                              :story:

We should explain why we decided to create a test model sanitizer
instead of just adding specs to the test models themselves. The
rationale behind it was that it would break the current diffing and
rebaselining logic; we would either have to ignore specs on the diff
or find a way to copy them after code generation. Both options are a
bit of a hack. So instead we created a model with all the specs.

*** Consider renaming dependencies to references in model             :story:

Dependencies is a map of reference; why not call it references?

*** IoC work                                                           :epic:

All stories related to IoC work are tracked here.

new understanding:

in reality, there is really only one place where IoC makes sense: in
the workflows. It would be great if one could pass in something akin
to a IoC container into the workflow's constructor and then use the
container to obtain access to all services via interfaces. Using
sml::workflow as an example, one could have:

- container_interface which returns grapher_interface,
  processor_interface, etc.
- the container could even return references to the these interfaces
  and own the lifetime of the objects.
- this would then allow us to provide mock container interface
  implementations returning mock services.

However:

- it seems like a lot of moving parts just to allow testing the
  workflow in isolation. this is particularly more so in the case of
  the workflows we have, which are fairly trivial. perhaps we should
  consider this approach when dogen is generating the interfaces
  automatically as this would require a lot of manual work for little
  gain.

old understanding:

- add workflow_interface to SML.
- we should be doing a bit more IoC, particularly with inclusion
  manager, location manager etc. In order to do so we could define
  interfaces for these classes and provide mocks for the tests. This
  would make the tests considerably smaller.

*** Refactor node according to composite pattern in dia to sml        :story:

This is not required if we decide to [[*Add%20composite%20stereotype][implement]] the composite
pattern. We should just follow the composite pattern.

*** Create a validator in SML                                         :story:

We need a class responsible for checking the consistency of the SML
model. There are several things we need to check for non-merged
models:

- ensure that we can only define identity once across concepts and
  parents
- concepts must have at least one property (or method).
- refined concepts must not have properties (or methods) with clashing
  names.
- type names, model names, etc must not contain spaces or other
  invalid characters. We should use a identifier parser for this.
- the qname of all keys in objects, etc must be part of the current
  model.
- the qnames of all types as keys are consistent with the values.
- type_name is non-empty; cannot be blank or a variable name
- type name must not exist on any model
- parent names and original parent names must exist in current model.
- leaves exist in current model.
- entity must have at least one key attribute.
- non entity must not have key attributes (value, service)
- keyed must be entity.
- aggregate root must be entity.
- all properties of types in current model must exist.
- properties of types in other models result in dependencies.
- enumeration must have at least one enumerator
- enumerator name must not be empty
- enumerator name must be unique
- external package path of the model matches all objects, etc in current
  model.
- model name is non-empty.
- documentation does not have non-printable characters.
- number of type arguments is consistent with objects type.
- objects marked as is comparable must follow the [[*Add%20is%20comparable%20to%20SML][comparison rules]].
- objects marked as is parent must have at least one child.
- property can only have a default value if primitive
- property default value must be castable to primitive type.
- property must have non-empty name.
- is versioned objects must have a property called version.
- string table cannot have duplicate entries.
- Issue error when a property is a value of an abstract class: SML
  should fail to merge if the user attempts to create a property of a
  base class. It should allow pointers to the base class though.
- Test relationships between objects and other meta types: We should
  validate that objects are only related to other objects - e.g. they
  cannot inherit from exception or enumeration or vice-versa. Add
  tests for this.
- Its not possible to be immutable and fluent.
- it is not possible to be immutable and be in an inheritance
  relationship. FIXME: why is that?
- user models cannot have stereotype of primitive.
- We don't support generic types (see [[Supporting%20user%20defined%20generic%20types][Supporting user defined generic
types]]) so we should throw if a user attempts to use them.

For merged models:

- issue error when a property is a value of an abstract class
- properties exist in merged model.

Validator should provide contextual validation error messages:

: error 1: properties must have a non-null name
: in model 'my_model' (Dia ID: O0)
: in object 'my_object (Dia ID: O0)
: property 'my_property' has empty name.

*** Add composite stereotype                                          :story:

It would be nice if one could just mark a object as =composite= and dogen
automatically created the composite structure. As we only support
boost shared pointer that's what we'd use. We have a few use cases for
this (node, nested qname, etc).

This would be part of the injection framework.

*** Add support for bitsets                                           :story:

We are using a lot of boolean variables in SML. In reality, these all
could be implemented with =std::bitset=, plus an enumeration. One
possible implementation is:

- add =std::bitset= to std model.
- create a new stereotype of bitset.
- classes with stereotype bitset are like enumerations, e.g. users are
  expected to add a list of names to the class.
- dogen will then implement the properties of type bitset as a
  =std::bitset= of the appropriate size, and also generate an
  enumeration which can be used for indexing the bitset. This may need
  to be a C++-03 enumeration, due to type safety in C++-11
  enumerations.
- we should also implement default bitsets with values corresponding
  to the flags.

Example usage:

#+begin_src c++
const unsigned int my_bitset_size(10);
std::bitset<my_bitset_size> bs;

bs[first_flag_index] = 1;
bs = first_flag_value;
#+end_src

Links:

- [[http://www.java2s.com/Tutorial/Cpp/0360__bitset/Usebitsetwithenumtogether.htm][Use bitset with enum together]]
- [[http://stackoverflow.com/questions/9857239/c11-and-17-5-2-1-3-bitmask-types][C++11 and {17.5.2.1.3} Bitmask Types]]

*** Vistor is only supported at the base class level                  :story:

Due to implementation constraints, we only support visitable at the
base class level. Add an exception if users attempt to use visitable
stereotype in a class that has parents.

*** Create an interface for the text reader                           :story:

In order to do performance testing of the dia model we should create
an interface for text reader and implement it as a mock. This will
avoid the overhead of reading stuff from the hard drive.

*** Add string table support                                          :story:

We need a way of creating "tables" of strings such as for example for
listing all the valid values for dia field names, etc. We could
implement this by creating a new stereotype where the name is the
string name and the default value is the string value. All strings
would be static public members of a class.

We should also add a validate method which checks to see if a string
is a valid value according to the string table. We could have a case
insensitive validate too.

*** Enumeration string conversion could be configurable               :story:

It should be possible to pass in one or more string values as implementation
specific parameters that tells dogen what valid values an enumerator
can have. We can then generate a from string method that does the
appropriate conversions.

These should be passed in as implementation specific parameters.

*** Enumeration string dumps could be configurable                    :story:

It should be possible to pass in a string value as an implementation
specific parameter that tells dogen what string to use for debug
dumping.

*** Add is comparable to SML                                          :story:
     CLOCK: [2013-05-10 Fri 07:48]--[2013-05-10 Fri 08:09] =>  0:21

A object can have a stereotype of comparable. If so, then at least one
property must be marked as comparable. Properties are marked as
comparable if they have an implementation specific parameter called
=comparison_order=. =comparison_order= is a sequence starting at 0 and
incrementing by 1; it determines the order in which properties are
compared between two objects of the same type.

In order for a property to qualify as a comparison candidate its type
must be:

- primitive;
- =std::string=;
- a object marked as comparable.

Some facts about comparable objects:

- they generate =operator<= as a global operator in the type
  header file.
- they can be keys in =std::map= and =std::set=.

Relation to keys:

- If all properties that are part of a key are also comparable then
  the key will be comparable.
- comparable versioned keys always compare the version after all other
  comparable properties.

If an object itself is marked as comparable, then it is equivalent to mark
all properties as comparable using their relative position as the
comparison order.

*** Private and public includes                                       :story:

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with [[*Build%20shared%20objects%20instead%20of%20dynamic%20libraries][using shared objects]], this
should improve build times.

 In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

*** Refactor boost and std helpers and enums                          :story:

We shouldn't really have std and boost enums. These are just a repeat
of the SML models. We should have a find object by name in a model which
returns the appropriate qname given a type name. Then the helpers bind
to those qnames; given a qname, they return the include information,
etc. In the current implementation, the enums are basically a
duplication of the static models.

In reality we should really load up these models from a file, such
that users can add their own bindings without having to change C++
code. This could be done with a config file using boost property
tree. However, one would need some kind of way of mapping types into
primitives, sequence containers etc - some kind of "concepts".

*** Type framing                                                      :story:

In places such as a cache or a socket, it may be useful to create a
basic "frame" around serialised types. The minimum requirements for a
frame would be a model ID, a type ID, a "format" (i.e. xml, text, etc)
and potentially a size, depending on the medium. The remainder of the
frame would be the payload - i.e. the serialised object.

In order for this to work we probably need the concept of a "model
group"; the type frame would be done for a group of models.

*** Model and type enums                                              :story:

It may be useful to create enumerations for models, types and
properties within objects. This would in the future form the basis of
reflection. One could use implementation specific properties to set
the model ID and objects IDs.

*** Add pimpl support                                                 :story:

It may be useful to mark classes as pimpl and generate a private
implementation. On the public header we could forward declare all
types.

*** Add C++-03 mode                                                   :story:

It shouldn't be too hard to generate C++-03 code in addition to
C++-11. We could follow the gcc/odb convention and have a =-std=
option for this. The only problem would be testing - at present the
language settings comes from PFH scripts, not cmake, and we'd have to
make sure the compiler is not in C++-11 mode when compiling 03.

*** Use dogen models to test dogen                                    :story:

We should really use the dogen models in the dogen unit tests. The
rationale is as follows:

- if somebody changes a diagram but forgets to code generate, we want
  the build to break;
- if somebody changes the code generator but forgets to regenerate all
  the dogen models and verify that the code generator still works, we
  want the build to break.

This will cause some inconvenience during development because it will
mean that some tests will fail until a feature is finished (or that
the developer will have to continuously rebase the dogen models), but
the advantages are important.

*** Add a property for the model name as meta-data                    :story:

It would be nice to be able to generate a model with a name other than
the diagram file. We should have a command line option for this that
overrides the default diagram name.

*** Add camel case option                                             :story:

It would be nice to have a command line option that switches names
from underscores into camel case. The default convention would be that
diagrams are always with underscores and then you can convert them at
generation time. There should be a regex for this conversion.

*** Warn if value or entity has methods                               :story:

We should issue a warning if a user defines methods in value or entity
objects as its most likely by mistake.

*** Add diff support                                                  :story:

New understanding: just create a new facet call diff and make these
classes generate a simple textual representation of differences,
inspired in =diff=. Where the object is an entity provide its ID. In
general just provide some "path" to the difference,
e.g. model/object/member variable/etc.

Old understanding:

Dogen should have a =diff= option. When switched on, it would generate
=diff= classes. These are system types like keys and live in a
sub-folder of =types=. They have full serialisation, hashing etc
support like any other model type. The generated classes are:

- =differ=: for each model type a differ gets generated. this is a
  top-level class that diffs two objects of the same type.
- =changeset=: for each model type a changeset gets generated. it has
  a variant called =changeset_types=, made up of all the types of all
  properties in the model. if a model property has a model type then
  it uses the changeset for that type rather than the type itself; for
  all other cases, including containers, it uses the type itself.

In addition, we need set of enumerations in =reflection=. To start off
with all it contains is a list of classes in the model and a list of
fields in each class.

The =changeset= then has a container of =changeset_types= against a
reflection class and field.

Diff support is injected into the model just like keys. It also
requires that reflection support gets injected too.

*** Container details in JSON dump                                    :story:

It would be nice to have the container type and size in the JSON
output.

*** Handling of include cmakelists in split projects is not correct   :story:

At present we are only generating a cmakelists file for include
folders on non-split projects. This means that the header files for
split projects won't be packaged up. It also means that for ODB
projects we won't get the ODB targets.

*** Partial matching of primitives doesn't work for certain types     :story:

We introduced a fix that allows users to create types that partially
match primitive types such as =in= or =integer=. The fix was copied
from the spirit documentation:

[[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/doc/html/spirit_repository/qi_components/directives/distinct.html][- Qi Distinct Parser Directive]]
- [[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/test/qi/distinct.cpp][distinct.cpp]]

However, we still haven't solved the following cases:

: BOOST_CHECK(test_primitive("longer"));
: BOOST_CHECK(test_primitive("unsigneder"));

As these are not so common they have been left for later.

*** Manual typedef generation                                         :story:

- We should be able to create a stereotype of =typedef group=. This is
  a object type with lots of attributes. The code generator will take
  the name and type of each attribute and generate a file with the
  name of the group and all the typedefs inside.
- We should be able to create a forward declarations like header that
  defines typedefs for =shared_ptr= etc at the users choosing. This
  could be implemented as a tag. We could create a =memory_fwd= header
  to avoid cluttering the main =fwd= file for the type. We will need
  another type of relationship to model this, as well as another type
  of file in tags; the file would then have several Boolean flags one
  can tick such as =std_shared_ptr=, =boost_shared_ptr= and so on.
- it should also be possible to add some meta-data to an attribute and
  get it to generate a typedef, e.g. cpp.typedef = "xyz" would result
  in the creation of typedef xyz using the type of the attribute;
  getters, setters and property would then be declared with the
  typedef.

*** Automatic typedef generation                                      :story:

We should generate typedefs for all smart pointers, containers, etc -
basically anything that has template arguments. This would make
generated code much more readable and could also be used by client
code. In theory all we need is:

1. determine if the property has type arguments;
2. if so, construct the typedef name by adding =_type= to the property
   name, e.g. =attribute_value= becomes =attribute_value_type=, etc;
3. create a typedef section at the top of the class declaring all
   typedefs;
4. add a property to the property view model containing the typedef
   name and use it instead of the fully qualified type name.
5. we should also generate a typedef for the key if the class is an
   entity. See Typedef keys for each type.

We could also always generate a typedef for smart pointers in the
class that uses the smart pointer, with a simple convention such as
=attribute_value_ptr_type= or =shared_attribute_value_type=.

*** Add support for iterable enumerations                             :story:

We should create an additional aspect for each enumerations which
creates a =std::array= with the enumerators (excluding invalid). This
would allow plugging the enumerations into for loops, boost ranges,
etc. The CPP should contain a static array; The HPP contains a method
which returns it, e.g. =my_enumeration_to_array.hpp=:

: std::array<my_enumeration, 5> my_enumeration_to_array();

We could make this slightly more generic by adding the notion of
enumeration groups. Out of the box we have:

- all: includes invalid;
- valid: excludes invalid

Users could then add implementation specific properties to create
other groups if needed.

*** Add support for user supplied test data sets                      :story:

*New understanding*:

we need to create a test data sets model. it should have an
enumeration for all of the available test data sets, and an
enumeration for the valid file formats. we should be able to pass in a
pair of file formats (input, actual/expected) and out should come a
triplet of directories. This would make maintenance really easy as
we'd only need to ad new strings to a string table. the service would
also handle things like the actual and expected directories, etc.

It should fix the following issues:

- [[*Adding%20new%20engine%20spec%20tests%20is%20hard][Adding new engine spec tests is hard]]
- [[*Naming%20of%20saved%20SML/Dia%20files%20is%20incorrect][Naming of saved SML/Dia files is incorrect]]

*Old understanding*:

The correct solution for test data and test data sets is as follows:

- the code generated by dogen in the test data directory is one of
  many possible ways of instantiating a model with test data.
- there are two types of instantiations: code and data. code is like
  dogen =test_data=; data is XML, text or binary - or any other
  supported boost archive; it also includes other external formats
  such as dia diagrams.
- a model should have a default enum with all the available test data
  sets: =test_data::sets=. If left to its default state it has only one
  entry (say =dogen=). The use is free to declare an enumeration on a
  diagram with the name test_data_sets and add other values to it.
- there must be a set of folders under test_data which match the
  enumerators of =test_data::sets=. Under each folder there must be an
  entry point such as =ENUMERATOR_generator=. Dogen will automatically
  ignore these folders via regular expressions.
- a factory will be created by dogen which will automatically include
  all such =ENUMERATOR_generator=. It will use static methods on the
  generator to determine what sort of capabilities the generator has
  (file, code, which formats supported, etc.) and throw if the user
  attempts to misuse it.
- all models must have a repository. Perhaps we need a stereotype of
  =repository= to identify it. This is what the factory will create.
- users will instantiate the factory and call =make=:

: my_model::test_data::factory f1;
: auto r = f1.make(my_model::test_data::sets::dogen);
:
: my_model::test_data::factory f2(expected_dir, actual_dir);
: auto r = f2.make(my_model::test_data_sets::some_set,
:   my_model::test_data::file_formats::boost_xml, file_locations::expected);

- if the user requires parsing a non-boost serialisation file then it
  should be make clear on the enum: =std_model, std_model_dia=. The
  second enumerator will read dia files. It will not support any file
  formats. The file must exist on either the expected or actual
  directory as per =file_locations= parameter.

Another topic which is also part of test data is the generation of
data for specific tests. At present we have lots of ad-hoc functions
scattered around different places. They should all live under test
data and be part of a test data set. The test data set should probably
be the spec name.

*** Add test to check if we are writing when file contents haven't changed :story:

We broke the code that detected changes and did not notice because we
don't have any changes around it. A simple test would be to generate
code for a test model, read the timestamp of a file (or even all
files), then regenerate the model and compare the timestamps. If there
are changes, the test would fail.

*** Add support for =std::function= in services                       :story:

At present its not possible to declare an attribute of type
=std::function= anywhere in a diagram. It won't really be possible to
do so for entities and values because boost serialisation will always
be a problem. If this was really a requirement, we could look into
serialising functions:

- [[https://groups.google.com/forum/?fromgroups%3D#!topic/boost-list/sHWRPlpPsf4][how to serialize boost::function]]

However we don't seem to need this quite just yet. What we do need is
a way of having attributes in services and that is slightly easier:

- the parser needs to be able to understand the function template
  syntax (e.g. =void(int)=). It seems this could be hacked easily
  enough into the parser.
- Nested qualified names need to be able to remember that in the case
  of a function, the first argument is a return type (they also need
  to know they represent a function). MC: is this actually necessary?
  all we need is to be able to reconstruct this syntax at format time.
- we need a =void= type in the primitives model. This is a bit more
  complicated since this type can't have values, only pointers, and we
  don't really support raw pointers at the moment. Adding the type
  blindly would open up all sorts of compilation errors.

This should be sufficient for services. At present we have a hack that
allows functions without any template arguments, e.g. =std::function=,
in services.

*** Add support for references and pointers to types                  :story:

At present its not possible to create a type that has a reference to
another type. This should be a case of updating the parser to cope
with references and adding reference to property or nested type
name. This would be a good time to inspect our support for raw
pointers, it probably suffers from exactly the same problem and
requires the same solution.

In addition we should also bear in mind moving. Ideally one should be
able to declare moveable attributes and the end result should be a
setter that takes the type by =&&=. The question then is should we
also move on the getter? Sometimes it may not be a copyable type
(e.g. asio's =socket=).

It seems we can't also cope with =const= or pointers. To be fair we
only need const for shared pointer for now. On all cases we need to
make the parser more clever:

: boost::shared_ptr<const my_type>
: std::string&

We should try to create tests for all the cases we consider important
and mark them as ignore until we can find a spirit expert to help out.

*** Add support for default values                                    :story:

It would be nice to be able to add a default value in dia and have it
set on the default constructor, if the type is a primitive or a =std::string=.

*** Add support for interfaces                                        :story:

This is a very blue-skies story. When dogen starts supporting service
types it would be useful to generate a service interface from
dogen. In order to do this we'd have to parse the method definitions
in dia and use those to construct an abstract base class.

*** Shared pointer to vector fails to build                           :story:

If one has a property with type
=boost::shared_ptr<std::vector<std::string>>=, we get the following
error:

: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp: In function ‘boost::shared_ptr<std::vector<std::basic_string<char> > > {anonymous}::create_boost_shared_ptr_std_vector_std_string_(unsigned int)’:
: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp:47:50: error: ‘create_std_vector_std_string_ptr’ was not declared in this scope

This is because the generated code is not creating a method to new
vectors:

: std::vector<std::string> create_std_vector_std_string(unsigned int position) {
:    std::vector<std::string> r;
:    for (unsigned int i(0); i < 10; ++i) {
:        r.push_back(create_std_string(position + i));
:    }
:    return r;
:}
:
:boost::shared_ptr<std::vector<std::string> >
:create_boost_shared_ptr_std_vector_std_string_(unsigned int position) {
:    boost::shared_ptr<std::vector<std::string> > r(
:        create_std_vector_std_string_ptr(position));
:    return r;
:}

*** Strange logging behaviour in tests                                :story:

As reported by JS for some reason if a test has a null pointer
de-reference, the next test will log to both files. This means the
logger is not being switched off properly in the presence of exceptions.

*** Mix-and-match of manual projects                                  :story:

With the ignore by regex feature its now possible to mix and match
projects. However, dogen generates a makefile which does not include
any manually generated projects. We need some intelligent logic in the
src cmakefile that looks for other cmakefiles and adds them
automatically in its =add_subdirectory=. This could be done by the
CMake backend when that exists.

*** Use error codes in exceptions                                     :story:

Avoid breaking tests every time the exception text changes by creating
a error code property in kitanda exceptions.

After some investigation it was found that boost already supports this
approach in =system=, as per [[http://en.highscore.de/cpp/boost/errorhandling.html][boost book]]. We could define a new
category per model and then create an enumeration of all error codes
in dia, for which the values would be the strings to use for the
error. The user could then create an exception and pass in the error
code in the constructor.

We should also make use of string tables to define all the error
messages.

Could we just have an exception factory that handles all of the
machinery of creating an exception with the right code, message etc?
it could also be responsible for appending more content to an existing
exception so that we'd have the tags all in one place.

*** Generation of cache code                                          :story:

New understanding for this story:

- create a cache interface in types;  all types marked as =cacheable=
  have gets, puts  etc.
- create a memcache implementation.
- create a type to string which converts a key made up of primitives
  into a underscore delimited string, used as a key in the cache.
- we should also consider external libraries like [[https://github.com/cripplet/cachepp][cachepp]].

Old text of story:

Some thought on adding caching to dogen:

- we could have "modes" in dogen; instead of the
  relational/procedural/etc approach, it would be more task based:
  domain, cache.
- in cache mode we do not need to a target. we load up all diagrams in
  references and we find all types which have a stereotype of
  versioned. we mark them as generatable.
- if a target is supplied, it can only have objects of stereotype
  =mapping= or =cache= (tbd). These are simply a key-value-pair and
  determine additional caches to create. the attributes must be called
  key and value. the key entity must be versioned, value doesn't have
  to.
- for each versioned type, we create the following "shadow" objects: get,
  put, erase. each has versioned and unversioned. these objects are in
  the namespace cache::type_namespaces,
  e.g. =cache::credit_risk::model_configuration::versioned_get= or
  maybe
  =cache::credit_risk::model_configuration::versioned::get=. they are
  protocol messages to be sent on the wire.
- new formatter: named cache, with unordered maps for an entity with
  key, value. Any additional mappings that were added manually using
  the target are also added to the kvp mapping.
- new formatter: raw named cache, with unordered maps for an entity with
  key, value. value is raw storage, with an indication of the type of
  data being stored (e.g. xml, binary or text).
- new formatter: repository. contains all of the named caches.
- new formatter: dispatcher. given a message of one of the known types
  (get/put/remove) it dispatches it to the correct location in the
  repository and takes appropriate action. we may need one per named
  cache.
- put/get/erase are regular domain objects so they go through the
  usual formatters

Sample flow:

: credit_risk::model_configuration_unversioned_key k(123);
: cache::near near;
: cache::credit_risk::model_configuration::named_cache nc(
:    near.named_cache<credit_risk::model_configuration>());
: std::future v(n.async_get(k));

- we need to re-read the coherence docs to clarify the roles of
  front/back cache, local/remote cache and near cache.

*** Unordered map of user type in package fails                       :story:

We seem to have a strange bug whereby creating a
=std::unordered_map<E1,E2>= fails sanity checks if E1 is in a
package. This appears to be some misunderstanding in namespacing
rules.

*** Create front-end interfaces                                       :story:

- create a front end component that defines the front end inteface,
  has a factory that returns a front end based on an enum.
- create the dia front end which contains the dia to SML code from
  modeling.
- create the JSON front end.
- engine should rely on the front-end factory.

*** Consider adding "modes of operation"                              :story:

Create "modes" of operation: relational, object-oriented and
procedural. they limit the types available in SML. relational only
allows primitives plus relational commands (FK and PK; FK is when
using a model type, PK is a marker on a property). procedural only
allows primitives plus model types. we will need pointer support for
this. object oriented is the current mode. the modes are validated in
the middle end.

*** Adding new engine spec tests is hard                              :story:

In order to test models at the engine level one needs to first
generate the dia input. This can be done as follows:

: ./dogen_knitter --save-dia-model xml --stop-after-merging
: -t ../../../../dogen/test_data/dia_sml/input/boost_model.dia

From the bin directory. We need to make these steps a bit more
obvious. Why do we even need this?

*** Naming of saved SML/Dia files is incorrect                        :story:

For some random reason when we use dogen to save SML/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.

*** Consider renaming specs to tests                                  :story:

We started using the terminology specs to mean specifications because
our unit tests follow the ideas outlined by Kevlin Henney. However, we
could easily use tests and still carry most of the meaning without
confusing every other developer. This would require:

- rename top-level =spec= folder to =tests=
- rename targets to =_tests=, e.g. =run_sml_tests=
- rename all test suites to =_tests=
- update the automatic detection of boost tests to use the new
  post-fix.
- we should also use =_tests= on the test suite name so we can do
  =using XYZ= without name clashes.

*** Support for components and groups                                 :story:

We recently added support for creating multiple packages from a single
source tree. We need generated models to have a new top-level cmake file:

: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/src)
: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/spec)
:
: install(
:     DIRECTORY include/
:     DESTINATION include
:     COMPONENT headers
:     FILES_MATCHING PATTERN "*.hpp")

And the =src= cmake file:

: install(TARGETS dia ARCHIVE DESTINATION lib COMPONENT libraries)

*** Option to diff generated code                                     :story:

It would be useful to have an option that would do everything except
writing the files to disk; instead, it would diff them with the
existing files and report if there are any differences. This would be
useful to make sure the source code matches the latest version of the
diagram.

We could use something like the [[https://code.google.com/p/dtl-cpp/wiki/Tutorial][DTL library]].

*** Option to validate diagram                                        :story:

It would be nice if one could just check if a dia diagram is valid for
code generation, e.g. =--validate= or something along those lines.

*** Shared pointers to primitive types                                :story:

At present we do not support shared pointers to primitive types. This
is because they require special handling in serialisation. See:

http://boost.2283326.n4.nabble.com/Serialization-of-boost-shared-ptr-lt-int-gt-td2554242.html

We probably need to iterate through all the nested types and find out
if there is a shared pointer to primitive; if there is, put in:

: // defined a "special kind of integer"
: BOOST_STRONG_TYPEDEF(int, tracked_int)
:
: // define serialization for a tracked int
: template<class Archive>
: void serialize(Archive &ar, tracked_int & ti, const unsigned int version){
:     // serialize the underlying int
:     ar & static_cast<int &>(ti);
: }

*** Full constructor parameter comments                               :story:

We could use the comments in properties to populate the comments for
the full constructor for each parameter. This would require taking the
first line of the documentation of each property and then stitching
them together for the full constructor.

*** Serialisation support for C++-11 specific containers              :story:

We can't add =std::array= or =std::forward_list= because there is no
serialisation support in boost 1.49. A mail was sent to the list to
see if this has changed in latter versions:

http://lists.boost.org/boost-users/2012/11/76458.php

However, it should be pretty trivial to generate serialisation code by
hand at least for =std::array= or to use a solution similar to
=std::unordered_map=.

*** Support for ordered containers                                    :story:

In order to provide support for ordered containers such as maps and
sets we need to define =operator<=. However, it makes no sense to code
generate this operator as its unlikely we'll get it right. We could
assume the user wants to always sort by key, but that seems like a bad
assumption. The alternatives are:

- to expect a user-defined =entity_name_less_than.hpp= in domain. we'd
  automatically ignore any files matching this patter so the user can
  create them and not lose it. The problem with this approach is that
  we may have different sort criteria. This is a good YAGNI start.
- to provide the =Compare= parameter in the template and then expect a
  user-defined =entity_name_Compare.hpp=. The same ignore
  applies. This would allow users to provide any number of comparison
  operations.

Either approach requires [[Ignore%20files%20and%20folders%20based%20on%20regex][Ignore files and folders based on regex]].

*** Rename =inserter_implementation=                                  :story:

We used =inserter_implementation= to provide all sorts of utility
methods for IO. This class should really be named IO utility or
something of the sort.

*** Cross model referencing tests                                     :story:

At present we do not have any tests were a object in one model makes use
of types defined in another model. This works fine but we should
really have tests at the dogen level.

*** Cross package referencing tests                                   :story:

Scenarios:

- object in root refers to object in package: A => pkg1::B;
- object in root refers to object in package inside of package: A =>
  pkg1::pkg2::B;
- object inside of package refers to object inside of the same
  package: pkg1::A => pkg1::B (must be qualified);
- object in package refers to root object: pkg1::A => B;
- object in package refers to object in other package: pkg1::A =>
  pkg2::B;
- object in package refers to object in package in package: pkg1::A =>
  pkg1::pkg2::B;
- object in package refers to object in other package in package: pkg1::A =>
  pkg2::pkg3::B;
- object in package in package refers to object in package in package:
  pkg1::pkg2::A => pkg3::pkg4::B.

*** Empty directories should be deleted                               :story:

When housekeeper finishes deleting all extra files, it should check
all of the processed directories to see if they are empty. If they
are, it should delete the directory.

We should probably have a command line option to control this
behaviour.

*** Header only models shall not generate projects                    :story:

A project with just exceptions does not need a make file, and fails to
compile if a makefile is generated. We need a way to not generate a
makefile if there are no implementation files generated.

*** Empty features should not show up                                 :story:

If there are no files for a feature, we should not generate includers
and folders for that feature.

*** Add support for configurable enumerations types                   :story:

At present our enumerations always use unsigned int as the underlying
type. It should be possible to override that from dia.

*** Add test model for disabling XML                                  :story:

At present we are not testing model generation with XML disabled.

*** IO header could depend on domain forward decl                     :story:

At present we are depending on the domain header but it seems we could
depend only on the forward declarations.

*** Format doubles, floats and bools properly                         :story:

At present we are using IO state savers but not actually setting the
formatting on the stream depending on the primitive type.

*** Add support for protected attributes                              :story:

We need to distinguish between public and protected attributes when in
the presence of inheritance. If not, issue a warning.

*** Add tests for disconnected connections                            :story:

We should throw if a diagram has a disconnected inheritance or
composition relationship.

At present the error message for an inheritance object in dia which
has less than two connections is less than helpful:

: 2013-06-26 22:58:50.236488 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2013-06-26 22:58:50.236917 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dia_to_sml::processing_error>
: std::exception::what: Expected 2 connections but found: 1
: [tag_workflow*] = Code generation failure.

We should really try to detail which object ID failed, as well as
details of the connected object if possible, etc.

*** Add tests for duplicate identifiers in Dia                        :story:

Detect if a diagram defines the same class or package multiple
times. Should throw an exception.

*** Test model sanity checks fail for enable facet serialisation      :story:

For some reason we are unable to compile the serialisation test for
the test model which focuses only on the serialisation facet. Test is
ignored for the moment.

*** Handle unnamed models properly                                    :story:

The option disable model name was meant to allow the generation of
flat models, without any folders or namespaces for the model
name. However, as a side-effect, this also means the artefacts being
generated do not have any names. This resulted in the creation of a
libSTATIC, purely because the next command in the cmake add_library is
STATIC (e.g. static library). As a quick hack, when an empty model
name is detected, a model named "unnamed_model" is created.

The correct solution for this is to have a flag (or flags) at the SML
level which state whether to use the model name for folders, packages,
etc. The view model generation will then take this into account.

*** Add SQL support to Dogen                                           :epic:
**** Note on formatters                                                :note:

- Use an attribute with the type to determine if we want only the ID of
  the foreign key in C++ code or if we want a whole type.

Formatters:

- File names are: FQN_ENTITY, e.g. kitanda_prototype_currency_table
- create: table, load, save, erase, test data generators, test
- drop: table, load, save, erase, test data generators, test
- domains
- create schema formatter
- create all tables
- create all procs
- drop all tables
- drop all procs
- drop all
- create all

**** Create SQL backend                                               :story:
***** Create new backend                                               :task:
***** Create new location manager                                      :task:
***** Create aspect and facet types                                    :task:
***** Create a view model for table and stored procedure               :task:
***** Create a transformer from SML to view model                      :task:
***** Add SQL command line options                                     :task:
**** Add table support                                                :story:
***** Create table formatter                                           :task:
***** Drop table formatter                                             :task:
***** Test formatters                                                  :task:

**** Add load support                                                 :story:
***** Create load formatter                                            :task:
***** Drop load formatter                                              :task:
***** Test formatters                                                  :task:

**** Add save support                                                 :story:
***** Create save formatter                                            :task:
***** Drop save formatter                                              :task:
***** Test formatters                                                  :task:

**** Add delete support                                               :story:
***** Create delete formatter                                          :task:
***** Drop delete formatter                                            :task:
***** Test formatters                                                  :task:

**** Add test support                                                 :story:
***** Create test formatter                                            :task:
***** Drop test formatter                                              :task:
***** Test formatters                                                  :task:

**** Analyse deployment support on CMake                              :story:

Ideally, get a state of affairs that resebles something like this:

- make deploy_database
- make undeploy_database

***** Review and fix existing targets                                  :task:

- Rename all =currency= targets to =prototype= targets
- Ensure the targets have correct dependencies

***** Add support for multiple databases                               :task:

**** Add database tests for generated code                            :story:
**** Test database deployment

We need to setup a build that deploys all the SQL (tables, procs, etc)
to a clean database, runs all SQL tests and un-deploys all the SQL.

**** Setup a postgres url in cmake file                               :story:

The database password is set to trust. We should really have user
passwords. To make things more secure we should also pass in the
database credentials to the unit tests. One potential approach is to
do so in cmake. Example from VTK:

#+begin_src cmake
IF ( BUILD_TESTING )
   SET ( VTK_PSQL_TEST_URL "" CACHE STRING "A URL for a PostgreSQL server
         of the form psql://[[username[:password]@]hostname[:port]]/[dbname]" )
ENDIF ( BUILD_TESTING )
#+end_src

Suggestion: maybe there's a possibility of using an environment
variable for all the used parameters (username, hostname, etc)

**** Add multiple database support to makefiles

Our makefiles don't cope very well with the test/development database
separation. There is a massive hack required to populate both
databases (changing makefile manually and then reverting the change).

There should be a way of passing in the database name as an
environment variable into the makefile (not into the cmake as we want
to be able to change databases without having to rebuild makefiles).

*** Missing =enable_facet_XYZ= tests                                  :story:

- test data

*** Log should use path for file names                                :story:

At present we are passing the log file name as a string and
concatenating using "/". This is not very good for Windows. We should
use =boost::filesystem::path= throughout and do a =.string= at the
very end if boost log does not support boost filesystem (or use the
path directly if it does).

*** Create model with invalid primitive type                          :story:

At present we are validating that all primitive types work but we
don't check that an invalid type doesn't work.

*** Private properties should be ignored                              :story:

At present we treat private properties as if they were public; we
should ignore them. We need to go through all the models and change
the private ones to public before we do this.

We should also log a warning.

*** Sanity check packages automatically                               :epic:

This work is also covered by tasks in the PFH backlog so update
accordingly. This task only refers to the dogen specific parts of the
task.

- sanity check that package installed correctly, e.g. check for a few
  key files.
- run sanity tests, e.g. create a dogen model and validate the results
- run uninstaller and sanity check that files are gone
  - this should actually be a build agent so we can see that deployment
    is green. We should create a deployment CMake script that does this.
- build package and drop them on a well known location;
- Create a batch script that polls this location for new packages;
  when one is found run package installer.
- we should create a set of VMs that are specific for testing - the
  test environments. One per OS. These are clean builds with nothing
  on them. To start off with they may contain postgres so we can
  connect locally.

*** Check if we've replaced =assert_object= with =assert_file=        :story:

Assert file is now able to do intelligent comparisons based on the
extension of the file. From a cursory look, all the usages we have of
assert object can be replaced by assert file. If that's the case we
can also remove this function.

*** Exception classes should allow inheritance                        :story:

We need to have a form of inheriting from a base exception for a given
model. We also need to be able to inherit from other exceptions in a
model. At present exceptions are not objects so the dependency graph
support is not there.

*** Investigate GDB visualisers for generated models                  :story:

It would be great if the code generator created GDB visualisers for
the types in a generated models such that one could inspect values of
STL containers with types of that model.

- [[http://sourceware.org/gdb/onlinedocs/gdb/Pretty-Printing.html][Pretty printing]]
- [[https://github.com/ruediger/Boost-Pretty-Printer][Boost pretty printer]]
- [[https://groups.google.com/group/boost-list/browse_thread/thread/ff232ac626bf41cf/18fbf516ceb091da?pli%3D1][Example for multi-index]]

*** Generator usage in template tests needs to be cleaned             :story:

At present some template tests in =utility/test= ask for a
generator, other for instances. We should only have one way of doing
this. We should probably always ask for generators as this means less
boiler plate code in tests. It does mean a fixed dependency on
generators.

*** Replace old style for iterations in IO                            :story:

At present we are still doing C++-03 iterations in the STL IO files
such as =vector_io=, =list_io=, etc. We should be using the new =for=
syntax for C++-11.

*** Add an includer for all includers                                 :story:

It would be nice to totally include a model. For that we need an
includer that includes all other includers. This should be as easy as
keeping track of the different includers for each facet in the map
inside of the includer service.

*** Add new equivalence operator to domain types                      :story:

We should have an operator that compares the state of two objects
ignoring the version.

*** Property types are always fully qualified                         :story:

When we code generate non-primitive properties we always fully qualify
them even if they are on the same namespace as the containing type.

*** Support "cross-facet interference"                                :story:

In a few cases its useful to disable bits of a facet when another
facet is switched off because those bits do not belong to the main
facet the formatter is working on. At present this happens in the
following cases:

- Forward declaration of serialisation in domain when serialisation is
  off
- Friend of serialisation in domain when serialisation is
  off
- declaration and implementation of to_stream when IO is off
- declaration and implementation of inserter when IO is off and
  integrated IO is on.

We need a way of accessing the on/off state of all facets from any
formatter so that they can make cross facet decisions. A quick hack
was to add yet another flag: =disable_io= which is disabled when the
IO facet is not present and passed on to the relevant formatters. This
needs to be replaced by a more general approach.

*** Add run spec targets for each test                                :story:

We could piggy back on the ctest functionality and add a target for
each test so one could =make enable_facet_domain= and =make
run_enable_facet_domain=. The targets need to be prefixed with module
name and test suite.

*** Clean up WinSock definition in CMakeLists                         :story:

We did a crude implementation of finding WinSock just to get windows
to build. There should be a FindWinSock somewhere. If not create one.

Do we need this anymore? we probably need it for linking the database
model, but we should check - maybe ODB has some magic around this.

*** Refactor engine's =persister=                                     :story:

- add documentation
- we put the decision on whether to persist on not based on settings
  inside of persister. It should really be up to the person calling
  the persister to decide. Persister should always persist.
- we should have an argument deciding the file format, perhaps an
  enumeration, instead of passing in the extension. The extension
  should be automatically determined.
- persister should support all archive types. At present it always
  outputs in XML; it should respect the archive type requested by the
  user.

*** Add unit test benchmarking                                        :story:

New understanding:

Create a set of performance specific tests. These wont get executed by
regular users (e.g. they are not part of =run_all_specs=) but they do
get executed in the build machine. These are selected tests with big
loops (say 1M times) doing things like reading dia diagrams etc. We
could chose a few key things just to give us some metrics around
performance.

In fact, we could create a set of colossi models: models with really
large number of classes (say 500), maybe 5 of these with
references. We could then use the diagrams to test the individual
workflows: dia, dia_to_sml, cpp and engine with no writing. We should
avoid writing files to filesystem to avoid number jitter caused by the
hard drive. There should be no comparisons between actual and expected
for the same reason.

We need to make sure the benchmark tests won't run on valgrind or else
the nightly builds will take over 24 hours. However, if we had it
running on continuous we'd spot regressions on every check-in. But we
don't want to delay continuous any more than necessary. Perhaps we
need a separate build called performance which is also continuous and
only runs these tests. We could pass in some kind of variable to CMake
so that if performance is on, it ignores all tests other than
performance and vice-versa. We'd also need a performance target that
only builds the performance binary, and a =run_performance= target
that executes it.

Perhaps we could use a ruby script to generate the test models?

Old understanding:

[[https://svn.boost.org/trac/boost/ticket/7082][Raised ticket]]

- nightly builds should run all unit tests in "benchmarking mode";
- for each test we should find the sweet spot for N repetitions;
- when plugged into ctest, make sure the benchmark tests have
  different names from the main tests otherwise the timing history
  will be nonsense.
- [[http://lists.boost.org/boost-users/2011/01/65790.php][sent]] email to boost users mailing list asking for benchmarking
  support.

** Won't fix

Stories which we do not think we are going to work on.

*** Tests for error conditions in libxml

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.

*** Check that custom targets in CMake have correct dependencies

At present we have a number of custom targets, which create a new Make
target. These are good because they do not require re-running CMake to
manage the files in the output directory; however, we do not have the
correct dependencies between the targets and the target
dependencies. For example, create_scripts should check to see if any
script has changed before re-generating the tarball; it seems to have
no dependencies so it will always regenerate the tarball. We need to:

- check all custom targets and see what their current behaviour is:
  a) change a dependency and rebuild the target and see if the
  change is picked up or not; b) change no dependencies and re-run the
  target and ensure that nothing happens.
- add dependencies as required.

*** Enable doxygen warnings for all undocumented code

At present doxygen only warns about undocumented parameters when a
function already has documented parameters. We should consider
enabling warnings for all undocumented code. We also need to figure
out how to mark code as ignored (for example serialisation helpers,
etc won't require documentation).

*** Add specification comments to tests

We started off by adding a technical specification as a doxygen
comment for a test but forgot to keep on doing it. Example:

: /**
:  * @brief It shall not be possible to create more terms than those
:  * supported by a finite sequence, using std::generate_n.
:  */

This helps make the purpose of the test clearer when the name is not
sufficient.
*** Supporting user defined generic types

At present we have a bit of a hack to support templates. However, all
that is required to allow users to create their own template types is:

- parse dia information for type arguments
- change object to have type arguments
- change merger to allow variables of the type of the type argument
- change view model to propagate type arguments
- change formatter to create template class if type arguments are
  present

However this would then mean that IO and serialisation would fail
since they are implemented on the cpp. As there is no need for
template types, this seems like an ok limitation.

*** Shared pointers as keys in associative containers

This is not supported; it would require generating the
hashing/comparison infrastructure for shared pointers. Further, as it
has been pointed out, keys should be immutable; having pointers as
keys opens the doors to all sorts of problems. We need to throw an
error at model building time if an user tries to do this.

*** Package names should follow a well-known convention

We need to make sure our package names are consistent with the
platform conventions.

- [[http://pastebin.com/TR17TUy9][Example of platform IFs]]
- [[http://libdivsufsort.googlecode.com/svn-history/r6/trunk/CMakeModules/ProjectCPack.cmake][Example CPack]]
- [[http://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.htmlhttp://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.html][Automatically add a revision number to the CPack installer name]]
- [[http://www.cmake.org/Wiki/CMake:CPackConfiguration][CPack Configuration]]

There are some known limitations in package naming:

- http://public.kitware.com/Bug/view.php?id=12997
