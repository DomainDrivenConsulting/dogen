#+title: Product Backlog
#+options: date:nil toc:nil author:nil num:nil
#+tags: { story(s) epic(e) }

* Stories

This document contains the [[http://www.mountaingoatsoftware.com/agile/scrum/product-backlog][product backlog]] for Dogen.

** In next few sprints

Stories that we intend to look at in the very near future.

*** Consider splitting =formattables::transformer=                    :story:

We have two different responsibilities within transformer:

- to perform an individual (1-1) transformation of an SML type into a
  formatable;
- to determine how many transformations of an SML type are required,
  and to do them.

Maybe we should have a top-level transformer that collaborates with
specific transformers, aligned to =cpp= types
(e.g. =class_info_transformer=, =enum_info_transformer= and so on,
each taking different (or the same) SML types). The role of the
top-level transformer is to call all of the sub-transformers for a
given SML type.

This work should be done when we finish all the work in the
transformer.

*** Handling of managed directories is incorrect                      :story:

At present we are querying the dia to sml transformer to figure out
what the managed directories are. These are basically the top-level
directories from where we want the housekeeper to operate. In reality
this is (or can be placed) in the meta-data. We should be able to
extract the managed directories from the meta-data as a step in one of
the workflows.

This can be done by the backend. It does mean that we should be
returning a composite type from generation:

- list of files;
- list of managed directories.

Alternatively we could have a =managed_directories= method that takes
in an SML model and then internally reads in the meta-data for a given
model to produce the list.

*Merged with previous story*

Compute managed directories from knitting options

At present the backend is returning empty managed directories. This
means housekeeping will fail in the new world. We need to change the
interface of this method to take in the knitting options and return
the managed directories.

This is not entirely trivial. At present the managed directories are
computed in the locator. It takes into account split project, etc to
come up with all the directories used by the backend. We need to make
these decisions during path expansion, expect we only need manged
directories for the root object. However we do not know which object
is the root object at present, during the expansion. We could identify
it via the QName and the SML model in context thought. We could then
populate the managed directories as a text collection. We then need
some settings and a factory to pull out the managed directories from
the root object. This could be done in =managed_directories=, by
having an SML model as input.

*** Consider removing the overwrite flag in =formatters::file=        :story:

Investigate if the overwrite flag makes sense in file; it seems we
only use it in two scenarios: force overwrite requested by user or
file contents have changed, both of which can be done in the
file_writer.

Actually this flag is needed. It is required to handle the case where
we do not code-generate files, unless they do not exist. For example,
for service headers and implementation we should create the files, but
then subsequently not touch them. The overwrite flag should be set to
false. We need to figure out how to implement this and remove the
hacks around file writing.

*** Split knitting from stitching settings                            :story:

At present we only have a single common directory with all of the
available fields. Not all fields apply to both stitching and
knitting - but some do. We need a way to filter these. One possibility
is to use an approach similar to the formatter groups in the ownership
hierarchy. For now we simply have fields that have no meaning in
stitching but can be supplied by users.

*** Update dynamic section in manual                                  :story:

We need to talk about the new fields, field templates, etc.

*** Add stitch section in manual                                      :story:

We need to document stitch:

- formal definition of the language and its limitations;
- command line usage of the tool.

*** Indent stitch output using clang format                           :story:

We need to indent the output coming out of stitch as it is not
suitable for reading as-is.

*** Solve the problem of "relocatable" formatters                     :story:

There may be cases where a formatter is the same for all facets, but
we still need to have it instantiated for every facet. At present that
is not possible because formatters have hard-coded ownership
hierarchies. In addition, there is the potential for inclusion
dependencies generation which is also facet specific.

This could be solved by instantiating the formatter in the initializer
of every facet, and supplying the facet name to the constructor; the
ownership hierarchy would then use this facet name. This would mean
that the same formatter would be registered for every facet.

We don't yet have a use case for this. It was thought to be needed for
forward declarations but at present we only need them for types. It is
probably needed in order to integrate knit and stitch.

*** Integration of stitch and dogen                                   :story:

Now that we have implemented stitch and proved it works (more or
less), we need to think how we can make using stitch from dogen
easier. At present there is not integration at all:

- users need to create regexes to ensure dogen does not trample on
  stitch files:

:    --ignore-files-matching-regex .*stitch
:    --ignore-files-matching-regex .*_stitch.hpp
:    --ignore-files-matching-regex .*_stitch.cpp

- users need to manually create a header file for each stitch
  template.
- users need to create stitch targets and run them to ensure the
  templates have been expanded. This means its possible to get dogen
  and stitch out of sync (but for now not a big problem).

In the ideal world, when we knit a model it would be nice if it could
also stitch as required. This could be achieved as follows:

- Create a meta-data tag that tells dogen a type has an associated
  stitch template with it.
- Create =cpp= types that represent the stitch header and
  implementation.
- Transformer needs to look for the meta-data tag and instantiate the
  =cpp= types.
- Create a =cpp= formatter for the header, as per regular
  formatters. The slight challenge here is that the formatter needs to
  be instantiable across facets, which we do not support at the
  moment.
- Create a cpp formatter for the implementation which instantiates
  stitch with the template and uses it to create a file. Same
  challenge as with the header.

*Previous Understanding*

- stitch can still be integrated with dogen. We could use meta-data to
  link a formatter (well, any class that needs stitch really, but at
  present just a formatter) with a stitch template. For example, a
  =class_header_formatter= could have a "is stitchable" flag set to
  on. This would then mean that dogen would look for a
  =class_header_formatter.stitch= file in the same directory as the
  CPP file. It would then use that to create a
  =class_header_formatter_stitch.cpp= file. It would also
  ignore/generate a =class_header_formatter_stitch.hpp= file and
  automatically add it to the inclusion dependencies of
  =class_header_formatter.cpp=. These are injected into stitch as we
  instantiate the template since stitch supports meta-data (we do need
  a way to inject the meta-data from dogen into the meta-data in the
  template; perhaps a kvp container passed in to the stitch workflow
  which could then be handed over to the parser). All these files are
  automatically added to the list of "exceptions" for housekeeping so
  that they do not get deleted. However, stitch would not know
  anything at all about any of this; this is all knitter's
  functionality. The problem is at present we haven't got a good place
  to perform the stitching as part of knitter's workflows. Perhaps as
  part of the expansion, we could set a number of stitch fields which
  would then be picked up by some knit-specific workflow classes.

*** Add kvp support to =identifier_parser=                            :story:

We have code to split kvps all over the place. We should do this in a
single pace, and use boost spirit or tokenizer. For one such
implementation with spirit see:

[[http://boost-spirit.com/home/2010/02/24/parsing-skippers-and-skipping-parsers/][Parsing Skippers and Skipping Parsers]]

*** Move =identifier_parser= to SML                                   :story:

We will require parsing identifiers from JSON so we need access to
identifier parser within SML.

*** Consider renaming includers                                       :story:

Its very confusing to have header files that include lots of other
header files called "includers". There is too much overloading. We
should consider calling them "master header files" as per Schaling
terminology in the [[http://theboostcpplibraries.com/boost.spirit][boost book]].

*** Adding a dependency to a non-existent expander crashes dogen      :story:

We are not checking that all dependencies exist when building the
graph. If we add a dependency to a expander that does not exist we
crash and burn:

: /home/marco/Development/DomainDrivenConsulting/dogen/projects/knit/spec/workflow_spec.cpp(550): last checkpoint
: dogen_knit_spec: /usr/include/boost/smart_ptr/shared_ptr.hpp:653: typename boost::detail::sp_member_access<T>::type boost::shared_ptr<dogen::dynamic::expansion::expander_interface>::operator->() const [Y = dogen::dynamic::expansion::expander_interface]: Assertion `px != 0' failed.
: unknown location(0): fatal error in "all_primitives_model_generates_expected_code": signal: SIGABRT (application abort requested)

*** Expand fields from command line options into dynamic              :story:

We need to ensure the following fields are populated, from the command
line options:

- integrated facets
- enabled

*** Add =static_formatter_name=                                       :story:

At present formatter names are defined as traits, we should use the
"static" approach. Facet names can stay as traits as they are common
across formatters.

*** Check for duplicate formatter names in formatter registrar        :story:

At present it is possible to register a formatter name more than
once. Registrar should keep track of the names and throw if the name
is duplicated.

*** Add validation for field definitions                              :story:

Perform some validation in repository workflow:

- that formatter fields are not duplicated on simple name.
- fields are not duplicated on qualified name.
- instances have qualified name populated.
- only instances are left after instantiation.

*** Perform expansion of properties and operations                    :story:

At present we are ignoring properties (and operations). This is ok as
we don't really have a use case for expansion there. However, it would
be nice if we could just expand them anyway. We just need to make sure
we don't do things like copying from root object.

*** Create an operation to populate c++ properties                    :story:

There are a number of properties such as "requires default
constructor" and so on that are specific to the c++ model. Some
require looking at related types (do they have the property enabled?)
some others require looking at the SML model graph. It seems they
should all live under one single operation (or perhaps a few), but we
do not have any good names for them.

*** Create settings expander and switcher                             :story:

*New Understanding*

The expansion process now takes on this work. We need to refactor this
story into an expander.

*Previous Understanding*

We need a class responsible for copying over all settings that exist
both locally and globally. The idea is that, for those settings, the
selector should be able to just query by formatter name locally and
get the right values. This could be the expander.

We also need a more intelligent class that determines what formatters
are enabled and disabled. This is due to:

- lack of support for a given formatter/facet by a type in the graph;
  it must be propagated to all dependent types. We must be careful
  with recursion (for example in the composite pattern).
- a facet has been switched off. This must be propagated to all
  formatters in that facet.
- user has switched off a formatter. As with lack of support, this
  must be propagated through the graph.

This could be done by the switcher. We should first expand the
settings then switch them.

In some ways we can think of the switcher as a dependency
manager. This may inform the naming of this class.

One thing to take into account is the different kinds of behaviours
regarding enabling facets and formatters:

- for serialisation we want it to be on and if its on, all types
  should be serialisable.
- for hashing we want it to be off (most likely) and if the user makes
  use of a hashing container we want the type that is the key of the
  container to have hashing on; no other types should have it on. We
  also may want the user to manually switch hashing on for a type.
- for forward declarations: if another formatter requires it for a
  type, we want it on; if no one requires it we want it off. The user
  may want to manually switch it on for a type.

*** Incorrect application of formatter templates                      :story:

At present we are applying formatter templates across all formatters
in C++ mode; this only makes sense because we do not have CMake and
ODB formatters. However, when these are added we will need to filter
the formatters further. For example, C++ formatters (both headers and
implementation) need inclusion dependencies but CMake files don't.

*** Consider supporting multiple formatter groups                     :story:

In some cases it would be nice for a field to belong to multiple
groups. For example =integrated_facet= is only applicable to class
header formatters. We could implement this by making the formatter
group a collection and having formatters belong to multiple groups.

*** Some test models do not build on run all specs                    :story:

For some reason we are not building some of the test models when doing
a run all specs, in particular:

- exception
- comments

this may be because we have no specs for them. We need to find a way
to build them somehow.

*** Improve error reporting around JSON                               :story:

At present when we break the JSON we get errors like so:

: Error: Failed to parse JSON file<unspecified file>(75): expected object name.

These are not very useful in diagnosing the problem. In the log file
we do a bit better:

: 2015-03-30 12:02:12.897202 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/fields/cpp.json
: 2015-03-30 12:02:12.897216 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON stream.
: 2015-03-30 12:02:12.897450 [ERROR] [dynamic.schema.json_hydrator] Failed to parse JSON file: <unspecified file>(75): expected object name
: 2015-03-30 12:02:12.897515 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dynamic/schema/src/types/json_hydrator.cpp(226): Throw in function std::list<field_definition> dogen::dynamic::schema::json_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen7dynamic6schema15hydration_errorEEE
: std::exception::what: Failed to parse JSON file<unspecified file>(75): expected object name
: [P12tag_workflow] = Code generation failure.

But it requires a lot of context to know whats going on. We need to
append more details to the exception.

*** Investigate boost log config files                                :story:

Our log files are growing quite a bit. We don't really want to log any
less since the logging is very useful for troubleshooting. However, at
any one time we just need to look at one or a couple of
components. What we really need is something like log4j, where we can
change log levels for a component or all components in a hierarchy. We
need to investigate boost log solutions for this.

It seems we cannot change severity per component ("channel") with our
current setup. We need something akin to this:

- [[http://www.boost.org/doc/libs/1_57_0/libs/log/doc/html/log/detailed/expressions.html#log.detailed.expressions.predicates.channel_severity_filter][Severity threshold per channel filter]]

This could be implemented as follows:

Create a log config file (say =logging.ini=) that contains a list of
strings and valid severities:

: root = fine_debug
: cpp = debug
: cpp.settings = info

and so on. When the log is being initialised, a sorted list with these
is loaded into memory. It is sorted by channel name. Note that =root=
is a special value and is always at the bottom of the list (or even
removed from the list altogether and handled specially). If root was
not defined in the config file, we set it to a default. Note also that
we convert the severity strings into enums, with adequate validation.

Once the list is setup, we then loop through all the channels that
have been defined. There is an assumption that all channels were
defined statically and thus have already been defined by the time we
initialise the log. This needs to be verified.

For each channel, we loop through all values from the file - other
than root - applying them as a regex against the channel name. Note
that we sorted them so the closest match should be last. For each
value that matches, we set the severity accordingly. If no matches are
found, we apply the root setting.

Some other tidbits:

- we can now remove the =verbose= option, or perhaps it should be used
  as a short-hand for the log configuration? if so we need a rule that
  determines which one to use when both are present.
- we could monitor the config file for changes, although for dogen
  this is overkill.
- if sorting proves too hard we could just say that the regexes are
  applied in the order provided by the user, with the exception of
  root.

*** Move some of the more verbose logging to fine debug               :story:

We have a category for fine debug logging (=FINE_DEBUG=) but we are
not making use of it. There is some rather verbose logging that could
be moved to it. Go through all the logging and move some to
=FINE_DEBUG=.

One strategy would be to put in the final object of each workflow as
=DEBUG= (say the expanded model, etc) but the intermediate steps as
=FINE_DEBUG=. This mirrors the way we investigate the problem: we
could check if each sub-system has done it's job correctly, and spot
the one that didn't; we can then just enable that one sub-system's
=FINE_DEBUG= (when that is supported).

*** Implement include generation for class header formatter           :story:

Now that we have finished generating the path spec details, we need to
make sure includes generation works as expected. Add both formatter
level includes as well as model level includes.

We also need to deal with:

- exposing formatter id as a static property so we can create
  dependencies between formatters;
- includes overrides via dynamic extensions, so we can start using
  STL, Boost etc classes.
- includes of STL, Boost etc that are formatter level dependencies -
  this needs to be handled via traits.

Notes:

- rename header file to file name override or something else quite
  distinct. We need to ensure it is obvious that this property is only
  used for non-dogen models. Actually we can now just call it include
  path.
- however we still have a problem: when we compute the include path we
  do not know if it is a system or user path. We need to change file
  properties to take this into account. We need a include path class
  with a include type: user or system. we could then use this.
- add field definition =no_inclusion_required= and populate all
  relevant primitives.

We need to add the notion of aspects. This is because a lot of things
we have dealt with individually are really all related to aspects. An
aspect is a part of a formatter that can be enabled or disabled. For
example:

- manual move constructor
- manual default constructor
- inserter operator
- swap function
- etc.

We could simply create aspect settings; local and global formatters
would have a map of =aspect_settings= against an =aspect_name=. We
need to remove the integrated facet approach and implement it in terms
of this. We should go through the types formatter and figure out which
aspects it has. For now we shall ignore relationships between aspects
and facets (and any other kind of switching logic). For example, it is
the responsibility of the user to ensure that if integrated io is
used, the io facet is switched off. This is because it is very
complicated to build in the generic logic of such dependencies. We
have a story to handle this properly in the backlog, but it will
require a lot of work.

*** Create a base formatter                                           :story:

We could implement most of the formatter interface in a common base
class and then only have the descendants override what they need.

In reality we need probably more than one base formatter: one for c++
types, etc. We could handle all of the path and include generation
here. Name: =cpp_base_formatter=?

*** Read =generate_preamble= from dynamic object                      :story:

We need to generate the field definitions and update the general
settings factory.

*** Implement class header formatter                                  :story:

- look at the old =om= types formatter implementation to see if there
  is any code to scavenge. This model was deleted around commit
  10157ad.

**** Tidy-up =types_main_header_file_formatter=                        :task:

Clean up internal functions in file and add documentation.

**** Copy across documentation from =om=                               :task:

We did a lot of doxygen comments that are readily applicable, copy
them across.

**** Make use of indenting stream                                      :task:

Remove uses of old indenter.

**** Copy across =om= types formatter tests                            :task:

Not sure how applicable this would be, but we may be able to scavenge
some tests.

*** Improve formatters code generation marker                         :story:

Things the marker can/should have:

- model level version;
- the dogen version too. However, this will make all our tests break
  every time there is a new commit so perhaps we need to have this
  switched off by default.

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders.

*** C++ workflow should perform a consistency check                   :story:

We should ensure that all facets and formatters available in the
registrar have corresponding field definitions and vice-versa. This
was originally to be done by some kind of "feature graph" class, but
since we need to use this data for other purposes, the main workflow
could take on this responsibility - or we could create some kind of
"validator" class to which the workflow delegates.

*** Add "model types"                                                 :story:

At present we have a number of dynamic extensions that exist purely to
deal with non-dogen models:

- supported: is the facet supported by the external model
- file_name: what is the external model naming for files for this
  facet
- is_system: is the file name a system include file or not?

In reality, all of this could be avoided if we had a way of
distinguishing between models that follow dogen conventions and those
who do not; a "model type" of sorts such as "external" and "dogen" -
naming needs more thought. With this we could infer the rest: if no
file name is supplied then a given formatter/facet is not supported;
if the model is_system then all types are system and so on.

We should also have a flag in field definitions that verify that a
parameter is only present if the model is a non-dogen model. For
example, it makes no sense to supply =cpp.type.family= in a dogen
model but it may make sense to do so in an external model. However,
this would mean that if a user manually adds a type to a dogen model
it cannot be extended. Requires a bit of thinking.

*** Consider renaming general settings                                :story:

A while ago we came up with this name for the settings of the generic
formatter model. This is the model with basic infrastructure to be
reused by the more specialised formatters. However, now that we have
many (many) settings classes, general settings may not be the most
appropriate name. We need to look a bit more deeply into the role of
this class and see if a better name is not available.

We could call it preamble settings because all settings are related to
the file preamble; annoyingly, we also generate a post-amble from
it. There doesn't seem to be any good names for the pair (preamble,
post-amble). In networking this would be called frame markers perhaps.

Now that we are not using =meta_data= any more, perhaps we could
re-purpose it for this (=meta_data_settings=). In a way, preambles and
post-ambles are meta-data, as opposed to the real file
contents. Having said that, one could say the same about any kind of
comments.

We could also use [[http://www1.appstate.edu/~yaleread/typographichierarchy.pdf][typography terminology]]: headers and footers.

Now that we have subsidiary settings and principal settings do we need
the rename? We should consider "universal settings" maybe.

*** Populate the "new" =class_info= properties                        :story:

We need a way of populating the class aspects via the type settings
and via information obtained in the SML model. We may want to create a
class to handle this logic or maybe it can be done in transformer.

*** Includer generation should be done from dynamic extensions        :story:

*New Understanding*

The true use case of this story is not to allow users to add includes
at random; it is actually only useful in one scenario:

- *merging code generation*: users add code which requires additional
  includes. Without support for this, merging code generation would
  have limited usefulness.

*Previous Understanding*

It would be nice if we could determine which includer files to create
by looking at the dynamic extensions. For this we need a notion of an
inclusion group, defined at the model level:

- =cpp.types.includers.general=
- =cpp.types.includers.value_objects=
- ...

Under each of these one would configure the aspect:

- =cpp.types.includers.general.generate=: =true=
- =cpp.types.includers.general.file_name=: =a/b/c=
- =cpp.types.includers.general.is_system=: =false=

Then, each type, module etc would declare its membership (as a list):

- =cpp.includers.member=: =cpp.types.includers.general=
- =cpp.includers.member=: =cpp.types.includers.value_objects=
- ...

*Previous understanding*

We should simply go through all the types in the SML model and for
each type and each facet create the corresponding inclusion
path. locator can be used to generate standard paths, and a model
specific mapping is required for other models such as std.

Include then takes the relationships extracted by extractor, the
mappings generated by this mapper and simply appends to the inclusion
list the file names. it also appends the implementation specific
headers.

*** Consider creating constants for common fields                     :story:

Fields such as =enabled=, =postfix= etc are common to all formatters
and facets. It may make more sense to define some string constants for
them, perhaps in =traits=?

*** Protect against double-initialisation                             :story:

We need to look into static initialisation and make sure the code can
cope with it being called several times.

At present it seems we would re-register fields, backends, etc so
multiple initialisation would fail.

In addition to this, we should also look into passing the registrars
into the initialisers. At present we are calling the static methods
directly. This is not ideal, because just like with singletons, we are
hiding the dependencies. We should really pass the registrars in the
initialise function so we can see the dependencies at the top-level.

*** Add field definition description                                  :story:

It would be useful to have a description of the purpose of the field
so that we could print it to the command line.

*** Rename ODB parameters                                             :story:

At present we use the following form:

: #DOGEN ODB_PRAGMA=no_id

We need to use the new naming style =cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

*** Rename =dia.comment=                                              :story:

This field hasn't got a sensible name. We need to continue thinking
about the right name for this - as we have already started doing with
processed comment; =applicable_to_parent_object= is not quite the
right name but it is moving on the right direction. Once we settle on
a good name we need to rename all usages of =dia.comment=.

*** Improve support for modules in JSON                               :story:

At present we are implying the existence of modules in JSON by looking
at the types qname. This is not ideal because it means one cannot
supply meta-data for modules. We should probably revisit the layout to
have a nested structure with namespaces containing types.

See also [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#sml-json-hydrator-read_module_path-needs-refactoring][SML json hydrator =read_module_path= needs refactoring]].

*** Handling of modelines is incorrect in general settings            :story:

At present the general settings are hard-coded to look for a C++
modeline. This will not work for CMake files, etc. We need to think
how multiple modelines will be supported. See general settings factory
in formatters.

*** Consider renaming model module to root module                     :story:

It would be more sensible to call it root module rather than model
module. We should also create a root module property in the model to
make it easier to locate.

*** Capture settings validation rules                                 :story:

Once all settings have been built (global and local) we must pass them
to a validator class that makes sure they all make sense. This story
captures all the rules we need to check for. We must also check the
SML validator story in backlog for rules that apply to settings.

- integrated IO must not be enabled if IO is enabled and vice-versa
  (opaque settings validator). actually it seems this is possible, we
  need to investigate the current implementation.
- types must be enabled
- if serialisation is enabled, types forward declaration of the
  serialisation classes must be enabled (opaque settings validator)

Actually what we need is a rules engine that infers all of this. It
does validation at the same time.

*** Tidy-up sml consumer interface                                    :story:

After implementing the includes for the class header formatter we
should figure out if we need the multiple passes machinery. Most
likely all of that should be scraped.

*** Add include files at the formatter level                          :story:

We need to remove all the include files from =includer= which are
related to formatter specific code. We need to inject these
dependencies inside of the formatters.

- implement includer in terms of json files
- get includer to work off of object relationships
- remove relationships from transformer
- remove helper models boost and std

*** Add frontends and backends to =info= command line option          :story:

#+begin_quote
*Story*: As a dogen user, I want to know what frontends and backends
are available in my dogen version so that I don't try to use features
that are not present.
#+end_quote

With the static registration of frontends and backends, we should add
some kind of mechanism to display whats on offer in the command line,
via the =--info= option. This is slightly tricky because the
=frontend= and =backend= models do not know of the command line. We
need a method in the frontends that returns a description and a method
in the workflow that returns all descriptions. These must be
static. The knitter can then call these methods and build the info
text.

*** Add support for disabling formatters                              :story:

We should be able to disable some formatters such as forward
declarations. Some users may not require them. We can do this using
dynamic extensions. We can either implement it in the backend or make
all the formatters return an =std::optional<dogen::formatters::file>=
and internally look for a =enabled= trait.

We need to be able to distinguish "optional" formatters - those that
can be disabled - and "mandatory" formatters - those that cannot. If a
user requests the disabling of a mandatory formatter, we must throw.

This story was merged with a previous one: Parameter to disable cpp
file.

#+begin_quote
*Story*: As a dogen user, I want to disable cpp files so that I don't
generate files with dummy content when I'm not using them.
#+end_quote

It would be really useful to define a implementation specific
parameter which disables the generation of a cpp file for a
service. This would stop us from having to create noddy translation
units with dummy functions just to avoid having to define exclusion
regexes.

*** Remove =cpp_formatters::formatting_error=                         :story:

Use the =formatters::formating_error= instead.

*** Delete content types                                              :story:

Now we have the type system representing the content, we can delete
this enumeration.

*** Delete aspect types                                               :story:

Now we have the type system representing the aspects, we can delete
this enumeration.

*** Delete key implementation formatter                               :story:

It doesn't seem like there is any good reason to treat the keys in a
special way so try to remove this.

** In current major release

The release will not be made unless these stories are closed, but we
won't be addressing them in the near term. This release is all about
getting the architecture right.

*** Consider moving dia diagrams to their own project                 :story:

We originally create a diagrams top-level directory for all of the
diagrams that generate dogen models. However it may make more sense to
have the diagrams closer to the project they generate; for example a
folder within the project. Also, the name "diagrams" is a bit
misleading. These are not just diagrams, they are also the dogen
input. We need a name that is a bit more meaningful.

*** Field definition templates do not support facet specific defaults :story:

At present we cannot use field definition templates for fields that
require facet specific default values such as =directory=. We could
either support something like a "variable", e.g. "find facet simple
name" or we could do overrides - the field definition is defined as a
template but then overriden at a facet level. Or we could handle
default values in a totally separate way - maybe a file with just the
default values.

In addition, we have the case where at the facet level we may have a
default value for a field but not at the formatter level - =postfix=.

*** Consider renaming =ownership_hierarchy=                           :story:

We came up with the name =ownership_hierarchy= because we could not
think of anything else. However, it is not a particularly good name,
and it is increasingly so now that we need to use it across models. We
need a better name for this value type.

*** Consider creating internal and external fields                    :story:

At present any dynamic field is automatically exposed to the outside
world, allowing users to set them. This is not always ideal; for
example, the file path should not be settable. Perhaps field
definitions should have a "internal" or "external" property that stops
users from being able to override certain fields.

*** Consider using the same API as boost property tree in selector    :story:

At present we have the type of the value in the method names in the
selector, e.g. =get_text_content=. It would be better to have a =get=
that takes in a template parameter, e.g. =get<text>=. However, in
order to do this we need to have some kind of mapping between the
schema value (=text=) and the raw value (=std::string=). This requires
some template magic.

Once this is done we can also make the API a bit more like the
property tree API such as for example returning =boost::optional= for
the cases where the field may not exist.

*** Add dynamic consistency validation                                :story:

We need to check that the default values supplied for a field are
consistent with the field's type. This could be done with a
=validate()= method in workflow.

Actually since we can only create fields from JSON, we should just add
a check there.

*** Rename name builder to name factory                               :story:

The name builder is just a factory so make the name reflect it.

*** Create a list of valid values for field definitions               :story:

In addition to default values, it should be possible to supply a list
of possible values for a field definition - a domain. When processing
the values we can then check that it is part of the domain and if not
throw. This is required for the include types and for the family
types. At present this is only applicable to string fields.

In this sense, =boolean= is just a special case where the list is know
up front. We should re-implement =boolean= this way. Possibly even add
synonyms (e.g. =true=, =false=, =0=, =1=)?

*** External module path and references as meta-data                  :story:

It actually does not make a lot of sense to allow users to supply
external module paths and references as command line options. This is
because the model will fail to build unless we provide the correct
ones; these are not configurable items in this sense. The project
path, etc are - and so should remain command line options.

We need to move these two into the meta-data. This would also mean we
no longer need to pass in external module paths for references, which
is much cleaner.

*** Hydrators provide no context when errors occur                    :story:

We tried to parse a JSON file using the INI parser and got the
following errors:

: 2015-03-27 15:16:05.291132 [DEBUG] [formatters.modeline_group_hydrator] Reading file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/modeline_groups/emacs.json
: 2015-03-27 15:16:05.291215 [ERROR] [formatters.modeline_group_hydrator] Failed to parse INI file: : <unspecified file>(1): '=' character not found in line
: 2015-03-27 15:16:05.291933 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/formatters/src/types/modeline_group_hydrator.cpp(172): Throw in function dogen::formatters::modeline_group dogen::formatters::modeline_group_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10formatters15hydration_errorEEE
: std::exception::what: Failed to parse INI file: <unspecified file>(1): '=' character not found in line
: [P12tag_workflow] = Code generation failure.

The exception provides no context to the file being parsed. We need to
catch the exception and augment it with the file name.

*** Add a validate JSON target                                        :story:

It would be nice to be able to validate all of the JSON we have in the
library by running a single target. We could detect [[http://stedolan.github.io/jq/][jq]] and then use it
to validate.

*** Rename dynamic models                                             :story:

At present we have named the dynamic models as follows:

- =schema=: for =dynamic::schema=
- =expansion=: for =dynamic::expansion=

This is because the name of the model is used to generate the
enclosing namespaces. Ideally we should name the models something like
=dynamic_schema= but still generate the namespaces correctly. One way
of doing this is to use the meta-data property for the model name,
which would allow having a file name that is different from the model
name. We have a dependency on [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-a-property-for-the-model-name-as-dynamic-extensions][this]] story.

*** Using types of non-referenced models produce bad error messages   :story:

By mistake we made a reference to =dynamic::object= in the schema
model, during the =dynamic= to =schema= refactoring. This resulted in
the following, non-obvious, error message:

: 2015-03-09 12:56:00.920766 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/merger.cpp(120): Throw in function void dogen::sml::merger::update_references()
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml13merging_errorEEE
: std::exception::what: Cannot find target dependency: dynamic
: [P12tag_workflow] = Code generation failure.

What this is trying to say is that the =dynamic= model is not being
referenced. We should make this a bit more obvious because it would be
very difficult for the user to figure out what type is bringing in
this dependency. It would make more sense to say "type X requires
model Y, which is not part of the list of reference models" or
something along these lines.

*** Nested external model path results in strange references          :story:

For the dynamic schema model we have an external model path of
=dogen::dynamic::schema=. This is because there are two things we need to place
under =dynamic= (also =dogen::dynamic::expansion=). This revealed a
more general problem: nesting of external model paths results in
"strange" references in dependent models. For example, in SML we have
to say =schema::dynamic_object= to refer to
=dogen::dynamic::schema::dynamic_object=. We need to figure out what
is the best approach for this.

*** Consider renaming qname                                           :story:

As part of dynamic we came up with a better way of modeling names:
type is name, fields:

- simple
- qualified

This is a better way of modeling, as opposed to the SML way with a
=qname= which then contains a =simple_name=. We should use this
approach in SML to.

*** Perform lexical casts once only for error reporting               :story:

There are a number of places in the code where we do lexical casts for
enumerations for the exception part:

: BOOST_LOG_SEV(lg, error) << unsupported_formatter_type << ft
:                          << " name: " << o.name();
: BOOST_THROW_EXCEPTION(workflow_error(unsupported_formatter_type +
:    boost::lexical_cast<std::string>(ft)));

We should just do the lexical cast once at the top and use it for both
logging and the exception message.

*** Consider renaming module path to internal module path             :story:

Since we have got a external module path, it would make sense for the
other to be the internal module path. This may be taking the symmetry
too far, so we need to have a think.

*** Names in C++ namespaces                                           :story:

It appears we are not using the entity name for a C++ namespace. If
that is the case, this is wrong and needs to be fixed. We are probably
inferring the name by looking at the =front= (or =back=) of the
namespaces list. Investigate this.

*** Consider renaming registrar in boost serialisation                :story:

At present we have a registrar formatter that does the boost
serialisation work. However, the name =registrar= is a bit too
generic; we may for example add formatters for static registrars. We
should rename this formatter to something more meaningful.

*** Create a single container of element in model                     :story:

We did an experiment to figure out if it would be more efficient to
have separate containers of elements in SML's model, the idea being
that we would avoid using the heap, dispatching etc. We would also
create code that is more type-safe (e.g. avoid cases where we pass in
elements but we want a specific descendant only).

However, predictably, the code now has lots of loops across the
different collections. This pattern is scattered everywhere we use
SML. In almost all cases this could be handled by a single loop
without loss of type-safety (e.g. loop and visit where we need
specific types; just loop where an element suffices). Using the
traversals (all types traversals, etc) didn't help because we then
need to create all the associated machinery (overload =operator()=
etc.) and that is not much different from having a visitor on
element. We should consider this experiment at an end and just add a
single container of element in model and deal with the fallout.

Alternatively, we need a "view" over the different containers. In
truth after the SML workflow has finished executing the model is
constant. This means we could then use pointers to the objects to
create a synthetic element container and use this for looping over all
entities.

In an ideal world, this would be a property of the model:
e.g. =std::forward_list<entity*>=. However, we do not support pointer
containers and this is a non-trivial change to the spirit parser so we
won't be able to do it quickly. The alternative is to generate the
container from within the backend workflow for now and pass it to each
workflow. Once we are done with the refactoring we can then replace
this with a model property.

We need to have a look at all instances of the code where we loop
across all elements and see if this is a win or not. Also, we can move
=Element= from a concept to a type (e.g. =element=) and make it the
base class for all elements. Validator would have to make sure the
model is not nonsensical (object inheriting from a primitive, etc).

*** Populate property =is_original_parent_visitable= in SML           :story:

To make life easier in C++ model, we've added a new SML property:
=is_original_parent_visitable=. We need to look at the SML
transformation and ensure this property is populated correctly.

*** Improve errors in dia objects                                     :story:

At present when adding blank spaces in a dia object we get the
following error:

: 2014-11-09 23:05:58.936785 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: std::unordered_map<std::string, facet_settings>
: 2014-11-09 23:05:58.938301 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: std::unordered_map<std::string, facet_settings>
: [P12tag_workflow] = Code generation failure.

There is no clue as to which object caused the error. Add a class name
and dia object ID to the exception.

*** Handling missing default facet settings                           :story:

At present we are just logging a warning when the user supplies
dynamic extensions for a facet that we do not have defaults
for. However, it may make more sense to just throw if someone is
assuming support for something which we do not support. We need to
think about this use case properly.

*** Improve resolution of partial types in resolver                   :story:

At present the code in =resolve_partial_type= is one big hack. We
should create a "lookup" routine that given a qname, tries it on all
containers and returns true or false. Then we should have different
"attempt" routines that try modifying the qname according to a rule
and call the "lookup" routine to see if it worked or not. We should
then continue to the next rule until we exhaust all rules or we find a
match. Each rule should provide some logging.

*** Improve error messages around dynamic extensions kvp's            :story:

Consider a dynamic extension "kvp" without a value, in a dia diagram
(model note):

: #DOGEN dia.comment'

At present the following error is triggered:

: 2014-09-27 10:07:32.761795 [ERROR] [dia_to_sml.comments_parser] Expected separator on KVP.

This provides very little context of what went wrong. Also, should we
allow a "kvp" that has no value, where the value is assumed to be
true. For cases like comment it would make life easier.

*** Remove unnecessary properties from model                          :story:

The model should be just dumb container of types. We have a few legacy
properties left behind from the days where the model was also used in
the transformation process. Remove all the concepts from the model
(=Element= etc) and deal with the fall out. Unnecessary properties:
documentation, containing module, extensions.

We need to keep the name because it is now used to locate the model's
module.

*** Add support for formatter and facet dependencies                  :story:

Once we are finished with the refactoring of the C++ model, we should
add a way of declaring dependencies between facets and between
formatters. We may not need dependencies between facets as these are
actually a manifestation of the formatter dependencies.

These are required to ensure users have not chosen some invalid
combination of formatters (for example disable serialisation when a
formatter requires it). It is also required when a given
facet/formatter is not supported (for example when an STL type does
not support serialisation out of the box).

Note that the dependencies are not just static. For example, the types
facet depends on the hash facet if the user decides to add a
=std::unordered_map= of a user defined type to another user defined
type. We need to make sure we take these run-time dependencies into
account too.

*** Use of disabled facets in non-generatable types                   :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a disabled
facet in a non-generatable type so that I don't generate
non-compilable code.
#+end_quote

It would be useful to set facets to disabled on non-generatable types,
when there are generatable types that depend on them. For example, if
we create some non-generatable types for which there is only a =types=
facet, we may still want to create generatable types that make use of
them. In this case, we would like Dogen to automatically disable all
facets except for =types=. Also, if a type is non-generatable, all
facets should be automatically disabled and its up to the user to
enable the ones he is interested in manually.

*** Failed facet dependencies should be treated as errors             :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a
non-supported facet from a system type so that I don't generate
non-compilable code.
#+end_quote

if a facet is not supported in a system module and the user tries to
make use of it, we should error. The user must then go and disable
explicitly the facet on the affected object via the meta data. We
should not silently disable facets.

*** References to objects in package should assume package            :story:

#+begin_quote
*Story*: As a dogen user, I don't want to have to specify fully
qualified names when referring to types in the same package so that I
don't have to type information that can be deduced by the system.
#+end_quote

At present if we define two objects in a package =p=, say =a= and =b=,
where =b= refers to =a= it must do so using a fully qualified path,
e.g.: =p::a=. Failure to do so results in an error:

: 2014-09-10 08:27:10.662113 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "simple_name": "registrar" }
: 2014-09-10 08:27:10.665861 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/resolver.cpp(178): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml16resolution_errorEEE
: std::exception::what: Object has property with undefined type: registrar
: [P12tag_workflow] = Code generation failure.

*** Handling of unsupported dia objects                               :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of Dia shapes that are
not supported by dogen so that my diagrams can be as expressive as
required.
#+end_quote

At present when we try to use a dia object that dogen knows nothing
about we get an error; for example using a standard line results in:

: 2014-09-10 08:09:43.480906 [ERROR] [dia_to_sml.processor] Invalid value for object type: Standard - Line
: 2014-09-10 08:09:43.487060 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/processor.cpp(124): Throw in function dogen::dia_to_sml::object_types dogen::dia_to_sml::processor::parse_object_type(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Invalid value for object type: Standard - Line

However, it may make more sense to just ignore these. To do so we
could relax the code in processor (object_types):

:    BOOST_LOG_SEV(lg, error) << invalid_object_type << ot;
:    BOOST_THROW_EXCEPTION(processing_error(invalid_object_type + ot));

We should also consider having a =strict= command line option to
enable/disable this behaviour.

*** Update manual with detailed model descriptions                    :story:

#+begin_quote
*Story*: As a dogen developer, I want to read about the architecture
of the application so that I don't have to spend a lot of time trying
to understand the source code.
#+end_quote

We should add CRCs for the main classes, with an explanation of what
each class does; we should also explain the separation of the
transformation logic between the core model (e.g. =dia=) and the
transformation model (e.g. =dia_to_sml=). We should describe what the
workflow does in each model.

We should only implement this story when all of the major refactoring
has been done.

*** Create a "post-processing" workflow in SML                        :story:

This story was spawned from "Refactor Dia to SML transformer". We need
to create a create a "post-processing" workflow in SML that handles
some of the work that is currently in Dia to SML. This will make life
easier in terms of supporting JSON as a fully supported front-end.

Post-processing tasks already identified:

- population of model references: should be distinct step in workflow,
  after transformation (=update_model_references=).
- computation of leaves: See  also [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#dia-to-sml-workflow-should-post-process-model-by-leaves][this]] ticket. 
- setting original parent name: in effect all of the relationship
  population.
- identity processing: this should be done as a post-processing step
  rather than during transformation.
- containing module: this is not done in Dia to SML at the moment, its
  only done in JSON hydrator. We are populating containment for the
  global module in injector though; see
  add_containing_module_to_non_contained_entities.

We need to look into Dia to SML to see if there are any additional
tasks that need to be moved across.

As part of this work, we should also rename =sml::workflow= to perhaps
=sml::merging_workflow= or something along these lines. This new
workflow would then be =sml::augmenting_workflow= perhaps. We should
also move any activities which are not related to merging into this
workflow (e.g. injecting of system types, etc).

*** Refactor Dia to SML transformer                                   :story:

- remove all properties from context which are used only internally in
  the transformer.
- split context into inputs and outputs: =transformation_result= as a
  candidate for the outputs.
- inputs are passed in at construction time and remain constant.
- each transformation method returns a value which can be slotted into
  the model by the workflow, contained in a transformation result.
- this does mean a lot of concatenation at the workflow level though.
- the only problem with this approach is that we do not have a common
  base class in SML for all the types that transformer generates. If
  we did we could just return it and let workflow visit the base class
  to dispatch it to the correct type. This may be the right solution
  (to create such a base class).
- actually the right thing to do may even be to take away dispatching
  from the transformer and have specific methods to call from the
  outside world: transform large package, etc. These return the
  expected SML type. The only exception is transform note, which
  returns some kind of =dia_to_sml= intermediate object.
- in workflow =transformation_activity= we should move the logic of
  defaulting to value object into the profiler.
- the transformer should ensure only zero or one notes can exist for a
  module.
- the setting of the documentation should be done as a separate step
  in transformation - i.e. look for =dia.comment= field, if set, use
  its value to populate documentation. This could be done to all types
  for completeness.
- the workflow should not be creating transformers half-way
  through. They should be as stateless as possible.
- tests need to be cleaned up - we need to check for text of the
  exception being thrown.

*** Update comments in C++ model                                      :story:

We have a very large blurb in this model that is rather old, and
reflects a legacy understanding of the role of the C++ model.

*** Dia to SML workflow should post-process model by leaves           :story:

At present in =dia_to_sml::workflow::post_process_model_activity= we
are post-processing by going through every single object; in reality
we only need to go through the leaves.

*** Refactor code around model origination                            :story:

In the past we added a number of knobs around generation, all with
their own problems:

- =origin_types=: was the model/type created by the user or the
  system. in reality this means did the model come from Dia or
  JSON. this is confusing as the user can also add JSON files (their
  own model library) and in the future the user can use JSON
  exclusively without needed Dia at all.

- =generation_types=: if the model is target, all types are to be
  generated /unless/ they are not properly supported, in which case
  they are to be "partially" generated (as is the case with
  services). This is a formatter decision and SML should not know
  anything about it.

These can be replaced by a single enumeration that indicates if the
type/model is target or not.

*** Tidy-up test models                                               :story:

#+begin_quote
*Story*: As a dogen developer, I want to be able to understand the
test models quickly so that I know at which model to look at when
doing a change.
#+end_quote

We have a lot of fine grained test models for historic reasons. A lot
of these could be collapsed into a smaller number of models, focused
on testing a set of well defined features.

**** Models that need changing

Merge the following models into a =basic= or =trivial= model (no
aggregation, no association):

- classes_in_a_package
- classes_inout_package
- classes_without_package
- class_in_a_package
- class_without_attributes
- class_without_package
- stand_alone_class

We should also check the combined model has all the scenarios
described in [[*Cross%20package%20referencing%20tests][Cross package referencing tests]].

Merge the following models into stereotypes:

- enumeration
- exception

Consider deleting the comments model and make sure we have comments in
all models with the same features:

- top-level comment for the model
- package level comment
- notes

These models are at the right level of granularity but need renaming:

- all_primitives: primitives or primitives_model to line up with boost
  and std.
- trivial_association: association
- trivial_inheritance: inheritance

**** Models that do not need changing and why

These models test other models, and we cannot remove the postfix
=_model= to avoid clashes with namespaces:

- boost_model
- std_model

These models test command line options, which means they cannot be
merged:

- disable_cmakelists
- disable_facet_folders
- disable_full_ctor
- dmp
- enable_facet_domain
- enable_facet_hash
- enable_facet_io
- enable_facet_serialization
- split_project

These models test features which have enough scenarios to justify
keeping them in isolation:

- database

These models test dia features and must be kept isolated:

- compressed
- two_layers_with_objects

**** Add objects, enumerations and exceptions to comments model

At present we are only testing packages in comments.

*** Improve error message for blank types                             :story:

#+begin_quote
*Story*: As a dogen user, I want a clear error message when I forget
to supply a type for a property so that I don't spend ages searching
the diagram for the missing type.
#+end_quote

If the user does not supply a type at all in Dia, dogen spits out a
message that is not very informative:

: Error: Failed to parse string: .

The log file is not much better:

: 2014-09-06 16:11:54.143249 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: 
: 2014-09-06 16:11:54.150595 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: 
: [P12tag_workflow] = Code generation failure.

We should instead mention that the string was empty or blank. We also
need to provide the property and class that contained this string. To
reproduce this problem create an enumeration but remove the
=enumeration= stereotype.

*** Add new warnings to compilation                                   :story:

- =-Wunused-private-field=: Seems like this warning is not part of
  =-Wall=
- =-Winconsistent-missing-override=: new clang warning, probably 3.6.

*** Move code to C++ 14                                               :story:

#+begin_quote
*Story*: As a dogen user or developer, I want to make use of C++-14
features so that I can generate more modern code.
#+end_quote

Now that the standard is out, we should move to it. Both clang and gcc
have some kind of support at present, so it should be a matter of
compiling on this mode. However, as we have gcc 4.7 on OSX and
Windows, we would have to upgrade these compilers first.

We have already proven that the code builds out of the box in sprint 50.

*** Usage of external module path in cmakelists                       :story:

It seems like we are not populating the target names
properly. Originally the target name for test model all primitives was:

: dogen_all_primitives

When we moved the test models into =test_models= the target name did
not change. It should have changed to:

: dogen_test_models_all_primitives

*** Rename the =database= test model to =odb=                         :story:

This name is a bit misleading, this is not a generic database model
but its designed to specifically test odb.

*** Enable package sanity tests for Linux                              :epic:

Now that we will be using docker, we could create a simple =systemd=
ctest script that runs as root in a docker container:

- it monitors the GDrive location for files that match a given regular
  expression (e.g. we need to make sure we match the bitness and the
  platform)
- if it finds one, it installs it and runs sanity scripts.
- it then uninstalls it and makes sure the docker image is identical
  to how we started (however that is done in docker)

*** The =types= facet should always be on                             :story:

At present users are given the option to enable or disable the
=domain= facet; this is not very wise because all facets depend on
it. It must always be on. We should remove these options.

In addition the facet is incorrectly named: when we performed the
rename of =domain= to =types= we left the command-line facet. We
should rename it to =types= too.

We should probably create a notion of "mandatory" facets to make this
more general.

*** Allow for generation of class with the same name as package       :story:

At present its not possible to generate a class inside a package with
the same name of that package, if the package documentation is being
generated. This is because they will both have the exact same file
name.

*** Type with the same name as the project does not compile           :story:

It seems that if we create a type with exactly the same name as the
model, we get strange compilation errors:

: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.4/stage/bin/dogen_examples/source/hello_world/include/hello_world/test_data/hello_world_td.hpp:37:13: error: ‘hello_world::hello_world::hello_world’ names the constructor, not the type
:     typedef hello_world::hello_world result_type;
             ^
We should do a test case for this and fix the errors.

*** Diagrams used in manual should be in sanity and in docs           :story:

Users should be able to follow the examples in the manual by using a
set of diagrams supplied in the dogen package. However, to ensure
these samples are actually working we need to test them as part of
sanity. This means we need the same diagrams packaged twice.

*** Move the mock factories into the test_data directory              :story:

There is no good conceptual reason to split the mock factories from
the test_data generators. However, we did it because we don't have a
good way to give dogen visibility of the existence of these files: we
could add regexes but then its not very maintainable and not visible
from the project diagram.

The correct solution for this may be to have some tags that state that
an object only has representations in certain facets.

*** Add tests for general settings factory                            :story:

Zero coverage on this one for some reason. Some simple tests come to
mind:

- empty data files directory results in empty factory;
- valid data files directory results in non-empty factory;
- invalid data files directory results in exception;
- more than one data files directory results in expected load;
- creating annotation for test model types works as expected.
- missing fields result in expected exceptions.

*** Remove references to PFH in makefiles                             :story:

Seems like the correct way of finding libraries is to use
=CMAKE_PREFIX_PATH= as explained [[https://blogs.kde.org/2008/12/12/how-get-cmake-find-what-you-want-it][in this article]]. We should stop using
any references to PFH and let the users provide a path to local
installs via this.

We need to add a note on the read me too.

*** Improve error messages for unconnected objects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to know exactly which object is not
connected correctly so that I can fix it.
#+end_quote

At present when a Dia object is not connected we get the following
error message to std out:

: Error: Expected 2 connections but found: 1. See the log file for details.

The log file is a bit more verbose but still not particularly helpful:

: 2014-01-23 08:25:28.115363 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2014-01-23 08:25:28.118718 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Expected 2 connections but found: 1
: [P12tag_workflow] = Code generation failure.

We should try to at least name the object that has the one connection
to make the user's life easier.

*** Error in log files when reading in Dia model                      :story:

For some reason the log file is full of errors like this:

: 2014-01-20 18:28:31.219549 [ERROR] [dia_to_sml.processor] Did not find expected attribute value type: composite

Presumably the errors are not fatal as code generation still
works. Investigate the errors and tidy-up the log.

*** Add tests for =name_builder=                                      :story:

We refactored tagger code related to flattening names into this class
but added no tests. We need good coverage, hopefully available from
C++ formatters.

*** Add tests for =annotation_factory=                                :story:

We added this class without any tests initially because we wanted to
first prove =om= worked. Once this is achieved we need to revisit this
class and add tests.

- missing licence
- missing modeline
- empty marker
- different marker for two objects
- consider moving generate preamble into annotation

*** Add tests for main header file formatter with optionality         :story:

We should add a couple of tests that exercise the annotation
factory. As it will have its own tests, we just need to make sure it
works in general. For example, pass in an empty annotation.

*** Remove speculative facet layers for hash and serialization        :story:

For some unfathomable reason we decided to add a layer of indirection
for both hash and serialization. This is for speculative reasons as in
the future we may want to add boost hash and other forms of
serialization. However, in keeping with the (often violated) rule that
we never add code without a use case, we need to remove this.

Instead of =serialization= we need to call it =boost_serialization=.

*** Feature models should always be tested by knit                     :epic:

#+begin_quote
*Story*: As a dogen user, I want to be sure that every feature is
comprehensively tested so that I don't have to worry about dogen bugs
when using it.
#+end_quote

We recently implemented features into dogen; these work off of CMake
detection, where by if a library is not detected, all tests associated
with it are not built and executed. However, we should still try to
codegen these models to make sure that a change we did elsewhere did
not introduce bugs in features we're not interested in. We need to
check that knit has tests for both EOS and ODB that get executed
regardless of these features being on or off.

*** Indent stream can be made a bit less inefficient                  :story:

Out first attempt at creating a stream with indentation support was a
bit naive: we are intercepting every character and then deciding if we
need to change any states in the state machine. Its probably wiser to
just use manipulators to perform the state transitions and leave the
=put= undisturbed. We can leave this until we have a good way of
getting metrics out of the system.

*** Consider creating an iostreams filter for comments                :story:

Seems logic to follow the filtering idea and add a doxygen (or
generic) commenting filter; one inserts into the stream and it
automatically inserts all the comment markers such as =/**=, =@brief=
and so on. Basically takes on the work of =comment_formatter=. This
would mean we would no longer need the =contents= vector, and we could
stream directly to the stream, after pushing the comments formatter on
to it. However, it would probably mean we need to cast the stream to a
=filtering_ostream= in order to do the push.

*** Use consistently the American spelling for license                :story:

We have a mix of American and British spelling of license (e.g. data
file folder is called licence. For details on the subject see [[http://www.future-perfect.co.uk/grammar-tip/is-it-license-or-licence/][this
article]].

We are going to take the easy approach as we did for serialisation and
make all the code artefacts American. Documentation etc is not that
important.

*** Create a new command line parameter for data files directories    :story:

#+begin_quote
*Story*: As a dogen user, I want dogen to use my own private data
libraries so that I don't have to supply them as diagrams.
#+end_quote

Users should be able to provide directories for their own JSON
models. We just need to add a new parameter to the knitter and
transport it all the way to OM's workflow.

*** Check packaging code for non-distro dependencies                  :story:

We are manually copying a lot of shared objects from locally built
third party libraries when creating packages, this should be replaced
with appropriate dependencies (at least for Debian packages).

*** Fix cp error on cmake with local third-party packages             :story:

We are getting strange errors in cmake:

: cp: cannot stat ‘/usr/lib/i386-linux-gnu/libpthread.so.1.54.0’: No such file or directory

*** Operations need to behave more like properties                    :story:

#+begin_quote
*Story*: As a dogen user, I want to specify operations via the
frontend so that I dogen can generate the header file and I can
manually add the implementation.
#+end_quote

This story is a requirement in order to implement merging support.

When we did the expansion and indexing work for properties, we omitted
operations altogether. This is fine for now, as we only have a
half-baked support for them anyway, but will need to be revisited as
we start to use it in anger. In particular:

- we need sets of operations: local, inherited, all
- we need an operations indexer

*** Use xtime-like stopwatch in selected places to log timings        :story:

We should log the time it takes for certain operations in dogen so
that users can figure out if we are becoming slower (or faster) at
doing them and report regressions.

Boost used to provide a nifty little utility class called xtime. It
appears to have been deprecated by [[http://www.boost.org/doc/libs/1_55_0/doc/html/chrono/users_guide.html#chrono.users_guide.examples.duration.xtime_conversions][chrono]].

We should also provide a command line option that prints a timing
report. This would be useful so that users can compare timings between
releases.

We should also be able to grep the log for all timings and save them
down to get trends. We should add a log severity for this, perhaps
PROFILE. Not sure what priority it would be at.

*** Consider renaming SML                                             :story:

Originally we intended to rename SML - the Simplified Modeling
Language - to DDL - the Domain Driven Language. This was because we
had envisioned that SML was a model of the ideas in Domain Driven
Design, and not at all a cut down version of UML as the name seems to
imply. However, its becoming increasingly clear that, whilst we use a
lot of the Domain Driven Design ideas, we are also morphing them
considerably. Perhaps a more apt name would be SDML - the Simplified
Domain Modeling Language?

Or instead we could follow the compiler theme and call it the =ir= or
intermediate representation, or =im= for intermediate model.

After more thinking on this, and since we now have a =backend= and a
=frontend=, it is obvious that SML is the =middleend=. However, this
is not a very good name for the model as we also do some front-end-y
things (such as the JSON implementation). =ir= and =im= still seem
like the more likely candidates. Actually, JSON is not a front-end; it
is a direct representation of the middle-end into a file. The same
could also be done in XML. It only becomes a front-end if there is an
intermediate representation (such as the =dia= model) that needs to be
transformed into SML.

Another idea: common representation or =cr=. It is what is common to
all modeled languages. What is not common goes into dynamic. This will
also make the vision for this model much more focused.

Some more thinking on this: SML is a meta-model, or a model that
provides a language to talk about programming objects in
general. There may be other models suitable for formatting; for
example one may want to take a model of a neuron and represent it in
[[http://www.neuron.yale.edu/neuron/static/docs/refman/hoc.html][HOC]]. In this case the formatters would bind directly to the neuron
model rather than SML. So the name of SML must reflect the fact that
it is a model of programming objects. Object-Oriented Programming
Language Model?

Another idea: the Middle-End Model (MEM) or just Middle-End (ME). SML
is the middle-end of the code generation process.

*** Consider renaming formatters                                      :story:

After reading the [[http://martinfowler.com/eaaDev/PresentationModel.html][Presentation Model]] pattern a bit more carefully, it
seems it provides a good approach for formatters. If one thinks of the
file as the view, then the formatters are the presenters and the model
representing all presentation logic (e.g. =cpp=) is the presentation
model. We could:

- create a top-level folder called =presentation=;
- rename =formatters= to =core= and move it to =presentation=;
- move =cpp= to =presentation=;
- in =cpp=:
  - rename =formattables= to =presentables=;
  - rename =formatters= to =presenters=;
- in this light, =backend= is really the "meta-workflow" for all
  possible presentations. It should really live under presentation. It
  would make more sense to merge it with =core=, if it were not that
  core contains all sorts of loose bits that are useful only in the
  guts of presentation. We could call it =orchestration= or some such
  name. Or we could leave it as =presentation::backends=.
- move =file= to =backends=. We don't really want external clients to
  have to know about =core= just to obtain a single type. Also,
  backends shouldn't really have any dependencies.
- grep for formatting, formattables, formatter, format, etc. and
  ensure all usages have been replaced with present*.

We should wait until the "great refactoring" is done so that we do not
have to rename the legacy models too.

*** Canned tests rely on copy constructors rather than cloning        :story:

If an object has pointers, the canned tests will not perform a deep
copy of the object. We need to [[*Add%20support%20for%20object%20cloning][implement cloning]] and then use it in
canned tests.

*** Clean up SML resolver tests by extending mock factory             :story:

Now that the mock factory has the concept of "stages" of processing,
we need to create a "stage" for merged but unresolved models and
remove the merger from the resolver tests. The flag for this has been
added, we just need to go through the different scenarios and add
handling code for them.

*** Refactor SML mock factory method names                            :story:

We have a zoo of naming conventions, some starting with =build_=, some
starting with =object_= etc.

*** Validate SML mock factory on its own tests                        :story:

At present we have a lot of code that ensures that the output of mock
factory actually corresponds to expectations. However, this validation
is in the tests that use the mock factory, resulting in duplication
and possibly missing coverage. We should really just have a mock
factory test with this validation.

*** Sanitizer: Add tests for empty objects                            :story:

This was mainly in the context of IO but could be useful for other
facets. Example:

: class empty_model_generator {
: public:
:     typedef dogen::sml::model result_type;
:
: public:
:     result_type operator()() {
:         dogen::sml::model r;
:         return r;
:     }
: };
: ...
: BOOST_AUTO_TEST_CASE(validate_io_for_empty_objects) {
:     SETUP_TEST_LOG("validate_io_for_empty_objects");
:
:     /* ensure we generate valid JSON for empty model. test was added
:      * because empty property trees were not correct, but its valid on
:      * its own right as we always use populated objects when testing
:      * JSON.
:      */
:     // test_io<empty_model_generator>();
: }

*** Split floating point stream settings from double                  :story:

We had a problem where the output of floating point numbers was being
truncated due to scientific notation being used. A quick fix was to
just update the properties of all streams which use either doubles,
floats or _bools_ with precision etc settings. The real fix is to
distinguish between the two such that we only enable =bool= related
settings when dealing with bools and floating point settings when
dealing with =double= or =float=.

*** Split is floating point like from int like in view model          :story:

At present we only have a single test data generator helper method for
any numeric type: =is_int_like=. This works ok, but it means we are not
generating useful test data for doubles, e.g: =1.0= instead of a
slightly more useful =1.2345= or some such number.

We need a =is_floating_point_like= method to be able to distinguish
between them, and then the associated changes in the generators to
create floating point numbers.

*** System models set meta-type to =invalid=                          :story:

Something is not quite right on the resolution logic

*** Create includers for value objects only                           :story:

#+begin_quote
*Story*: As a dogen user, I don't want to include every object in a
model when I use includers.
#+end_quote

At present we are using the facet includers in unit tests. This is not
ideal because it means that every time we do a change in a service
header, all tests recompile. In reality we should have two types of
inclusions:

- canned tests should include only value objects, etc - e.g. no
  services.
- service tests should include the header for the service and any
  additional dependencies the service may require.

Perhaps we could have a second type of includer that only has value
objects, etc.

*** Do not copy models in merger                                      :story:

At present we are adding the partial models into the merger by copying
them into an associative container. It would be nicer to avoid the
copying as it adds no value. This should wait until we have a way to
get performance numbers out.

*** Adding linking libraries is not handled                           :story:

#+begin_quote
*Story*: As a dogen user, I want to link against libraries without
having to manually generate CMakeFiles.
#+end_quote

At present whenever a model requires additional link library targets
we need to disable CMake generation and do it by hand. However:

- for well-known dependencies such as boost we could create a
  convention (e.g. assume/require that the CMake boost libraries flags
  are set via find boost)
- for user level dependencies we should add dynamic extensions at the
  model level.

*** Test data generator does not detect cycles in object graph        :story:

At present we handle composition correctly, but not other forms of
cycles in the object graph.

Let model M be composed of class A with a member of type class B, and
class B with a member of type =shared_ptr= to class A. The test data
generated for such model will contain an infinite loop. We need a way
to detect such loops, potentially in SML, and then generate code which
breaks the loop.

This could be done by explicitly checking if the type of any member
variable loops back into the type itself. Of course one could conceive
cycles that involve many edges in the object graph, and for these we'd
still generate invalid code.

Another approach would be to have an unordered map of type
association; the map would have the IDs of every type as we go further
into the association graph. It would be pushed and popped as we go in
and out of branches; at the same time we need to have a look back
capacity to see the few elements in the stack. When a pattern emerges
that involved types of a certain ID, they would stop creating any
further associations.

*** Create a =key_extractor= service                                  :story:

We need a way to automatically extract a key for a =keyed_entity=.
The right solution is to create a service to represent this
concept.

Injector creates objects for these just like it does with keys; the
C++ transformer intercepts them and generates the correct view models.

*** Use explicit casting for versioned to unversioned conversions     :story:

At present we have to_versioned; in reality this would be dealt much
better using explicit casts:

: explicit operator std::string() { return "explicit"; }

Actually the real solution for this is to make the versioned key
contain the unversioned key; then dogen will generate all the
required code.

At this point in time we do not have enough use cases to make the
correct design decisions in this area. We need to wait until we start
using keys in anger in Creris and then design the API around the use
cases.

It is not possible to use global cast operators so we need to
introduce a dependency between versioned and unversioned keys in order
for this to work.

*** Consider not creating unversioned keys for single property        :story:

If a key is made up of a single property, its a bit nonsensical to
create an unversioned key. We should only generate the versioned
key. However, it does make life easier. Wait for real world use cases
to decide.

*** Detect invalid child nodes                                        :story:

#+begin_quote
*Story*: As a dogen user, I want to know when there are invalid child
nodes in diagram so that I can fix them.
#+end_quote

When copying a set of classes from a diagram, where these classes
where contained in a package, dia seems to copy across the =childnode=
id. This is a problem because when pasted in a new diagram, if those
classes are not in a package there is now the potential for total
mismatching - for instance, they could be children of an
association. Dogen should validate that children belong to UML
elements which can have children, and if not issue good error
messages - perhaps even talking about the possible cause for the
error.

*** Add tests for SML workflow                                        :story:

We don't seem to have any. A few come to mind:

- model with no generatable types returns false
- model with generatable types returns true
- multiple models get merged
- system models get injected

*** Register types for multiple models is misbehaving                 :story:

It seems that somehow we're clobbering the type registration of one
model with another in register types. This is probably because we are
reusing type id's somehow. This wasn't a problem until now because we
were not using inheritance in anger but with the sml changes, it is a
problem as one cannot load dia and sml types off the same registration
(e.g. as in XML serialisation helper).

One solution for this problem would be to create serialisers which
hide the machinery of serialisation internally; one should be able to
just pass in a stream in and get a type out.

*** Comments seem to be trimmed                                       :story:

For some reason we seem to be munching any blank lines at the end of
comments. We should only remove the lines with the well known dogen
marker, all other lines should be left untouched.

*** Type resolution in referenced models                              :story:

We did a hack a while ago whereby if a type is of a referenced model,
we don't bother resolving it. As an optimisation this is probably
fine, but however, it hides a bug which is that we fail to resolve
properties of referenced models properly. The reason why is that these
properties have a blank model name. We could simply force it to be the
name of the referenced model but then it would fail to find
primitives. So we leave it blank during the dia to sml translation and
then if it gets to the resolver, it will not be able to resolve the
type. We could add yet another layer of try-logic (e.g. try every
model name in the references) but it seems that this is just another
hack to solve a more fundamental problem. The sort of errors one gets
due to this are like so:

: 2013-06-29 23:10:34.831009 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "type_name": "qname", "meta_type": { "__type__": "meta_types", "value": "invalid" } }
: 2013-06-29 23:10:34.831294 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/sml/src/types/resolver.cpp(202): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname&) const
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::sml::resolution_error>

*** Sort model dependencies                                           :story:

It seems the order of registration of models has moved with recent
builds of dogen (1418). Investigate if we sort the dependencies and if
not, sort them.

*** Assignment operator should be protected in ABC                    :story:

As per MEC 33. We should probably do the same for the move and copy
constructors.

*** Change transformation code to use a type visitor                  :story:

Now we have a base type, we could probably simplify some of the
transformation code:

- dia to sml
- sml to c++
- potentially merger

*** Test data generator with immutability looks wrong                 :story:

We are using the full constructor for immutability, but its not clear
how that would work on a inheritance tree. Ensure we have test cases
for this.

*** Consider renaming formatters                                      :story:

These are not really formatters; not sure what the right name should
be though; templates?

*** Inserter for enumerations shouldn't throw                         :story:

We only use the inserter for debug dumping and it could happen that we
are about to write the message for an exception when we decide to
throw. Instead we should just print unexpected/invalid value and cast
it to a numeric value in brackets.

*** Add comments to test model sanitizer                              :story:

We should explain why we decided to create a test model sanitizer
instead of just adding specs to the test models themselves. The
rationale behind it was that it would break the current diffing and
rebaselining logic; we would either have to ignore specs on the diff
or find a way to copy them after code generation. Both options are a
bit of a hack. So instead we created a model with all the specs.

*** Consider renaming dependencies to references in model             :story:

Dependencies is a map of reference; why not call it references?

*** Create a validator in SML                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to know exactly why my diagram is not
correct so that I can fix the issues. I also want dogen to pick up
errors and generate valid code so that I don't have to figure out what
went wrong by looking at the generated code and the compiler errors.
#+end_quote

We need a class responsible for checking the consistency of the SML
model. There are several things we need to check for non-merged
models:

- ensure that we can only define identity once across concepts and
  parents
- concepts must have at least one property (or method).
- refined concepts must not have properties (or methods) with clashing
  names.
- type names, model names, etc must not contain spaces or other
  invalid characters. We should use a identifier parser for this.
- the qname of all keys in objects, etc must be part of the current
  model.
- the qnames of all types as keys are consistent with the values.
- type_name is non-empty; cannot be blank or a variable name
- type name must not exist on any model
- parent names and original parent names must exist in current model.
- leaves exist in current model.
- entity must have at least one key attribute.
- non entity must not have key attributes (value, service)
- keyed must be entity.
- aggregate root must be entity.
- all properties of types in current model must exist.
- properties of types in other models result in dependencies.
- enumeration must have at least one enumerator
- enumerator name must not be empty
- enumerator name must be unique
- external package path of the model matches all objects, etc in current
  model.
- model name is non-empty.
- documentation does not have non-printable characters.
- number of type arguments is consistent with objects type.
- objects marked as is comparable must follow the [[*Add%20is%20comparable%20to%20SML][comparison rules]].
- objects marked as is parent must have at least one child.
- property can only have a default value if primitive
- property default value must be castable to primitive type.
- property must have non-empty name.
- is versioned objects must have a property called version.
- string table cannot have duplicate entries.
- duplicate checks: properties cannot have duplicate names; classes in
  a package cannot have the same name; namespaces at the same level
  cannot have the same name;
- Issue error when a property is a value of an abstract class: SML
  should fail to merge if the user attempts to create a property of a
  base class. It should allow pointers to the base class though (raw,
  shared pointers, boost optional etc).
- Test relationships between objects and other meta types: We should
  validate that objects are only related to other objects - e.g. they
  cannot inherit from exception or enumeration or vice-versa. Add
  tests for this.
- Its not possible to be immutable and fluent.
- it is not possible to be immutable and be in an inheritance
  relationship. FIXME: why is that?
- user models cannot have stereotype of primitive.
- We don't support generic types (see [[Supporting%20user%20defined%20generic%20types][Supporting user defined generic
types]]) so we should throw if a user attempts to use them.

For merged models:

- issue error when a property is a value of an abstract class
- properties exist in merged model.

Validator should provide contextual validation error messages:

: error 1: properties must have a non-null name
: in model 'my_model' (Dia ID: O0)
: in object 'my_object (Dia ID: O0)
: property 'my_property' has empty name.

*** Validation-only or dry-run mode                                   :story:

Both stitcher and knitter could do with a "dry-run" mode in which we'd
do everything except for actually outputting.

*For Knitter*

It would be nice if one could just check if a dia diagram is valid for
code generation, e.g. =--validate= or something along those lines.

*For Stitch*

We are interested in performing the parsing. This would be useful for
example for a flymake mode in emacs.

*** Vistor is only supported at the base class level                  :story:

Due to implementation constraints, we only support visitable at the
base class level. Add an exception if users attempt to use visitable
stereotype in a class that has parents.

Note: is this true? We are using derived visitable in C++ model.

*** Refactor boost and std helpers and enums                          :story:

We shouldn't really have std and boost enums. These are just a repeat
of the SML models. We should have a find object by name in a model which
returns the appropriate qname given a type name. Then the helpers bind
to those qnames; given a qname, they return the include information,
etc. In the current implementation, the enums are basically a
duplication of the static models.

In reality we should really load up these models from a file, such
that users can add their own bindings without having to change C++
code. This could be done with a config file using boost property
tree. However, one would need some kind of way of mapping types into
primitives, sequence containers etc - some kind of "concepts".

*** Add a property for the model name as dynamic extensions           :story:

#+begin_quote
*Story*: As a dogen user in a constrained environment, I am forced to
use file names that are not suitable for a model name so that I need
to supply an override somewhere else.
#+end_quote

It would be nice to be able to generate a model with a name other than
the diagram file. We should have a command line option for this that
overrides the default diagram name.

This could also be supplied as part of dynamic extensions. The command
line option is useful when we want to use the same diagram to test
different aspects of the generation, as we do with the tests. The
dynamic extensions option is useful when we don't want the file name
to have the full name of the model.

We now have a use case for this: the dynamic models. See Rename
dynamic models.

*** Warn if value or entity has methods                               :story:

We should issue a warning if a user defines methods in value or entity
objects as its most likely by mistake.

*** Handling of include cmakelists in split projects is not correct   :story:

At present we are only generating a cmakelists file for include
folders on non-split projects. This means that the header files for
split projects won't be packaged up. It also means that for ODB
projects we won't get the ODB targets.

*** Partial matching of primitives doesn't work for certain types     :story:

We introduced a fix that allows users to create types that partially
match primitive types such as =in= or =integer=. The fix was copied
from the spirit documentation:

[[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/doc/html/spirit_repository/qi_components/directives/distinct.html][- Qi Distinct Parser Directive]]
- [[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/test/qi/distinct.cpp][distinct.cpp]]

However, we still haven't solved the following cases:

: BOOST_CHECK(test_primitive("longer"));
: BOOST_CHECK(test_primitive("unsigneder"));

As these are not so common they have been left for later.

*** Shared pointer to vector fails to build                           :story:

If one has a property with type
=boost::shared_ptr<std::vector<std::string>>=, we get the following
error:

: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp: In function ‘boost::shared_ptr<std::vector<std::basic_string<char> > > {anonymous}::create_boost_shared_ptr_std_vector_std_string_(unsigned int)’:
: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp:47:50: error: ‘create_std_vector_std_string_ptr’ was not declared in this scope

This is because the generated code is not creating a method to new
vectors:

: std::vector<std::string> create_std_vector_std_string(unsigned int position) {
:    std::vector<std::string> r;
:    for (unsigned int i(0); i < 10; ++i) {
:        r.push_back(create_std_string(position + i));
:    }
:    return r;
:}
:
:boost::shared_ptr<std::vector<std::string> >
:create_boost_shared_ptr_std_vector_std_string_(unsigned int position) {
:    boost::shared_ptr<std::vector<std::string> > r(
:        create_std_vector_std_string_ptr(position));
:    return r;
:}

*** Strange logging behaviour in tests                                :story:

As reported by JS for some reason if a test has a null pointer
de-reference, the next test will log to both files. This means the
logger is not being switched off properly in the presence of exceptions.

*** Unordered map of user type in package fails                       :story:

We seem to have a strange bug whereby creating a
=std::unordered_map<E1,E2>= fails sanity checks if E1 is in a
package. This appears to be some misunderstanding in namespacing
rules.

*** Naming of saved SML/Dia files is incorrect                        :story:

For some random reason when we use dogen to save SML/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.

*** Consider renaming specs to tests                                  :story:

We started using the terminology specs to mean specifications because
our unit tests follow the ideas outlined by Kevlin Henney. However, we
could easily use tests and still carry most of the meaning without
confusing every other developer. This would require:

- rename top-level =spec= folder to =tests=
- rename targets to =_tests=, e.g. =run_sml_tests=
- rename all test suites to =_tests=
- update the automatic detection of boost tests to use the new
  post-fix.
- we should also use =_tests= on the test suite name so we can do
  =using XYZ= without name clashes.

*** Shared pointers to primitive types                                :story:

At present we do not support shared pointers to primitive types. This
is because they require special handling in serialisation. See:

http://boost.2283326.n4.nabble.com/Serialization-of-boost-shared-ptr-lt-int-gt-td2554242.html

We probably need to iterate through all the nested types and find out
if there is a shared pointer to primitive; if there is, put in:

: // defined a "special kind of integer"
: BOOST_STRONG_TYPEDEF(int, tracked_int)
:
: // define serialization for a tracked int
: template<class Archive>
: void serialize(Archive &ar, tracked_int & ti, const unsigned int version){
:     // serialize the underlying int
:     ar & static_cast<int &>(ti);
: }

*** Full constructor parameter comments                               :story:

#+begin_quote
*Story*: As a dogen user, I want the complete constructor to be
documented automatically so that I don't have to do it manually.
#+end_quote

We could use the comments in properties to populate the comments for
the full constructor for each parameter. This would require taking the
first line of the documentation of each property and then stitching
them together for the full constructor.

*** Serialisation support for C++-11 specific containers              :story:

We can't add =std::array= or =std::forward_list= because there is no
serialisation support in boost 1.49. A mail was sent to the list to
see if this has changed in latter versions:

http://lists.boost.org/boost-users/2012/11/76458.php

However, it should be pretty trivial to generate serialisation code by
hand at least for =std::array= or to use a solution similar to
=std::unordered_map=.

*** Rename =inserter_implementation=                                  :story:

We used =inserter_implementation= to provide all sorts of utility
methods for IO. This class should really be named IO utility or
something of the sort.

*** Cross model referencing tests                                     :story:

At present we do not have any tests were a object in one model makes use
of types defined in another model. This works fine but we should
really have tests at the dogen level.

*** Cross package referencing tests                                   :story:

Scenarios:

- object in root refers to object in package: A => pkg1::B;
- object in root refers to object in package inside of package: A =>
  pkg1::pkg2::B;
- object inside of package refers to object inside of the same
  package: pkg1::A => pkg1::B (must be qualified);
- object in package refers to root object: pkg1::A => B;
- object in package refers to object in other package: pkg1::A =>
  pkg2::B;
- object in package refers to object in package in package: pkg1::A =>
  pkg1::pkg2::B;
- object in package refers to object in other package in package: pkg1::A =>
  pkg2::pkg3::B;
- object in package in package refers to object in package in package:
  pkg1::pkg2::A => pkg3::pkg4::B.

*** Empty directories should be deleted                               :story:

#+begin_quote
*Story*: As a dogen user, I want empty directories to be removed so
that I don't have to do it manually.
#+end_quote

When housekeeper finishes deleting all extra files, it should check
all of the processed directories to see if they are empty. If they
are, it should delete the directory.

We should probably have a command line option to control this
behaviour.

*** Header only models shall not generate projects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to generate models with just headers
that do not result in full blown projects.
#+end_quote

A project with just exceptions does not need a make file, and fails to
compile if a makefile is generated. We need a way to not generate a
makefile if there are no implementation files generated.

*** IO header could depend on domain forward decl                     :story:

At present we are depending on the domain header but it seems we could
depend only on the forward declarations.

*** Sanity check packages automatically                                :epic:

This work is also covered by tasks in the PFH backlog so update
accordingly. This task only refers to the dogen specific parts of the
task.

- sanity check that package installed correctly, e.g. check for a few
  key files.
- run sanity tests, e.g. create a dogen model and validate the results
- run uninstaller and sanity check that files are gone
  - this should actually be a build agent so we can see that deployment
    is green. We should create a deployment CMake script that does this.
- build package and drop them on a well known location;
- Create a batch script that polls this location for new packages;
  when one is found run package installer.
- we should create a set of VMs that are specific for testing - the
  test environments. One per OS. These are clean builds with nothing
  on them. To start off with they may contain postgres so we can
  connect locally.

*** Property types are always fully qualified                         :story:

When we code generate non-primitive properties we always fully qualify
them even if they are on the same namespace as the containing type.

** Maybe in current major release

If we have enough time and disposition, we may sneak some of these
in. This release is all about getting the architecture right.

*** Create a "utility" model like formatters for frontends            :story:

We have a number of utilities that are common to several backends,
similar to what happened to formatters. We should probably extract
those into a common model. At present we have:

- =identifier_parser=: in dia to sml but should also be used from JSON
when we support full models.
- "method identifier": this will be used by the merger to identify
methods and to link them back to language specific methods. Not
quite frontend, but not far.

*** Rename =codgen= targets                                           :story:

These are really the =knitting= or =knitter= targets because we are
calling =knitter=.

*** Indent generated c++ code using clang                             :story:

We should generate un-indented c++ code and then rely on clang-format
to do the indentation. We can allow users to supply their own
configurations and supply those to clang. This can be done via the
meta-data, or if there is a well defined file for clang, we could use
it instead.

Note that using clang to manage indentation will make things a lot
slower. Note also that clang supports Java and may in future support
C#. See [[http://clang.llvm.org/docs/LibFormat.html][LibFormat]].

Another option is to create fallback modes. The preferred indenter for
a given language (say c++) may not exist for another language (say
c#); for these we use a dogen created indenter that is very basic. It
may support some of the configuration parameters supplied for the
clang indenter. The key thing is that we take away indenting from the
formaters - they become flat - and then we always apply the indenter;
either a clang based one or a simplified one. Either way, the code
should live in formatters and make use of the language-specific
folders as required.

*** Dia models are not always user models                             :story:

At present there is an assumption that all models read in from dia are
user models. In reality, it is entirely possible to have a system
model such as LAM (Language Agnostic Model) which is a system
model. We should tell Dia to SML if the model is a) system or not b)
expandable or not.

*** Consider adding =with= support for fluent properties              :story:

It seems the java guys have decided to add the prefix =with= when
using fluent interfaces, e.g.:

: x.with_property_x(false).with_property_y(true);

We could easily add this via dynamic extensions.

*** Support for transactional writes                                  :story:

It would be nice if dogen either generated all files or didn't touch
the directory at all, at least as an option. We could simply generate
into a temporary directory and then swap them at the end.

*** Consider using boost pointer container for formatters             :story:

At present we are using a container of shared pointers to house the
different formatter types. These are then encased on a "container"
class. However, in reality we are passing around references to that
container class; it seems we do not need shared pointers at all. We
should look into using a [[http://www.boost.org/doc/libs/1_57_0/libs/ptr_container/doc/ptr_container.html#motivation][boost pointer container]]. We do not have dogen
support for this so we would have to add it first.

*** Remove new lines from all text to be logged                       :story:

We should strive to write to the log one line per "record". This makes
grepping etc much easier. We should create a method to convert new
lines to a marker (say =<new_line>= or whatever we are already doing
for JSON output). This should be applied to all cases where there is a
potential to have new lines (comments, etc).

*** Add unit test benchmarking                                        :story:

#+begin_quote
*Story*: As a dogen developer, I would like to know if any of my
changes impact performance so that I can address these problems early.
#+end_quote

*New understanding*:

Create a set of performance specific tests. These wont get executed by
regular users (e.g. they are not part of =run_all_specs=) but they do
get executed in the build machine. These are selected tests with big
loops (say 1M times) doing things like reading dia diagrams etc. We
could chose a few key things just to give us some metrics around
performance.

In fact, we could create a set of colossi models: models with really
large number of classes (say 500), maybe 5 of these with
references. We could then use the diagrams to test the individual
workflows: dia, dia_to_sml, cpp and engine with no writing. We should
avoid writing files to filesystem to avoid number jitter caused by the
hard drive. There should be no comparisons between actual and expected
for the same reason.

We need to make sure the benchmark tests won't run on valgrind or else
the nightly builds will take over 24 hours. However, if we had it
running on continuous we'd spot regressions on every check-in. But we
don't want to delay continuous any more than necessary. Perhaps we
need a separate build called performance which is also continuous and
only runs these tests. We could pass in some kind of variable to CMake
so that if performance is on, it ignores all tests other than
performance and vice-versa. We'd also need a performance target that
only builds the performance binary, and a =run_performance= target
that executes it.

Perhaps we could use a ruby script to generate the test models?

Also, investigate nonius:

https://github.com/rmartinho/nonius

*Old understanding*:

[[https://svn.boost.org/trac/boost/ticket/7082][Raised ticket]]

- nightly builds should run all unit tests in "benchmarking mode";
- for each test we should find the sweet spot for N repetitions;
- when plugged into ctest, make sure the benchmark tests have
  different names from the main tests otherwise the timing history
  will be nonsense.
- [[http://lists.boost.org/boost-users/2011/01/65790.php][sent]] email to boost users mailing list asking for benchmarking
  support.
- some tips on using chrono to benchmark [[http://www.cookandcommit.eu/2014/11/simple-macro-for-algorithms-time.html][here]].

*** SML json hydrator =read_module_path= needs refactoring            :story:

We have a very misleading function =read_module_path= which reads
modules but also creates modules that do not exist. We need to split
this into a =read= function without side-effects and a =create=
function that creates the missing modules.

*** Clean up of stereotyepes                                          :story:

At present we use the dia stereotypes for two things: a) the
"internal" things like =visitable=, etc and b) concepts, which can be
thought of as "external" as they are defined by users. It would be
nice if we could move one or the other to dynamic extensions to make
things cleaner.

*** Attributes versus properties                                      :story:

At present we have assumed that all attributes in objects should be
generated as properties. This is not quite the right thing to do; one
may actually want to generate a member variable which is not a
property. One solution would be to create a dynamic extension at the
class level that defaults all attributes to properties (or to member
variables). This could be the default for objects but not for
services.

We would have to extend SML to understand member variables as well as
properties.

*** Returning optional of base class results in invalid code          :story:

When defining a model with a type with a field of =boost::optional<x>=
where =x= is an abstract base class, we get compilation errors in test
data. The problem appears to be that our test data factories try to
instantiate =x= rather than go through the abstract base class
machinery. We need to build a test model for this and fix the code.

We should also question if this is a valid scenario - if not we must
add it to the validation rules.

*** Remove references to namespace when within namespace              :story:

Due to moving classes around, we seem to have lots of cases where code
in a namespace (say =sml=) refers to types in that namespace with
qualification (say =sml::qname=). We need to do a grep in each project
to look for instances of a namespace and ensure they are valid.

*** Add support for spaces in template types                          :story:

At present we do not allow any spaces when declaring a type that has
template parameters:

: std::exception::what: Failed to parse string: std::unordered_map<std::string, facet_settings>

We need to look into how to add this to the spirit parsing rules as it
causes a lot of pain.

*** Add support for =std::forward_list=                               :story:

We have been using =std::list= quite liberally. However, on hindsight,
for the vast majority of cases, we don't require a full blown list; a
simple forward list would do. Problem is Dogen does not support
forward lists just yet. We need to add support for these, including
solving the missing boost serialisation problem.

*** Use diagram files to setup test models in cmakefile               :story:

In the CMakeLists for the test models we are already looping through
all the diagrams:

: foreach(dia_model ${all_dia_test_models})

We should take advantage of this to define =include_directories= and
=add_subdirectory=. At present we are doing these manually.

*** Consider using a proper JSON library                              :story:

We could use a full-blow JSON parser rather than the property tree
one. One option is [[https://github.com/cierelabs/json_spirit][json_spirit]].

Another option is [[https://github.com/miloyip/rapidjson][RapidJson]].

*** Persisters only support XML                                       :story:

Persister should support all archive types. At present it always
outputs in XML; it should respect the archive type requested by the
user.

*** Persisters should throw on invalid archive types                  :story:

At present we are checking to see if the archive type is invalid, and
if so ignoring it:

:     if (at == archive_types::invalid)
:        return; // FIXME: should we not throw?
:
:    const auto& dp(create_debug_file_path(at, p));
:    sml::persister persister;
:    persister.persist(m, dp);

We should:

- pass the archive type into persister;
- throw if the archive type is not supported.

*** Generalise persister and remove serialisation helpers             :story:

With the move of the knit persister into each model, it became obvious
that users need a way to hydrate and dehydrate certain types by just
supplying a path. The ideal setup would be where each supported
serialisation mechanism registers a number of extensions with an
hydrator / dehydrator and the user can supply a path; the path gets
dispatched to the correct serialisation. In this world we wouldn't
need the XML serialisation helper (in utilities) because we would code
generate a complete serialisation solution. This only works for files
(and not for streams) because we infer the format from the
extension. Having said that, if there was a way to supply an enum or
such-like with the stream, we could create a class for streams and
then implement the file one as an adaptor to the stream class.

*** All model items traversal should resolve types                    :story:

This traversal was designed for tagger but yet it does not resolve
=type= into one of the sub-classes, forcing tagger to implement
visitation to resolve the types. We should improve the traversal.

*** Version number relies on latest commit in master                  :story:

When trying to build off of a tag, we noticed that the version number
is always of the latest commit in master. This means that trying to
generate packages for tag =v0.50.2410= results in packages with a
version after that like say =v0.50.2415=. We should look at the
current commit in master rather than the latest one.

The current workaround is to manually sett the minor version just
before closing the sprint and then reset it back.

*** Add support for ignoring types                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to ignore certain types I am working
on so that I can evolve my diagram over time, whilst still being able
to commit it.
#+end_quote

Sometimes when changing a diagram it may be useful to set some types
to "ignore", i.e. make dogen pretend they don't exist at all. For
instance one may want to introduce new types one at a time. It would
be nice to have a dynamic extension flag for ignoring.

We should probably have some kind of warning to ensure users are aware
of the types being ignored.

*** Setup containing module correctly in mock factory                 :story:

We did not update the SML mock model factory to populate the
containing type. We also did not setup the members of the module.

*** Consider moving =add_model_module= to flags                       :story:

When we implemented support for =add_model_module= in SML mock factory
we added the flag to all relevant methods. We could have added it to
the flags instead. The downside of this approach is that we have
static factories in specs, so all tests will have the same set of
flags. Still, intuitively it sounds like all tests should have it
either on or off for a given class being tested. Patch:

: @@ -82,7 +82,8 @@ public:
:              const bool resolved = false,
:              const bool concepts_indexed = false,
:              const bool properties_indexed = false,
: -            const bool associations_indexed = false);
: +            const bool associations_indexed = false,
: +            const bool add_model_module = false);
: 
:      public:
:          /**
: @@ -139,6 +140,14 @@ public:
:          void associations_indexed(const bool v);
:          /**@}*/
: 
: +        /**
: +         * @brief If true, adds a module for the model.
: +         */
: +        /**@{*/
: +        bool add_model_module() const;
: +        void add_model_module(const bool v);
: +        /**@}*/
: +
:      private:
:          bool tagged_;
:          bool merged_;
: @@ -146,6 +155,7 @@ public:
:          bool concepts_indexed_;
:          bool properties_indexed_;
:          bool associations_indexed_;
: +        bool add_model_module_;
:      };

*** Allow placing types in the global module in Dia                   :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate certain types in the
global namespace so that I don't have to manually code them.
#+end_quote

At present all types in a Dia diagram are placed in the model
module. However, there may be cases where one may wish to place types
in the global module. At present this is only done in the hardware
model, and that is supplied via JSON. However, we may need to do this
from Dia. Find example use cases for this first.

In terms of implementation, a trait could be added to dia
=dia.use_global_module=. This would force the type to be contained
directly in the global module rather than the model module. If the
trait is used in the model or a package, all types in the containing
scope will inherit it.

*** Make features optional at compile time                            :story:

#+begin_quote
*Story*: As a dogen user, I want to ignore all facets in a model that
I don't need so that I don't have to install unnecessary third-party
dependencies.
#+end_quote

One scenario we haven't accounted for is for compile time
optionality. For example, say we have several serialisation facets,
all of them useful to a general model; however, individual users of
that model may only be interested in one of the several
alternatives. In these cases, users should be able to opt out from
compiling some of the facets and only include those that they are
interested in. This is different from the current optionality we
support in that we allow the user to determine what to code
generate. In this case, the mainline project wants to code generate
all facets, but the users of the model may choose to compile only a
subset of the facets.

To implement this we need a trait - say =optional= - that when set
results in a set of macros that get defined to protect the facet. The
user can then pass in that macro to cmake to disable the facet. This
is not the same as the "feature" macros we use for ODB and EOS. These
are actually not Dogen macros, just hand-crafted macros we put in to
allow users to compile Dogen without support for EOS and ODB.

The macros should follow the standard notation of =MODEL.FACET= or
perhaps =MODEL.FACET.FEATURE=, e.g. =cpp.boost_serialization= to make
the whole of serialisation optional or
=cpp.boost_serialization.main_header= to make the header optional. Not
sure if the latter has any use.

*** Add a configuration class to SML mock factory                     :story:

Every time we need to extend the mock factory we are finding we need
to modify every single function. This is particularly painful due to
the fact we rely on defaults. For example, we can't easily add an
external module path because we need to modify every single method. We
need to look into patterns for this. One option would be to create a
factory configuration class that has the super set of all parameters
required and pass that configuration to each function.

We did add the flags to the constructor, but it would be better if we
could pass in the configuration for each method invocation rather than
for the entire factory.

*** Concepts cannot be placed inside of packages                      :story:

#+begin_quote
*Story*: As a dogen user, I want to place concepts in packages so that
I can scope them when required.
#+end_quote

At present it is not possible to create a concept inside a
package. This is because the concept qualified name is assumed to be
at top-level. In the future it may be useful to use scoping for
concept names in the stereotype. We do not yet have a use case for
this.

*** Re-enable schema updates in database model                        :story:

We are deleting the entire DB schema and re-applying it for every
invocation of the tests. This does not work on a concurrent world. We
commented it out for now, but we need a proper solution for this.

*** Move test model diagrams into main diagrams directory             :story:

For some reason - lost in the mists of time - we decided to split the
test model diagrams from the main models; the first is in the =diagrams=
directory, the latter is in the rather non-obvious location of
=test_data/dia_sml/input/=. All source code goes into =projects=
though, so this seems like a spurious split. Also, the test data
directory should really only have data that we generate as part of
testing (e.g. where there is a pairing of expected and actual) and
the test model diagrams are not of this kind - we never output dia
diagrams, at least at present.

The right thing to do is to move them into the =diagrams=
directory. This is not an easy undertaking because:

- there is hard-coding in the test model sets pointing to these
- the CMake scripts rely on the location of the diagrams to copy them
  across

We should create =production= and =test= sub-directories for diagrams.

*** Include forward declaration in visitable types                    :story:

There doesn't seem to be any good reason to include the full visitor
header in visitable types - we should be able to get away with
including only the forward declaration for the visitor.

*** Investigate the integration of =boost::log= with throw exception  :story:

At present we write a lot of code like this:

: BOOST_LOG_SEV(lg, error) << object_not_found << qn;
: BOOST_THROW_EXCEPTION(indexing_error(object_not_found +
:     boost::lexical_cast<std::string>(qn)));

This is to ensure we log the fact that an exception occurred to make
debugging problems easier. However, it leads to a lot of duplicated
code. We need to figure out a way of simplifying this, most likely
through a macro.

*** Investigate integration of =boost::log= with =boost::test=        :story:

At present whenever there is a test failure, we get a compiler-style
error in the console, which is great for emacs integration - its easy
to go to the source code that generated the failure. However, we do
not write it to the log file of the test. Its very difficult to
understand the log file without the context of the =boost::test=
failures. Due to this we end up manually logging before doing boost
test assertions - a lot of duplicated effort. What would be ideal is
if =boost::test= logged to _both_ the console and to our log
file. There is a file output for boost log, but its not configurable
enough to accept a =boost::log= stream. We should send an email to
mailing list asking for help.

*** Make test data generator more configurable                        :story:

#+begin_quote
*Story*: As a dogen user, I want to configure test data generation so
that I don't have to handle corner cases manually.
#+end_quote

One thing that would be useful is to have a way to attach lambdas to
test data generator. Let =a= be a class with a property =prop= of type
string. It would be nice to be able to do:

: a_generator g;
: g.prop([](const unsigned int seed) {
:     std::ostringstream s;
:     s << "my property " << seed * 10;
:     return s.str();
: });

And so on, for all member variables. The generators would have some
default behaviour, but it could be overridden at any point by the
user. With this, test data generator would be a great starting point
as a way of generating random data for test systems.

See also [[http://www.json-generator.com/][JSON generator]].

*** Consider using a graph in SML for indexing                        :story:

To keep things simple we created a number of specialised indexers,
each performing a complete loop, recursion, etc over the merged
model. A better way of doing things would be to do a DAG of the model
that includes both concepts and objects and then DFS it; at each
vertex we could plug in a set of indexers, each acting on the
vertex. We could also have dependencies between the indexers (for
example concept indexing must take place before property indexing and
so on).

*** Self-contained build files                                        :story:

#+begin_quote
*Story*: As a dogen user, I would like to build models without having
to add any code so that I use dogen without needing to learn lots
about build systems.
#+end_quote

It would be nice to be able to generate a complete application from a
given model, or a library. At present there is an expectation that the
user will slot in the generated CMake files into a larger, more
comprehensive CMake build. All we need is:

- some kind of binary type: e.g. executable or library. we should have
  this anyway. meta data at the model level could be used to convey
  this.
- if executable, we should automatically ignore a main.cpp in the
  source directory.
- generate a stand-alone CMake template.

The idea is that with this the user could immediately generate a
binary without any further configuration required.

*** Caching qname lookups                                             :story:

Once the model has been merged and resolved, all qnames in the model
all known to resolve to a valid type, model or module. This means we
could cache in the qname itself a pointer to the object the qname
resolves into. There are two problems with this approach:

- we do not have a base class that covers types, models and
  module. one could be created (=modeling_entity=?) with an associated
  visitor. but then:
- formatters are not designed to think at the =modeling_entity= level;
  a formatter that does types may not necessarily be able to do
  modules or models. Thus we would need to convert from a
  =modeling_entity= to a type, model or module before we get to the
  formatter.

However one imagines that a great number of lookups would be avoided
if this was possible.

*** Forward declaration is not always correct for services            :story:

In cases where we used a service as a way of declaring a stand alone
function (such as the traversals in SML), the forward declarations do
not match the header file at all. In this cases we should use
=nongeneratable= rather than =service= stereotypes, and perhaps when
that happens we should switch off forward declarations?

In addition, in some cases we may want to use a =struct= rather than a
=class=. At present we are always forward declaring as =class= but
sometimes declaring as =struct=.

*** Add tests for tagging of modules, primitves and enumerations      :story:

We've tested abstract objects et al quite a lot but forgotten the
other aspects of the model.

*** Add tests for all permutations of the domain formatter            :story:

_All_ may be too strong a word as there quite a few. We need good
coverage around the combinations one can do within the domain
formatter.

*** Formatters should cache qname formatting                          :story:

We seem to re-format the same qname lots of times. We should just use
a =std::ostringstream= to format once and reuse the resulting
string. Probably worth doing this change after the performance tests
are in.

*** Consider model as a container of types                            :story:

At present model is composed of objects, primitives, concepts,
modules, etc. We could bring together all descendants of types into a
single container (e.g. types). However, in places we do thinks like
looking at the primitive container to see if the container has any
primitive types - these would become slower as we'd now be looking at
the entire type collection. Need to look at all usages of these
containers in the code to see if this would be a win or not.

*** Split a fully formed model from partial models                    :story:

We should really have two distinct types to represent the model that
is returned from the dia to sml transformer from the model returned by
the merger. Potentially this could be called =partial_model=.

*** Rename nested qname to composite qname                            :story:

*New understanding*:

This story requires further analysis. Blindly following the composite
pattern was tried but it resulted in a lot of inconsistencies because
we then had to follow MEC-33 and create =abstract_qname=; however, the
nested qname does not really behave like a composite qname; its more
like the difference between a type in isolation and a type
instantiated as an argument of a function. For example, whilst the
type in isolation may have unknown template parameters, presumably, as
an argument of a function these have been instantiated with real
types.

*Previous understanding*:

We should just follow the composite pattern in the naming.

*** Injection framework                                                :epic:

We need a more generic way of handling system types injection into
models. This is because there is a number of things that can be
derived from the existing model types:

- keys
- diff support
- reflection
- cache code
- etc.

So we need to:

- make injector a composite of injectors that do the real work such as
  =key_injector=. internally =injector= just delegates the work to
  these classes.
- injector decides which internal injectors to use based on options
  passed in.
- in the IoC spirit, we should probably create a =injector_interface=.

*** Check concept properties for identity                             :story:

When we added concepts we didn't had a link to the processing of
identity attributes. This means that if we get a property via modeling
a concept it is not processed and added to the keys.

Update injector to follow concepts.

*** SML models could have a model classification                       :epic:

Consider creating an enumeration for model classification (e.g. type
of the model):

- relational model
- core domain model
- generic sub-domain model
- segregated core model

This still requires a lot of analysis work around the DDD book.

*Merged with modes of operation story:*

Create "modes" of operation: relational, object-oriented and
procedural. they limit the types available in SML. relational only
allows primitives plus relational commands (FK and PK; FK is when
using a model type, PK is a marker on a property). procedural only
allows primitives plus model types. we will need pointer support for
this. object oriented is the current mode. the modes are validated in
the middle end.

*** Add support for boost and/or std tuple                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of tuples in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

It would be nice to be able to use =std::tuple= and/or =boost::tuple=
from dogen. The processing would be rather similar to containers. It
would be even nicer if one could associate an enumeration to a tuple
so that the gets would be more meaningful, e.g.:

: std::get<my_field>()

rather than

: std::get<0>()

Using =std::tuple= would mean we'd have to create our own serialisers
for it most likely.

*** Add support for posix_time_zone                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost posix_time_zone
so that I don't have to manually generate code for types that use it.
#+end_quote

At present we need to use std::string to convey time zone
information. We should be able to use the time zones available in
boost date time library.

See boost documentation: [[http://www.boost.org/doc/libs/1_53_0/doc/html/date_time/local_time.html#date_time.local_time.posix_time_zone][Posix Time Zone]]

*** Build shared objects instead of dynamic libraries                 :story:

#+begin_quote
*Story*: As a dogen user, I want to generate models as shared objects
so that I don't have to statically link models all the time.
#+end_quote

With the increase in tests build speeds have started to suffer,
especially on low hardware. One potential way to mitigate this is to
avoid unnecessary linking. The problem we have at present is that
every time something changes in any model we have to relink all the
binaries that use that model as it is consumed as a static library. If
all the static libraries were converted to shared objects this would
no longer be necessary.

We probably need a dogen command line option to determine what to
build so that users are not forced to always build static / shared
libraries. We should make sure one of the tests is using a static
library to make sure this scenario doesn't get borked.

*** IoC work                                                           :epic:

All stories related to IoC work are tracked here.

*New Understanding*:

in reality, there is really only one place where IoC makes sense: in
the workflows. It would be great if one could pass in something akin
to a IoC container into the workflow's constructor and then use the
container to obtain access to all services via interfaces. Using
sml::workflow as an example, one could have:

- container_interface which returns grapher_interface,
  processor_interface, etc.
- the container could even return references to the these interfaces
  and own the lifetime of the objects.
- this would then allow us to provide mock container interface
  implementations returning mock services.

However:

- it seems like a lot of moving parts just to allow testing the
  workflow in isolation. this is particularly more so in the case of
  the workflows we have, which are fairly trivial. perhaps we should
  consider this approach when dogen is generating the interfaces
  automatically as this would require a lot of manual work for little
  gain.

*Old understanding*:

- add workflow_interface to SML.
- we should be doing a bit more IoC, particularly with inclusion
  manager, location manager etc. In order to do so we could define
  interfaces for these classes and provide mocks for the tests. This
  would make the tests considerably smaller.

*** Refactor node according to composite pattern in dia to sml        :story:

This is not required if we decide to [[*Add%20composite%20stereotype][implement]] the composite
pattern. We should just follow the composite pattern.

*** Create an interface for the text reader                           :story:

In order to do performance testing of the dia model we should create
an interface for text reader and implement it as a mock. This will
avoid the overhead of reading stuff from the hard drive.

*** Use dogen models to test dogen                                    :story:

We should really use the dogen models in the dogen unit tests. The
rationale is as follows:

- if somebody changes a diagram but forgets to code generate, we want
  the build to break;
- if somebody changes the code generator but forgets to regenerate all
  the dogen models and verify that the code generator still works, we
  want the build to break.

This will cause some inconvenience during development because it will
mean that some tests will fail until a feature is finished (or that
the developer will have to continuously rebase the dogen models), but
the advantages are important.

*** Container details in JSON dump                                    :story:

#+begin_quote
*Story*: As a dogen user, I would like to know how many elements
containers have so that I don't have to count it manually.
#+end_quote

It would be nice to have the container type and size in the JSON
output.

*** Add test to check if we are writing when file contents haven't changed :story:

We broke the code that detected changes and did not notice because we
don't have any changes around it. A simple test would be to generate
code for a test model, read the timestamp of a file (or even all
files), then regenerate the model and compare the timestamps. If there
are changes, the test would fail.

*** Adding new engine spec tests is hard                              :story:

In order to test models at the engine level one needs to first
generate the dia input. This can be done as follows:

: ./dogen_knitter --save-dia-model xml --stop-after-merging
: -t ../../../../dogen/test_data/dia_sml/input/boost_model.dia

From the bin directory. We need to make these steps a bit more
obvious. Why do we even need this?

*** Support for cmake components and groups                           :story:

#+begin_quote
*Story*: As a dogen user, I need to integrate the generated models
with my existing packaging code.
#+end_quote

We recently added support for creating multiple packages from a single
source tree. We need generated models to have a new top-level cmake file:

: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/src)
: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/spec)
:
: install(
:     DIRECTORY include/
:     DESTINATION include
:     COMPONENT headers
:     FILES_MATCHING PATTERN "*.hpp")

And the =src= cmake file:

: install(TARGETS dia ARCHIVE DESTINATION lib COMPONENT libraries)

*** Add test model for disabling XML                                  :story:

At present we are not testing model generation with XML disabled.

*** Format doubles, floats and bools properly                         :story:

At present we are using IO state savers but not actually setting the
formatting on the stream depending on the primitive type.

Ideally we should pass in some dynamic extensions to determine the
formatting. We should also consider using =boost::format= for this.

*** Check if we've replaced =assert_object= with =assert_file=        :story:

Assert file is now able to do intelligent comparisons based on the
extension of the file. From a cursory look, all the usages we have of
assert object can be replaced by assert file. If that's the case we
can also remove this function.

*** Add tests for disconnected connections                            :story:

We should throw if a diagram has a disconnected inheritance or
composition relationship.

At present the error message for an inheritance object in dia which
has less than two connections is less than helpful:

: 2013-06-26 22:58:50.236488 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2013-06-26 22:58:50.236917 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dia_to_sml::processing_error>
: std::exception::what: Expected 2 connections but found: 1
: [tag_workflow*] = Code generation failure.

We should really try to detail which object ID failed, as well as
details of the connected object if possible, etc.

*** Add tests for duplicate identifiers in Dia                        :story:

Detect if a diagram defines the same class or package multiple
times. Should throw an exception. We should also detect multiple
properties with the same name.

*** Test model sanity checks fail for enable facet serialisation      :story:

For some reason we are unable to compile the serialisation test for
the test model which focuses only on the serialisation facet. Test is
ignored for the moment.

*** Handle unnamed models properly                                    :story:

The option disable model name was meant to allow the generation of
flat models, without any folders or namespaces for the model
name. However, as a side-effect, this also means the artefacts being
generated do not have any names. This resulted in the creation of a
libSTATIC, purely because the next command in the cmake add_library is
STATIC (e.g. static library). As a quick hack, when an empty model
name is detected, a model named "unnamed_model" is created.

The correct solution for this is to have a flag (or flags) at the SML
level which state whether to use the model name for folders, packages,
etc. The view model generation will then take this into account.

*** Missing =enable_facet_XYZ= tests                                  :story:

- test data

*** Create model with invalid primitive type                          :story:

At present we are validating that all primitive types work but we
don't check that an invalid type doesn't work.

*** Private properties should be ignored                              :story:

At present we treat private properties as if they were public; we
should ignore them. We need to go through all the models and change
the private ones to public before we do this.

We should also log a warning.

*** Generator usage in template tests needs to be cleaned             :story:

At present some template tests in =utility/test= ask for a
generator, other for instances. We should only have one way of doing
this. We should probably always ask for generators as this means less
boiler plate code in tests. It does mean a fixed dependency on
generators.

*** Replace old style for iterations in IO                            :story:

At present we are still doing C++-03 iterations in the STL IO files
such as =vector_io=, =list_io=, etc. We should be using the new =for=
syntax for C++-11.

*** Investigate GDB visualisers for generated models                  :story:

#+begin_quote
*Story*: As a dogen user, I would like pretty-printing for my types in
GDB so that I debug more easily programs using dogen models.
#+end_quote

It would be great if the code generator created GDB visualisers for
the types in a generated models such that one could inspect values of
STL containers with types of that model.

- [[http://sourceware.org/gdb/onlinedocs/gdb/Pretty-Printing.html][Pretty printing]]
- [[https://github.com/ruediger/Boost-Pretty-Printer][Boost pretty printer]]
- [[https://groups.google.com/group/boost-list/browse_thread/thread/ff232ac626bf41cf/18fbf516ceb091da?pli%3D1][Example for multi-index]]

*** Add an includer for all includers                                 :story:

#+begin_quote
*Story*: As a dogen user, I need a quick and dirty way of including
all files in a model so that I can test them without having to
include every file manually.
#+end_quote

It would be nice to totally include a model. For that we need an
includer that includes all other includers. This should be as easy as
keeping track of the different includers for each facet in the map
inside of the includer service.

We need to find a good use case for this.

*** Add new equivalence operator to domain types                      :story:

#+begin_quote
*Story*: As a dogen user, I would like to know if two objects are
equal ignoring the version properties so that I can model my domain
more accurately.
#+end_quote

We should have an operator that compares the state of two objects
ignoring the version.

*** Add run spec targets for each test                                :story:

We could piggy back on the ctest functionality and add a target for
each test so one could =make enable_facet_domain= and =make
run_enable_facet_domain=. The targets need to be prefixed with module
name and test suite.

*** Clean up WinSock definition in CMakeLists                         :story:

We did a crude implementation of finding WinSock just to get windows
to build. There should be a FindWinSock somewhere. If not create one.

Do we need this anymore? we probably need it for linking the database
model, but we should check - maybe ODB has some magic around this.

*** Tests for error conditions in libxml                              :story:

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.

*** Check that custom targets in CMake have correct dependencies      :story:

At present we have a number of custom targets, which create a new Make
target. These are good because they do not require re-running CMake to
manage the files in the output directory; however, we do not have the
correct dependencies between the targets and the target
dependencies. For example, create_scripts should check to see if any
script has changed before re-generating the tarball; it seems to have
no dependencies so it will always regenerate the tarball. We need to:

- check all custom targets and see what their current behaviour is:
  a) change a dependency and rebuild the target and see if the
  change is picked up or not; b) change no dependencies and re-run the
  target and ensure that nothing happens.
- add dependencies as required.

*** Enable doxygen warnings for all undocumented code                 :story:

At present doxygen only warns about undocumented parameters when a
function already has documented parameters. We should consider
enabling warnings for all undocumented code. We also need to figure
out how to mark code as ignored (for example serialisation helpers,
etc won't require documentation).

*** Add specification comments to tests                               :story:

We started off by adding a technical specification as a doxygen
comment for a test but forgot to keep on doing it. Example:

: /**
:  * @brief It shall not be possible to create more terms than those
:  * supported by a finite sequence, using std::generate_n.
:  */

This helps make the purpose of the test clearer when the name is not
sufficient.

** In next major release

These stories are good candidates for the subsequent release. This
release will be all about adding new features.

*** Implement stitch with merging                                     :story:

When we have merging support we can actually implement stitch in a
good way. We could mark the formatters with meta-data stating they
have a method which is a stitch template. That then results in a c++
operation which makes reference to stitch. When converting SML into
the CPP model we can add a "content" property to the stitchable
operations that contains the expansion of the stitch
template. Finally, when inside of the stitch template that is
code-generating, we will loop through all of the operations. For the
stitched ones, we can then dump the content property. Merge support is
required because formatters are always expected to have a component of
hand-crafting (include providers etc) and cannot be implemented
without it.

*** Allow cross model inheritance                                     :story:

#+begin_quote
*Story*: As a dogen user, I want to inherit types from existing models
so that I can extend them.
#+end_quote

At present we can only inherit within the same model. This is a
limitation of how to express inheritance in a Dia diagram - either the
parent is part of that diagram or it is not, and if it's not we have
no way of connecting the generalisation relationship to it.

Having said that, it would actually be quite simple to allow cross
model inheritance by using dynamic extensions:

- create a field that forces a type to behave like a parent,
  regardless of whether there are any children or not;
- create a field that contains a qualified name of a parent,
  regardless of whether it's in this model or not;
- change the transformer to convert these fields into SML inheritance
  relationships;

There may be some fallout in places where we assume that the
descendents are all in this model such as serialisation, visitors.

*** Add support for inner classes                                     :story:

Inner classes could be expressed in the same way as the short-hand for
namespaces. For example, given a class =a=, an inner class =b= could
be declared as =a::b=. The system would have to recognise that =a= is
a class and then treat it accordingly in SML. The formatters would
have to be taught to express inner classes when formatting the main
class. This probably requires merging two SML entities into a single
cpp entity. Finally some dynamic extension support would be required
to determine if the inner class is public or private.

We just need a use case for where inner classes would be useful.

*** Add support for user defined final types                          :story:

At some point we started talking about the possibility of having types
defined as final by the user, via dynamic extensions. This was to be
done using =dia.is_final=. We never did implement it properly.

- define attribute in dia to sml
- propagate it to sml

*** Consider representing namespaces in file names rather than directories :story:

Languages like .Net represent namespacing using dots rather than
separate folders. Perhaps we should support a mode of operation where
all files are placed in a single folder but have the namespacing
encoded in the file name. For example:

: /a_project/types/a.cpp
: /a_project/io/a_io.cpp

would become:

: /a_project/types_a.cpp
: /a_project/io_a_io.cpp

or, using dot notation, so we can distinguish namespaces from
"composite" names:

: /a_project/types.a.cpp
: /a_project/io.a_io.cpp

*** Allow manual overrides to facets                                  :story:

#+begin_quote
*Story*: As a dogen user, I sometimes want to provide my own
implementation for a given facet.
#+end_quote

Sometimes it may make sense to provide a user-supplied implementation
for a given facet. For example for qname it may make sense to use the
string converter approach to do the actual JSON serialisation. However
its not possible to just disable code generation of a given type's
inserters and then provide a manual implementation. This could easily
be achieved via dynamic extensions.

*** Consider adding support for inline hashing                         :epic:

At present we have an entire facet for hashing. However, it is
conceivable that some users may find it a bit of an overkill and would
rather have it added to the types facet instead. We already do
something similar for the =operator<<=. We need to consider a more
generic mechanism for allowing the inlining of certain features when
they are more core.

In fact, one wonders if we are not getting towards a multi-option
implementation for certain things:

- as a facet;
- as a formatter;
- as a toggleable aspect of an existing formatter.

Where the user gets to choose one of the three possible
implementations. In some cases we may not support all of these
options; for example, for operators we could support the second and
the third option and so on.

See also Support "cross-facet interference".

*** Control the emission of pragma once with dynamic extensions       :story:

At present we are always adding =#pragrma once= to the header guard:

: #if defined(_MSC_VER) && (_MSC_VER >= 1200)
: #pragma once
: #endif

This should really be optional and controlled via dynamic extensions,
probably at the c++ model level.

*** Consider making header guards configurable                        :story:

It may be nice to be able to switch parts of the header guards off (do
not use namespaces, etc) or even to have a completely different policy
to generate header guards. We do not have a use case for this yet, but
this story is a place-holder for it.

*** Adding types to package namespace                                 :story:

Whilst it is possible to document a namespace, it is not possible to
add any classes etc to that namespace. For example, it may make sense
to add some constants at the namespace level. This is not possible
with the current setup.

*** Code generation of registrars for static registration             :story:

We are using a lot of static registration and we have converged into
what appears to be a useful pattern. It would be nice to be able to
mark a type as registrable and to have the registrar automatically
generated for it, with a name configurable via dynamic extensions. We
would also need to configure the registration method (name,
arguments - we may want to register against a string or just have a
list of registered types).

The next logical step would be to code-generate the static
initialisers too. For this we would have to be aware of all types to
register in a given model (perhaps by looking at inheritance across
models) and for each of these generate the appropriate initialiser
code. This is more tricky but it would be really useful.

*** Serialisable and ioable exceptions                                 :epic:

#+begin_quote
*Story*: As a dogen user, I want to send exceptions across the wire so
that I can report errors to remote users. I also want to dump
exceptions to the log file.
#+end_quote

At present we only generate the types facet for exceptions. However,
there is nothing stopping us from adding serialisation support for
exceptions. This would be useful for example for services to convey
errors on the remote end point. The same logic applies to io.

*** Load system models intelligently                                   :epic:

#+begin_quote
*Story*: As a dogen user, I want to load only the system models
required for the model I want to generate so that generation is as
quick as possible.
#+end_quote

At present we are loading all library models. This is not a problem
because they are small and there are only a few of them. However, in a
distant future, one can imagine a very large number of system models,
each of which with large number of types (say the C# system models,
the C++ system models, etc). In this world we may need to disable the
loading of some system models: either by programming language or more
explicitly by choosing individual models in a given language.

It may even make more sense to load just what is required: load the
target model, infer all of its dependencies (including at the
programming language level) and then load only the system models that
are required for those languages.

This may not be as hard as it seems: we already infer that all models
the target depends on are present by looking at the list of distinct
model names required by the target qualified names. We could use the
same logic to determine what system models to load. The only exception
is the hardware model, which must always be loaded (or we need some
kind of mapping between "empty" model name and the hardware model).

*** Make JSON SML a fully supported frontend                          :story:

#+begin_quote
*Story*: As a dogen user, I want to be able to write my domain models
in JSON since I don't have any need for UML visualisation.
#+end_quote

At present we are using an SML JSON format to supply Dogen the system
libraries. However, there is nothing stopping us from having a
full-blown JSON frontend useful for code generation. For this we need:

- flag to state if its a target model or not;
- ability to supply external module path;
- ability to supply all of the missing information for SML types
  (properties for object, stereotypes, enumerations, etc).

In order to test this we could generate a model from both Dia and JSON
and make sure we arrive at the same SML.

*** Consider adding facet specific types                               :epic:

#+begin_quote
*Story*: As a dogen user, I want to code-generate simple types for
facets other than =types= so that I don't have to create them manually.
#+end_quote

Types in dogen are somewhat "uni-dimensional"; that is, the main focus
of all work is types and the other facets are thought to either be
code generated in total (serialisation, hashing, etc) or manually
generated in total (test for mock factories). However, in some cases
it may make sense to add a type directly to a facet. For example, we
may want to add simple value objects to the mock factory. We don't
want to pollute =types= with these classes, but at the same time we'd
rather not have to manually generate them. It would be nice to be able
to associate a type with just a facet via dynamic extensions. Of
course, this does mean we would not be able to rely on all other
facets such as serialisation and even streaming or else things would
get a bit confusing. But it would still be useful.

Another possible (less clean) approach is this:

#+begin_quote
It would be great if we could use dynamic extensions to enable and
disable facets (there probably already is a story for this). But in
addition to this, it would also be great if one could override the
default name for an object in a facet; for instance: one could add an
object called =serialization_manager=, disable all facets bar
serialisation, disable the serialisation postfix of this file and
disable code generation. This way one could add manual code to any of
the facets, independently.

At present we support this, but only for types as it is hard-coded.
#+end_quote

*** Add support for deprecation                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to mark certain properties, classes
or methods as deprecated so that I can tell my users to stop using
them.
#+end_quote

We should be able to mark classes and properties as deprecated and
have that reflected in both doxygen and C++-11 deprecated attributes.

Note that at present nothing stops the users from adding the marker
themselves.

Perhaps we should add general support for attributes. This would be
useful for languages like C# and Java, to control serialisation, etc.

*** Control JSON output via traits                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the output JSON so that
I get exactly what the external users are looking for.
#+end_quote

Once we add support for JSON we will face the same sort of problems
that Json.net has already solved: we may want to have keys that do not
match the property names (for instance we may want to use human
readable names in the json), we may want to translate enumerations to
numbers or to human readable descriptions, we may want to collapse a
class into some less verbose JSON, etc. Some of these are describable
via traits, very much like Json.Net uses C# attributes. We should look
into the available attributes and see if they make sense as dogen
traits to control JSON. Some of these may have wider application and
be used to control other serialisation formats.

*** Add support for JSON serialisation                                :story:

We should have proper JSON serialisation support, for both reading and
writing. We can then implement IO in terms of JSON.

*Raw JSON vs cooked JSON*

If we do implement customisable JSON serialisation, we should still
use the raw format in streaming. We need a way to disable the cooked
JSON internally. We should also re-implement streaming in terms of
this JSON mode.

*** Add targets to output manual in downloadable formats              :story:

#+begin_quote
*Story*: As a dogen user, I would like to read the manual offline and
have the ability to print them.
#+end_quote

We should build HTML and PDF representations of the manual.

*** Include groups                                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to create includers for user defined
groups of files so that I don't have to do it manually.
#+end_quote

One of my personal preferences has always been to group includes by
"library". Normally first come the C includes, then the standard
library ones, then boost, then utilities and finally types of the same
model. Each of these can be thought of as a group. Inside each group
the file names are normally ordered by size, smallest first. It would
be nice to have support for such a feature in Dogen.

Formatters would then push their includes into the correct
group. Group names could be the model name (=std=, etc).

A bit of a nitpick but nice nonetheless.

*** Special purpose formatters                                         :epic:

In the future, when the creation of formatters is made easier, we may
start designing formatters that are totally a application specific and
may not have any particular use for any other application. They should
be accepted in mainline Dogen:

- to make sure we don't break this code;
- to allow other people to copy and paste to generate their own
  formatters;
- because sometimes what one thinks is special purpose actually much
  more general.

However, we need to make sure we don't start cluttering the code base
with these formatters. We will also have to start to worry about
things like defining stable interfaces:

- at which point do we decide that some code has bitrot and
  deprecated, so will have to be removed?
- what happens when a formatter moves from version 1 to version 2 of
  some dependent library, must we create a version 1 and version 2
  formatter or just update the existing one? what if it breaks code
  for people using version 1 that do not wish to move to version 2?
- do we mandate compilation tests for all formatters? This would mean
  our build machine would be full of third-party libraries (some
  potentially not available in Debian), and quite hard to
  maintain. Alternatively we could mandate that if you have a
  formatter you must setup a CTest agent with a compilation for that
  formatter and publish the results of the build to dashboard; if your
  build becomes consistently red we are allowed to remove the
  formatter.
- for the diff tests, is it acceptable if someone refactors the code?
  Once "your" formatter is merged in it is now owned by the community
  and it is entirely possible that someone will improve it/extend it,
  etc. In order for this to work they need to be very sure they have
  not broken the original use case.

We probably just need to setup a very simple policy to start off with,
but its best to keep track of these potential pitfalls.

Merged with this story:

*Private formatters*

We should look into code we do in dogen that is highly repetitive and
create "private formatters" for it. For example, field definitions are
more or less exclusive to dogen so it doesn't make it any sense to add
it to the "public" side of dogen; but it would be nice to create a
formatter to generate them so that we don't have to do it
manually. For these "private formatters" we would need to load a SO
with them into a dogen binary.

*** Investigate the possibility of creating a mock facet              :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate mocks for interfaces
so that I don't have to generate this code manually.
#+end_quote

This is straight out of left-field, but may actually be a good
idea. One annoying thing with mocking frameworks such as [[http://turtle.sourceforge.net/index.html][turtle]] is
the amount of macros. However, =dogen= already has all the required
information needed to create an expectation based mock - the
meta-model. We could mimic the turtle API with a mock facet that is
made up of real C++ objects. When a class is marked as an interface,
we could automatically generate its mock in a mock facet.

This will require proper operations support.

*** Add support for user defined literals                             :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of literals so that I can
make my code more type safe.
#+end_quote

With user defined literals in C++11, defining one's own numeric types
became more convenient. We should look into adding support for this in
dogen.

See [[http://www.codeproject.com/Articles/447922/Application-of-Cplusplus11-User-Defined-Literals-t][Application of C++11 User-Defined Literals to Handling Scientific
Quantities, Number Representation and String Manipulation]]

*** Types that share one file                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to generate a single file for a
number of related classes so that I don't have to deal with lots of
files when they are not needed.
#+end_quote

At present we force all types etc to have their own file. However, in
cases it may be useful to have multiple types sharing the same
file. For instance, one may want to have all enumerations in one file,
or all exceptions, etc.

We could implement this using dynamic extensions.

*** Add export macros support                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to export types selectively so that I
can control what external users can depend on.
#+end_quote

We should add export macros for shared objects/DLLs:

: #ifdef ExportDeclaration
:    #undef ExportDeclaration
: #endif
:
: #ifdef
:    #define ExportMacro __declspec(dllexport)
: #else
:    #define ExportMacro __declspec(dllimport)
: #endif

There is also a GCC equivalent explained [[http://pic.dhe.ibm.com/infocenter/tpfhelp/current/index.jsp?topic%3D%252Fcom.ibm.ztpf-ztpfdf.doc_put.cur%252Fgtpl2%252Fexport.html][here]].

We should have some dynamic extensions to control the outputting of
these.

*** Create a visitor interface with multiple implementations          :story:

#+begin_quote
*Story*: As a dogen user, I need multiple visitor interfaces so that I
can decide which one is best for my requirements.
#+end_quote

We decided to use a base class for visitor; it would have been better
to create an interface, with multiple implementations:

- negative visitor: any unimplemented methods throw
- default visitor: all methods do nothing
- [[*Visitor%20with%20%3Dstd::function%3D%20for%20each%20%3Dvisit%3D%20method][std::function visitor]]
- ...

Users can then inherit from these visitors where appropriate
(e.g. negative and default visitors).

*** Disable =invalid= value in enumerations                           :story:

#+begin_quote
*Story*: As a dogen user, I may not want to allow invalid values in
enumerations because they do not model my problem domain accurately.
#+end_quote

At present all enumerations must have an invalid value. One can
conceive cases where that is not a useful thing. We should have a
dynamic extension flag that disables it.

*** Bitmask enumeration                                               :story:

#+begin_quote
*Story*: As a dogen user, I want to define a bitmask enumeration in
dogen so that I don't have to create it manually.
#+end_quote

We should have a dynamic extension flag that generates enumerators
with values that are powers of two. These can then be used for flags,
as per the [[*Add%20support%20for%20bitsets][bitset story]].

*** Replace Boolean attributes with flags                             :story:

We have a number of Boolean attributes in SML which could easily be
replaced by a single int and a flag enumeration. We would also need a
set of utility methods to access the values.

This story has a dependency on [[*Add%20support%20for%20bitsets][bitset support]].

*** Support for file level comments via dynamic extensions            :story:

#+begin_quote
*Story*: As a dogen user, I may want to add comments at the file level
so that I can provide documentation to the model users.
#+end_quote

We could easily have a tag for file level comments and transport that
all the way to the output. The only problem is that it would be a one
liner only so it may not be that useful.

Multi-line support could be simulated by concatenating multiple
entries - cumbersome but workable...

*** Models should have an associated language                          :epic:

#+begin_quote
*Story*: As a dogen user, I want to make sure I only use valid system
models so that I don't generate models that code generate but do not
compile.
#+end_quote

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as dynamic extensions in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in SML
which is a property of the model, and one entry of which is "language
agnostic".

*** Add getter and setter prefixes                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to change the default getter and
setter conventions so that I can integrate my code with
dogen-generated code.
#+end_quote

External users may have getter and setter prefix conventions such as
=set_prop= or =SetProp=. It would be nice if we could pass in a
getter/setting prefix and then dogen would append them when converting
the diagram, e.g. =--getter-prefix=set_=.

We should check what ODB has done for this and implement the same
pattern.

*** Add support for qualified class names in dia                      :story:

#+begin_quote
*Story*: As a dogen user, I don't want to have to define packages in
certain cases.
#+end_quote

It has become apparent that creating large packages in dia and placing
all classes in a large package is cumbersome:

- there are issues with the large package implementation in dia,
  making copying and pasting a dark art; its not very obvious how one
  copies into a package (e.g. populating the child node id correctly).
- models do not always have a neat division between packages; in
  dogen, where packages would be useful, there are all sorts of
  connections (e.g. inheritance, association) between the package and
  the model "package" or other packages. Thus is very difficult to
  produce a representative diagram.

A solution to this problem would be to support qualified names in
class names; these would be interpreted as being part of the current
model. One would still have to define a large package, but it could be
empty, or contain only the types which only have connections inside
the package, plus comments for the package, etc.

*** Consider adding merging code generation support                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to manually change some code in
generated files so that I can add functionality that is missing in
dogen.
#+end_quote

At present it is not possible to manually add methods to a class that
was code generated; one must stop code generating the class and
maintain the whole class manually. However, in some cases it makes
sense to have a combination of both:

- value objects need helper methods such as for example boolean
  properties (e.g. =is_empty=) that make use of other properties, or
  simple methods such as population etc that really belong to the
  object rather than an external service
- services sometimes need state and it would be good if we could
  manage that via code generation.

For this we need a merging code generator: that is, a code generator
that is aware of code that was crafted manually and does not overwrite
it - but instead "intelligently" merges manual with code generated
code.

From the beginning we avoided this because we thought it would be too
complicated for dogen. However, its increasingly becoming apparent
that this is a needed feature for the real world - there are many
cases where we are working around this deficiency. A few solutions are
possible:

- let the code generator manage the header file and create two types
  of CPP files, one which includes the other: a manual and an
  "automatic" one. This would effectively separate the two types of
  code. For this dogen would have to be able to generate complex types
  in operations (e.g. we'd have to solve the lack of support for
  =const std::string&=).
- use clang to do the merging. this probably means adding some kind of
  attribute to every method - possibly using C++ attribute support
  (e.g. =[ [generated ] ]= and/or =[ [ manual ] ]= (spaces due to org
  mode). We could then say to clang: read current state of the file,
  grab every non-generated method and copy them across to the newly
  code generated file. Merging could be the final stage before
  writing. In addition, we should also have some dynamic extensions to
  determine which files require merging. The dynamic extension could
  be populated automatically (e.g. grep for the manual attribute) or
  manually. Note that using clang to do merging will make things a lot
  slower.

*Random Thoughts*

- include management with be a mix of manual versus automatically
  generated. This is not possible because there will be no way to
  determine which one is which. To solve this problem we need to allow
  users to add include files from the dynamic extensions and get those
  processed like all the other includes. In the new world this means
  adding includes to the formatter settings. These are local
  settings. As at present, we cannot identify a use case for adding an
  include file for all types, there is no need to support this feature
  at the global settings level, so this fits nicely with the existing
  settings infrastructure.
- merging could be done without needing clang, which would also make
  it cross language. All that is required is for the language to
  support some kind of meta-data to mark a method as "manually
  generated". This could even be done using comments but this is not
  ideal. The process would then be: dogen would open up an existing
  source file and locate the attribute; then look for a open brackets
  to indicate the start of the method (={=) and then find the matching
  close brackets (=}=). We could keep a counter and increment it when
  a new open bracket is found and decrement it when a close bracket is
  found. When its zero we are done. All the code from the attribute to
  the close brackets would be lifted. A very simple regex matching
  would be done to find the method name - or perhaps some trivial
  parsing could be done, but it should be kept as simple as
  possible. The objective is simply to figure out the method name. The
  method is copied across and stored in the =cpp= model, in the
  correct method. When code generating, if a method is marked as
  "manually generated" and if there is implementation content, we dump
  that; otherwise we generate the skeleton of the method as if it was
  not "manually generated".
- merging could be done as part of SML, in meta-data. That is, we
  could annotate the merged method into language specific properties
  in the meta-data and then query those in the language specific model
  generation. We could have another SML workflow to look for files; it
  could use the meta-data for file path. The extension will tell it
  what "function parser" to use. We could literally look through the
  meta-data extensions looking for file path, and for each run the
  "function parser"; it will return a set of "manual" functions. These
  we can then slot into the meta-data and reuse later on.

*** Visitor adaptor for usage in ranges                               :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of visitor in ranges so
that I generate code that follows the modern C++ approach.
#+end_quote

It would be great if we automatically generated an adaptor to visitors
which could be plugged into a range. Internally the adaptor would
perform the accept on its =operator()=. We could also have an adaptor
for a =std::pair= which would be templatised on the first member of
the pair. Or should one just use a keys or values range iterator.

*** Visitor with =std::function= for each =visit= method              :story:

#+begin_quote
*Story*: As a dogen user, I want an extensible visitor so that I don't
have to manually generate one.
#+end_quote

It would be nice if the code generator created a visitor which has as
its properties a set of =std::function= which match the signature of
the visit functions; then the visit functions would just check that
the functions have been assigned and call them. If not, throw.

*** Use pimpl for a few "one-shot" services                           :story:

We have quite a few services where it would be great to have
transactional semantics. For example, when building a graph in
=sml::grapher=, it would be great if one could have a list of objects
to graph as an input and some kind of =grapher_result= as the
output. From a potential =grapher_interface= it would look like a
simple method in the interface, almost static. The problem with this
approach of course is that it makes the =grapher_interface=
implementations cumbersome because one has to pass all parameters to
all internal methods instead of using class state. The present
approach is to make it a "prepare" and then "use" sort of service,
causing the usual nonsensical methods of "is it finished yet" and "are
you trying to use the service a second time" (e.g. =is_built=,
etc). Even if we pass in all the inputs in the constructor, its still
not ideal. There are two options:

- set member variables inside the "one-shot" function and then unset
  them at the end;
- have a =grapher= implementation which uses a =grapher_impl= that
  does provide a sensible implementation. We used to do this inside
  the =.cpp= files but then they became too big to manage.

*** Add support for boost concept                                     :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate boost concepts so
that I don't have to manually create them.
#+end_quote

Now dogen supports concepts, the natural thing to do is to express
them in C++ code. This could easily be done using boost concept, or
the C++-14 concepts light.

See [[http://www.boost.org/doc/libs/1_53_0/libs/concept_check/creating_concepts.htm][Creating Concepts]].

*** Add support for object cloning                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to be able to clone object state so
that I don't have to do this manually.
#+end_quote

We should have a clone method which copy constructs all non-pointer
types, and then creates new objects for pointer types.

*** Generate service skeleton                                          :epic:

Since we already have all of the boiler plate code for services such
as licence, header guards, etc - we could just create a service
skeleton to stop us from having to copy it from the forward
declarations.

In addition to the class definition, it should also define all of the
automatic constructors, and add a private section at the bottom.

*** Add versioning support                                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make changes to diagrams in a
backwards compatible way so that I can upgrade the users of my code
incrementally.
#+end_quote

*New understanding*:

- Add versioning support by adding versions at the object level and at
  the property level. Properties with 0 version will have no special
  handling. Properties with non-zero version (V) will have the
  following code added in serialisation:

: if (version > V)
:    // read or write property

- If a number of consecutive properties all share the same version,
  dogen will group them under the same version if. There will be no
  other special grouping or otherwise changing of order of properties.
- The object version will be max(version) of all properties for that
  class, excluding inherited properties.
- The object version will be stamped using boost serialisation class
  version macro, unless the object version is zero.
- Dogen will make no validation or otherwise dictate the management of
  version numbers; its up to the users to ensure they make sensible
  backwards compatibility decisions such as adding only new properties
  and always adding to the end.
- The model version is a human level concept and has no direct
  relation to class versioning. It will be implemented as an
  implementation specific parameter in the Dia model and as a string
  in the SML model class. See [[*Improve%20OM's%20code%20generation%20marker][this story]].
- Model version will be used for the following:
  - stamped on doxygen documentation for the model namespace;
  - stamped on DLLs, etc.
  - used by humans to convey the "type" of changes made to the
    diagram/model (e.g. a minor version bump is a small change, etc).

Previous understanding:

Versioning support is now available in SML, so we need to apply it to
SML itself. That is, we need a way of having two versions of an SML
model coexist, and allow Dogen to diff those two versions to make code
generation decisions so that we can add basic backwards compatibility
support.

Before we can do this, we need a way of stamping a model version into
models. This can easily be done via implementation specific
parameters. See [[*Improve%20OM's%20code%20generation%20marker][this story]].

We then need to create some kind of strategy for version number
management:

- minor bumps are backwards compatible; e.g. only adding new fields.
- major bumps are not backwards compatible: e.g. deleting fields,
  classes, etc.

However, at present we only support a single version number. Perhaps
we should just declare which versions are backwards compatible and
which ones are not.

Once all of these are in place we should add versioning support to
dogen:

- add a new command line argument: =--previous-version= or something
  of the kind.
- the model supplied by this argument must have the same name as the
  model supplied by =--target=.
- change all SML types to be versioned.
- dogen will load up both models, and stamp the versions in each
  type. Merger will then be responsible for stamping the versions on
  each property, taking previous and new as input.
- for every field which is in new model but not in previous, add boost
  serialisation code to handle that.
- add unit tests with v1, v2 models.
- in order for dia diagrams with multiple versions to coexist in the
  same directory we will probably need to add the version to the
  diagram name, e.g. =sml_1.dia= or =sml_v1.dia=. We probably need
  some parsing code that looks for the last few characters of the file
  name and if it obeys a simple convention such as =_v= followed by a
  number, it ignores these for the model name and uses it for the
  version.

With this in place, when rebasing we can now do a proper comparison
between expected and actual.

Potential future feature: to put the files of different versions in
separate folders. This would allow the creation of "conversion" apps
which take types for one version and transform them into the next
version.

*** Add support for boost parameter                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost parameter so
that I can generate classes with named parameters without having to
manually create this code.
#+end_quote

It would be nice to have boost parameter support. [[http://www.boost.org/doc/libs/1_53_0/libs/parameter/doc/html/index.html#named-function-parameters][Documentation here]].

Ideally one would mark a type with a stereotype such as =named
parameter= and this would result in a full constructor with named
parameters. However since it seems one has to add a lot of boiler
plate code, perhaps its better to have a create function on a separate
header which internally calls the appropriate setters.

*** Add composite stereotype                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate composite objects so
that I don't have to do it manually.
#+end_quote

It would be nice if one could just mark a object as =composite= and dogen
automatically created the composite structure. As we only support
boost shared pointer that's what we'd use. We have a few use cases for
this (node, nested qname, etc).

This would be part of the injection framework.

*** Add support for bitsets                                           :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of bitsets in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

We are using a lot of boolean variables in SML. In reality, these all
could be implemented with =std::bitset=, plus an enumeration. One
possible implementation is:

- add =std::bitset= to std model.
- create a new stereotype of bitset.
- classes with stereotype bitset are like enumerations, e.g. users are
  expected to add a list of names to the class.
- dogen will then implement the properties of type bitset as a
  =std::bitset= of the appropriate size, and also generate an
  enumeration which can be used for indexing the bitset. This may need
  to be a C++-03 enumeration, due to type safety in C++-11
  enumerations.
- we should also implement default bitsets with values corresponding
  to the flags.

Example usage:

#+begin_src c++
const unsigned int my_bitset_size(10);
std::bitset<my_bitset_size> bs;

bs[first_flag_index] = 1;
bs = first_flag_value;
#+end_src

Links:

- [[http://www.java2s.com/Tutorial/Cpp/0360__bitset/Usebitsetwithenumtogether.htm][Use bitset with enum together]]
- [[http://stackoverflow.com/questions/9857239/c11-and-17-5-2-1-3-bitmask-types][C++11 and {17.5.2.1.3} Bitmask Types]]
- [[https://www.justsoftwaresolutions.co.uk/cplusplus/using-enum-classes-as-bitfields.html][Using Enum Classes as Bitfields]]

*** Add string table support                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate "enumerations" of
strings so that I don't have to create them manually.
#+end_quote

We need a way of creating "tables" of strings such as for example for
listing all the valid values for dia field names, etc. We could
implement this by creating a new stereotype where the name is the
string name and the default value is the string value. All strings
would be static public members of a class.

We should also add a validate method which checks to see if a string
is a valid value according to the string table. We could have a "case
insensitive" validate too.

*** Enumeration string conversion could be configurable               :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the input strings that
get converted into enumerations so that I can adapt it to my
requirements.
#+end_quote

It should be possible to pass in one or more string values as implementation
specific parameters that tells dogen what valid values an enumerator
can have. We can then generate a from string method that does the
appropriate conversions.

These should be passed in as dynamic extensions. At present
enumerators do not have dynamic extensions support so we need to add
it too (e.g. add the concept to them).

*** Enumeration string dumps could be configurable                    :story:

#+begin_quote
*Story*: As a dogen user, I want to output user defined strings from
enumerations so that I can adapt it to my requirements..
#+end_quote

It should be possible to pass in a string value as a dynamic extension
that tells dogen what string to use for debug dumping. At present
enumerators do not have dynamic extensions support so we need to add
it too (e.g. add the concept to them).

*** Add is comparable to SML                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to define types as comparable so that
I can use them in ordered containers.
#+end_quote

A object can have a stereotype of comparable. If so, then at least one
property must be marked as comparable. Properties are marked as
comparable if they have an implementation specific parameter called
=comparison_order=. =comparison_order= is a sequence starting at 0 and
incrementing by 1; it determines the order in which properties are
compared between two objects of the same type.

In order for a property to qualify as a comparison candidate its type
must be:

- primitive;
- =std::string=;
- a object marked as comparable.

Some facts about comparable objects:

- they generate =operator<= as a global operator in the type
  header file.
- they can be keys in =std::map= and =std::set=.

Relation to keys:

- If all properties that are part of a key are also comparable then
  the key will be comparable.
- comparable versioned keys always compare the version after all other
  comparable properties.

If an object itself is marked as comparable, then it is equivalent to mark
all properties as comparable using their relative position as the
comparison order.

*Merged with ordered containers story:*

In order to provide support for ordered containers such as maps and
sets we need to define =operator<=. However, it makes no sense to code
generate this operator as its unlikely we'll get it right. We could
assume the user wants to always sort by key, but that seems like a bad
assumption. The alternatives are:

- to expect a user-defined =entity_name_less_than.hpp= in domain. we'd
  automatically ignore any files matching this patter so the user can
  create them and not lose it. The problem with this approach is that
  we may have different sort criteria. This is a good YAGNI start.
- to provide the =Compare= parameter in the template and then expect a
  user-defined =entity_name_Compare.hpp=. The same ignore
  applies. This would allow users to provide any number of comparison
  operations.

Either approach requires [[Ignore%20files%20and%20folders%20based%20on%20regex][Ignore files and folders based on regex]].

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with [[*Build%20shared%20objects%20instead%20of%20dynamic%20libraries][using shared objects]], this
should improve build times.

 In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

*** Add support for type framing                                      :story:

#+begin_quote
*Story*: As a dogen user, I want to send model types over the wire so
that I can build networked applications.
#+end_quote

In places such as a cache or a socket, it may be useful to create a
basic "frame" around serialised types. The minimum requirements for a
frame would be a model ID, a type ID, a "format" (i.e. xml, text, etc)
and potentially a size, depending on the medium. The remainder of the
frame would be the payload - i.e. the serialised object.

In order for this to work we probably need the concept of a "model
group"; the type frame would be done for a group of models.

This can be done with or without boost fusion. See this presentation
for details on a boost fusion approach:

- [[https://www.youtube.com/watch?v%3DwbZdZKpUVeg][CppCon 2014: Thomas Rodgers "Implementing Wire Protocols with Boost Fusion"]]

*** Add pimpl support                                                 :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate PIMPL objects so
that I don't have to do it manually.
#+end_quote

It may be useful to mark classes as pimpl and generate a private
implementation. On the public header we could forward declare all
types.

*** Add camel case option                                             :story:

#+begin_quote
*Story*: As a dogen user, I want my models to use camel case so that I
can integrate dogen code with my code base.
#+end_quote

It would be nice to have a command line option that switches names
from underscores into camel case. The default convention would be that
diagrams are always with underscores and then you can convert them at
generation time. There should be a regex for this conversion.

*** Manual typedef generation                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of manually generated
typedefs in automated code so that I don't have to manually generate
all code that makes use of these typedefs when when its trivial code.
#+end_quote

- We should be able to create a stereotype of =typedef group=. This is
  a object type with lots of attributes. The code generator will take
  the name and type of each attribute and generate a file with the
  name of the group and all the typedefs inside.
- We should be able to create a forward declarations like header that
  defines typedefs for =shared_ptr= etc at the users choosing. This
  could be implemented as a tag. We could create a =memory_fwd= header
  to avoid cluttering the main =fwd= file for the type. We will need
  another type of relationship to model this, as well as another type
  of file in tags; the file would then have several Boolean flags one
  can tick such as =std_shared_ptr=, =boost_shared_ptr= and so on.
- it should also be possible to add some dynamic extensions to an
  attribute and get it to generate a typedef, e.g. cpp.typedef = "xyz"
  would result in the creation of typedef xyz using the type of the
  attribute; getters, setters and property would then be declared with
  the typedef.

*** Automatic typedef generation                                      :story:

#+begin_quote
*Story*: As a dogen user, I want dogen to generate typedefs so that I
don't have to create them manually.
#+end_quote

We should generate typedefs for all smart pointers, containers, etc -
basically anything that has template arguments. This would make
generated code much more readable and could also be used by client
code. In theory all we need is:

1. determine if the property has type arguments;
2. if so, construct the typedef name by adding =_type= to the property
   name, e.g. =attribute_value= becomes =attribute_value_type=, etc;
3. create a typedef section at the top of the class declaring all
   typedefs;
4. add a property to the property view model containing the typedef
   name and use it instead of the fully qualified type name.
5. we should also generate a typedef for the key if the class is an
   entity. See Typedef keys for each type.

We could also always generate a typedef for smart pointers in the
class that uses the smart pointer, with a simple convention such as
=attribute_value_ptr_type= or =shared_attribute_value_type=.

*** Add support for iterable enumerations                             :story:

#+begin_quote
*Story*: As a dogen user, I want enumerations to be iterable so that I
don't have to manually create code to do this.
#+end_quote

We should create an additional aspect for each enumerations which
creates a =std::array= with the enumerators (excluding invalid). This
would allow plugging the enumerations into for loops, boost ranges,
etc. The CPP should contain a static array; The HPP contains a method
which returns it, e.g. =my_enumeration_to_array.hpp=:

: std::array<my_enumeration, 5> my_enumeration_to_array();

We could make this slightly more generic by adding the notion of
enumeration groups. Out of the box we have:

- all: includes invalid;
- valid: excludes invalid

Users could then add implementation specific properties to create
other groups if needed.

*** Add support for user supplied test data sets                      :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of test data without
having to manually add code for it.
#+end_quote

*New understanding*:

we need to create a test data sets model. it should have an
enumeration for all of the available test data sets, and an
enumeration for the valid file formats. we should be able to pass in a
pair of file formats (input, actual/expected) and out should come a
triplet of directories. This would make maintenance really easy as
we'd only need to ad new strings to a string table. the service would
also handle things like the actual and expected directories, etc.

It should fix the following issues:

- [[*Adding%20new%20engine%20spec%20tests%20is%20hard][Adding new engine spec tests is hard]]
- [[*Naming%20of%20saved%20SML/Dia%20files%20is%20incorrect][Naming of saved SML/Dia files is incorrect]]

*Old understanding*:

The correct solution for test data and test data sets is as follows:

- the code generated by dogen in the test data directory is one of
  many possible ways of instantiating a model with test data.
- there are two types of instantiations: code and data. code is like
  dogen =test_data=; data is XML, text or binary - or any other
  supported boost archive; it also includes other external formats
  such as dia diagrams.
- a model should have a default enum with all the available test data
  sets: =test_data::sets=. If left to its default state it has only one
  entry (say =dogen=). The use is free to declare an enumeration on a
  diagram with the name test_data_sets and add other values to it.
- there must be a set of folders under test_data which match the
  enumerators of =test_data::sets=. Under each folder there must be an
  entry point such as =ENUMERATOR_generator=. Dogen will automatically
  ignore these folders via regular expressions.
- a factory will be created by dogen which will automatically include
  all such =ENUMERATOR_generator=. It will use static methods on the
  generator to determine what sort of capabilities the generator has
  (file, code, which formats supported, etc.) and throw if the user
  attempts to misuse it.
- all models must have a repository. Perhaps we need a stereotype of
  =repository= to identify it. This is what the factory will create.
- users will instantiate the factory and call =make=:

: my_model::test_data::factory f1;
: auto r = f1.make(my_model::test_data::sets::dogen);
:
: my_model::test_data::factory f2(expected_dir, actual_dir);
: auto r = f2.make(my_model::test_data_sets::some_set,
:   my_model::test_data::file_formats::boost_xml, file_locations::expected);

- if the user requires parsing a non-boost serialisation file then it
  should be make clear on the enum: =std_model, std_model_dia=. The
  second enumerator will read dia files. It will not support any file
  formats. The file must exist on either the expected or actual
  directory as per =file_locations= parameter.

Another topic which is also part of test data is the generation of
data for specific tests. At present we have lots of ad-hoc functions
scattered around different places. They should all live under test
data and be part of a test data set. The test data set should probably
be the spec name.

*** Add support for template classes                                  :story:

At present we can create classes in dia that require template
parameters; however, when we try to create member variables that
instantiate that class the parser fails to parse. For example,
=expansion::inclusion_dependencies_provider= has a template parameter:

: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>

The parser fails:

: 2015-04-02 15:55:02.084366 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>
: 2015-04-02 15:55:02.086929 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(197): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>
: [P12tag_workflow] = Code generation failure.

In addition, even if the parser succeeded, we still need a way to tell
dogen that the class has a template parameter; this is hard-coded at
the moment for containers (we determine if the type is an associative
container, etc). We need it to be dynamically determined when
inspecting the SML type. For example, we could have a "number of
template parameters" in the type. This could be set in the meta-data
for STL containers. Or we could actually specify the template
parameters as "type arguments" just like we do with operations and
properties (preferred). The parser would then use this info.

*** Add support for =std::function= in services                       :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of =std::function= so
that I don't have to manually generate code for types that use it.
#+end_quote

At present its not possible to declare an attribute of type
=std::function= anywhere in a diagram. It won't really be possible to
do so for entities and values because boost serialisation will always
be a problem. If this was really a requirement, we could look into
serialising functions:

- [[https://groups.google.com/forum/?fromgroups%3D#!topic/boost-list/sHWRPlpPsf4][how to serialize boost::function]]

However we don't seem to need this quite just yet. What we do need is
a way of having attributes in services and that is slightly easier:

- the parser needs to be able to understand the function template
  syntax (e.g. =void(int)=). It seems this could be hacked easily
  enough into the parser.
- Nested qualified names need to be able to remember that in the case
  of a function, the first argument is a return type (they also need
  to know they represent a function). MC: is this actually necessary?
  all we need is to be able to reconstruct this syntax at format time.
- we need a =void= type in the primitives model. This is a bit more
  complicated since this type can't have values, only pointers, and we
  don't really support raw pointers at the moment. Adding the type
  blindly would open up all sorts of compilation errors.

This should be sufficient for services. At present we have a hack that
allows functions without any template arguments, e.g. =std::function=,
in services.

*** Add support for references and pointers to types                  :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of references and
pointers so that I don't have to manually generate code for types that
use it.
#+end_quote

At present its not possible to create a type that has a reference to
another type. This should be a case of updating the parser to cope
with references and adding reference to property or nested type
name. This would be a good time to inspect our support for raw
pointers, it probably suffers from exactly the same problem and
requires the same solution.

In addition we should also bear in mind moving. Ideally one should be
able to declare moveable attributes and the end result should be a
setter that takes the type by =&&=. The question then is should we
also move on the getter? Sometimes it may not be a copyable type
(e.g. asio's =socket=).

It seems we can't also cope with =const= or pointers. To be fair we
only need const for shared pointer for now. On all cases we need to
make the parser more clever:

: boost::shared_ptr<const my_type>
: std::string&

We should try to create tests for all the cases we consider important
and mark them as ignore until we can find a spirit expert to help out.

*** Add support for default values                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to set default values for certain
primitives so that I can model my domain more accurately.
#+end_quote

It would be nice to be able to add a default value in dia and have it
set on the default constructor, if the type is a primitive or a
=std::string=.

These could be added as dynamic extensions; however, they should not
be model specific - the same defaults should apply to all
languages. However, note that in certain cases the default values may
be language specific (e.g. =0.0f= for float, etc).

Note also that there are two levels of default values: the type-level
(all primitives of type =x= default to =y=) and the property level
(instances of type =x= in class =z= default to =y=). The meta-data can
be applied at each level to achieve the desired effect.

*** Add support for interfaces                                         :epic:

This is a very blue-skies story. When dogen starts supporting service
types it would be useful to generate a service interface from
dogen. In order to do this we'd have to parse the method definitions
in dia and use those to construct an abstract base class.

*** Use error codes in exceptions                                     :story:

Avoid breaking tests every time the exception text changes by creating
a error code property in kitanda exceptions.

After some investigation it was found that boost already supports this
approach in =system=, as per [[http://en.highscore.de/cpp/boost/errorhandling.html][boost book]]. We could define a new
category per model and then create an enumeration of all error codes
in dia, for which the values would be the strings to use for the
error. The user could then create an exception and pass in the error
code in the constructor.

We should also make use of string tables to define all the error
messages.

Could we just have an exception factory that handles all of the
machinery of creating an exception with the right code, message etc?
it could also be responsible for appending more content to an existing
exception so that we'd have the tags all in one place.

*** Option to diff generated code                                     :story:

#+begin_quote
*Story*: As a dogen user, I want to know what has changed with the
next code generation so that I can evaluate if the changes are as
expected or not.
#+end_quote

It would be useful to have an option that would do everything except
writing the files to disk; instead, it would diff them with the
existing files and report if there are any differences. This would be
useful to make sure the source code matches the latest version of the
diagram.

We could use something like the [[https://code.google.com/p/dtl-cpp/wiki/Tutorial][DTL library]].

*** Add support for configurable enumerations types                   :story:

#+begin_quote
*Story*: As a dogen user, I need to configure the primitive type of my
enumerations so that I model my domain accurately.
#+end_quote

At present our enumerations always use unsigned int as the underlying
type. It should be possible to override that from dia. We could do
this by supplying a type via dynamic extensions.

*** Exception classes should allow inheritance                        :story:

#+begin_quote
*Story*: As a dogen user, I need to generate object graphs for my
exception classes so that I can model my domain better.
#+end_quote

We need to have a form of inheriting from a base exception for a given
model. We also need to be able to inherit from other exceptions in a
model. At present exceptions are not objects so the dependency graph
support is not there.

*** Add support for protected attributes                              :story:

#+begin_quote
*Story*: As a dogen user, I would like to define properties as
 protected so that I only expose them to derived types.
#+end_quote

We need to distinguish between public and protected attributes when in
the presence of inheritance. If not, issue a warning.

*** Shared pointers as keys in associative containers                 :story:

This is not supported; it would require generating the
hashing/comparison infrastructure for shared pointers. Further, as it
has been pointed out, keys should be immutable; having pointers as
keys opens the doors to all sorts of problems. We need to throw an
error at model building time if an user tries to do this.

*** Package names should follow a well-known convention               :story:

We need to make sure our package names are consistent with the
platform conventions.

- [[http://pastebin.com/TR17TUy9][Example of platform IFs]]
- [[http://libdivsufsort.googlecode.com/svn-history/r6/trunk/CMakeModules/ProjectCPack.cmake][Example CPack]]
- [[http://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.htmlhttp://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.html][Automatically add a revision number to the CPack installer name]]
- [[http://www.cmake.org/Wiki/CMake:CPackConfiguration][CPack Configuration]]

There are some known limitations in package naming:

- http://public.kitware.com/Bug/view.php?id=12997

*** Supporting user defined generic types                             :story:

At present we have a bit of a hack to support templates. However, all
that is required to allow users to create their own template types is:

- parse dia information for type arguments
- change object to have type arguments
- change merger to allow variables of the type of the type argument
- change view model to propagate type arguments
- change formatter to create template class if type arguments are
  present

However this would then mean that IO and serialisation would fail
since they are implemented on the cpp. As there is no need for
template types, this seems like an ok limitation.

** Tooling, infrastructure, blog posts, etc.

*** Log analysis tool                                                 :story:

We should create a log analyser tool (=logan=?), as follows:

- separate repo.
- use a dogen model to describe the tool's domain. Very simple domain.
- use the dogen version line to determine the application, the version
  and the run time. All other entries are foreign-keyed against this
  entry.
- use JSON object markers to extract JSON objects from the log line
  into a postgres JSON field.
- use ODB to create the database schema.
- create a simple parser that is hard-coded to the log lines in dogen,
  with perhaps an addition for threads.
- when profiling is present, have a way to split profiling information
  from the rest.
- create some simple stored procs that compare two runs from a
  performance perspective.
- create a stored proc to list all errors and all warnings, with
  perhaps some lines around it.
- create a stored proc that does a text search using postgres text
  search facilities.
- we need to figure out how splunk decides to start loading the log
  files (only after roll, incrementally - and if so, how does it keep
  track, etc).

*** Use clang format to format the code base                          :story:

It seems clang-format is being used by quite a lot of people to save
time with the formatting of the code. More info:

http://clang.llvm.org/docs/ClangFormat.html

Emacs support:

https://llvm.org/svn/llvm-project/cfe/trunk/tools/clang-format/clang-format.el

*** Investigate NeoVim's emblems                                      :story:

They appear to have emblems for issues, connected to waffle as well as
clang's analysis checks:

https://github.com/neovim/neovim

*** Consider c++ itself as a front-end                                 :epic:

One can imagine a clang-based front-end that reads C++ code suitably
annotated, perhaps with =Generatable= or some such attributes -
basically all attributes required to build a sensible SML model. The
frontend will parse the code and generate SML. We can then generate
serialisation, hashing etc for the hand-crafted code.

Previous story merged with this one:

*Add a utility that converts C++ code into JSON model*

#+begin_quote
*Story*: As a dogen user, I want to generate system models
 automatically so that I don't have to create them manually.
#+end_quote

We should be able to create a clang based utility that given a set of
libraries returns a basic JSON model for dogen for them, with as much
filled in as possible such as include directories, etc. This would
save us a lot of time instead of manually adding these.

*** Consider adding issues emblem                                     :story:

Boost.DI seems to have a new emblem for issues:

https://github.com/krzysztof-jusiak/di

In the future, this may be useful if users submit issues.

*** Consider adding NuGet support                                     :story:

Seems like AppVeyor supports deployment into nugget. Example:

http://www.nuget.org/packages/RxCpp/

It also comes with a couple of useful emblems:

https://github.com/Reactive-Extensions/RxCpp

We should push both the C++ libraries as well as the dogen binary.

*** Investigate AppVeyor's windows support for builds                 :story:

These guys seem to have good windows support:

http://www.appveyor.com/

Similar to travis. Used by RxCpp:

https://github.com/Reactive-Extensions/RxCpp

Yml:

https://github.com/Reactive-Extensions/RxCpp/blob/master/appveyor.yml

*** Investigate the emblems used by Bit7z                             :story:

This project uses emblems for version, platform and compiler:

https://github.com/rikyoz/Bit7z

This may or may not be useful to dogen.

*** Add support for coveralls                                         :story:

Seems like all we need to do to have code coverage from travis is to
enable it in the YML file.

*Direct use of Coveralls failed*

We had to remove coveralls:

: - coveralls --gcov "$GCOV" --gcov-options '\-lp' -e /usr

This was generating over 10 MB of logging so the build got terminated.

We also add to remove debug builds:

: -DWITH_DEBUG=on -DWITH_PROFILING=on

We were getting a lot of internal compiler errors:

: FAILED: /usr/bin/g++-4.9   -DBOOST_ALL_DYN_LINK -g -O0 -Wall -Wextra -pedantic -Werror -Wno-system-headers -Woverloaded-virtual -Wwrite-strings -fprofile-arcs -ftest-coverage -std=c++11 -frtti -fvisibility-inlines-hidden -fvisibility=default -isystem /usr/include/libxml2 -Istage/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/dia/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/dia_to_sml/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/frontend/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/backend/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/sml/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/config/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/cpp/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/cpp_formatters/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/sml_to_cpp/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/formatters/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/utility/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/knit/include -I/home/travis/build/DomainDrivenConsulting/dogen/projects/knitter/include -MMD -MT projects/sml_to_cpp/src/CMakeFiles/sml_to_cpp.dir/types/transformer.cpp.o -MF "projects/sml_to_cpp/src/CMakeFiles/sml_to_cpp.dir/types/transformer.cpp.o.d" -o projects/sml_to_cpp/src/CMakeFiles/sml_to_cpp.dir/types/transformer.cpp.o -c /home/travis/build/DomainDrivenConsulting/dogen/projects/sml_to_cpp/src/types/transformer.cpp
: g++-4.9: internal compiler error: Killed (program cc1plus)
: Please submit a full bug report,
: with preprocessed source if appropriate.
: See <file:///usr/share/doc/gcc-4.9/README.Bugs> for instructions.

Finally note also that we must add coverage _after_ the script
executes or else we risk doing coverage whilst the build is taking
place. Hopefully this is the reason for these errors:

: /home/travis/build/DomainDrivenConsulting/output/projects/test_models/class_without_attributes/src/CMakeFiles/class_without_attributes.dir/io/package_1/class_1_io.cpp.gcda:cannot open data file, assuming not executed
: File '/usr/include/c++/4.9/bits/basic_ios.h'
: No executable lines

We should read up on the [[http://docs.travis-ci.com/user/build-lifecycle/][life-cycle]] properly.

*Travis Examples*

Seems like all we need to do to have code coverage from travis is to
enable it in the YML file. We should look into copying it from the
[[https://github.com/apolukhin/Boost.DLL][Boost.DLL]] [[https://raw.githubusercontent.com/apolukhin/Boost.DLL/master/.travis.yml][example]]. We also need to enable coverage on all builds,
separately from nightlies. The key parts appear to be these:

:  - ../../../b2 cxxflags="--coverage -std=$CXX_STANDARD" linkflags="--coverage"

and

: after_success:
:    - find ../../../bin.v2/ -name "*.gcda" -exec cp "{}" ./ \;
:    - find ../../../bin.v2/ -name "*.gcno" -exec cp "{}" ./ \;
:    - sudo apt-get install -qq python-yaml lcov
:    - lcov --directory ./ --base-directory ./ --capture --output-file coverage.info
:    - lcov --remove coverage.info '/usr*' '*/filesystem*' '*/container*' '*/core/*' '*/exception/*' '*/intrusive/*' '*/smart_ptr/*' '*/move/*' '*/fusion/*' '*/io/*' '*/function/*' '*/iterator/*' '*/preprocessor/*' '*/system/*' '*/boost/test/*' '*/boost/detail/*' '*/utility/*' '*/dll/example/*' '*/dll/test/*' '*/pe_info.hpp' '*/macho_info.hpp' -o coverage.info
:    - gem install coveralls-lcov
:    - cd .. && coveralls-lcov test/coverage.info

Another way seems to be using gcov, as per [[https://github.com/fabianschuiki/Maxwell][Maxwell]] [[https://raw.githubusercontent.com/fabianschuiki/Maxwell/master/.travis.yml][travis.yml]]:

: - if [ "$CXX" = "g++" ]; then sudo apt-get install -qq g++-4.8; export CXX="g++-4.8" CC="gcc-4.8" GCOV="gcov-4.8"; fi
:  - sudo pip install cpp-coveralls

and

: script:
:  - export CTEST_OUTPUT_ON_FAILURE=1
:  - cmake -DCMAKE_BUILD_TYPE=gcov . && make && make test
: after_success:
:  - coveralls --gcov "$GCOV" --gcov-options '\-lp' -e CMakeFiles -E ".*/test/.*" -E ".*/mock/.*" -e maxwell/gen -e language -e thirdparty -e maxwell/ast/nodes -e maxwell/driver/gramdiag.c -e maxwell/driver/Parser.cpp -e maxwell/driver/Parser.hpp -e maxwell/driver/Scanner.cpp -e maxwell/driver/position.hh -e maxwell/driver/stack.hh -e maxwell/driver/location.hh

Yet another way seems to be creating a script to do coverage, as per
[[https://github.com/BoostGSoC13/boost.afio][boost.afio]] [[https://raw.githubusercontent.com/BoostGSoC13/boost.afio/master/.travis.yml][travis.yml]]. The script is available [[https://raw.githubusercontent.com/BoostGSoC13/boost.afio/master/test/update_coveralls.sh][here]].

*** Travis deployment of tags fails                                   :story:

As per [[https://github.com/travis-ci/travis-ci/issues/2577][issue 2577]] in travis, it does not support wildcards at the
moment. We need to find another way to upload packages into GitHub
without using wildcards.

Error:

: dpl.1
: Installing deploy dependencies
: Fetching: addressable-2.3.6.gem (100%)
: Successfully installed addressable-2.3.6
: Fetching: multipart-post-2.0.0.gem (100%)
: Successfully installed multipart-post-2.0.0
: Fetching: faraday-0.9.0.gem (100%)
: Successfully installed faraday-0.9.0
: Fetching: sawyer-0.5.5.gem (100%)
: Successfully installed sawyer-0.5.5
: Fetching: octokit-3.5.2.gem (100%)
: Successfully installed octokit-3.5.2
: 5 gems installed
: Fetching: mime-types-2.4.3.gem (100%)
: Successfully installed mime-types-2.4.3
: 1 gem installed
: error: could not lock config file .git/config: No such file or directory
: error: could not lock config file .git/config: No such file or directory
: dpl.2
: Preparing deploy
: Logged in as Marco Craveiro
: Deploying to repo: DomainDrivenConsulting/dogen
: Current tag is: v0.56.2767
: dpl.3
: Deploying application
: /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `initialize': No such file or directory - stage/pkg/*.deb (Errno::ENOENT)
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `new'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `upload_asset'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:118:in `block in push_app'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:102:in `each'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:102:in `push_app'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider.rb:122:in `block in deploy'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:41:in `fold'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider.rb:122:in `deploy'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:32:in `run'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:7:in `run'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/bin/dpl:5:in `<top (required)>'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/bin/dpl:23:in `load'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/bin/dpl:23:in `<main>'
: failed to deploy

*** Usage of rev-list in version causes problems                      :story:

Now we are using travis, we have found problems with the way we are
using the number of git commits to assign versions. The problem stems
from the git clone command:

:  git clone --depth=50 --branch=master git://github.com/DomainDrivenConsulting/dogen.git DomainDrivenConsulting/dogen

This means we are always at version 50/51:

: -- Product version: 0.56.51

This is in addition to the problems we had with tagging (where we have
to manually stamp the version) and branching (where we are using the
number of commits in master rather than the branch). We need a better
solution than using rev-list for this.

*** Consider hosting documentation in read the docs                   :story:

We should consider hosting the documentation here:

- https://readthedocs.org/

*** Create a demo of installing dogen and generating hello world      :story:

#+begin_quote
*Story*: As a dogen user, I want basic instructions on using the
program so that I don't have to figure it all out by myself.
#+end_quote

We need to start creating a series of quick videos demoing dogen. The
script for the first video of the series is as follows:

- download packages from Google Drive and install them.
- obtain the hello world model from git.
- generate the hello world model.
- create a hello world main with make files and compile it.
- give a quick overview of the available facilities.

*** Write next interesting instalment in blog                         :story:

We have a number of links backlogged and we need to offload them in an
"interesting..." post.

*** Write blog post on reflexive programming                          :story:

Basic idea is that domain expertise tightens the agile circle,
allowing for much faster progress. Check Soros definition of
reflexive.

*** Create a trivial Linux gcc script                                 :story:

The previous attempts to clean up the build environment were too
elaborate given the available time. We need to go back to basics with
a trivial script that works for Linux 32-bit and 64-bit with gcc.

*** Create a trivial Linux clang script                               :story:

We need to be able to build Linux clang 32-bit and 64-bit again.

*** Implement flymake from the EDE project                            :story:

This move of directories highlighted the fragility of the current
flymake hack: every time the top-level directory changes we need to
update =cunene=. Ideally what we want is to have a top-level file -
most ideally =dogen.ede= with some lisp code that would setup the
dogen paths for flymake. Users would only need to load this up to use it.

*** Add support for automatic upload packages into GDrive             :story:

We need to upload the packages created by the build to a public Google
Drive (GDrive) location.

- Google drive folder created [[https://drive.google.com/folderview?id%3D0B4sIAJ9bC4XecFBOTE1LZEpINUE&usp%3Dsharing][here]].
- See [[https://developers.google.com/drive/quickstart-ruby][this article]].
- [[http://stackoverflow.com/questions/15798141/create-folder-in-google-drive-with-google-drive-ruby-gem][Create folders]] to represent the different types of uploads:
  =tag_x.y.z=, =last=, =previous=. maybe we should only have latest
  and tag as this would require no complex logic: if tag create new
  folder, if latest, delete then create.

*** Add a CMake module for git                                        :story:

We are finding git manually at the moment, which means we are probably
not doing it right. It appears there is a CMake script for it:

- [[%20http://gnuradio.org/redmine/projects/gnuradio/repository/revisions/accb9f2fe8fd8f6a1e114adac5b15304b0e0012d/entry/cmake/Modules/FindGit.cmake][FindGIT.cmake]]

*** Add support for the =scan-build= static analyser                  :story:

scan-build is a command line utility that enables a user to run the
static analyzer over their codebase as part of performing a regular
build.

- [[%20http://clang-analyzer.llvm.org/scan-build.html][scan-build]] project page

*** Add support for iwyu                                              :story:

There is a clang based tool that checks if which includes are actually
used by the translation unit. We should have a build for this that
breaks whenever one includes something which is not required.

- [[http://code.google.com/p/include-what-you-use/][iwyu project page]]
- [[http://mpd.jinr.ru/svn/mpdroot/trunk/cmake/modules/FindIWYU.cmake][FindIWYU.cmake]]
- [[https://github.com/christophgysin/addp/blob/master/cmake/iwyu.cmake][iwyu.cmake]]

*** Add support for Address Sanitizer (ASan)                          :story:

This seems like another interest dynamic analysis tool:

[[https://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction][Address Sanitizer]]

*** Add support for CPPCheck                                          :story:

Seems like CPPCheck has a different take on dynamic analysis when
compared to Valgrind. We should look into how hard it is to integrate
it with CTest.

Links:

- [[http://cmake.3232098.n2.nabble.com/Static-code-analysis-with-CDash-td6079787.html][CMake and CPPCheck]]

** Visionary work and random ideas

These stories are far in the future or just plain crazy.

*** Model groups and multi-language support                           :story:

At present we have hard-coded knit to support a single C++ model,
cpp. However, in reality the world looks more like this:

- there are "groups of models" that have models that target specific
  languages. We need to give a name to the "default" model group in
  dogen. We should choose something from the [[http://en.wikipedia.org/wiki/Glossary_of_sewing_terms][sewing terms]]; for now
  lets call it =quilt=. =quilt= contains a number of languages such as
  =cpp=. A user can only generate one model group at a time. Users can
  generate one or more languages within a group (depending on what the
  group supports).
- we should have a top-level folder to house all model groups:
  =backends=. The existing =backend= model becomes =backends::core=.
- there may be facilities that are language specific, shared by model
  groups. These can be housed in language specific folders:
  =backends::cpp= and so on. For instance, the language specific stuff
  now in =formatters= should move here.
- different groups may express SML models differently; almost by
  definition, they will, or else there is little purpose in having
  multiple groups. For example, one can imagine a model group (say
  =pleat=) which expresses [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-language-agnostic-models-lam][LAM]] as a model that is identical on every
  supported language, ignoring native types; that is, expresses LAM as
  a LAM model. However, =quilt= would still express LAM as a mapping
  between generic LAM types and concrete native types (e.g. LAM
  dictionary is a C++ unordered map). A good candidate for =pleat=
  would be [[http://www.eclipse.org/modeling/emf/][eCore]].
- if one was to try to generate code that is identical to =protobuf=,
  the xsd tool, =odb=, etc one would generate model groups for these.
- we may need multiple "needles" for each model group. For example,
  the supporting libraries for =quilt= may be (and almost certainly,
  will be) totally different than that of those in =pleat=. And of
  course, needle would have different expressions in each programming
  language. So perhaps needle is more of a concept than a physical
  thing. We should rename it to something meaningful that represents
  "a library with supporting code for a given model group". However,
  it does make sense to have a top-level folder to house all of the
  supporting libraries, so maybe needle does exist physically as the
  namespace to house all of the different supporting libraries. For
  example: =dogen::needle::quilt=, etc.
- the different needle libraries should be pushed to the appropriate
  repositories (e.g. nuget for C# and maybe C++, biicode for C++,
  maven for Java, etc).
- in the model groups world, each model most likely will only support
  a single model group: for example either quilt or pleat, etc. This
  is because some types only make sense with a given model group (say
  for example a cross platform =String= type in pleat won't exist in
  quilt and so forth). This means one must filter the models one is
  loading depending on the model group. This applies to both internal
  and external models. Also a model group may support a different
  subset of programming languages compared to another model group.
- we need a better name than "model group".

*** Add support for type "labels" or "tags"                           :story:

It may be useful to "tag" a number of types with a "label", and to
allow users to access these at run time. This only makes sense in the
context of reflection. This story needs a bit more fleshing out.

*** Splitting facets into their own projects                          :story:

It is not always desirable to generate a facet as part of the main
model. For example, say we want to support coherence. We don't want to
generate a single shared object with the model and also include
coherence in it because this means everyone will have to link against
the coherence libraries. Instead, one would like to create "project
facets"; that is, to be able to somehow create a top-level project
just for that facet (or perhaps group of facets) such that they would
end up in a different project and thus different shared object. For
example:

: project_a/cpp/types
: project_a/cpp/coherence

would become

: project_a/cpp/types
: project_a.coherence/cpp/coherence

Or some such approach.

*** Support for platform specific code                                :story:

There are some features which may only make sense on a given platform,
or may have different expressions depending on a platform. For
example, [[https://msdn.microsoft.com/en-us/library/aa370448%2528v%3Dvs.85%2529.aspx][DLL Main]] is required on Windows but not on UNIX. These files
must be correctly handled by CMake such that they are excluded on UNIX
and added on Windows. Same with [[http://en.wikipedia.org/wiki/Precompiled_header][StdAfx.h]] and cpp, which will require
looking into pre-compiled headers support in CMake.

*** Add support for plugins                                           :story:

An interesting idea is to generate a model that contains formatters,
create a dynamic library and then have some kind of loading mechanism
in Dogen. The interesting thing is that with static factories, dogen
could make use of this without any code changes at all (e.g. loading
the library into the process is sufficient to trigger registration,
and then its up to the dynamic extensions to decide whether to use the
formatters or not). So a user could create a model with formatters,
add its own text templates, compile and link it and then add it to
Dogen and then make use of the new formatters. The usual constraints
apply such as ABI (ensure one is using the same compiler as used to
compile Dogen, flags, etc).

This may also make sense for the front-end, but less so.

See [[http://apolukhin.github.io/Boost.DLL/index.html][Boost.DLL]].

*** Add vagrant and docker support                                    :story:

It would be nice to provide vagrant and docker support to dogen in
terms of development. The idea is that you can =git clone= the repo
and then =vagrant up= it and you would be ready to start coding. This
would make drive-by patches much easier.

*** Add support for Language Agnostic Models (LAM)                    :story:

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N SML merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

FIXME: we believed this story was already backloged but could not find
it on a quick search. Do a more thorough search.

*** Improving test data generation via SML "reflection"               :story:

A really random but perhaps implementable idea: to create a
description of test data as JSON objects. For example, one could
supply a "SML instantiation description language" for a user model,
such as:

: {
:    "__type__" : {
:        "model_name": "my_model",
:        "external_module_path": []
:        "module_path": [],
:        "simple_name": "my_type"
:    },
:    "property_0" : true
: }

Where =__type__= is the "meta-type" for the object we want to
instantiate, allowing us to locate it in the SML model, and the
remaining properties are as per user SML model. Once the type is
located, one could then iterate through the properties in SML and
locate their instance values in the JSON.

With this JSON and an SML instance of =my_model= we could generate
code that looks like so:

: my_model m;
: m.property_0(true);

This would allow users to provide JSON descriptions of the test data
factories. If we took this approach we should consider renaming test
data to something a bit better (sample data?).

In this light, the current test data is akin to a "generic
instantiation language" in that all strings are instantiated with the
same values (or algorithm of value generation), all ints etc.

This is almost like reflection, in that if one had a strongly typed
model for the instantiation description language, it would look like a
reflected SML model. The problem, of course, is that we do not code
generate SML; we code generate in language specific models such as
=cpp=, etc. We would also have to have some kind of reflection support
for these models, and a transformation layer between the SML
reflection and the implementation model. This is what the formatter
would then use to output.

*Notes*

- See also [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#make-test-data-generator-more-configurable][Make test data generator more configurable]]
- this idea stems from the need to generate [[http://www.neuron.yale.edu/neuron/static/docs/refman/hoc.html][Hoc]] files from a neuron
  model described SML. For that particular use case we would have a
  language model (=hoc=) that uses the instantiation description
  language to generate hoc files.

*** Consider using bounded integers                                   :story:

This library seems to improve static error checking with integers,
etc:

https://bitbucket.org/davidstone/bounded_integer

However, off the top of my head, there are no cases for bounded
integers in dogen yet.

*** Investigate support for biicode                                   :story:

[[https://www.biicode.com/][Biicode]] is a nuget-like repo for c++. We should look into both
consuming dependencies from it and pushing dogen into it. In addition
there are associated emblems:

https://github.com/Manu343726/snail

We should also look into [[https://www.biicode.com/biicode-open-source-challenge][the challenge]].

We should push both the C++ libraries as well as the dogen binary.

*** Investigate ModelQ for ideas                                      :story:

We need to mine this project for potential ideas - see how their
approach compares to ours, see if we can learn any lessons from it:

*ModelQ*: ModelQ is a code generator for creating Golang codes/models
to access RDBMS database/tables (only MySQL supported for now).

ModelQ: https://github.com/mijia/modelq

*** Evaluate all of our data structures based on usage                 :epic:

This presentation is very interesting:

[[https://www.youtube.com/watch?v%3DrX0ItVEVjHc][CppCon 2014: Mike Acton "Data-Oriented Design and C++"]]

The presenter makes a lot of points that are directly applicable to
Dogen. The main one is that we need to look at all the data structures
to see how they are used, and to try to extract deeply nested if's
that in many cases can easily be extracted from the bottom and moved
to the top. There are many other excellent points, we probably need to
watch the presentation again and write each of them down.

The key point though is that the re-engineering exercise should only
be done after we finished all of the current refactorings - we must
make sure the code does all that it is intended to do first and then
tackle the Acton's suggestions. This is to ensure that we have
captured all the main use cases. Data analysis can be done after this.

*** Consider generating dependency injection code                      :epic:

If one could mark constructors as =injectable= in a diagram, we could
then generate something like a castle windsor container and do all of
the management of dependency injection from generated code. We also
have access to all interfaces and their implementations so a lot of
the clever logic done at run time by castle/guice etc could be done in
the generated code.

*** Consider using Given-When-Then for unit tests                      :epic:

We should check the Martin Fowler article on [[http://martinfowler.com/bliki/GivenWhenThen.html][Given-When-Then]] as a way
of specifying unit tests, to see if it would make our tests
clearer. We are already following some kind of Given and Then, but we
should consider making it explicit.

The best approach may be to move all unit tests to [[https://github.com/philsquared/Catch][Catch]], since it
natively supports GWT.

*** Multi-purpose models per language                                  :epic:

#+begin_quote
This story is a very vague story that keeps track of ideas on making
dogen useful for code generators of other kinds.
#+end_quote

One of the stories in the backlog covers other targets of code generation:

[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-thrift-and-protocol-buffers][Add support for thrift and protocol buffers]]

Originally we thought about adding support for these within a model;
that is to say, one would have additional serialisation "kinds"
available with a given dogen model. However, there is another way to
look at this; one could make other kinds of code generators using the
dogen infrastructure.

That is, contrary to it's name, dogen isn't just for "domain model
generation". Nothing stops one from building a protocol buffers or
thrift compiler using dogen infrastructure that outputs *exactly* the
same code as the original tools. All that would be required to do so
is:

- create a front-end that reads in their specification;
- to ensure SML is expressive enough to cover all of the aspects of
  the code that needs to be generated;
- to create the formatters.

In this view of the world, we have two options:

- create groups of facets within the =cpp= model; for example,
  the thrift group, the domain generation group etc. These are
  mutually incompatible sets of formatters and only one of them can be
  enabled for a given execution.
- create models at the same level of the =cpp= model. We could group
  them by language (e.g. the =cpp= namespace). However, this seems
  less practical because these models would probably have a lot in
  common. This is yet to be seen as we need to finish the large
  formatters refactor before we can answer this question.

Taking this to its logical consequence, even a tool as complex as ODB
could potentially be implementable in this way: one can conceive a
clang front-end that reads in source code and generates an SML model;
this model then can be used to generate C++ code that is identical to
the code produced by ODB (again assuming that SML is extended to be
expressive enough to represent all the constructs required by ODB).

This would be a compelling proposition if we had =stitch= because it
would make the generation of formatters quite trivial and would also
mean that people that want to create code generators don't have to
worry about a lot of the boilerplate code. However, the biggest
problem is that we'd be imposing a large and complex "framework" on
them with all the evilness that that entails.

Food for thought:

- in this light, a better name for dogen would be =codegen= (or =cogen=
  to make it a bit more unique in google). The tag line is then The
  Generic Code Generator. Unfortunately there are already a few
  projects with the name =cogen= so we may need to find a better
  name. Alternatively we can maintain the name dogen, but take away
  its meaning (i.e. no longer "The Domain Generator").
- the merge of =cpp= and =cpp_formatters= may not have been for the
  best in this case; it would make more sense to have a =cpp::dogen=
  where we collect all of the formatters related to domain
  generation - after the =cogen= rename; if no rename then we need
  some other name to imply domain generation. At this level we could
  then have =cogen::cpp::odb=, =cogen::cpp::protobuf= and so on. They
  all make use of the core types defined in =cogen::cpp=. The problem
  with this approach is that dogen is not really designed to share a
  namespace in this way. We won't be able to have a =cpp= project as
  well as placing other projects inside of the =cpp= namespace. We can
  have one or the other in the current setup, but not both. We could
  take the same approach as we did for test models: create a cpp
  folder and then put the model under a different name such as =model=
  or =domain= etc. Note that we still have to define all of the
  formatter interfaces in the "main" model, as well as workflows
  etc. However, some interfaces may not make sense for other models:
  what is a registrar in protocol buffers? If it exists at all, its
  probably something very different from boost serialisation and as
  such will require other data.
- note that this kind of grouping is not necessarily at the language
  level. For example, domain generation should be common to a set of
  languages, and so would protocol buffers. This means that rather
  than a facet or formatter grouping, we need a higher level construct
  to aggregate things; "domain generation" is made up of languages,
  languages are made of of facets, facets have formatters. We need a
  name/classification for "domain generation" in this context.

We should bear in mind [[http://st-www.cs.illinois.edu/users/droberts/evolve.html][this quote]]:

#+begin_quote
People develop abstractions by generalizing from concrete
examples. Every attempt to determine the correct abstractions on paper
without actually developing a running system is doomed to failure. No
one is that smart. A framework is a reusable design, so you develop it
by looking at the things it is supposed to be a design of. The more
examples you look at, the more general your framework will be.
#+end_quote

*** Consider adding XML schema as a front-end                          :epic:

Tools like =Xsd= and =ejc= use the XML schema to provide the input to
the code generator. There is a simple mapping between the XML schema
and the language. [[http://en.wikipedia.org/wiki/Java_Architecture_for_XML_Binding][JAXB]] is a good example of this. As we already have a
dependency on libxml, we can already load XML schemas.

Note though that the idea is not to generate another xsd tool since we
already have a good [[http://www.codesynthesis.com/products/xsd/][C++ xsd tool]]. The point is to figure out if a XML
schema based front-end to Dogen is more convenient than Dia XML or
JSON.

It may also be worth looking at the [[https://jaxb.java.net/2.2.4/docs/xjc.html][xjc]] command line tool interface to
pick up ideas for Dogen.

We should also support the annotations used by JAXB such as
inheritance, etc.

*** XML serialisation for interoperability                             :epic:

A very good point raised by Boris on his [[https://www.youtube.com/watch?v%3DAuamDUrG5ZU][XML talk]] is that XML is
useful as an interchange format, mainly for interoperability and
future-proofing. It would be nice if we could use dogen to generate a
"controlled" XML, using a well defined schema (also
code-generated). This would be a stable format that could allow
third-parties to hook into the serialisation - as opposed to boost
serialisation's XML.

*** Add support for CSV serialisation                                 :story:

It would be nice to be able to serialise some simpler types into CSV
files, and to be able to read them from CSV files. We should use a
third-party library to perform the serialisation. We should check that
the types only have simple types. We should also provide some dynamic
extensions for options such as "use quoted strings", etc.

*** Consider generation of validators                                  :epic:

It would be nice if we could constraint the domain of a type via some
kind of rules; for example, provide a regular expression or an EBNF
definition with a string that tells dogen how to validate it. We could
then construct simple validators. As usual these expressions can be
supplied via dynamic extensions.

*** Providing view model hints                                         :epic:

Once we start supporting view models, it would be nice to be able to
take an inheritance tree of objects and collapse it into a list view,
handling all of the use cases (reorder columns, remove columns, add
columns, updating rows, etc). All of this code can be inferred from
the type hierarchy.

*** Consider replacing out libxml bindings with RapidXML              :story:

We rolled our own libxml bindings for reading the dia XML. However, it
may make more sense to use [[https://github.com/dwd/rapidxml][RapidXML]] instead. It seems basic but our
needs are also very basic.

*** Add support for BSON serialisation                                :story:

It would be useful to support Mongo DB's BSON. There is a C++ stand
alone library for this:

https://github.com/jbenet/bson-cpp

For examples on how to use the C++ API see the tutorial:

https://github.com/mongodb/mongo-cxx-driver/wiki/Tutorial

*** Consider adding SWIG and Boost.Python support                      :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose models to other languages
so that I can make use of them from there.
#+end_quote

We could generate the code required to expose the C++ types into ruby
or python by creating a formatter for it. Boost.Python would be more
straightforward as it is plain C++ code; SWIG would require generating
an interface file (IDL-like) and as such is closer to [[*Add%20support%20for%20thrift%20and%20protocol%20buffers][this]] story.

*** Add yuml markup language support                                  :story:

#+begin_quote
*Story*: As a dogen user, I want to generate diagrams using yuml so
that I don't have to install Dia.
#+end_quote

It should be fairly straightforward to add a yuml front end that reads
a file using their markup language and generates an SML model from it.

*** Add support for thrift and protocol buffers                        :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose dogen models to other
languages so that I can make use of them on these languages.
#+end_quote

Amongst other things, these technologies provide cross-language
support, allowing one to create c++ services and consume them from say
ruby, python, etc. At their heart they are simplified versions of
CORBA/DCOM, with IDL equivalents, IDL compilers, specification for
wire formats, etc. As they all share a number of commonalities, we
shall refer to these technologies in general as Distributed Services
Technologies (DST). We could integrate DST's with Dogen in two
ways. First approach A:

- generate the IDL for a model; we have enough information to produce
  something that is very close to it's Dogen representation,
  translated to the type system of the IDL; e.g. map =std::string=,
  =std::vector=, etc to their types. This IDL is then compiled by the
  DST's IDL to C++ compiler.
- possibly generate the transformation code that takes a C++ object
  generated by Dogen and converts it into the C++ object generated by
  the DST's C++ compiler and vice-versa. We probably have enough
  information to generate these transformers automatically, after some
  analysis of the code generated by the DST's C++ compiler.

In order for this to work we need to have the ability to understand
function signatures for services so that we can generate the correct
service IDL for the DST. In fact, we should be able to mark certain
services as DST-only so that we do not generate a Dogen representation
for them. The DST service then internally uses the transformer to take
the DST's domain types and convert them into Dogen domain types, and
then uses the Dogen object model to implement the guts of the
service. When shipping data out, the reverse process takes place.

Approach A works really well when a service has a very narrow
interface, and performs most of it's work internally without exposing
it via the interface. Once the service requires the input (and/or
output) of a large number of domain types, we hit a cost limitation;
we may end up defining as many types in Dogen as there are in the IDL,
thus resulting in a large amount of transformations between the two
object models.

In these cases one may be tempted in ignoring Dogen and implementing
the service directly in terms of the DST's object model. This is not
very convenient as the type system is not as expressive as regular
C++ - there are a number of conventions that must be adopted, and
limitations imposed too due to the expressiveness of the IDL. We'd
also loose all the services provided by Dogen, which was the main
reason why we created it in the first place.

Approach B is more difficult. We could look into the wire format of
each DST and implement it as serialisation mechanism. For this to
work, the DST must:

- provide some kind of raw interface that allows one to plug in types
  serialisation manually. Ideally we wouldn't have to do this for
  services, just for domain types, but it depends on the low-level
  facilities available. A cursory look at both thrift and protocol
  buffers does not reveal easy access to such an interface
- provide either a low-level wire format library (e.g. =std::string=
  to =string=, etc) or a well specified wire format that we could
  easily implement from scratch.

This approach is the cleaner technically, but its a lot of work, and
very hard to get right. We would have to have a lot of round-trip
tests. In addition, DST's such as thrift provide a wealth of wire
formats, so if there is no easy-access low-level wire format library,
it would be very difficult to get this right.

*** IOable services                                                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to output all the state of my
services without having to manually create code for it.
#+end_quote

Even though we do not code generate services, it would be nice if we
could still setup their IO infrastructure - something basic just
outputting the type and taking inheritance into account. We end up
doing a lot of this manually anyway.

Also, if a service has a bunch of attributes that are IOable, we
should set them up too.

*** Code generation as a service                                       :epic:

One way of testing new functionality added to dogen is to try to
exercise it as part of the code generation itself. We have been doing
this with the bootstrapping, but there were limitations on
functionality such as ODB and EOS where we couldn't see any obvious
use for it in code generation. However, there is one way of exercising
this and a lot more of these sort of features: to create a Web-based code
generation service, along the lines of Web Sequence Diagrams or
YUML. We could create a simple bootstrap based website that forwards
requests to a set of end-points, all done within the dogen project.

We'd create a casablanca REST layer with a simple interface, with
functionality such:

- create workspace: returns a UUID and creates some kind of internal
  storage area.
- upload target: uploads a Dia or JSON model to be used as the code
  generation target.
- upload reference: uploads a Dia or JSON model to be used as a
  reference.
- set options: which facets to generate, which languages, etc.
- codegen: runs the code generation and returns a tarball with
  generated files and the log file; or returns a set of code
  generation errors.
- we could integrate with google drive to load the files from there.

As a further layer we could create an ASIO service that is queried by
the casablanca REST. This would exercise all of the messaging
infrastructure. Internally it would create the engine and run code
generation. It could also exercise ODB by writing session information
to a database and keeping track of the historical usage of the
service, log files etc.

This stack would allow us to continuously exercise pretty much every
feature we need out of dogen. As an added bonus, when we get to the UI
we could also exercise that (Wt, GTK).

Finally, this would also allow us to play with Docker, and place each
service in their own container, create load balancing etc.

Links:

- [[http://codeplanet.io/principles-good-restful-api-design/][REST API Design]]
- [[http://www.drdobbs.com/tools/json-and-the-microsoft-c-rest-sdk/240164821][Using Microsoft REST SDK]]

*** Automatic generation of C interfaces for C++ code                  :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of c++ models from C so
that I can create "bridge" APIs.
#+end_quote

Once we have proper C support, it should be doable to have a C++ facet
that automatically exposes a C interface.

*** Support for COM/CORBA                                              :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of COM/CORBA so that I
can create code to interface with legacy systems.
#+end_quote

We should investigate how hard it is to add support for these IDLs.

*** Add a utility that converts a dia model into JSON                  :epic:

#+begin_quote
*Story*: As a dogen user, I want to convert some Dia models into JSON
documents whenever I don't require UML and diagram formatting, so that
I don't have to generate the documents manually.
#+end_quote

It would be great if one could take a dia model and convert it into a
JSON representation. This would allow users to take models that are
not particularly useful in UML and convert them into JSON.

Name according to new convention: tailor.

*** Investigate support for automatic diagram updates                  :epic:

For classes that are manually generated, it would be really nice if we
could update the properties of the class in the diagram from the
source code. This would work as follows:

- user creates a class =x= and marks it as non-generatable; executes
  dogen.
- dogen creates the initial file and adds as much boilerplate as
  possible. For instance if the user manually added properties or
  operations to the class, dogen generates skeletons for these.
- once the file exists, dogen will no longer touch it (see also the
  merging code generation story, for a different take on this).
- the user runs a second tool (=diup=? the Diagram Updator) which uses
  clang internally; it reads the diagram and looks for all of the
  non-generatable classes; for each of these, it updates the dia class
  with the properties found in the source file. Everything else is
  left untouched.

This feature would be extremely powerful when in presence of many
other features such as mocking, remote method invocation, etc - the
user would have no effort at all in generating the
code. Implementation-wise we'd have to:

- create an XML writer;
- add write support for the dia model and ensure we generate valid dia
  models;
- integrate clang libraries with dogen;
- create =diup= - or perhaps we should just have an "update diagram"
  mode in dogen?

*** Add POF serialisation support                                      :epic:

If coherence has open source C++ libraries, we should add support for
serialisation to and from POF.

Links:

- [[http://docs.oracle.com/cd/E24290_01/coh.371/e22839/cpp_api.htm][Understanding the Coherence for C++ API]]

*** Generate Visual Studio solutions                                   :epic:

#+begin_quote
*Story*: As a dogen user, I want to use visual studio solutions
directly so that I don't have to rely on CMake.
#+end_quote

At present we rely on CMake as the C++ meta-build system. There is
nothing stopping us from supporting more native build systems such as
Visual Studio. Consider adding direct support for Visual Studio.

*** Improve the integration of dogen with dia                          :epic:

It would be great if the model generation in dia was slightly more
interactive:

- dia could have a button to run/configure an external tool, where the
  setup for dogen would be kept.
- pushing an execute button would code generate.
- pushing a validate button would validate the current diagram, taking
  into account declared references. references to types that are not
  resolved could make the class or function go red.

The idea is to do the least intrusive changes in dia that would
provide us with this support. In order to access dogen, instead of
running the executable and parsing the command line output, it would
make more sense to create a C interface that supports these specific
use cases (and nothing else).

*** Dia limitations that impact dogen usage                            :epic:

Collection of limitations we found in Dia that are annoying when using
it in anger with dogen:

- moving types in and out of packages does not work very well.
- comments for packages are missing.

*** Consider adding YQL support                                        :epic:

YQL offers a REST based API with lots of interesting information; an
example of the information provided is available [[https://github.com/yql/yql-tables/blob/master/yahoo/finance/yahoo.finance.quant.xml][here]]. There should be
somewhere a matching XML schema for each of these queries, at least
for the end points that return XML. It would be great if one could
take one of those schemas and generate an SML representation for them.

More generally, it would be great if dogen was able to create a domain
model off of an XML schema. However, we already have the Code
Synthesis [[http://www.codesynthesis.com/products/xsd/][XSD tool]] for that, so maybe this is just scope creep.

*** Add support for GtkBuilder / Glade XML files                       :epic:

There is nothing stopping us from using a GtkBuilder / Glade XML file
as an input, create some SML from it and then generate code which
would do the boiler plate setup of the UI. With a bit more work one
could potentially even generate the bindings for a presentation model.

*** Remote method invocation                                           :epic:

See [[*Type%20framing][type framing]], [[*Model%20and%20type%20enums][Model and type enums]].

It seems fairly straightforward to add remote method invocation to a
few select types. The following would have to be done:

- create a new stereotype like =dispatchable=, =remotable= or suchlike
- create a new stereotype: interface.
- add support for interface code generation.
- validation: model must have a model ID, thought to be unique across
  models.
- validation: types must be marked as both =remotable= and
  =interface= and have a unique type ID in the model.
- validation: types must have at least one public method
- injector: if at least one type is =remotable=, a new system
  package is created: =rmi=.
- injector: a system enumeration will be created with all the
  supported serialisation types. actually, we should create this
  anyway in serialisation or reflection.
- rmi will contain one class that represents a "frame". this
  frame will be composed as follows: model ID, type ID, serialisation
  type, raw buffer. we need to look at RMI terminology to come up with
  a good name for this frame.
- messages: for each method that exists in each dispatchable service,
  a message class will be created with a name following some well
  defined convention such as =CLASS_NAME_METHOD_NAME=. we need
  examples to make up a sensible convention. or perhaps an
  implementation specific parameter can override the class name. the
  message class is a data object and has as attributes all of the
  parameters of the method.
- a dispatcher class will be created in dispatching. it will have as
  constructor arguments references to all the dispatchable
  services. when passed in a frame, it will hydrate it and dispatch it
  to the correct service.
- a "framer" class will be created in dispatching. it will be
  configured for a given serialisation type. it will take a message
  object, serialise it and frame it.
- we could support the notion of callbacks. for this we need to be
  able to serialise stubs as references such that when the other end
  receives it, it calls a registrar to activate a client stub.

Now we just need a way of creating some generic interfaces that take a
wire client and a wire service and plug the framer and the dispatcher
into it.

*** Generate state diagrams                                            :epic:

There is nothing stopping us from reading the UML State Chart objects
in Dia and generating an FSM off of it, using one (or both) of boost's
state machines. We could make the state machine contain inheritable
methods which could be overridden by the user manually.

*** Add reflection support by using model and type enums               :epic:

#+begin_quote
*Story*: As a dogen user, I want to use reflection on generated models
so that I can do meta-programming.
#+end_quote

It may be useful to create enumerations for models, types and
properties within objects. This would in the future form the basis of
reflection. One could use implementation specific properties to set
the model ID and objects IDs.

*** Add C++-03 mode                                                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to create models in C++ 03 so that I
can interface with legacy code.
#+end_quote

It shouldn't be too hard to generate C++-03 code in addition to
C++-11. We could follow the gcc/odb convention and have a =-std=
option for this. The only problem would be testing - at present the
language settings comes from PFH scripts, not cmake, and we'd have to
make sure the compiler is not in C++-11 mode when compiling 03.

*** Add diff support                                                   :epic:

*New understanding*:

Just create a new facet call diff and make these classes generate a
simple textual representation of differences, inspired in
=diff=. Where the object is an entity provide its ID. In general just
provide some "path" to the difference, e.g. model/object/member
variable/etc.

*Old understanding*:

Dogen should have a =diff= option. When switched on, it would generate
=diff= classes. These are system types like keys and live in a
sub-folder of =types=. They have full serialisation, hashing etc
support like any other model type. The generated classes are:

- =differ=: for each model type a differ gets generated. this is a
  top-level class that diffs two objects of the same type.
- =changeset=: for each model type a changeset gets generated. it has
  a variant called =changeset_types=, made up of all the types of all
  properties in the model. if a model property has a model type then
  it uses the changeset for that type rather than the type itself; for
  all other cases, including containers, it uses the type itself.

In addition, we need set of enumerations in =reflection=. To start off
with all it contains is a list of classes in the model and a list of
fields in each class.

The =changeset= then has a container of =changeset_types= against a
reflection class and field.

Diff support is injected into the model just like keys. It also
requires that reflection support gets injected too.

*** Generation of cache code                                           :epic:

*New understanding*:

- create a cache interface in types;  all types marked as =cacheable=
  have gets, puts  etc.
- create a memcache implementation.
- create a type to string which converts a key made up of primitives
  into a underscore delimited string, used as a key in the cache.
- we should also consider external libraries like [[https://github.com/cripplet/cachepp][cachepp]].

*Old Understanding*:

Some thought on adding caching to dogen:

- we could have "modes" in dogen; instead of the
  relational/procedural/etc approach, it would be more task based:
  domain, cache.
- in cache mode we do not need to a target. we load up all diagrams in
  references and we find all types which have a stereotype of
  versioned. we mark them as generatable.
- if a target is supplied, it can only have objects of stereotype
  =mapping= or =cache= (tbd). These are simply a key-value-pair and
  determine additional caches to create. the attributes must be called
  key and value. the key entity must be versioned, value doesn't have
  to.
- for each versioned type, we create the following "shadow" objects: get,
  put, erase. each has versioned and unversioned. these objects are in
  the namespace cache::type_namespaces,
  e.g. =cache::credit_risk::model_configuration::versioned_get= or
  maybe
  =cache::credit_risk::model_configuration::versioned::get=. they are
  protocol messages to be sent on the wire.
- new formatter: named cache, with unordered maps for an entity with
  key, value. Any additional mappings that were added manually using
  the target are also added to the kvp mapping.
- new formatter: raw named cache, with unordered maps for an entity with
  key, value. value is raw storage, with an indication of the type of
  data being stored (e.g. xml, binary or text).
- new formatter: repository. contains all of the named caches.
- new formatter: dispatcher. given a message of one of the known types
  (get/put/remove) it dispatches it to the correct location in the
  repository and takes appropriate action. we may need one per named
  cache.
- put/get/erase are regular domain objects so they go through the
  usual formatters

Sample flow:

: credit_risk::model_configuration_unversioned_key k(123);
: cache::near near;
: cache::credit_risk::model_configuration::named_cache nc(
:    near.named_cache<credit_risk::model_configuration>());
: std::future v(n.async_get(k));

- we need to re-read the coherence docs to clarify the roles of
  front/back cache, local/remote cache and near cache.

*** Add SQL support to Dogen                                           :epic:
**** Note on formatters                                                :note:

- Use an attribute with the type to determine if we want only the ID of
  the foreign key in C++ code or if we want a whole type.

Formatters:

- File names are: FQN_ENTITY, e.g. kitanda_prototype_currency_table
- create: table, load, save, erase, test data generators, test
- drop: table, load, save, erase, test data generators, test
- domains
- create schema formatter
- create all tables
- create all procs
- drop all tables
- drop all procs
- drop all
- create all

**** Analyse deployment support on CMake

Ideally, get a state of affairs that resebles something like this:

- make deploy_database
- make undeploy_database

**** Test database deployment

We need to setup a build that deploys all the SQL (tables, procs, etc)
to a clean database, runs all SQL tests and un-deploys all the SQL.

**** Setup a postgres url in cmake file

The database password is set to trust. We should really have user
passwords. To make things more secure we should also pass in the
database credentials to the unit tests. One potential approach is to
do so in cmake. Example from VTK:

#+begin_src cmake
IF ( BUILD_TESTING )
   SET ( VTK_PSQL_TEST_URL "" CACHE STRING "A URL for a PostgreSQL server
         of the form psql://[[username[:password]@]hostname[:port]]/[dbname]" )
ENDIF ( BUILD_TESTING )
#+end_src

Suggestion: maybe there's a possibility of using an environment
variable for all the used parameters (username, hostname, etc)

**** Add multiple database support to makefiles

Our makefiles don't cope very well with the test/development database
separation. There is a massive hack required to populate both
databases (changing makefile manually and then reverting the change).

There should be a way of passing in the database name as an
environment variable into the makefile (not into the cmake as we want
to be able to change databases without having to rebuild makefiles).
