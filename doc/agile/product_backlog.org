#+title: Product Backlog
#+options: date:nil toc:nil author:nil num:nil
#+tags: { reviewing(r) }
#+tags: { story(s) epic(e) }

* Stories

This document contains the [[http://www.mountaingoatsoftware.com/agile/scrum/product-backlog][product backlog]] for Dogen.

** In next few sprints

Stories that we intend to look at in the very near future.

*** Add support for bintray                                           :story:

There is a new web application that interfaces with GitHub:
[[https://bintray.com/][bintray]].

- add JSON validation to bintray deployment descriptor.

Notes:

- an account was created [[https://bintray.com/mcraveiro][linked to GitHub]].
- it supports the uploads of [[https://bintray.com/mcraveiro/deb][debs]].
- [[https://bintray.com/docs/usermanual/uploads/uploads_howdoiuploadmystufftobintray.html][How Do I Upload My Stuff to Bintray?]]
- [[https://github.com/tim-janik/beast][Beast]] project [[https://raw.githubusercontent.com/tim-janik/beast/master/README.md][README]] with emblems
- Beast [[https://github.com/tim-janik/beast/blob/master/.travis.yml][travis.yml]]
- [[https://github.com/tim-janik/rapicorn/blob/master/citool.sh.][citool.sh]] script from [[https://github.com/tim-janik/rapicorn][Rapicorn]] project and their [[https://raw.githubusercontent.com/tim-janik/rapicorn/master/README.md][README]].

*** Remove =optional<list>=                                           :story:

We should not really be using optional<list>. The empty list is
sufficient for this.

*** Consider renaming formatters                                      :story:

After reading the [[http://martinfowler.com/eaaDev/PresentationModel.html][Presentation Model]] pattern a bit more carefully, it
seems it provides a good approach for formatters. If one thinks of the
file as the view, then the formatters are the presenters and the model
representing all presentation logic (e.g. =cpp=) is the presentation
model. We could:

- create a top-level folder called =presentation=;
- rename =formatters= to =core= and move it to =presentation=;
- move =cpp= to =presentation=;
- in =cpp=:
  - rename =formattables= to =presentables=;
  - rename =formatters= to =presenters=;
- in this light, =backend= is really the "meta-workflow" for all
  possible presentations. It should really live under presentation. It
  would make more sense to merge it with =core=, if it were not that
  core contains all sorts of loose bits that are useful only in the
  guts of presentation. We could call it =orchestration= or some such
  name. Or we could leave it as =presentation::backends=.
- move =file= to =backends=. We don't really want external clients to
  have to know about =core= just to obtain a single type. Also,
  backends shouldn't really have any dependencies.
- grep for formatting, formattables, formatter, format, etc. and
  ensure all usages have been replaced with present*.

We should wait until the "great refactoring" is done so that we do not
have to rename the legacy models too.

*Merged with duplicate*

These are not really formatters; not sure what the right name should
be though; templates?

*** Analysis work on handling varying levels of formatter optionality :story:

In some cases the formatter may decide that it does not need to create
a file. The only use case we have is the =namespace_info= where if
there is no documentation one does not want to create a formatter. At
present we filter out empty namespaces in the formatters' workflow,
but this is not very clean because it now means the workflow needs to
know about the formatter's logic.

This would also make things cleaner for the services hack where we do
not want to generate services for now. Actually not quite; for
services we still need to generate skeletons. There are three cases:
a) nothing should be generated, in which case we should filter these
elements before hand b) something should be generated, but we may not
actually overwrite the existing file and c) generate and write,
regardless.

Note that we do not need to change the stitch templates for this; the
decision is done before we call the template.

Tasks:

- change formatter interfaces to return =boost::optional<file>=
- change all formatters.

*** Consider removing the overwrite flag in =formatters::file=        :story:

Investigate if the overwrite flag makes sense in file; it seems we
only use it in two scenarios: force overwrite requested by user or
file contents have changed, both of which can be done in the
file_writer.

Actually this flag is needed. It is required to handle the case where
we do not code-generate files, unless they do not exist. For example,
for service headers and implementation we should create the files, but
then subsequently not touch them. The overwrite flag should be set to
false. We need to figure out how to implement this and remove the
hacks around file writing.

For now we have abused this flag to allow legacy files overrides of
the new world formatters. This is just until we move totally to new
world though.

*** Consider automatic injection of helpers                           :story:

At present we are manually calling:

: a.add_helper_methods();

On each of the class implementation formatters in order to inject
helpers. This is fine for existing cases, but its a bit less obvious
when adding the first helper to an existing template: one does not
quite know why the helper is not coming through without
investigating. One possible solution is to make the helper generation
more "mandatory". Its not entirely obvious how this would work.

*** Element properties includes non-target types                      :story:

We seem to be generating a lot of element properties and formatter
properties as well. We should only be generating these for the target
model.

*** Consider renaming settings to annotations                         :story:

Whilst its pretty clear now that settings are a strongly-typed
representation of the meta-data and properties are the post-processed
version, the names "settings" and "properties" still sound far too
similar. It would be nicer to have something more meta-data-like for
settings such as annotations. Read up the past discussions on
naming. One possible reason not to use annotations was because we used
it already in the formatters model. Perhaps that could be renamed to
something else, freeing up the name?

*** Consider creating a single top-level settings class               :story:

Since settings are nothing but meta-data, we should be able to read
them all in one go. Further: we should be able to compute up front the
inputs (root object, all other objects; sliced from the model) and the
size of the outputs (vector of settings). It would be a totally
parallelisable task. This also means we only need a single repository
by id for all settings.

This repository is then the input for the property workflow. Because
properties follow a dependency graph, we would still need to compute
them in some kind of order.

Actually, this is not entirely true: for all elements in the target
model we will have a single top-level class with all settings (or
almost all, since some settings only make sense to the root object
such as directory settings). However, for the reference models we will
have less settings. We should probably do some taxonomy work here and
try to figure out what categories of settings we have.

*** Run tests that are passing on windows                             :story:

At present we have a release build on windows but we are not running
any tests. This is because some of the tests are failing at the
moment. We should run all test suites that are green to ensure we
don't regress without noticing.

*** Add "namespaces" to name                                          :story:

Name should have a flat class with all namespaces in yarn, instead of
generating it on every formatter.

*** =always_in_heap= is not a very good name                          :story:

What the name is trying to say is: I have a type parameter and that
type parameter is always allocated in the heap. But it does not quite
convey that at all - it seems like the type itself is always in heap
the way we use it in resolver.

*** Check generation type before dispatching element                  :story:

At present we are doing this check in =visit=:

:     if (o.generation_type() == yarn::generation_types::no_generation)
:        return;

If we did it before the =visit= call we'd save the cost of
dispatching.

*** Add test with smart pointer in base class                         :story:

At present we have the following helper formatters registered against
SmartPointer:

:      {
:        "quilt.cpp.types.class_implementation_formatter": [
:          "<quilt.cpp.types><smart_pointer_helper>",
:          "<quilt.cpp.io><smart_pointer_helper>"
:        ]
:      }

This should have caused something to break. It didn't because we don't
seem to have a test case with a smart pointer on the base class. This
raises the interesting point: do we ever need more than one helper for
a given family and a given file formatter? If so, we should change it
from a list to a single shared pointer.

Interestingly, for AssociativeContainer we have:

:    "AssociativeContainer": [
:      {
:        "quilt.cpp.types.class_implementation_formatter": [
:          "<quilt.cpp.io><associative_container_helper>"
:        ]
:      },
:      {
:        "quilt.cpp.io.class_implementation_formatter": [
:          "<quilt.cpp.io><associative_container_helper>"
:        ]
:      },

*** Clean-up helper terminology                                       :story:

The name "helper" was never really thought out. It makes little
sense - anything can be a helper. In addition, we have helpers that do
not behave in the same manner (inserter vs every other helper). We
need to come up with a good vocabulary around this.

- static aspects: those that are baked in to the file formatter.
- dynamic aspects: those that are inserted in to the file formatter at
  run time.
- type-dependent dynamic aspects: those that are connected to the
  types used in the file formatter.

*** Dump container of files in formatter workflow                     :story:

At present we are polluting the log file with lots of entries for each
file name in formatter's workflow. Ideally we want a single entry with
a container of file names. The problem is, if we dump the entire
container we will also get the file contents. But if we create a
temporary container we will have to pay the cost even though log level
may not be enabled.

*** Type-bound helpers and generic helpers                            :story:

Not all helpers are bound to a type. We have the case of inserter
helper in io which is used by main formatters directly. We need to
make this distinction in the manual.

*** Check which properties need to loop through the entire model      :story:

In certain cases such as helpers we probably don't need to go through
all types; only the target types matter. Ensure we are not processing
other types for no reason.

*** Add validation for helper families                                :story:

At present we are checking that the name tree has the expected number
of type arguments:

:    const auto children(t.children());
:    if (children.size() != 1) {
:        BOOST_LOG_SEV(lg, error) << invalid_smart_pointer;
:        BOOST_THROW_EXCEPTION(formatting_error(invalid_smart_pointer));
:    }
:    smart_pointer_helper_stitch(fa, t);

In the future with dynamic helpers we will remove these checks. In
order to implement them we need to declare the type families up front
in a JSON file, with a name and number of type arguments. When
constructing the type helpers, we can check the name tree to make sure
the number of type arguments is correct.

This can be done as a helper setting (number of type arguments?).

Actually this is a core yarn property. So:

- add number of type arguments to object;
- read this as a dynamic field;
- during validation, check that all name trees that instantiate this
  object have the expected number of type arguments.
- in order to cope with cases such as variant we also need some kind
  of enum, e.g. type parameterisation: none, variable, fixed. if
  fixed, then number of type parameters must be non-zero.

*** Initialise formatters in the formatter's translation unit         :story:

At present we are initialising the formatters in each of the facet
initialisers. However, it makes more sense to initialise them on the
translation unit for each formatter. This will also make life easier
when we move to a mustache world where there may not be a formatter
header file at all.

*** Incorrect generation of forward declarations for all facets       :story:

Up to know we generated forward declarations for all facets for all
types that needed a =types= forward declaration. This was not a
problem for enumerations, objects, exceptions and so forth because
they were all serialisable. However with the introduction of visitor
forward declarations, we are now generating an invalid serialisation
forward declaration (because visitors are not serialisable). We need
to find a way to determine when to generate a forward declaration for
a facet. This could be done via enablement, but we don't want to do
too much hard-coding (e.g. if visitor then disable serialisation
facet, etc).

*** Generate formatter interfaces                                      :epic:

We should create another template language, in addition to stitch:
"wale". Wale is a very simple language that has templates that just do
token replacement. The tokens must have a special format:
={{{TOKEN}}}=. We receive a map of keys to values and do a blind
replacement to the keys on the wale document.

This links to stitch as follows:

- create a single file implementation of a formatter. It will
  implement both the provider interface and the appropriate formatter
  interface. It will call the stitch method to start off with. There
  are no headers, just cpp. It does the formatter registration.
- add support in stitch for "named sections": its possible to start a
  section and assign it a name. A stitch template will have two
  sections: inclusion provision and formatting.
- add support in stitch for "wale variables". These are just kvp's
  defined at the top:

: <#@ wale.variable="formatter_name=abcd" #>

  wale variables and sections are converted into a kvp container for
  wale input. Examples: facet, formatter name, etc.
- convert the formatter code into a wale template, adding wale
  variables as required.
- update stitch to detect wale usage and to call wale in those
  cases. This could be done by supplying a wale template:

: <#@ wale.template="abcd.wale" #>

- note that wale could be useful outside of stitch, for example for
  dart: we could wale-lise utility and then instantiate it for a given
  project.

*Previous Understanding*

It should be possible to generate some trivial types such as formatter
interfaces, formatter container, registrar and so on. For this we
need:

- a mustache type template;
- a set of fields from yarn types to be exposed to mustache;
- a list of types to iterate through.

Once we got this we could instantiate the templates. To integrate this
with knit we would need some way of specifying which types the
iteration would be over. We could mark a specific type with a given
stereotype, and then supply say the base class ("all leaf descendants
of xyz"). Dogen would then locate the descendants and for each call
the template.

For registrar and container its a bit trickier because we want a
collection of types in one go.

We also need a way to keep these templates away from the main (user
visible) code, since they are useful only for dogen.

See also [[https://github.com/cierelabs/boostache/tree/develop][boostache]].

Notes:

- we will need some "special" tags for copyright, includes
  etc. Includes will be particularly special because we need to
  augment the include list with additional includes. However, we may
  not even need to be aware of this.

*** Add support for inlining                                          :story:

We should be able to set a model-wide property that tells dogen to
generate inline methods for properties. We could be more flexible and
allow inlining at class level or just for a single property. We don't
have any use cases for these at present.

*** Strong type aliasing                                              :story:

One extremely useful feature would be to create "aliases" for types
which could be implemented as strongly-typed aliases where there is
language support. The gist of the problem is as described in here:

[[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3515.pdf][Toward Opaque Typedefs for C++1Y]]

This is also similar to the problem space of boost dimensions,
although their problem is more generic. The gist of it is that one
should be able to "conceptually" sub-class primitives such as int and
even types such as string and have the code generator create some
representation of that type that has the desired properties (including
a "to underlying" function). These types would not be interchangeable
with their aliased types. For example, if we define a "book id" as an
unsigned int, it should not be interchangeable with unsigned
int. Potentially it should also not have certain int abilities such as
adding/multiplication and so forth.

Links:

- [[http://www.boost.org/doc/libs/1_37_0/boost/strong_typedef.hpp][Boost Strong Typedef]]
- [[http://stackoverflow.com/questions/23726038/how-can-i-create-a-new-primitive-type-using-c11-style-strong-typedefs][How can I create a new primitive type using C++11 style strong
  typedefs?]]
- [[http://stackoverflow.com/questions/28916627/strong-typedefs][Strong typedefs]]
- [[http://programmers.stackexchange.com/questions/243154/c-strongly-typed-typedef][C++ strongly typed typedef]]

Note: the other stories in the backlog about typedefs are just about
the C++ feature, not this extension to it. Hence we called it "type
aliasing" to avoid confusion.

*** Computation of enablement values                                   :epic:

Note: this story is still *very* sketchy.

At present we have a very simple way of determining what formatters
are enabled: if a facet has been enabled by the user then all
formatters on that facet are enabled. This is a good starting point
but results in a lot of manual work:

- if we add a type which does not support all facets, we will generate
  invalid code. Users should be able to mark which facets are
  supported and then the graph of dependencies should do the right
  thing, propagating the disabled status.
- we are enabling all formatters in a facet. For hashing and forward
  declarations, it would make more sense to have a "dependency based
  enablement": if we determine that someone in the model needs that
  feature, we enable it, if not its disabled. Users can always
  override this and force it to be globally enabled.
- if a user creates a "service", all facets other than types are
  disabled. Ideally we should be able to define "enablement profiles"
  and then set an element's enablement profile. Each enablement
  profile is made up of a set of enabled facets. They could be
  supplied as a KVP. In fairness we probably just need "types and io"
  or "default".

One way to think of this problem is to imagine a matrix for each
element in element space. Each matrix is two-dimensional: one
dimension is the facets and the other are "dependent elements". These
are effectively made up of all attributes for each element, with a
name tree expansion. Each value of the matrix can either be 0
(disabled), 1 (enabled) or 2 (not computable). Not computable is a
hack to cope with cycles in the graph of dependencies.

Each value is computed by looking up an element's matrix and looking
for zeros. If there is one or more zero against a facet, the element's
value for that facet is zero. If there is a two we need to do a
two-pass whereby we first compute the matrix ignoring all the two's;
then, for each cycle we create a list of all the elements on that path
and the pair of elements that causes the cycle. We then compute the
enablement for this pair with a simple table (OR the computed
enablement values). We then traverse the cycle in reverse, updating
the twos to real values.

We could start with one large matrix with rows by element and columns
by feature. All values on this matrix are set to 1. We would then
multiply it against the global enablement matrix. We would then
multiply it by the local enablement matrix, for each element. We would
then compute the dependency matrices for all elements only taking into
account facets that are still enabled. We need to find the linear
algebra operation that takes a column with zeros and ones and returns
one if all rows are one and zero otherwise.

This produces the enabled facets. We then need to worry about the
formatters. There are a few sources of information:

- the facet enablement.
- the user local or global decision for that formatter.
- some kind of default formatter property (e.g. disabled by default).
- dependencies.

For these we need to create a "get dependencies" method in
each formatter which returns dependent formatters. For example, the
visitor formatter depends on the forward declarations formatter. This
is a static dependency. The more complex case is where there are
dynamic dependencies. For example, if hashing is detected for a given
type, we then need to enable the hashing facet for the containee. We
should probably hard-code this scenario for now.

We may want to make these computations disableable. For example: a)
all: no computation, everything is enabled b) all supported: all that
is supported is enabled c) by dependencies.

*** Indent stitch output using clang format                           :story:

We need to indent the output coming out of stitch as it is not
suitable for reading as-is.

This article has a good example of how to consume clang, but
unfortunately its not about indenting:

- [[https://bbannier.github.io/blog/2015/05/02/Writing-a-basic-clang-static-analysis-check.html][Writing a basic clang static analysis check]]
- [[http://zed0.co.uk/clang-format-configurator/][clang-format configurator]]
- [[http://clangformat.com/][online clang format]] - older site

*** Use clang format in knit                                          :story:

We need to indent the output coming out of knit. At present our stitch
templates are super-complex purely because we are trying to get the
indentation right. In most cases we don't. We need to:

- remove indent filter and any other indentation "helpers"
- update all templates to output everything as simply as possible, in
  one long line if need be. We may still need to use sequence helper
  but hopefully for very trivial cases.
- plug in clang format at the end of the knit pipeline, using either a
  default set of options or a user supplied set of options (via a
  command line parameter).

Merged with other story:

We should generate un-indented c++ code and then rely on clang-format
to do the indentation. We can allow users to supply their own
configurations and supply those to clang. This can be done via the
meta-data, or if there is a well defined file for clang, we could use
it instead.

Note that using clang to manage indentation will make things a lot
slower. Note also that clang supports Java and may in future support
C#. See [[http://clang.llvm.org/docs/LibFormat.html][LibFormat]].

Another option is to create fallback modes. The preferred indenter for
a given language (say c++) may not exist for another language (say
c#); for these we use a dogen created indenter that is very basic. It
may support some of the configuration parameters supplied for the
clang indenter. The key thing is that we take away indenting from the
formaters - they become flat - and then we always apply the indenter;
either a clang based one or a simplified one. Either way, the code
should live in formatters and make use of the language-specific
folders as required.

*** Formatters need different =enabled= defaults                      :story:

We should be able to disable some formatters such as forward
declarations. Some users may not require them. We can do this using
dynamic extensions. We can either implement it in the backend or make
all the formatters return an =std::optional<dogen::formatters::file>=
and internally look for a =enabled= trait.

We need to be able to distinguish "optional" formatters - those that
can be disabled - and "mandatory" formatters - those that cannot. If a
user requests the disabling of a mandatory formatter, we must
throw. This must be handled in enabler.

This story was merged with a previous one: Parameter to disable cpp
file.

#+begin_quote
*Story*: As a dogen user, I want to disable cpp files so that I don't
generate files with dummy content when I'm not using them.
#+end_quote

It would be really useful to define a implementation specific
parameter which disables the generation of a cpp file for a
service. This would stop us from having to create noddy translation
units with dummy functions just to avoid having to define exclusion
regexes.

In some cases we may need a "enable by usage". For example,
it would be great to be able to enable forward declarations only for
those types for which we required them. Same with hash. We can detect
this by looking at the generated include dependencies. However,
because the include dependency only has a directive, we cannot tell
which formatter it belonged to. This would require some augmenting of
the directive to record the "origination" formatter.

*** Consider using a proper JSON library                              :story:

We could use a full-blow JSON parser rather than the property tree
one. One option is [[https://github.com/cierelabs/json_spirit][json_spirit]].

Another option is [[https://github.com/miloyip/rapidjson][RapidJson]].

Actually there is a project comparing JSON libraries:

[[https://github.com/miloyip/nativejson-benchmark][nativejson-benchmark]]

One interesting library is [[https://github.com/dropbox/json11][Json11]].

When we implement this we should first provide support for JSON with
roundtripping tests and only then change io to be implemented in terms
of JSON. These should be two totally separate stories.

For the previous attempt to integrate RapidJson see this commit:

b2cce41 * third party: remove includes and rapid json

Merged stories:

Add support for JSON serialisation

We should have proper JSON serialisation support, for both reading and
writing. We can then implement IO in terms of JSON.

*Raw JSON vs cooked JSON*

If we do implement customisable JSON serialisation, we should still
use the raw format in streaming. We need a way to disable the cooked
JSON internally. We should also re-implement streaming in terms of
this JSON mode.

*** Supply model references via meta-data rather than command line    :story:

It doesn't make any sense to have model dependencies in the command
line. After all, the model cannot be interpreted without them. A
better way to do this would be to split this functionality into two:

- command line supplies "import directories", that is, directories
  to search when looking for models. By default the system directory
  is already in the path.
- model supplies "import statements". The problem here is that we need
  to also supply the file name of the model. We could perhaps omit the
  extension and then load all files that match (e.g. =.dia=, =.json=,
  etc). If more than one matches we should error. Actually we should
  just supply the full filename, as well as keep the current notation
  for the external project path.

This is also a nice way to avoid loading system models unnecessary;
users still need to declare the models they depend on, regardless if
system or user.

Each model should also supply the external module path as meta-data.

Merged stories

*External module path and references as meta-data*

It actually does not make a lot of sense to allow users to supply
external module paths and references as command line options. This is
because the model will fail to build unless we provide the correct
ones; these are not configurable items in this sense. The project
path, etc are - and so should remain command line options.

We need to move these two into the meta-data. This would also mean we
no longer need to pass in external module paths for references, which
is much cleaner.

*** Identifier parser has hard-coded primitives                       :story:

Instead of using the hardware model, we have hard-coded all of the
primitives. In addition, there are some primitives which are C++
specific (=wchar_t=), as well as others which are only valid in
certain cases such as =void=. This needs a bit of thinking.

We could look for all primitives in the global namespace. Or we could
have a tag in the types that describes them in a way that we can
filter: =hardware_type= flag? The problem is that we need the
identifier parser in order to load models and we need the loaded
models in order to locate these types.

One solution for this problem is to move the properties expansion to
later on after the front end workflow has finished executing. Once we
have a merged model we can then easily take the primitives container
and inject that into the identifier parser. The only slight problem is
that we need to know of the top-level modules for a given model in
order to use the identifier parser. This means we need to expand
unparsed types before merging. There is a circular dependency here.

We somehow need a first pass to obtain all the primitives and a second
pass to parse.

*** Remove =service= stereotype                                       :story:

This really just means non-generatable, or do not generate. We already
have a stereotype for this. Remove =service= and any other stereotype
which is not being used such as =value_object= etc.

Actually, non-generatable is not a stereotype really. We should
instead have some meta-data that can affect generation:

- do not generate: do nothing at all. For references only. If a file
  exists with this file name, it will be deleted as part of
  housekeeping.
- generate blank file if it doesn't exist: we don't even want a
  template.
- generate with content if it doesn't exist, do not touch otherwise:
  what we call services at the moment. Generate a "template" that then
  gets filled in manually.
- generate and merge: merge the contents of the generated file with
  the current contents in the file system. When we support merging.
- generate and overwrite: generate the file and overwrite whatever
  exists in the file system.

This could be called "generation policy".

The second behaviour we get for free with services is that we disable
all facets except for types. A few points:

- we may want to have io, serialisation, etc. This is not possible at
  present. If a state of a service is made up of supported types, we
  could even use existing code generation.
- in order for this to be implemented correctly we need to hook in to
  the enablement management somehow. In addition, it seems each facet
  can have its own generation policy. For example we may want to
  manually create types but automatically generate io.
- the best way to handle this may be to setup "enablement profiles"
  that the user can hook up to. For example we could have a "default"
  profile that enables all facets (or uses facet defaults), a second
  "service" profile that enables types with partial generation and io
  with full generation and so on. We probably also need "generation
  profiles" to go with "enablement profiles".

*** Handle registration of services properly                          :story:

We need a way to determine if a type which is part of a generalisation
should be added to the registrar or not. In =generalisation_indexer=:

:     // FIXME: massive hack. must not add leafs for services.

One way would be to check if serialisation is enabled for that type
and if not, skip the type.

Another way is to check if the type is generatable. If not, skip
it. If we do it this way we need to wait for the generatable clean up.

*** Refactor code around model origination                            :story:

- remove origin types and generation types, replacing it with just a
  boolean for is target. Actually we need something like:
  proxy_reference, non_proxy_reference, target. We also need a good
  name for this enumeration.
- add a model-level flag: is empty. It is true if there are no model
  elements. has_generatable_types is then is_target && !is_empty.
- at present we are using origin type to determine whether to create a
  registrar, etc in cpp model. There is no other use case for
  this. This is done in several places due to the bad handling of C++
  specific types. Grep for =references= in =cpp= to find all
  locations. We could split references into two (dogen, non-dogen). Or
  references could have a origin type too.
- we should also replace has generatable types with something more
  like "target model has types" or "is target model empty". The idea
  we are trying to capture is that the target model contained at least
  one type. This could be set by the merger when it processes the
  target model.

*Previous Understanding*

In the past we added a number of knobs around generation, all with
their own problems:

- =origin_types=: was the model/type created by the user or the
  system. in reality this means did the model come from Dia or
  JSON. this is confusing as the user can also add JSON files (their
  own model library) and in the future the user can use JSON
  exclusively without needed Dia at all.
- =generation_types=: if the model is target, all types are to be
  generated /unless/ they are not properly supported, in which case
  they are to be "partially" generated (as is the case with
  services). This is a formatter decision and yarn should not know
  anything about it. Actually this is not quite true; users may want
  to stop generation.

These can be replaced by a single enumeration that indicates if the
type/model is target or not.

This work should be integrated with the model types story.

Merged stories:

*Split references into dogen and non-dogen models*

If we had two containers of references, one for dogen models and
another one for non-dogen models - which we could give a nice name, to
imply its foreign origin - we could then use the dogen references for
registrar, etc. This is a replacement for the origin type.

We need a good name for these. Candidates:

- proxy model: represents something that exists in the outside
  world. e.g. =is_proxy=.

*** Update copyright notices                                          :story:

We need to update all notices to reflect personal ownership until DDC
was formed, and then ownership by DDC.

- first update to personal ownership has been done, but we need to
  test if multiple copyright entries is properly supported.

*** Implement all formatter interfaces                                :story:

We still have a couple of skeleton interfaces:

- primitve
- concepts

*** Do not compute inclusion directives for system models             :story:

It seems we are computing inclusion directives and other path
derivatives for system models:

: {
:   "__type__": "dogen::cpp::expansion::path_derivatives",
:   "file_path": "/home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../test_data/all_primitives/actual/std/include/std/serialization/unique_ptr_fwd_ser.hpp",
:   "header_guard": "STD_SERIALIZATION_UNIQUE_PTR_FWD_SER_HPP",
:   "inclusion_directive": "<quote>std/serialization/unique_ptr_fwd_ser.hpp<quote>"
: }

This comes out of the workflow, so we possibly are then ignoring it
for the non-target types. So:

- can we avoid computing these altogether?
- are we ignoring it?

Actually this is the usual problem with the "origin" of the type. We
need a way to determine if this type needs computations or not. We
need to create a story to clean up the =origin_type= and
=generation_type= and then we can make use of it to determine if we
need to compute inclusion, path etc or not.

*** Consider renaming registrar in boost serialisation                :story:

At present we have a registrar formatter that does the boost
serialisation work. However, the name =registrar= is a bit too
generic; we may for example add formatters for static registrars. We
should rename this formatter to something more meaningful. Also the
name registrar is already well understood to mean static registrar.

This is a big problem now that we cannot add a type with the name
registrar to the main model as it clashes with the serialisation
registrar.

We could simply name it serialisation registrar or some such name that
is very unlikely to clash. We should then have a validation rule that
stops users from defining types with that name.

We need to go through all of the renamed registrars and fix them.

Another option is to allow users to supply a name via meta-data to
avoid name clashes. We could error when the user has defined a type.

Actually, since the clash is only internal - the names we are
generating on the fly are clashing with the user defined names - we
should probably have a "postfix" that can be added in case of
clashes. The generated code will not cause problems, its just the
formattables pipeline.

*** Duplicate fields in JSON result in non-intuitive errors           :story:

By mistake we added the same field twice in JSON:

:            "extensions" : {
:                "cpp.types.class_header_formatter.inclusion_directive" : "<boost/property_tree/ptree.hpp>",
:                "cpp.serialization.class_header_formatter.inclusion_directive" : "<boost/property_tree/ptree_serialization.hpp>",
:                "cpp.io.class_header_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.io.class_implementation_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.io.class_implementation_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.hash.class_header_formatter.inclusion_required" : false

The resulting error message was not particularly helpful:

: 2015-06-17 13:56:06.658500 [DEBUG] [sml.json_hydrator] Processing type: <boost><property_tree><ptree>
: 2015-06-17 13:56:06.658519 [ERROR] [dynamic.field_instance_factory] Expected at most one element

*** Formatters with duplicate names result in non-intuitive errors    :story:

We added two formatters to io with the same name by mistake and the
resulting error was not particularly enlightening:

: std::exception::what: Qualified name defined more than once: cpp.io.enum_header_formatter.inclusion_required

We should have a very early on validation to ensure formatters have
distinct names.

Merged stories:

*Check for duplicate formatter names in formatter registrar*

At present it is possible to register a formatter name more than
once. Registrar should keep track of the names and throw if the name
is duplicated.

*** Consider adding =fileset= to formatters                           :story:

We are using collections of files quite a bit, and it makes sense to
create an abstraction for it such as a =fileset=. However, for this to
work properly we need to add at least one basic behaviours: the
ability to merge two file sets. Or else we will end up having to
unpack the files, then merging them, then creating a new fileset.

Problem is, we either create the fileset as a non-generatable type -
not ideal - or we create it as generatable and need to add this as a
free function. We need to wait until dogen has support for merging
code generation.

*** Make JSON yarn a fully supported frontend                          :story:

#+begin_quote
*Story*: As a dogen user, I want to be able to write my domain models
in JSON since I don't have any need for UML visualisation.
#+end_quote

At present we are using an yarn JSON format to supply Dogen the system
libraries. However, there is nothing stopping us from having a
full-blown JSON frontend useful for code generation. For this we need:

- flag to state if its a target model or not;
- ability to supply external module path;
- ability to supply all of the missing information for yarn types
  (properties for object, stereotypes, enumerations, etc).

In order to test this we could generate a model from both Dia and JSON
and make sure we arrive at the same yarn.

As part of this work we probably need to create a new stage in the yarn
pipeline where we populate:

- inheritance related properties (is_parent, leaves, is_final)

We need to look at the dia to sml transformer and see what it is doing
that is also required by JSON and move it to yarn.

We should have a look at the Boost Fusion approach:

- [[http://jrruethe.github.io/blog/2015/05/21/boost-fusion-json-serializer/][Boost Fusion JSON Serialiser]]

*** Integration of stitch and dogen                                   :story:

Now that we have implemented stitch and proved it works (more or
less), we need to think how we can make using stitch from dogen
easier. At present there is not integration at all:

- users need to create regexes to ensure dogen does not trample on
  stitch files:

:    --ignore-files-matching-regex .*stitch
:    --ignore-files-matching-regex .*_stitch.hpp
:    --ignore-files-matching-regex .*_stitch.cpp

- users need to manually create a header file for each stitch
  template.
- users need to create stitch targets and run them to ensure the
  templates have been expanded. This means its possible to get dogen
  and stitch out of sync (but for now not a big problem).

In the ideal world, when we knit a model it would be nice if it could
also stitch as required. This could be achieved as follows:

- Create a meta-data tag that tells dogen a type has an associated
  stitch template with it.
- Create =cpp= types that represent the stitch header and
  implementation.
- Transformer needs to look for the meta-data tag and instantiate the
  =cpp= types.
- Create a =cpp= formatter for the header, as per regular
  formatters. The slight challenge here is that the formatter needs to
  be instantiable across facets, which we do not support at the
  moment.
- Create a cpp formatter for the implementation which instantiates
  stitch with the template and uses it to create a file. Same
  challenge as with the header.

*Previous Understanding*

- stitch can still be integrated with dogen. We could use meta-data to
  link a formatter (well, any class that needs stitch really, but at
  present just a formatter) with a stitch template. For example, a
  =class_header_formatter= could have a "is stitchable" flag set to
  on. This would then mean that dogen would look for a
  =class_header_formatter.stitch= file in the same directory as the
  CPP file. It would then use that to create a
  =class_header_formatter_stitch.cpp= file. It would also
  ignore/generate a =class_header_formatter_stitch.hpp= file and
  automatically add it to the inclusion dependencies of
  =class_header_formatter.cpp=. These are injected into stitch as we
  instantiate the template since stitch supports meta-data (we do need
  a way to inject the meta-data from dogen into the meta-data in the
  template; perhaps a kvp container passed in to the stitch workflow
  which could then be handed over to the parser). All these files are
  automatically added to the list of "exceptions" for housekeeping so
  that they do not get deleted. However, stitch would not know
  anything at all about any of this; this is all knitter's
  functionality. The problem is at present we haven't got a good place
  to perform the stitching as part of knitter's workflows. Perhaps as
  part of the expansion, we could set a number of stitch fields which
  would then be picked up by some knit-specific workflow classes.

*** Rename hash and serialization facets                              :story:

We originally called our support for =std::hash= just =hash= and our
support for =boost::serialization= just =serialization=. The problem
is:

- we may want to also support =boost::hash=.
- we may want to support other serialisation types.

We should rename these. Perhaps:

- =std_hash=
- =boost_serialization=: a tad verbose, but quite explicit.

*** Do not copy models in merger                                      :story:

At present we are adding the partial models into the merger by copying
them into an associative container. It would be nicer to avoid the
copying as it adds no value. This should wait until we have a way to
get performance numbers out.

In fact do we even need to have a two step process? Can we not add and
merge as we go along.

*** Add top-level module names to yarn                                :story:

In a couple of places we are computing the top-level modules. We
should just compute it once and have this as part of the yarn model.

Look for stories related to "model module" in case we have covered
this already.

*** "current" is not the best of names in name tree                   :story:

We need to find a slightly more meaningful name for the "current" name
on the name tree. It was just about alright and then we went and
introduced =is_current_simple_type=, which is unintelligible.

*** Code generation of dynamic instances                              :story:

Note: this story probably should wait until we rename settings to
annotations.

We seem to have a pretty well established usage pattern for dynamic,
so it may be a candidate for code generation. All we need is:

- a stereotype to mark a class as dynamic; the attributes of the class
  are dynamic fields, and their types must be one of the valid values
  for dynamic fields. The default value is used for the field's
  default value. Qualified name, ownership hierarchy, definition type,
  scope, etc are supplied as meta-data.
- stereotype name should be something like =DynamicFieldGroup=.
- the injection of the settings class is done by looking at the
  =DynamicFieldGroup= class and mapping the dynamic types to C++
  types. Note: this mapping should be dynamic too so that we can use
  it for other languages. We just need a meta-data tag for this, like
  we do with default enum value.
- the injection of the settings factory class is a bit more
  complicated; we need to mark the object as a settings factory. At
  present we have object types, but it was supposed to be removed
  after a refactoring. Actually we just need to create a new kind of
  element (=dynamic_settings_factory=?). In addition, settings factory
  may also need to take in some parameters such as facet/formatter.
- a stitch template that generates the settings factory.
- a stitch template that registers the dynamic field definition;
  instead of JSON we can just generate c++ code to perform the
  injection.
- we could also generate the repository and in most cases the
  repository factory. The only case where this breaks down is when we
  need to look at properties too.
- we should have a number of knobs to control generation: a) generate
  field injection b) generate settings factory c) generate repository
  d) generate repository factory.

We also need to merge the traits class directly into the factory. In
the majority of cases, we have traits just to access the fields. But
there are a few cases where we use traits for other purposes such as
formatter naming.

*** Consider using indices rather than associative containers          :epic:

Once we generate the final model the model becomes constant; this
means we can easily assign an [[https://en.wikipedia.org/wiki/Ordinal_number][ordinal number]] to each model
element. These could be arranged so that we always start with
generatable types first; this way we always generate dense
containers - there are some cases where we need both generatable types
and non-generatable types; in other cases we just need generatable
types; we never need just non-generatable types. We also need to know
the position of the first non-generatable type (or alternatively, the
size of the generatable types set).

Once we have this, we can start creating vectors with a fixed size
(either total number of elements or just size of generatable
types). We can also make it so that each name has an id which is the
ordinal (another model post-processing activity). Actually we should
call it "type index" or some other name because its a transient
id. This means both properties and settings require no lookups at all
since all positions are known beforehand (except in cases where the
key of the associative container must be the =yarn::name= because we
use it for processing).

In theory, a similar approach can be done for formatters too. We know
upfront what the ordinal number is for each formatter because they are
all registered before we start processing. If formatters obtained
their ordinal number at registration, wherever we are using a map of
formatter name to a resource, we could use a fixed-size
vector. However, formatters may be sparse in many cases (if not all
cases?). For example, we do not have formatter properties for all
formatters for every =yarn::name= because many (most) formatters don't
make sense for every yarn type. Thus this is less applicable, at least
for formatter properties. We need to look carefully at all use cases
and see if there is any place where this approach is applicable. It is
probably going to be more useful for formatters than elements.

Tasks:

- in resolver, assign element indices and update property names with
  them.
- change final model to have a vector of size maximum index (a
  property of the intermediate model).
- in the final model generation, for each type, look at its index and
  populate the slot accordingly.
- update quilt to use the indices where possible.

*** Handle "special includes" correctly                     :reviewing:story:

We did a quick hack to handle "special includes": we simply "detected"
them in include builder and then did the appropriate action in each of
the include providers. In order to make this work dynamically, we need
somehow to have "associated includes" on a per type basis. For
example:

- type =x= requires include =y= in formatter =f=.

This can easily be achieved via an "additional inclusion directive"
which is a container. For example:

:        "extensions" : {
:                "quilt.cpp.helper.family" : "Dereferenceable",
:                "quilt.cpp.types.class_header_formatter.inclusion_directive" : "<boost/weak_ptr.hpp>",

Could have:

:                "quilt.cpp.types.class_header_formatter.additional_inclusion_directive" : "<some_include.hpp>",

If multiple are provided then they are all added. This highlights an
important point: we need a way to inject type specific includes from a
formatter. It makes no sense to declare all of these up front in a
library since we do not know what all possible formatters are, nor
what requirements they may have for inclusion. At the same time,
formatters cannot be expected to declare types. The solution is to be
able to "inject" these dependencies from a JSON file associated with
the formatter. We could supply the qualified name and the properties
to inject. This problem can be solved later on - create a separate
story for this.

*** Registrar in serialisation is not stable sorted                   :story:

We seem to have a traffic light diff on =registrar_ser.cpp=:

: -    dogen::config::register_types(ar);
:      dogen::quilt::cpp::register_types(ar);
:      dogen::yarn::register_types(ar);
: +    dogen::config::register_types(ar);

This is probably a lack of a stable sort in model dependencies.

*** Support only specific attributes for certain facets               :story:

Whenever an object has a unique identifier, it may make sense to make
use of it for:

- hashing
- equality
- less than

And so forth. For example, names and name trees don't really require
comparing the entire state of the object. We need a way to mark
properties against each facet in the meta-data.

*** Rename methods parsing name trees                                 :story:

We have a variety of names for the methods parsing name trees
recursively. The best one seems to be =walk_name_tree=. We should use
this name consistently.

*** Add support for selectively disabling helpers on a family         :story:

At present when a type belongs to a helper family it must provide all
helpers across all facets. This means that we can't support the cases
where a helper is required for one facet for one type but not for
others. For example, we cannot create a family for =Dereferenceable=
including both smart pointers and optionals because optional does not
need a helper for =types=.

One solution for this is to allow disabling the helper for a given
type on a given facet. However, our templating mechanism in dynamic is
not able to cope with this use case. Changes required:

- add a "component" to ownership hierarchy. This would be "helper" in
  our case. We should also set "type" which has been hacked via the
  qualified name.
- create a supported/enabled field with a component of helper and a
  facet template. We could change this to formatter template if
  required in the future.
- merge the families of optional and smart pointer into
  =Dereferenceable=.
- disable the helper for types for optional.
- update the helper settings to read this new field.
- enabled method now checks the helper properties.

*** Consider caching "all modules" in location                        :story:

At present we are adding the module lists together to build the
qualified name; location could have a "all modules" list that
concatenates external, model and internal modules. We should look at
performance before doing this change though.

*** Consider reducing the number of qname lookups in cpp model        :story:

At present we are still using =yarn::name= in a lot of repositories in
quilt. We already had one go in moving to id's but there are still
quite a few left. Investigate to see if there are more that can be
moved.

*** Windows build release test failures                               :epic:

Dia tests:

: [00:27:30] C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: "cmd.exe" exited with code -1073741515. [C:\projects\dogen\build\output\projects\dia\tests\run_dia.tests.vcxproj]

Dia hydrator tests:

: [00:27:31] unknown location : fatal error : in "modeline_group_hydrator_tests/hydrating_emacs_modeline_group_results_in_expected_modelines": class std::runtime_error: Error during test [C:\projects\dogen\build\output\projects\formatters\tests\run_formatters.tests.vcx
: [00:27:31] proj]
: [00:27:31]   C:\projects\dogen\projects\formatters\tests\modeline_group_hydrator_tests.cpp(142): last checkpoint: hydrating_emacs_modeline_group_results_in_expected_modelines
: [00:27:31]
: [00:27:31]   *** 1 failure is detected in the test module "formatters_tests"

Knit:

: [00:27:35] C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: "cmd.exe" exited with code -1073741515. [C:\projects\dogen\build\output\projects\knit\tests\run_knit.tests.vcxproj]
: [00:27:35] Done Building Project "C:\projects\dogen\build\output\projects\knit\tests\run_knit.tests.vcxproj" (default targets) -- FAILED.

Stitch:
: [00:27:36]   C:\projects\dogen\projects\utility\src\test_data\validating_resolver.cpp(39): Throw in function class boost::filesystem::path __cdecl dogen::utility::test_data::validating_resolver::resolve(class boost::filesystem::path)
: [00:27:36]   Dynamic exception type: class boost::exception_detail::clone_impl<class dogen::utility::filesystem::file_not_found>
: [00:27:36]   std::exception::what: File not found: C:\projects\dogen\build\output\bin\../test_data\stitch/input/simple_template.stitch
: [00:27:36] unknown location : fatal error : in "workflow_tests/simple_template_results_in_expected_output": class std::runtime_error: Error during test [C:\projects\dogen\build\output\projects\stitch\tests\run_stitch.tests.vcxproj]
: [00:27:36]   C:\projects\dogen\projects\stitch\tests\workflow_tests.cpp(48): last checkpoint: simple_template_results_in_expected_output
: [00:27:36]
: [00:27:36]   C:\projects\dogen\projects\utility\src\test_data\validating_resolver.cpp(39): Throw in function class boost::filesystem::path __cdecl dogen::utility::test_data::validating_resolver::resolve(class boost::filesystem::path)
: [00:27:36]   Dynamic exception type: class boost::exception_detail::clone_impl<class dogen::utility::filesystem::file_not_found>
: [00:27:36]   std::exception::what: File not found: C:\projects\dogen\build\output\bin\../test_data\stitch/input/complex_template.stitch
: [00:27:36]
: [00:27:36]   C:\projects\dogen\projects\utility\src\test_data\validating_resolver.cpp(39): Throw in function class boost::filesystem::path __cdecl dogen::utility::test_data::validating_resolver::resolve(class boost::filesystem::path)
: [00:27:36]   Dynamic exception type: class boost::exception_detail::clone_impl<class dogen::utility::filesystem::file_not_found>
: [00:27:36]   std::exception::what: File not found: C:\projects\dogen\build\output\bin\../test_data\stitch/input/empty_template.stitch
: [00:27:36]
: [00:27:36]   *** 3 failures are detected in the test module "stitch_tests"
<snip>

Test model sanitizer:

: [00:27:39]   CMake does not need to re-run because C:\projects\dogen\build\output\projects\test_models\test_model_sanitizer\tests\CMakeFiles\generate.stamp is up-to-date.
: [00:27:39]   Running 127 test cases...
: [00:27:39] C:/projects/dogen/projects/utility/include/dogen/utility/test/hash_tester.hpp(57): error : in "std_model_tests/validate_hashing": check hasher(a) == hasher(b) has failed [C:\projects\dogen\build\output\projects\test_models\test_model_sanitizer\tests\run_test_model_sanitizer.tests.vcxproj]
: [00:27:39] C:/projects/dogen/projects/utility/include/dogen/utility/test/hash_tester.hpp(57): error : in "std_model_tests/validate_hashing": check hasher(a) == hasher(b) has failed [C:\projects\dogen\build\output\projects\test_models\test_model_sanitizer\tests\run_te
: [00:27:39] st_model_sanitizer.tests.vcxproj]
: [00:27:40]
: [00:27:40]   *** 2 failures are detected in the test module "test_model_sanitizer_tests"

Yarn.dia:

: [00:27:42]   C:\projects\dogen\projects\utility\src\test_data\validating_resolver.cpp(39): Throw in function class boost::filesystem::path __cdecl dogen::utility::test_data::validating_resolver::resolve(class boost::filesystem::path)
: [00:27:42]   Dynamic exception type: class boost::exception_detail::clone_impl<class dogen::utility::filesystem::file_not_found>
: [00:27:42]   std::exception::what: File not found: C:\projects\dogen\build\output\bin\../test_data\yarn.dia/expected/class_in_a_package.diaxml
: [00:27:42] unknown location : fatal error : in "workflow_tests/class_in_a_package_dia_transforms_into_expected_yarn": class std::runtime_error: Error during test [C:\projects\dogen\build\output\projects\yarn.dia\tests\run_yarn.dia.tests.vcxproj]
: [00:27:42]   C:\projects\dogen\projects\yarn.dia\tests\workflow_tests.cpp(85): last checkpoint: class_in_a_package_dia_transforms_into_expected_yarn

Yarn.Json

: [00:27:42]   Building Custom Rule C:/projects/dogen/projects/yarn.json/tests/CMakeLists.txt
: [00:27:42]   CMake does not need to re-run because C:\projects\dogen\build\output\projects\yarn.json\tests\CMakeFiles\generate.stamp is up-to-date.
: [00:27:42]   Running 12 test cases...
: [00:27:42]
: [00:27:42]   C:\projects\dogen\projects\yarn.json\src\types\hydrator.cpp(251): Throw in function class dogen::yarn::intermediate_model __cdecl dogen::yarn::json::hydrator::hydrate(class std::basic_istream<char,struct std::char_traits<char> > &) const
: [00:27:42]   Dynamic exception type: class boost::exception_detail::clone_impl<class dogen::yarn::json::hydration_error>
: [00:27:42]   std::exception::what: Failed to parse JSON file<unspecified file>(1): expected value
: [00:27:42] unknown location : fatal error : in "hydrator_tests/cpp_std_model_hydrates_into_expected_model": class std::runtime_error: Error during test [C:\projects\dogen\build\output\projects\yarn.json\tests\run_yarn.json.tests.vcxproj]
: [00:27:42]   C:\projects\dogen\projects\yarn.json\tests\hydrator_tests.cpp(386): last checkpoint: cpp_std_model_hydrates_into_expected_model

Yarn:

: [00:27:42] C:/projects/dogen/projects/utility/include/dogen/utility/test/hash_tester.hpp(57): error : in "hashing_tests/validate_hashing": check hasher(a) == hasher(b) has failed [C:\projects\dogen\build\output\projects\yarn\tests\run_yarn.tests.vcxproj]
: [00:27:42] C:/projects/dogen/projects/utility/include/dogen/utility/test/hash_tester.hpp(57): error : in "hashing_tests/validate_hashing": check hasher(a) == hasher(b) has failed [C:\projects\dogen\build\output\projects\yarn\tests\run_yarn.tests.vcxproj]
: [00:27:42] C:/projects/dogen/projects/utility/include/dogen/utility/test/hash_tester.hpp(57): error : in "hashing_tests/validate_hashing": check hasher(a) == hasher(b) has failed [C:\projects\dogen\build\output\projects\yarn\tests\run_yarn.tests.vcxproj]
: [00:27:44]
: [00:27:44]   *** 3 failures are detected in the test module "yarn_tests"

*** Windows build debug failures                                      :epic:

The windows debug build fails to link with the following errors:

: C:\projects\dogen\build\output\projects\config\tests\config.tests.vcxproj" (default target) (14) ->
: (Link target) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\config\tests\config.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\dia\tests\dia.tests.vcxproj" (default target) (17) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\dia\tests\dia.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\dynamic\tests\dynamic.tests.vcxproj" (default target) (21) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\dynamic\tests\dynamic.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\formatters\tests\formatters.tests.vcxproj" (default target) (29) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\formatters\tests\formatters.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\knit\tests\knit.tests.vcxproj" (default target) (36) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\knit\tests\knit.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\knitter\src\knitter.vcxproj" (default target) (37) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\knitter\src\knitter.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\knitter\tests\knitter.tests.vcxproj" (default target) (38) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\knitter\tests\knitter.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\quilt.cpp\tests\quilt.cpp.tests.vcxproj" (default target) (39) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\quilt.cpp\tests\quilt.cpp.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\quilt\tests\quilt.tests.vcxproj" (default target) (40) ->
:   libboost_log-vc140-mt-gd-1_60.lib(default_attribute_names.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\quilt\tests\quilt.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\test_models\seam\tests\seam.tests.vcxproj" (default target) (41) ->
:   libboost_log-vc140-mt-gd-1_60.lib(core.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\test_models\seam\tests\seam.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\stitch\tests\stitch.tests.vcxproj" (default target) (46) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\stitch\tests\stitch.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\stitcher\src\stitcher.vcxproj" (default target) (47) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\stitcher\src\stitcher.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\stitcher\tests\stitcher.tests.vcxproj" (default target) (48) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\stitcher\tests\stitcher.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\test_models\test_model_sanitizer\tests\test_model_sanitizer.tests.vcxproj" (default target) (49) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\test_models\test_model_sanitizer\tests\test_model_sanitizer.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\utility\tests\utility.tests.vcxproj" (default target) (53) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\utility\tests\utility.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\yarn.dia\tests\yarn.dia.tests.vcxproj" (default target) (54) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\yarn.dia\tests\yarn.dia.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\yarn.json\tests\yarn.json.tests.vcxproj" (default target) (55) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\yarn.json\tests\yarn.json.tests.vcxproj]
: "C:\projects\dogen\build\output\ALL_BUILD.vcxproj" (default target) (1) ->
: "C:\projects\dogen\build\output\projects\yarn\tests\yarn.tests.vcxproj" (default target) (56) ->
:   libboost_log-vc140-mt-gd-1_60.lib(unhandled_exception_count.obj) : fatal error LNK1112: module machine type 'X86' conflicts with target machine type 'x64' [C:\projects\dogen\build\output\projects\yarn\tests\yarn.tests.vcxproj]
:     910 Warning(s)
:     18 Error(s)

This appears to be a mixing of 32-bit and 64-bit settings somewhere in
the boost tests, according to SO:

[[http://stackoverflow.com/questions/3563756/fatal-error-lnk1112-module-machine-type-x64-conflicts-with-target-machine-typ][fatal error LNK1112: module machine type 'x64' conflicts with target
machine type 'X86']]

Debug build is back down to =knit= target again.

*** Add tests to inheritance test model                               :story:

We should make sure types' use of IO kicks in via the inheritance test
model. For this we need a base class with associative containers, etc
and a derived class.

*** Add tests to association model                                    :story:

We need a test for composition / recursion.

*** Add new c++ warnings to compilation                               :story:

- =-Wunused-private-field=: Seems like this warning is not part of
  =-Wall=
- =-Winconsistent-missing-override=: new clang warning, probably 3.6.

*** Detect knitter and disable code generation accordingly            :story:

At present you can try to build the codegen knitting targets even
before you built knitter. We should make them conditional on detecting
=knitter=. We just need to make sure this is not cached by CMake.

*** Group the file related fields under a prefix                      :story:

Now we have =element= as a prefix, it probably makes sense to also
group the fields that are related to file names, paths etc. These
could be under =file= or perhaps =paths=? Examples:

- =quilt.cpp.file.include_directory_name=
- =quilt.cpp.source_directory_name=

*** Implement qualified name efficiently                              :story:

We used a =std::map= to store qualified names. In practice, we don't
need something this expensive.

- instead of mapping names to languages, we could map them to
  "styles". There are only a few "styles" across all programming
  languages (e.g. =.= separated, =::= separated and so on).
- we can also create an array of these styles. We know up front how
  many styles there are.
- finally we can create a enumeration to access the array. At present
  this is not possible because we cannot disable invalid, nor is it
  possible to move it to a different position (e.g. last). Also we
  will have to static cast the enum to access the int, which is not
  very pretty.

Once all of this is done we can simply do, at O(1):

: name.qualified[static_cast<unsigned int>(styles::double_colon_separated_style)]

We can prettify it a bit: [[http://stackoverflow.com/questions/8357240/how-to-automatically-convert-strongly-typed-enum-into-int][How to automatically convert strongly typed
enum into int?]]

: template <typename E>
: constexpr typename std::underlying_type<E>::type to_underlying(E e) {
:     return static_cast<typename std::underlying_type<E>::type>(e);
: }
:
: std::cout << foo(to_underlying(b::B2)) << std::endl;

Giving us:

: name.qualified[to_underlying(styles::double_colon_separated_style)]

*** Create utility methods for =__type__= etc                         :story:

At present we've hard-coded the field name for =__type__= and so forth
in each formatter. This is not ideal. Create a simple utility method
that returns it and update all formatters to use it instead. List of
hard-coded things:

- =__type__=
- =<empty>=
- =data=
- =value=
- =memory=
- string helper variables: =<new_line>=, =<quote>=
- =tidy_up_string=

*** Add support for file properties overrides                         :story:

At present we have hard-coded the file properties (old general
settings) to be read from the root object only. In an ideal world, we
should be able to override some of these such as the copyrights. It
may not make sense to be able to override them all though.

*** Why do we need helpers and io for some types?                     :story:

At present we have helper support for maps, sets, pairs etc. We also
seem to have utility support for these. Originally the idea was that
we needed utility so that users could have a map of dogen types and
still have streaming support. This is useful. However, what is
slightly less clear is why we don't just use the utility methods
inside the IO subsystem to output these types, but instead use
helpers. We should try doing that and see what breaks, there may be a
reasons for this.

In theory we just have to remove the helpers in IO for utility
supported types and add the includes to the meta-data; regenerate and
see what breaks. It could be related to the ordering of template
functions or some such problem. If so we need to document this in
manual. We should also do a quick search in backlog for this.

*** Character member variables are not tidied up on io                :story:

At present there is no code to convert non-printable chars into
something acceptable in JSON. We probably never noticed this before
because test data generates printable chars. Code generated is as
follows (all primitives model):

: << "\"char_property\": " << "\"" << v.char_property() << "\"" << ", "

We need a "tidy-up char" function to handle this properly.

For now we've hacked this and set =remove_unprintable_characters= to
false to keep backwards compatibility with legacy.

*** Lists of strings are not properly tidied up on io                 :story:

In the log file, when we dump include dependencies we see invalid
JSON:

: [ "<iosfwd>", ""dogen/sml/types/merger.hpp"" ]

This implies we are not calling =tidy_up_string=. This can be tested
by creating a container of =filesystem::path=.

*** Element formatter should have a container api                     :story:

In general, where the client is performing a loop over a well known
container and then calling a method, we should add an API for that
well known container. This is the case with the element formatter.

This also reduces the number of splices done by the calling code. All
the logging should be done in the element formatter as well.

*** Add logging to all top-level workflow activities                  :story:

We need to make sure the log file is narrating a story. For this we
need to add logging to all start and end of activities by the
workflows. This means that when we filter by workflow name we should
be able to quickly figure out where things went wrong.

*** Add logging to test suite                                         :story:

At present its not possible to figure out where a test suite starts or
ends in the log file. We should also move the asserts from =DEBUG= to
=TRACE=, unless there is an error.

*** Fix cmake emacs variable for tab width                            :story:

We need to replace uses of =tab-width= in cmake files with
=cmake-tab-width=, as explained here:

[[http://stackoverflow.com/questions/25751408/controlling-the-indent-offset-for-cmake-in-emacs][Controlling the indent/offset for CMake in emacs]]

We need to do this for both code generated and manually generated
files.

*** Recursive structures result in crashes                            :story:

If one defines a tree node with a parent and children (such as =node=
in =yarn=) dogen generates code that recurses inifinitely. This is
because the structure contains a parent and we loop through the parent
back to itself and so on. To stop this from happening we need to tell
dogen to exclude certain fields. For example, we could mark =parent=
as a cycle. This is then interpreted by the io feature as a "do not
follow the pointer" (just dump its memory address). We could have a
manipulator that tells the =boost::shared_ptr= io to skip its payload,
much like we do when the pointer is empty.

In summary:

- add a tag to mark a property as circular. Do not confuse this with
  name tree cycles which are at the type level.
- create a manipulator that is set when a circular property is
  found. Set it appropriately.
- on all pointer code (io, comparisons, etc) check for the
  manipulator; if set, do not dereference the pointer. For equality do
  a pointer comparison, for io dump the address, etc.

*** Using =std::set<std::string>= causes compilation errors           :story:

In theory sets of strings (and any other type that has =operator<=
should work out of the box, even though we do not support sets of
dogen types. However, when we tried to use a set of strings we got a
whole load of compilation errors in serialisation, etc.

*** Using =std::unordered_map<my_enum, ...>= fails equality           :story:

We changed the map in =name= to an unordered map and suddenly the
equality tests started to fail. Since we use it for strings quite a
lot, it may be related to the fact that we used an enum? Add a test
case on the test models and see if we can reproduce it.

*** Stitcher log file names look weird                                :story:

At present we are writing files with names like:

: dogen.stitcher...log

*** ODB options file is generated to incorrect location               :story:

Models with composite names seem to have their ODB options file
generated under the =projects= directory, e.g.:

: projects/vtk/geometry/src/options.odb

*** Reference to non-existent dynamic fields produce unhelpful errors :story:

When renaming fields, we get the following dogen errors:

: 2016-01-09 22:54:27.703708 [ERROR] [dynamic.workflow] Field definition not found: cpp.odb.class_header_formatter.inclusion_required

This is not particularly helpful. We should state:

- that the field instance is in the user model but does not exist in
  the library;
- the type in which the field instance was used;
- for extra bonus points use the [[http://en.wikipedia.org/wiki/levenshtein_distance][levenshtein distance]] for spelling
  suggestions. See story on this.

In addition this also depends on the field. For example, while
renaming =dia.comment= to =yarn.dia.comment=, we had no errors at all,
but then all fields defaulted. We should have gotten an error message
stating that the field did not exist.

*** Consider renaming test data to sequence                           :story:

Test data is a strange name. We need something slightly more idiomatic
such as perhaps sequence? We need to look into STL generator
terminology. We should also look into Rx and transducers - these
should be pluggable into these. Ranges also come to mind.

*** Helper methods should have their own includes                     :story:

When a formatter relies on the helper methods, we have a problem: we
need to determine the required includes from the main formatter
without knowing what the helper methods may need. We have hacked this
with things like the "special includes" but there must be a cleaner
way of doing this. For example, we could ask the helper methods
formatter to provide its includes and it would be its job to either
delegate further or to compute the includes. This would at least
remove the duplication of code between io and types.

This task will be made much easier once we have stitch support
for named regions.

As part of the work to make helpers dynamic we reached the following
conclusions:

Note: when time comes to support includes in helper methods, we can
take a similar approach as we do for formatters now. The helper method
implements some kind of include provider interface, which is then used
by the inclusion dependencies builder. The only slight snag is that we
need to first resolve the type into a type family and then go to the
helper interface.

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders.

*** Filter out unused types from final model                          :story:

When we finished assembling the model we should be able to determine
which supporting types are in use and drop those that are not. This
can be done just before building the final model (or as part of that
task).

We should have a class responsible for removing all types from a model
which are not in use. This could be done as part of model assembly.

One way this could be achieved is by adding a "usages" property,
computed during resolution. Resolver could keep track of the
non-target names that are in use and return those.

*** Services and leaves are not properly handled                      :story:

We are manually ignoring services when calculating leaves.

*** Use dots in data files extensions                                 :story:

At the moment we use extensions such as =xmlyarn=. It should really be
=.xml.yarn= or something of the kind.

*** Add tests to identifier parser with invalid names                 :story:

We need to handle properly the following cases:

- totally blank name.
- template with angle brackets but nothing inside: =a<>=.
- template with angle brackets, type and then a comma: =a<b,>=.

*** Stitch meta-templates                                             :story:

*Note*: re-read story [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_64.org#code-generating-formatters-as-text-templates][Code-generating formatters as text templates]] as
some of these ideas were already there. Also: see [[https://github.com/no1msd/mstch][mstch]].

In the quest for defining a single stitch template which then becomes
a formatter - without any additional infrastructure required at all -
we hit on an idea: stitch meta-templates. Basically we would have two
different kinds of inputs to stitch: the template itself and the
meta-template. Meta-template is a provisional name. The meta-template
would define the formatter layout:

- class definition, using a stitch variable for the yarn element type
- registration of the formatter
- definition of a method for the includes
- definition of a method for the stitching

These last two would result in the creation of "regions". These
regions must then be "instantiated" in the template. This could easily
be achieved with some kind of new element:

: <#% region "includes">

Or some such stitch construct. All lines after this line are part of
the region "includes" until a new region is defined. The region is
stitched and then transposed to the place in the meta-template where
it was defined, for example:

: int f(int a, int b) {
: <#% region "includes">
: }

Would result in copying across the region into these brackets. This
will make defining multiple functions very easy, without having to
supply command line arguments, etc.

Notes:

- meta-templates are supplied as command line arguments.
- potential extension: =meta.stitch=
- stitch should still work on non-meta-template mode.
- some of these ideas had already been covered on another story but
  can't find it in backlog. It could be part of the original stitch
  epic. We need to revisit it to see if it contains additional
  insights.
- when an error occurs, it would be great if we could pin point the
  error to the template or to the meta-template. This is more of a
  concern when we add clang compilation support.

Further thoughts:

- there are two approaches for this: we could integrate stitch tighter
  with knit and have it return "chunks" of processed code instead of
  files. As per story "Integration of stitch and dogen", dogen would
  then be responsible for writing the header file as per methods
  defined in the class diagram. Each method would be marked as a
  region. Meta-data in the class associates a template with the
  class. Knitter uses stitch to convert the template into regions, and
  then takes these regions and inserts them into a generated
  file. This approach is very clever and requires a lot of machinery.
- the easier approach uses meta-templates. Class diagram associates
  both meta-template and template with class via meta-data. We could
  possibly also have a stitch stereotype to make it clearer. Yarn has
  a stitch class with attributes of these parameters. Dogen
  instantiates stitch (probably within quilt) with the parameters and
  generates the file. Actually we probably can't have this in quilt
  because we still need formatter properties.

*** Perform an in-depth product backlog groom                          :epic:

We now have lots of references to types (and models) that have been
refactored away - either renamed or deleted altogether. As we are
reaching the final form for =yarn= and =quilt=, we need to go
through all the stories and update them to the new world.

- add two todos to the backlog: not reviewed, reviewed
  (=<REVIEWING>=). Actually, added org mode tag support for this to
  make it more obvious and filterable.
- mark all stores as not reviewed
- go through all the stories and mark them suitably as we review them.

*** Create a set of definitions for tagging and meta-data             :story:

We still use these terms frequently. We should define them in dynamic
to have specific meanings.

*** Refactor ownership hierarchy                                      :story:

Start implementing the archetype logic. Basically there is a artefact
unique identifier

- rename it to =artefact_descriptor=.
- remove all dia fields; these are now file importer specific and
  never reach dynamic.
- add =kernel= field. This is set to =stitch= or =quilt=.
- rename formatter field to =kind=

Merged stories:

*Consider adding "application" to ownership hierarchy*

Not all fields make sense to all tools in the dogen suite; some are
knit specific, some are stitch specific and some are shared. At
present this is not a problem because stitch loads up all of knit's
fields and assumes users won't make use of them. If they do, nothing
bad "should" happen. But a better way to solve this may be to only
load fields that belong to an application. We could add "application"
to ownership hierarchy, and filter on that. Note though that we would
need some way of saying "all applications" (e.g. at present, leave the
field blank).

*Consider renaming =ownership_hierarchy=*

We came up with the name =ownership_hierarchy= because we could not
think of anything else. However, it is not a particularly good name,
and it is increasingly so now that we need to use it across models. We
need a better name for this value type.

This work must be integrated with the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].

*Split knitting from stitching settings*

*Rationale*: with "kernel" we will have quilt and stitch.

At present we only have a single common directory with all of the
available fields. Not all fields apply to both stitching and
knitting - but some do. We need a way to filter these. One possibility
is to use an approach similar to the formatter groups in the ownership
hierarchy. For now we simply have fields that have no meaning in
stitching but can be supplied by users.

*** Models should have an associated language                          :epic:

#+begin_quote
*Story*: As a dogen user, I want to make sure I only use valid system
models so that I don't generate models that code generate but do not
compile.
#+end_quote

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as dynamic extensions in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in yarn
which is a property of the model, and one entry of which is "language
agnostic".

*** Handling of managed directories is incorrect                      :story:

At present we are querying the yarn dia importer to figure out what
the managed directories are. These are basically the top-level
directories from where we want the housekeeper to operate. In reality
this is (or can be placed) in the meta-data. We should be able to
extract the managed directories from the meta-data as a step in one of
the workflows.

This can be done by the backend. It does mean that we should be
returning a composite type from generation:

- list of files;
- list of managed directories.

Alternatively we could have a =managed_directories= method that takes
in an yarn model and then internally reads in the meta-data for a given
model to produce the list.

*Merged with previous story*

Compute managed directories from knitting options

At present the backend is returning empty managed directories. This
means housekeeping will fail in the new world. We need to change the
interface of this method to take in the knitting options and return
the managed directories.

This is not entirely trivial. At present the managed directories are
computed in the locator. It takes into account split project, etc to
come up with all the directories used by the backend. We need to make
these decisions during path expansion, expect we only need manged
directories for the root object. However we do not know which object
is the root object at present, during the expansion. We could identify
it via the QName and the yarn model in context thought. We could then
populate the managed directories as a text collection. We then need
some settings and a factory to pull out the managed directories from
the root object. This could be done in =managed_directories=, by
having an yarn model as input.

*** Add include providers for all types                               :story:

We need to implement the provider container support for primitives,
modules and concepts.

Update:

- inclusion dependencies factory
- provider container

*** Header guard in formatters should be optional                     :story:

At present we are relying on empty header guards to determine what to
do in boilerplate. We should use boost optional.

*** Create more "utility" members for formatters                      :story:

One way of making the templates a bit more manageable is to avoid
having really complex conditions. We could simplify these by giving
them intelligible names and making them properties of the
formattables - mainly class info as that's where the complexity seems
to stem from. For example:

: if ((!c.all_properties().empty() || c.is_parent()) && !c.is_immutable()) {

could be replaced with =has_swap=, or perhaps even =has_public_swap= /
=has_protected_swap=.

*** Use pointer container in registrars                               :story:

At present we are using shared pointers on all registrars. This makes
no sense as the pointer ownership is clear (the registrar owns the
pointer). We should use a boost pointer container and pass references
around, via reference wrapper where required.

*** Consider creating a "locator" like class for path management      :story:

At present we are using path settings to compute paths in several
places. Most of these exist because of hacks but it still seems that
it needs to be done in more than one place. We should consider
something like we had in =sml_to_cpp::locator= that is initialised
with the path settings and can then be used to create paths.

*** Add support for formatter and facet dependencies                  :story:

Once we are finished with the refactoring of the C++ model, we should
add a way of declaring dependencies between facets and between
formatters. We may not need dependencies between facets as these are
actually a manifestation of the formatter dependencies.

These are required to ensure users have not chosen some invalid
combination of formatters (for example disable serialisation when a
formatter requires it). It is also required when a given
facet/formatter is not supported (for example when an STL type does
not support serialisation out of the box).

Note that the dependencies are not just static. For example, the types
facet depends on the hash facet if the user decides to add a
=std::unordered_map= of a user defined type to another user defined
type. We need to make sure we take these run-time dependencies into
account too.

*** Consider renaming formatter groups and model groups to sets       :story:

We should try to keep the words groups and sets to their mathematical
as much as possible - modulus our limited understanding. As such,
where we are using "group" we probably mean "set" since there is no
associated operation with the set; it is merely a way of gathering
elements.

*** Consider generating the diagram targets from files in directory   :story:

Once references are supplied as meta-data, we could conceivably create
a loop in CMake to generates all of the knitting targets based on the
contents of the diagrams directory.

*** Top-level "inclusion required" should be "tribool"                :story:

One of the most common use cases for inclusion required is to have it
set to true for all types where we provide an override, but false for
all other cases. This makes sense in terms of use cases:

- either we need to supply some includes; in which case where we do
  not supply includes we do not want the system to automatically
  compute include paths;
- or we don't supply any includes, in which case:
  - we either don't require any includes at all (hardware primitives);
  - or we want all includes to be computed by the system.

The problem is that we do not have a way to express this logic in the
meta-data. The only way would be to convert the top-level
=requires_includes= to an enumeration:

- yes, compute them
- yes, where supplied
- no

We need to figure out how to implement this. For now we are manually
adding flags.

*** Improve container details in JSON dump                            :story:

#+begin_quote
*Story*: As a dogen user, I would like to know how many elements
containers have so that I don't have to count it manually.
#+end_quote

It would be nice to have the container type and size in the JSON
output. In addition, it seems we are not outputting all containers
correctly. For example, for associative containers we have:

:  "elements": [
:    [
:      {
:        "__type__": "key",
:        "data": "<std><unordered_map>"
:      },
:      {
:        "__type__": "value",
:        "data": {
:          "__type__": "boost::shared_ptr",

We should really be outputting the container type, as well as the key
and value types:

:  "elements": {
:     "__type__": "std::unordered_map",
:     "count": 10,
:     "entries": [
:         {
:             "__type__": "std::pair",
:             "first": "<std><unordered_map>", ==> NOTE: just a string
:             "second": {
:                 "__type__": "boost::shared_ptr",
: ...

And so forth. The only problem with this approach is with simple
types. If we have a key

*** Do not include algorithm if swap is disabled                      :story:

At present we always include =algorithm= in types' class header - both
in new and old world. However, it is there for swap, so we should only
include it if we are going to generate swap. This could be achieved
with:

: if ((!c.all_properties().empty() || c.is_parent()) && !c.is_immutable()) {

As per stitch template.

*** Create the =needle= library                                       :story:

We need to create a library with support code that is used by the
models. At present it is needed for =io= and =test_data=. However, we
ran into [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_67.org#update-legacy-formatters-to-use-needle-for-io][a lot of difficulties]] when we tried to implement it for =io=
using templates.

For the previous attempt to create the needle library see this commit:

feb4750 * integration needle: remove project and includes

*** Consider changing fields where "qualified name" is not qualified  :story:

At present, the the qualified field name is not always a prefix +
simple name. For example, for general settings and for stitch, the
qualified field names do not have a prefix. We could just add a prefix
to make everything symmetric (e.g. =formatters.copyright_notice=) but
it would make the fields less readable at the usage point and this was
the reason why we didn't add it in the first place. For now, we will
leave stitch as it is. This is a bit more meaningful with the
annotation rename.

This may even be a more wide-ranging question: why do users need to
know who owns the field? e.g. =dia.comment=, do I care?

*** Improve streaming of empty expressions in stitch                  :spike:

We have a problem with empty expressions:

: <#= #>

Results in:

: s << <<

We need to ignore empty expressions.

*** Contents change check is done twice                               :story:

We seem to check twice if a file has changed:

: 2015-04-26 12:37:28.451464 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.
: 2015-04-26 12:37:28.451486 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.

This is in stitch but it should be the same for knit.

*** Stitch does not handle directories very well                      :story:

At present we seem to generate log files called =.= when we use stitch
against a directory. This should only happen if we use =.= on the
target parameter, e.g.:

: --target .

Not sure why it is happening when we call stitch from CMake since it
should use the full path to the =cpp= directory.

*** Update dynamic section in manual                                  :story:

We need to talk about the new fields, field templates, etc.

*** Add stitch section in manual                                      :story:

We need to document stitch:

- formal definition of the language and its limitations;
- command line usage of the tool.
- describe the t4 grammar, our similarities and differences. See the
  stories around using t4 terminology in sprint 64 and 68 (most
  important bits copied below).
- describe available directives.
- note on how we don't support class feature blocks and how we use the
  class feature block start marker to mean standard control block
  start marker.

Relevant comments from previous stories:

We found [[https://msdn.microsoft.com/en-us/library/bb126478.aspx][a page]] documenting the elements of T4. These are:

- *Directives*: Text template directives provide general instructions to
  the text templating engine about how to generate the transformation
  code and the output file.
- *Text blocks*: A text block inserts text directly into the output
  file. There is no special formatting for text blocks.
- *Control blocks*: Control blocks are sections of program code that
  are used to transform the templates. Two types:
  - *Standard control blocks*: A standard control block is a section
    of program code that generates part of the output file.
  - *Expression control blocks*: An expression control block evaluates
    an expression and converts it to a string.

Additional definitions we made up because we could not find anything
suitable in documentation:

- *Block*: one of: text block, control block or any of its descendants.
- *Statement*: either a directive or a control block.
- *Inline statement*: statement that starts and ends in one line.
- *Marker*: one of <#, <#@, <#=, #>. Mark-up that delimits statements.
- *Start Marker*: one of <#, <#@, <#=. Can also be specialised to
  "start X marker", e.g. start control block marker is <#, and so on.
- *End Marker*: #>. Can also be specialised to "end X marker",
  e.g. end directive marker is #>.

*** Add kvp support to =identifier_parser=                            :story:

We have code to split kvps all over the place. We should do this in a
single pace, and use boost spirit or tokenizer. For one such
implementation with spirit see:

[[http://boost-spirit.com/home/2010/02/24/parsing-skippers-and-skipping-parsers/][Parsing Skippers and Skipping Parsers]]

*** Assignment operator seems to pass types by value                  :story:

The code for the operator is as follows:

:         stream_ << indenter_ << ci.name() << "& operator=(" << ci.name()
:                << " other);" << std::endl;

If this is the case we need to fix it and regenerate all models.

Actually we have implemented assignment in terms of swap, so that is
why we copy. We need to figure out if this was a good idea. Raise
story in backlog.

: diff --git a/projects/cpp/src/types/formatters/types/class_header_formatter.stitch b/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: index f9f91af..663f0ac 100644
: --- a/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: +++ b/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: @@ -253,7 +253,7 @@ public:
:  <#+
:                  if (!c.is_parent()) {
:  #>
: -    <#= c.name() #>& operator=(<#= c.name() #> other);
: +    <#= c.name() #>& operator=(<#= c.name() #>& other);
:  <#+
:                  }
:              }
: diff --git a/projects/cpp_formatters/src/types/class_declaration.cpp b/projects/cpp_formatters/src/types/class_declaration.cpp
: index c2eeb3c..534ab69 100644
: --- a/projects/cpp_formatters/src/types/class_declaration.cpp
: +++ b/projects/cpp_formatters/src/types/class_declaration.cpp
: @@ -457,8 +457,8 @@ void class_declaration::swap_and_assignment(
:
:      // assignment is only available in leaf classes - MEC++-33
:      if (!ci.is_parent()) {
: -        stream_ << indenter_ << ci.name() << "& operator=(" << ci.name()
: -                << " other);" << std::endl;
: +        stream_ << indenter_ << ci.name() << "& operator=(const " << ci.name()
: +                << "& other);" << std::endl;
:      }
:
:      utility_.blank_line();
: diff --git a/projects/cpp_formatters/src/types/class_implementation.cpp b/projects/cpp_formatters/src/types/class_implementation.cpp
: index 5c9fe50..9276701 100644
: --- a/projects/cpp_formatters/src/types/class_implementation.cpp
: +++ b/projects/cpp_formatters/src/types/class_implementation.cpp
: @@ -456,8 +456,8 @@ assignment_operator(const cpp::formattables::class_info& ci) {
:          return;
:
:      stream_ << indenter_ << ci.name() << "& "
: -            << ci.name() << "::operator=(" << ci.name()
: -            << " other) ";
: +            << ci.name() << "::operator=(const " << ci.name()
: +            << "& other) ";
:
:      utility_.open_scope();
:      {

*** Rename sequence formatter                                         :story:

The =sequence_formatter= is actually not a formatter, but a helper or
assistant.

*** Add validation for field definitions                              :story:

Perform some validation in repository workflow:

- that formatter fields are not duplicated on simple name.
- fields are not duplicated on qualified name.
- instances have qualified name populated.
- only instances are left after instantiation.

*** Incorrect application of formatter templates in field expansion   :story:

At present we are applying formatter templates across all formatters
in C++ mode; this only makes sense because we do not have CMake and
ODB formatters. However, when these are added we will need to filter
the formatters further. For example, C++ formatters (both headers and
implementation) need inclusion dependencies but CMake files don't.

*** Consider supporting multiple formatter groups                     :story:

In some cases it would be nice for a field to belong to multiple
groups. For example =integrated_facet= is only applicable to class
header formatters. We could implement this by making the formatter
group a collection and having formatters belong to multiple groups.

This work must be integrated with the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].

*** Some test models do not build on run all specs                    :story:

For some reason we are not building some of the test models when doing
a run all specs, in particular:

- exception
- comments

this may be because we have no specs for them. We need to find a way
to build them somehow.

*** Improve error reporting around JSON                               :story:

At present when we break the JSON we get errors like so:

: Error: Failed to parse JSON file<unspecified file>(75): expected object name.

These are not very useful in diagnosing the problem. In the log file
we do a bit better:

: 2015-03-30 12:02:12.897202 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/fields/cpp.json
: 2015-03-30 12:02:12.897216 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON stream.
: 2015-03-30 12:02:12.897450 [ERROR] [dynamic.schema.json_hydrator] Failed to parse JSON file: <unspecified file>(75): expected object name
: 2015-03-30 12:02:12.897515 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dynamic/schema/src/types/json_hydrator.cpp(226): Throw in function std::list<field_definition> dogen::dynamic::schema::json_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen7dynamic6schema15hydration_errorEEE
: std::exception::what: Failed to parse JSON file<unspecified file>(75): expected object name
: [P12tag_workflow] = Code generation failure.

But it requires a lot of context to know whats going on. We need to
append more details to the exception.

*** Investigate boost log config files                                :story:

Our log files are growing quite a bit. We don't really want to log any
less since the logging is very useful for troubleshooting. However, at
any one time we just need to look at one or a couple of
components. What we really need is something like log4j, where we can
change log levels for a component or all components in a hierarchy. We
need to investigate boost log solutions for this.

It seems we cannot change severity per component ("channel") with our
current setup. We need something akin to this:

- [[http://www.boost.org/doc/libs/1_57_0/libs/log/doc/html/log/detailed/expressions.html#log.detailed.expressions.predicates.channel_severity_filter][Severity threshold per channel filter]]

This could be implemented as follows:

Create a log config file (say =logging.ini=) that contains a list of
strings and valid severities:

: root = trace
: cpp = debug
: cpp.settings = info

and so on. When the log is being initialised, a sorted list with these
is loaded into memory. It is sorted by channel name. Note that =root=
is a special value and is always at the bottom of the list (or even
removed from the list altogether and handled specially). If root was
not defined in the config file, we set it to a default. Note also that
we convert the severity strings into enums, with adequate validation.

Once the list is setup, we then loop through all the channels that
have been defined. There is an assumption that all channels were
defined statically and thus have already been defined by the time we
initialise the log. This needs to be verified.

For each channel, we loop through all values from the file - other
than root - applying them as a regex against the channel name. Note
that we sorted them so the closest match should be last. For each
value that matches, we set the severity accordingly. If no matches are
found, we apply the root setting.

Some other tidbits:

- we can now remove the =verbose= option, or perhaps it should be used
  as a short-hand for the log configuration? if so we need a rule that
  determines which one to use when both are present.
- we could monitor the config file for changes, although for dogen
  this is overkill.
- if sorting proves too hard we could just say that the regexes are
  applied in the order provided by the user, with the exception of
  root.

*** Consider adding support for formatter tagging                     :story:

At present there is a presumption that if a formatter belongs to say
=types= it cannot belong to any other facet. This means facets are
used purely for hierarchical purposes. However, in certain cases it
may make sense to "tag" or "label" formatters. For example, we may
need to know of all header or implementation files; or of all build
files, or of all files that belong to the main class, and so
forth. For this tags are more appropriate. We have started to hack
things slightly (such as =file_types=) but a generic solution for this
would be preferable.

*** Read =generate_preamble= from dynamic object                      :story:

We need to generate the field definitions and update the general
settings factory.

*** C++ workflow should perform a consistency check                   :story:

We should ensure that all facets and formatters available in the
registrar have corresponding field definitions and vice-versa. This
was originally to be done by some kind of "feature graph" class, but
since we need to use this data for other purposes, the main workflow
could take on this responsibility - or we could create some kind of
"validator" class to which the workflow delegates.

*** Add field definition description                                  :story:

It would be useful to have a description of the purpose of the field
so that we could print it to the command line. We could simply add a
JSON attribute to the field called description to start off with. But
ideally we need a command line argument to dump all fields and their
descriptions so that users know what's available.

*** Rename ODB parameters                                             :story:

At present we use the following form:

: #DOGEN ODB_PRAGMA=no_id

We need to use the new naming style =cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

*** Improve support for modules in JSON                               :story:

At present we are implying the existence of modules in JSON by looking
at the types qname. This is not ideal because it means one cannot
supply meta-data for modules. We should probably revisit the layout to
have a nested structure with namespaces containing types.

We should still support "implied" modules because it makes the file
format less verbose for the common use case though.

*** Consider renaming model module to root module                     :story:

It would be more sensible to call it root module rather than model
module. We should also create a root module property in the model to
make it easier to locate.

*** Add importers and backends to =info= command line option          :story:

#+begin_quote
*Story*: As a dogen user, I want to know what importers and backends
are available in my dogen version so that I don't try to use features
that are not present.
#+end_quote

With the static registration of importers and backends, we should add
some kind of mechanism to display whats on offer in the command line,
via the =--info= option. This is slightly tricky because the
=importer= and =backend= models do not know of the command line. We
need a method in the importers that returns a description and a method
in the workflow that returns all descriptions. These must be
static. The knitter can then call these methods and build the info
text.

** In current major release

The release will not be made unless these stories are closed, but we
won't be addressing them in the near term. This release is all about
getting the architecture right.

*** Consider making fully generated files read-only                   :story:

We could add emacs/vi tags to make fully generated files read-only -
as opposed to partially generated files such as services, which are
expected to be modified by the user. Example:

: /* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 buffer-read-only: t -*-

There must be a vi equivalent.

*** Using default value with text collection throws                   :story:

We don't support default values with text collection, but if the user
tries to use it, the error that comes out is not particularly helpful:

: Invalid or unsupported value type:
: { "__type__": "value_types", "value": "text_collection" }

This is because we attempt to instantiate the field value in the
hydrator, but there is no support for text collection there:

: 2016-08-05 08:10:03.749580 [ERROR] [knitter] Error: ../../../../projects/dynamic/src/types/json_hydrator.cpp(150): Throw in function boost::shared_ptr<dogen::dynamic::value> dogen::dynamic::json_hydrator::create_value(dogen::dynamic::value_types, const string&) const
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dynamic::hydration_error>
: std::exception::what: Invalid or unsupported value type: { "__type__": "value_types", "value": "text_collection" }
: [tag_workflow*] = Code generation failure.

The right thing to do is to throw a more sensible exception such as
"default value is not supported for text collections".

Once we have a use case for default values in text collections we
should add it.

*** Consider adding enumerations in dynamic                           :story:

This story is bound to already exist in backlog so do another
search. The idea is that we should be able to define a field and all
of its valid values. For extra bonus points, we should be able to
assign an enumeration and get the string conversion done
automatically; for example by having a string to enum code generated,
and supplying that function as a type parameter into dynamic. Then
dynamic's field selector would create the instances of the enumeration.

Previous stories:

*Create a domain field definitions*

In addition to default values, it should be possible to supply a list
of possible values for a field definition - a domain. When processing
the values we can then check that it is part of the domain and if not
throw. This is required for the include types and for the family
types. At present this is only applicable to string fields.

In this sense, =boolean= is just a special case where the list is know
up front. We should re-implement =boolean= this way. Possibly even add
synonyms (e.g. =true=, =false=, =0=, =1=)?

*** Consider allowing renaming of "internal" types                    :story:

Users may want to change the =_visitor= postfix for visitors or the
boost serialisation registrar name. This could be achieved via
meta-data.

*** Add =interface= stereotype                                        :story:

Even though we can't generate much outside of plain types, we should
already have support for a stereotype of =interface= which for now
behaves just like =service=. In the future we may be able to code
generate the interface. This should be implemented in yarn as a type
on its own right.

- add an interface which is: element, operatable, relatable. Not
  stateful. We should also have a "is abstract" flag
  somewhere. Perhaps in relatable?
- this should only be done after the UML profile for yarn.

*** Implement module expander test                                    :story:

We copied across the code for the module expander test from yarn json
but didn't actually finished implementing it.

*** Create =src= and =include= facets                                 :story:

At present we have some formatters that are not in the traditional
facets such as =types=, etc. We should make facets for them. We need
to check what the current facet name is. There should only be one case
of this, the CMakeLists formatters.

*** Add more types to =quilt::cpp= canned tests                       :story:

Originally we used the =*_info= types in the canned tests, but these
are all about to be removed. We need to hunt for types in the
=quilt::cpp= model and add those to the canned tests.

*** Create a UML profile to formalise yarn concepts                    :epic:

Profile should include the hashable, etc changes.

*** Create a map between UML/MOF terminology and yarn                  :epic:

It would be helpful to know what a yarn type means in terms of
UML/MOF, and perhaps even explain why we have chosen certain names
instead of the UML ones. We should also cover the modeling of
relationships and the relation between yarn concepts and UML/MOF
classes. This will form a chapter in the manual.

The UML specification is available [[http://www.omg.org/spec/UML/2.5/][here]] and MOF specification is
available [[http://www.omg.org/spec/MOF/2.5][here]].

We need a way to uniquely identify a property. This could be done by
appending the containing type's qualified name to the property name.

See also [[http://www.uml-diagrams.org/][The Unified Modeling Language]] for a more accessible
treatment.

See [[http://www-01.ibm.com/support/knowledgecenter/SS5JSH_9.1.2/com.ibm.xtools.transform.uml2.cpp.doc/topics/rucppprofile.html][Stereotypes of the UML-to-C++ transformation profile]] for ideas.

*** Add support for Language Agnostic Models (LAM)                    :story:

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N yarn merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

Tasks:

- create the LAM model with a set of basic types.
- add a set of mapping fields into yarn: =yarn.mapping.csharp=, etc
  and populate the types with entries for each supported language.
- create a notion of mapping of intermediate models into
  languages. The input is the merged intermediate model and the output
  is N models one per language. We also need a way to associate
  backends with languages. Each model is sent down to its backend.
- note that reverse mapping is possible: we should be able to
  associate a type on a given language with it's lam type. This means
  that, given a model in say C#, we could reconstruct a yarn lam model
  (or tell the user about the list of failures to map). This should be
  logged as a separate story.

*** Protect against double-initialisation                             :story:

We need to look into static initialisation and make sure the code can
cope with it being called several times.

At present it seems we would re-register fields, backends, etc so
multiple initialisation would fail.

In addition to this, we should also look into passing the registrars
into the initialisers. At present we are calling the static methods
directly. This is not ideal, because just like with singletons, we are
hiding the dependencies. We should really pass the registrars in the
initialise function so we can see the dependencies at the top-level.

*** Field definition templates do not support facet specific defaults :story:

At present we cannot use field definition templates for fields that
require facet specific default values such as =directory=. We could
either support something like a "variable", e.g. "find facet simple
name" or we could do overrides - the field definition is defined as a
template but then overriden at a facet level. Or we could handle
default values in a totally separate way - maybe a file with just the
default values.

In addition, we have the case where at the facet level we may have a
default value for a field but not at the formatter level - =postfix=.

For variables, the simple way is to have some "special names". For
example =$(facet_name)= could be made to mean the facet name. With
just support for this we could probably handle all of the use cases
except for =postfix=.

*** Consider using the same API as boost property tree in selector    :story:

At present we have the type of the value in the method names in the
selector, e.g. =get_text_content=. It would be better to have a =get=
that takes in a template parameter, e.g. =get<text>=. However, in
order to do this we need to have some kind of mapping between the
schema value (=text=) and the raw value (=std::string=). This requires
some template magic.

Once this is done we can also make the API a bit more like the
property tree API such as for example returning =boost::optional= for
the cases where the field may not exist.

We have started introducing =try_select...=. This was preferred to
=get_optional= because we are not getting an optional but instead
trying to get.

*** Add dynamic consistency validation                                :story:

We need to check that the default values supplied for a field are
consistent with the field's type. This could be done with a
=validate()= method in workflow.

Actually since we can only create fields from JSON, we should just add
a check there.

*** Consider renaming cpp's name builder to name factory              :story:

The name builder is just a factory so make the name reflect it.

Actually, we don't just build names either.

*** Hydrators provide no context when errors occur                    :story:

We tried to parse a JSON file using the INI parser and got the
following errors:

: 2015-03-27 15:16:05.291132 [DEBUG] [formatters.modeline_group_hydrator] Reading file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/modeline_groups/emacs.json
: 2015-03-27 15:16:05.291215 [ERROR] [formatters.modeline_group_hydrator] Failed to parse INI file: : <unspecified file>(1): '=' character not found in line
: 2015-03-27 15:16:05.291933 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/formatters/src/types/modeline_group_hydrator.cpp(172): Throw in function dogen::formatters::modeline_group dogen::formatters::modeline_group_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10formatters15hydration_errorEEE
: std::exception::what: Failed to parse INI file: <unspecified file>(1): '=' character not found in line
: [P12tag_workflow] = Code generation failure.

The exception provides no context to the file being parsed. We need to
catch the exception and augment it with the file name.

*** Using types of non-referenced models produces bad error messages  :story:

By mistake we made a reference to =dynamic::object= in the schema
model, during the =dynamic= to =schema= refactoring. This resulted in
the following, non-obvious, error message:

: 2015-03-09 12:56:00.920766 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/merger.cpp(120): Throw in function void dogen::sml::merger::update_references()
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml13merging_errorEEE
: std::exception::what: Cannot find target dependency: dynamic
: [P12tag_workflow] = Code generation failure.

What this is trying to say is that the =dynamic= model is not being
referenced. We should make this a bit more obvious because it would be
very difficult for the user to figure out what type is bringing in
this dependency. It would make more sense to say "type X requires
model Y, which is not part of the list of reference models" or
something along these lines.

*** Nested external model path results in strange references          :story:

The external model path does not contribute to path resolution in a
model. Up til now that has actually been a feature; it would have been
annoying to have to dype =dogen::= on every type for every
model. Instead, we refer to say =dogen::a::b= as simply =a::b= in all
models that use =a=. However this masks a deeper problem: this is not
the desired behaviour at all times. We saw this problem when we
created multiple models under dynamic: =dynamic::schema= and
=dynamic::expansion=. In this case, users of these models referred to
them as =schema= and =expansion= respectively, and this was not
ideal. In general:

- external module path should contribute to references just like
  internal module path does - there should be no difference;
- dogen should be clever enough to determine if two models share a
  top-level namespace (regardless if it was obtained from the external
  or internal module path) that there is no need to have an absolute
  path. So in the case of =dogen=, since every model has =dogen= as
  their external module path, according to this rule we should not
  have to type it.

*** Perform lexical casts once only for error reporting               :story:

There are a number of places in the code where we do lexical casts for
enumerations for the exception part:

: BOOST_LOG_SEV(lg, error) << unsupported_formatter_type << ft
:                          << " name: " << o.name();
: BOOST_THROW_EXCEPTION(workflow_error(unsupported_formatter_type +
:    boost::lexical_cast<std::string>(ft)));

We should just do the lexical cast once at the top and use it for both
logging and the exception message.

In addition we should be using =string_converter= for qnames now
instead of io'ing them directly.

*** Names in C++ namespaces                                           :story:

It appears we are not using the entity name for a C++ namespace. If
that is the case, this is wrong and needs to be fixed. We are probably
inferring the name by looking at the =front= (or =back=) of the
namespaces list. Investigate this.

*** Improve errors in dia objects                                     :story:

At present when adding blank spaces in a dia object we get the
following error:

: 2014-11-09 23:05:58.936785 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: std::unordered_map<std::string, facet_settings>
: 2014-11-09 23:05:58.938301 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: std::unordered_map<std::string, facet_settings>
: [P12tag_workflow] = Code generation failure.

There is no clue as to which object caused the error. Add a class name
and dia object ID to the exception.

*** Improve error messages around dynamic extensions kvp's            :story:

Consider a dynamic extension "kvp" without a value, in a dia diagram
(model note):

: #DOGEN dia.comment'

At present the following error is triggered:

: 2014-09-27 10:07:32.761795 [ERROR] [dia_to_sml.comments_parser] Expected separator on KVP.

This provides very little context of what went wrong. Also, should we
allow a "kvp" that has no value, where the value is assumed to be
true. For cases like comment it would make life easier.

*** Use of disabled facets in non-generatable types                   :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a disabled
facet in a non-generatable type so that I don't generate
non-compilable code.
#+end_quote

It would be useful to set facets to disabled on non-generatable types,
when there are generatable types that depend on them. For example, if
we create some non-generatable types for which there is only a =types=
facet, we may still want to create generatable types that make use of
them. In this case, we would like Dogen to automatically disable all
facets except for =types=. Also, if a type is non-generatable, all
facets should be automatically disabled and its up to the user to
enable the ones he is interested in manually.

*** Failed facet dependencies should be treated as errors             :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a
non-supported facet from a system type so that I don't generate
non-compilable code.
#+end_quote

if a facet is not supported in a system module and the user tries to
make use of it, we should error. The user must then go and disable
explicitly the facet on the affected object via the meta data. We
should not silently disable facets.

*** References to objects in package should assume package            :story:

#+begin_quote
*Story*: As a dogen user, I don't want to have to specify fully
qualified names when referring to types in the same package so that I
don't have to type information that can be deduced by the system.
#+end_quote

At present if we define two objects in a package =p=, say =a= and =b=,
where =b= refers to =a= it must do so using a fully qualified path,
e.g.: =p::a=. Failure to do so results in an error:

: 2014-09-10 08:27:10.662113 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "simple_name": "registrar" }
: 2014-09-10 08:27:10.665861 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/resolver.cpp(178): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml16resolution_errorEEE
: std::exception::what: Object has property with undefined type: registrar
: [P12tag_workflow] = Code generation failure.

This should be fairly trivial to implement: all we need to do is to
add =owner= to =resolve_name= in =resolver= and add an extra
resolution step that uses the owner's location.

*** Handling of unsupported dia objects                               :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of Dia shapes that are
not supported by dogen so that my diagrams can be as expressive as
required.
#+end_quote

At present when we try to use a dia object that dogen knows nothing
about we get an error; for example using a standard line results in:

: 2014-09-10 08:09:43.480906 [ERROR] [dia_to_sml.processor] Invalid value for object type: Standard - Line
: 2014-09-10 08:09:43.487060 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/processor.cpp(124): Throw in function dogen::dia_to_sml::object_types dogen::dia_to_sml::processor::parse_object_type(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Invalid value for object type: Standard - Line

However, it may make more sense to just ignore these. To do so we
could relax the code in processor (object_types):

:    BOOST_LOG_SEV(lg, error) << invalid_object_type << ot;
:    BOOST_THROW_EXCEPTION(processing_error(invalid_object_type + ot));

We should also consider having a =strict= command line option to
enable/disable this behaviour.

*** Update manual with detailed model descriptions                     :epic:

#+begin_quote
*Story*: As a dogen developer, I want to read about the architecture
of the application so that I don't have to spend a lot of time trying
to understand the source code.
#+end_quote

We should add CRCs for the main classes, with an explanation of what
each class does; we should also explain the separation of the
transformation logic between the core model (e.g. =dia=) and the
transformation model (e.g. =dia_to_sml=). We should describe what the
workflow does in each model.

We should only implement this story when all of the major refactoring
has been done.

*** Refactor =yarn.dia= transformer                                   :story:

- rename context to repository and make it const for the
  transformer. The only reason why we are mutating it now is because
  of the =id_to_name= container. It must be possible to update this
  container from outside the transformer.
- transformer should just return a list of elements for a given
  processed object; we should then dispatch the list and insert the
  elements into the appropriate containers.
- in workflow =transformation_activity= we should move the logic of
  defaulting to value object into the profiler.
- the transformer should ensure only zero or one notes can exist for a
  module.
- the setting of the documentation should be done as a separate step
  in transformation - i.e. look for =dia.comment= field, if set, use
  its value to populate documentation. This could be done to all types
  for completeness.
- the workflow should not be creating transformers half-way
  through. They should be as stateless as possible.
- tests need to be cleaned up - we need to check for text of the
  exception being thrown.

Actually this is not quite so straight forward. We could move the
logic of dispatching outside of transformer, but we have to bear in
mind we are traversing a graph, so this would have to be done in the
graph itself - not ideal. This story needs more thinking.

*** Update comments in C++ model                                      :story:

We have a very large blurb in this model that is rather old, and
reflects a legacy understanding of the role of the C++ model.

*** Improve error message for blank types                             :story:

#+begin_quote
*Story*: As a dogen user, I want a clear error message when I forget
to supply a type for a property so that I don't spend ages searching
the diagram for the missing type.
#+end_quote

If the user does not supply a type at all in Dia, dogen spits out a
message that is not very informative:

: Error: Failed to parse string: .

The log file is not much better:

: 2014-09-06 16:11:54.143249 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string:
: 2014-09-06 16:11:54.150595 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string:
: [P12tag_workflow] = Code generation failure.

We should instead mention that the string was empty or blank. We also
need to provide the property and class that contained this string. To
reproduce this problem create an enumeration but remove the
=enumeration= stereotype. This is a very common error when creating
enumerations (forgetting to set the stereotype). We should supply some
kind of clue ("did you mean to set the stereotype to enumeration?").

*** The =types= facet should always be on                             :story:

At present users are given the option to enable or disable the
=domain= facet; this is not very wise because all facets depend on
it. It must always be on. We should remove these options.

In addition the facet is incorrectly named: when we performed the
rename of =domain= to =types= we left the command-line facet. We
should rename it to =types= too.

We should probably create a notion of "mandatory" facets to make this
more general.

Actually, we did find [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-c-itself-as-a-front-end][one use case]] where types needs to be off.

*** Allow for generation of class with the same name as package       :story:

At present its not possible to generate a class inside a package with
the same name of that package, if the package documentation is being
generated. This is because they will both have the exact same file
name.

*** Type with the same name as the project does not compile           :story:

It seems that if we create a type with exactly the same name as the
model, we get strange compilation errors:

: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.4/stage/bin/dogen_examples/source/hello_world/include/hello_world/test_data/hello_world_td.hpp:37:13: error: ‘hello_world::hello_world::hello_world’ names the constructor, not the type
:     typedef hello_world::hello_world result_type;
             ^
We should do a test case for this and fix the errors.

*** Diagrams used in manual should be in sanity and in docs           :story:

Users should be able to follow the examples in the manual by using a
set of diagrams supplied in the dogen package. However, to ensure
these samples are actually working we need to test them as part of
sanity. This means we need the same diagrams packaged twice.

*** Consider moving the mock factories into the test_data directory   :story:

There is no good conceptual reason to split the mock factories from
the test_data generators. However, we did it because we don't have a
good way to give dogen visibility of the existence of these files: we
could add regexes but then its not very maintainable and not visible
from the project diagram.

The correct solution for this may be to have some tags that state that
an object only has representations in certain facets. This is captured
by this story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-adding-facet-specific-types][Consider adding facet specific types]].

*** Add tests for general settings factory                            :story:

Some simple tests come to mind:

- empty data files directory results in empty factory;
- valid data files directory results in non-empty factory;
- invalid data files directory results in exception;
- more than one data files directory results in expected load;
- creating annotation for test model types works as expected.
- missing fields result in expected exceptions.

*** Remove references to PFH in makefiles                             :story:

Seems like the correct way of finding libraries is to use
=CMAKE_PREFIX_PATH= as explained [[https://blogs.kde.org/2008/12/12/how-get-cmake-find-what-you-want-it][in this article]]. We should stop using
any references to PFH and let the users provide a path to local
installs via this.

We need to add a note on the read me too.

*** Improve error messages for unconnected objects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to know exactly which object is not
connected correctly so that I can fix it.
#+end_quote

At present when a Dia object is not connected we get the following
error message to std out:

: Error: Expected 2 connections but found: 1. See the log file for details.

The log file is a bit more verbose but still not particularly helpful:

: 2014-01-23 08:25:28.115363 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2014-01-23 08:25:28.118718 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Expected 2 connections but found: 1
: [P12tag_workflow] = Code generation failure.

We should try to at least name the object that has the one connection
to make the user's life easier.

*** Error in log files when reading in Dia model                      :story:

For some reason the log file is full of errors like this:

: 2014-01-20 18:28:31.219549 [ERROR] [dia_to_sml.processor] Did not find expected attribute value type: composite

Presumably the errors are not fatal as code generation still
works. Investigate the errors and tidy-up the log. Since the errors
are not fatal we should at least downgrade them to warnings.

*** Add tests for =general_settings_factory=                          :story:

Tests:

- missing licence
- missing modeline
- empty marker
- different marker for two objects
- consider moving generate preamble into annotation

*** Feature models should always be tested by knit                     :epic:

#+begin_quote
*Story*: As a dogen user, I want to be sure that every feature is
comprehensively tested so that I don't have to worry about dogen bugs
when using it.
#+end_quote

We recently implemented features into dogen; these work off of CMake
detection, where by if a library is not detected, all tests associated
with it are not built and executed. However, we should still try to
codegen these models to make sure that a change we did elsewhere did
not introduce bugs in features we're not interested in. We need to
check that knit has tests for both EOS and ODB that get executed
regardless of these features being on or off.

*** Use consistently the American spelling for license                :story:

We have a mix of American and British spelling of license (e.g. data
file folder is called licence. For details on the subject see [[http://www.future-perfect.co.uk/grammar-tip/is-it-license-or-licence/][this
article]].

We are going to take the easy approach as we did for serialisation and
make all the code artefacts American. Documentation etc is not that
important.

*** Create a new command line parameter for data files directories    :story:

#+begin_quote
*Story*: As a dogen user, I want dogen to use my own private data
libraries so that I don't have to supply them as diagrams.
#+end_quote

Users should be able to provide directories for their own JSON
models. We just need to add a new parameter to the knitter and
transport it all the way to OM's workflow.

*** Check packaging code for non-distro dependencies                  :story:

We are manually copying a lot of shared objects from locally built
third party libraries when creating packages, this should be replaced
with appropriate dependencies (at least for Debian packages).

*** Fix =cp= error on cmake with local third-party packages           :story:

We are getting strange errors in cmake:

: cp: cannot stat ‘/usr/lib/i386-linux-gnu/libpthread.so.1.54.0’: No such file or directory

*** Add support for operations                                        :story:

#+begin_quote
*Story*: As a dogen user, I want to specify operations via the
frontend so that I dogen can generate the header file and I can
manually add the implementation.
#+end_quote

This story is a requirement in order to implement merging support.

When we did the expansion and indexing work for properties, we omitted
operations altogether. This is fine for now, as we only have a
half-baked support for them anyway, but will need to be revisited as
we start to use it in anger. In particular:

- we need sets of operations: local, inherited, all
- we need an operations indexer

Actually we removed the half-baked support.

*** Use xtime-like stopwatch in selected places to log timings        :story:

We should log the time it takes for certain operations in dogen so
that users can figure out if we are becoming slower (or faster) at
doing them and report regressions.

Boost used to provide a nifty little utility class called xtime. It
appears to have been deprecated by [[http://www.boost.org/doc/libs/1_55_0/doc/html/chrono/users_guide.html#chrono.users_guide.examples.duration.xtime_conversions][chrono]].

We should also provide a command line option that prints a timing
report. This would be useful so that users can compare timings between
releases.

We should also be able to grep the log for all timings and save them
down to get trends. We should add a log severity for this, perhaps
PROFILE. Not sure what priority it would be at.

We should also be able to get a command-line report, e.g. =--profile=
would show all the timings for all the components.

It should also be possible to support some kind of uploading of
metrics to a metrics server with a database etc.

*** Canned tests rely on copy constructors rather than cloning        :story:

If an object has pointers, the canned tests will not perform a deep
copy of the object. We need to [[*Add%20support%20for%20object%20cloning][implement cloning]] and then use it in
canned tests.

*** Clean up yarn resolver tests by extending mock factory            :story:

Now that the mock factory has the concept of "stages" of processing,
we need to create a "stage" for merged but unresolved models and
remove the merger from the resolver tests. The flag for this has been
added, we just need to go through the different scenarios and add
handling code for them.

*** Refactor yarn mock factory method names                           :story:

We have a zoo of naming conventions, some starting with =build_=, some
starting with =object_= etc.

*** Validate yarn mock factory on its own tests                       :story:

At present we have a lot of code that ensures that the output of mock
factory actually corresponds to expectations. However, this validation
is in the tests that use the mock factory, resulting in duplication
and possibly missing coverage. We should really just have a mock
factory test with this validation.

*** Add tests for empty objects                                       :story:

This was mainly in the context of IO but could be useful for other
facets. Example:

: class empty_model_generator {
: public:
:     typedef dogen::sml::model result_type;
:
: public:
:     result_type operator()() {
:         dogen::sml::model r;
:         return r;
:     }
: };
: ...
: BOOST_AUTO_TEST_CASE(validate_io_for_empty_objects) {
:     SETUP_TEST_LOG("validate_io_for_empty_objects");
:
:     /* ensure we generate valid JSON for empty model. test was added
:      * because empty property trees were not correct, but its valid on
:      * its own right as we always use populated objects when testing
:      * JSON.
:      */
:     // test_io<empty_model_generator>();
: }

*** Split floating point stream settings from double                  :story:

We had a problem where the output of floating point numbers was being
truncated due to scientific notation being used. A quick fix was to
just update the properties of all streams which use either doubles,
floats or _bools_ with precision etc settings. The real fix is to
distinguish between the two such that we only enable =bool= related
settings when dealing with bools and floating point settings when
dealing with =double= or =float=.

*** Split is floating point like from int like in view model          :story:

At present we only have a single test data generator helper method for
any numeric type: =is_int_like=. This works ok, but it means we are not
generating useful test data for doubles, e.g: =1.0= instead of a
slightly more useful =1.2345= or some such number.

We need a =is_floating_point_like= method to be able to distinguish
between them, and then the associated changes in the generators to
create floating point numbers.

*** Create different kinds of master header files                     :story:

#+begin_quote
*Story*: As a dogen user, I don't want to include every object in a
model when I use includers.
#+end_quote

At present we are using the facet includers in unit tests. This is not
ideal because it means that every time we do a change in a service
header, all tests recompile. In reality we should have two types of
inclusions:

- canned tests should include only value objects, etc - e.g. no
  services.
- service tests should include the header for the service and any
  additional dependencies the service may require.

Perhaps we could have a second type of includer that only has value
objects, etc.

Another way to look at this is that there should be user-configurable
master header files:

#+begin_quote
*Story*: As a dogen user, I want to create master header files for
user defined sets of files so that I don't have to do it manually.
#+end_quote

*** Adding linking libraries is not handled                           :story:

#+begin_quote
*Story*: As a dogen user, I want to link against libraries without
having to manually generate CMakeFiles.
#+end_quote

At present whenever a model requires additional link library targets
we need to disable CMake generation and do it by hand. However:

- for well-known dependencies such as boost we could create a
  convention (e.g. assume/require that the CMake boost libraries flags
  are set via find boost). Alternatively, the types should contain
  meta-data that has information about linking requirements; e.g. if
  you use a type from a boost model, it should provide you with
  linking information in its meta-data. Each boost type could have
  different information depending on which boost library they come
  from.
- for user level dependencies we should add dynamic extensions at the
  model level. Also, references provide sufficient information to link
  against other dogen models.

*** Detect invalid child nodes in dia diagram                         :story:

#+begin_quote
*Story*: As a dogen user, I want to know when there are invalid child
nodes in diagram so that I can fix them.
#+end_quote

When copying a set of classes from a diagram, where these classes
where contained in a package, dia seems to copy across the =childnode=
id. This is a problem because when pasted in a new diagram, if those
classes are not in a package there is now the potential for total
mismatching - for instance, they could be children of an
association. Dogen should validate that children belong to UML
elements which can have children, and if not issue good error
messages - perhaps even talking about the possible cause for the
error.

*** Add tests for yarn main workflow                                  :story:

A few come to mind:

- model with no generatable types returns false
- model with generatable types returns true
- multiple models get merged
- system models get injected

*** Register types for multiple models is misbehaving                 :story:

It seems that somehow we're clobbering the type registration of one
model with another in register types. This is probably because we are
reusing type id's somehow. This wasn't a problem until now because we
were not using inheritance in anger but with the sml changes, it is a
problem as one cannot load dia and sml types off the same registration
(e.g. as in XML serialisation helper).

One solution for this problem would be to create serialisers which
hide the machinery of serialisation internally; one should be able to
just pass in a stream in and get a type out.

*** Comments seem to be trimmed                                       :story:

For some reason we seem to be munching any blank lines at the end of
comments. We should only remove the lines with the well known dogen
marker, all other lines should be left untouched.

*** Type resolution in referenced models                              :story:

We did a hack a while ago whereby if a type is of a referenced model,
we don't bother resolving it. As an optimisation this is probably
fine, but however, it hides a bug which is that we fail to resolve
properties of referenced models properly. The reason why is that these
properties have a blank model name. We could simply force it to be the
name of the referenced model but then it would fail to find
primitives. So we leave it blank during the dia to sml translation and
then if it gets to the resolver, it will not be able to resolve the
type. We could add yet another layer of try-logic (e.g. try every
model name in the references) but it seems that this is just another
hack to solve a more fundamental problem. The sort of errors one gets
due to this are like so:

: 2013-06-29 23:10:34.831009 [ERROR] [sml.resolver] Object has property with undefined type:  { "__type__": "dogen::sml::qname", "model_name": "", "external_module_path": [ ] , "module_path": [ ] , "type_name": "qname", "meta_type": { "__type__": "meta_types", "value": "invalid" } }
: 2013-06-29 23:10:34.831294 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/sml/src/types/resolver.cpp(202): Throw in function dogen::sml::qname dogen::sml::resolver::resolve_partial_type(const dogen::sml::qname&) const
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::sml::resolution_error>

*** Sort model dependencies                                           :story:

It seems the order of registration of models has moved with recent
builds of dogen (1418). Investigate if we sort the dependencies and if
not, sort them.

*** Assignment operator should be protected in ABC                    :story:

As per MEC 33. We should probably do the same for the move and copy
constructors.

*** Change transformation code to use a type visitor                  :story:

Now we have a base type, we could probably simplify some of the
transformation code:

- dia to sml
- sml to c++
- potentially merger

*** Test data generator with immutability looks wrong                 :story:

We are using the full constructor for immutability, but its not clear
how that would work on a inheritance tree. Ensure we have test cases
for this.

*** Inserter for enumerations shouldn't throw                         :story:

We only use the inserter for debug dumping and it could happen that we
are about to write the message for an exception when we decide to
throw. Instead we should just print unexpected/invalid value and cast
it to a numeric value in brackets.

*** Add comments to test model sanitizer                              :story:

We should explain why we decided to create a test model sanitizer
instead of just adding specs to the test models themselves. The
rationale behind it was that it would break the current diffing and
rebaselining logic; we would either have to ignore specs on the diff
or find a way to copy them after code generation. Both options are a
bit of a hack. So instead we created a model with all the specs.

*** Consider renaming dependencies to references in model             :story:

Dependencies is a map of reference; why not call it references?

*** Create a validator in yarn                                        :story:

#+begin_quote
*Story*: As a dogen user, I want to know exactly why my diagram is not
correct so that I can fix the issues. I also want dogen to pick up
errors and generate valid code so that I don't have to figure out what
went wrong by looking at the generated code and the compiler errors.
#+end_quote

We need a class responsible for checking the consistency of the yarn
model. There are several things we need to check for non-merged
models:

- ensure that we can only define identity once across concepts and
  parents
- concepts must have at least one property (or method).
- refined concepts must not have properties (or methods) with clashing
  names.
- type names, model names, etc must not contain spaces or other
  invalid characters. They should also not be a keyword on the target
  language (e.g. =if=, etc.). We should use a identifier parser for
  name validation.
- the name of all keys in objects, etc must be part of the current
  model.
- the qnames of all types as keys are consistent with the values.
- type_name is non-empty; cannot be blank or a variable name
- type name must not exist on any model: basically detect duplicate
  element names. At present we simply silently drop duplicates.
- parent names and original parent names must exist in current model
  (resolver?)
- leaves exist in current model.
- entity must have at least one key attribute.
- non entity must not have key attributes (value, service)
- keyed must be entity.
- aggregate root must be entity.
- all properties of types in current model must exist.
- properties of types in other models result in dependencies.
- enumeration must have at least one enumerator
- enumerator name must not be empty
- enumerator name must be unique
- external package path of the model matches all objects, etc in current
  model.
- model name is non-empty.
- documentation does not have non-printable characters.
- number of type arguments is consistent with objects type.
- objects marked as is comparable must follow the [[*Add%20is%20comparable%20to%20yarn][comparison rules]].
- objects marked as is parent must have at least one child.
- property can only have a default value if primitive
- property default value must be castable to primitive type.
- property must have non-empty name.
- is versioned objects must have a property called version.
- string table cannot have duplicate entries.
- duplicate checks: properties cannot have duplicate names; classes in
  a package cannot have the same name; namespaces at the same level
  cannot have the same name;
- Issue error when a property is a value of an abstract class: yarn
  should fail to merge if the user attempts to create a property of a
  base class. It should allow pointers to the base class though (raw,
  shared pointers, boost optional etc).
- Test relationships between objects and other meta types: We should
  validate that objects are only related to other objects - e.g. they
  cannot inherit from exception or enumeration or vice-versa. Add
  tests for this.
- Its not possible to be immutable and fluent.
- it is not possible to be immutable and be in an inheritance
  relationship. FIXME: why is that?
- user models cannot have stereotype of primitive.
- we don't support generic types (see [[Supporting%20user%20defined%20generic%20types][Supporting user defined generic
types]]) so we should throw if a user attempts to use them.
- a type marked as final cannot have descendants.
- types in global namespace must have an empty location.
- if model module path is empty, location must also be empty.
- check the number of type parameters in the type definition and
  ensure that all name tree's have the expected number of type
  parameters.

For merged models:

- issue error when a property is a value of an abstract class
- properties exist in merged model.

Existing validation code:

:    if ((o.is_parent() || o.is_child()) && p.is_immutable())  {
:        BOOST_LOG_SEV(lg, error) << immutabilty_with_inheritance
:                                 << o.name().id();
:
:        BOOST_THROW_EXCEPTION(
:            transformation_error(immutabilty_with_inheritance +
:                o.name().id()));
:    }

: BOOST_AUTO_TEST_CASE(inheritance_with_immutability_throws) {
:     SETUP_TEST_LOG_SOURCE("inheritance_with_immutability_throws");
:     auto c(mock_context(model_name));
:
:     const auto po(mock_processed_object_factory::make_generalization());
:     const auto con(po[0].connection());
:     BOOST_REQUIRE(con);
:     const auto parents = std::list<std::string> { con->first };
:     c.child_id_to_parent_ids().insert(std::make_pair(con->second, parents));
:
:     transform(c, {po[1]});
:
:     auto po1(po[2]);
:     po1.stereotype(immutable_stereotype);
:     const auto op1(mock_profile(po1));
:     contains_checker<transformation_error> cc(immutability_inheritance);
:     BOOST_CHECK_EXCEPTION(transform(c, po1, op1), transformation_error, cc);
:
:     c.child_id_to_parent_ids().clear();
:     auto po2(po[1]);
:     po2.stereotype(immutable_stereotype);
:     const auto op2(mock_profile(po2));
:     BOOST_CHECK_EXCEPTION(transform(c, po2, op2), transformation_error, cc);
: }

*** Provide contextual error messages during validation               :story:

As per the validator story, we need to ensure the model we are
processing is valid. However, the validator must provide contextual
validation error messages:

: error 1: properties must have a non-null name
: in model 'my_model' (Dia ID: O0)
: in object 'my_object (Dia ID: O0)
: property 'my_property' has empty name.

We should also try to make this compatible with compiler errors so
that we can go to the file where the error occurred:

: file.dia:5:10: properties must have a non-null name

For this to work we need to label everything with a file, column and
line. We can't call this class a =location= since we already use that
name for positions in model space. We could probably call it
=source_location= like [[http://clang.llvm.org/doxygen/classclang_1_1SourceLocation.htlm][clang does]]. We can use [[https://developer.apple.com/library/prerelease/content/documentation/Darwin/Reference/usr_APIs/tree/index.html][XML_GET_LINE]] to get
information about the line number of dia documents. We need something
similar for JSON.

The context should be provided using Boost Exception diagnostics and
dumped into =std::cerr= at the very top.

Merged stories:

*Improve file importer errors*

It would be nice if one could know the file, line and column where an
error has occurred when importing any imported file. Even for Dia,
this would make troubleshooting much easier. We need to add some
information to the meta-model to keep track of the source of the
types: file, line start, line end perhaps.

*** Add warning support for validation                                :story:

Once we implement a validator, we will soon run into warnings: cases
where the user has done something silly but we still want to code
generate. These are best handled as warnings rather than errors.

This story keeps track of things we think should be a warning.  List
of known warnings:

- unconnected dia object
- comment greater than 80 columns

We will probably soon need a way to enable/disable warnings. We could
use a similar scheme as GCC: =-Wname= and =-Wno-name=.

*** Validation-only or dry-run mode                                   :story:

Both stitcher and knitter could do with a "dry-run" mode in which we'd
do everything except for actually outputting.

*For Knitter*

It would be nice if one could just check if a dia diagram is valid for
code generation, e.g. =--validate= or something along those lines.

*For Stitch*

We are interested in performing the parsing. This would be useful for
example for a flymake mode in emacs.

*** Vistor is only supported at the base class level                  :story:

Due to implementation constraints, we only support visitable at the
base class level. Add an exception if users attempt to use visitable
stereotype in a class that has parents.

Note: is this true? We are using derived visitable in C++ model.

*** Add a property for the model name as dynamic extensions           :story:

#+begin_quote
*Story*: As a dogen user in a constrained environment, I am forced to
use file names that are not suitable for a model name so that I need
to supply an override somewhere else.
#+end_quote

It would be nice to be able to generate a model with a name other than
the diagram file. We should have a command line option for this that
overrides the default diagram name.

This could also be supplied as part of dynamic extensions. The command
line option is useful when we want to use the same diagram to test
different aspects of the generation, as we do with the tests. The
dynamic extensions option is useful when we don't want the file name
to have the full name of the model.

We now have a use case for this: the dynamic models. See Rename
dynamic models.

*** Warn if value or entity has methods                               :story:

We should issue a warning if a user defines methods in value or entity
objects as its most likely by mistake.

*** Partial matching of primitives doesn't work for certain types     :story:

We introduced a fix that allows users to create types that partially
match primitive types such as =in= or =integer=. The fix was copied
from the spirit documentation:

[[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/doc/html/spirit_repository/qi_components/directives/distinct.html][- Qi Distinct Parser Directive]]
- [[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/test/qi/distinct.cpp][distinct.cpp]]

However, we still haven't solved the following cases:

: BOOST_CHECK(test_primitive("longer"));
: BOOST_CHECK(test_primitive("unsigneder"));

As these are not so common they have been left for later.

*** Shared pointer to vector fails to build                           :story:

If one has a property with type
=boost::shared_ptr<std::vector<std::string>>=, we get the following
error:

: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp: In function ‘boost::shared_ptr<std::vector<std::basic_string<char> > > {anonymous}::create_boost_shared_ptr_std_vector_std_string_(unsigned int)’:
: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp:47:50: error: ‘create_std_vector_std_string_ptr’ was not declared in this scope

This is because the generated code is not creating a method to new
vectors:

: std::vector<std::string> create_std_vector_std_string(unsigned int position) {
:    std::vector<std::string> r;
:    for (unsigned int i(0); i < 10; ++i) {
:        r.push_back(create_std_string(position + i));
:    }
:    return r;
:}
:
:boost::shared_ptr<std::vector<std::string> >
:create_boost_shared_ptr_std_vector_std_string_(unsigned int position) {
:    boost::shared_ptr<std::vector<std::string> > r(
:        create_std_vector_std_string_ptr(position));
:    return r;
:}

*** Unordered map of user type in package fails                       :story:

We seem to have a strange bug whereby creating a
=std::unordered_map<E1,E2>= fails sanity checks if E1 is in a
package. This appears to be some misunderstanding in namespacing
rules.

*** Naming of saved yarn/Dia files is incorrect                        :story:

For some random reason when we use dogen to save yarn/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.

*** Shared pointers to primitive types                                :story:

At present we do not support shared pointers to primitive types. This
is because they require special handling in serialisation. See:

http://boost.2283326.n4.nabble.com/Serialization-of-boost-shared-ptr-lt-int-gt-td2554242.html

We probably need to iterate through all the nested types and find out
if there is a shared pointer to primitive; if there is, put in:

: // defined a "special kind of integer"
: BOOST_STRONG_TYPEDEF(int, tracked_int)
:
: // define serialization for a tracked int
: template<class Archive>
: void serialize(Archive &ar, tracked_int & ti, const unsigned int version){
:     // serialize the underlying int
:     ar & static_cast<int &>(ti);
: }

*** Full constructor parameter comments                               :story:

#+begin_quote
*Story*: As a dogen user, I want the complete constructor to be
documented automatically so that I don't have to do it manually.
#+end_quote

We could use the comments in properties to populate the comments for
the full constructor for each parameter. This would require taking the
first line of the documentation of each property and then stitching
them together for the full constructor.

*** Serialisation support for C++-11 specific containers              :story:

We can't add =std::array= or =std::forward_list= because there is no
serialisation support in boost 1.49. A mail was sent to the list to
see if this has changed in latter versions:

http://lists.boost.org/boost-users/2012/11/76458.php

However, it should be pretty trivial to generate serialisation code by
hand at least for =std::array= or to use a solution similar to
=std::unordered_map=.

*** Cross model referencing tests                                     :story:

At present we do not have any tests were a object in one model makes use
of types defined in another model. This works fine but we should
really have tests at the dogen level.

*** Cross package referencing tests                                   :story:

Scenarios:

- object in root refers to object in package: A => pkg1::B;
- object in root refers to object in package inside of package: A =>
  pkg1::pkg2::B;
- object inside of package refers to object inside of the same
  package: pkg1::A => pkg1::B (must be qualified);
- object in package refers to root object: pkg1::A => B;
- object in package refers to object in other package: pkg1::A =>
  pkg2::B;
- object in package refers to object in package in package: pkg1::A =>
  pkg1::pkg2::B;
- object in package refers to object in other package in package: pkg1::A =>
  pkg2::pkg3::B;
- object in package in package refers to object in package in package:
  pkg1::pkg2::A => pkg3::pkg4::B.

*** Empty directories should be deleted                               :story:

#+begin_quote
*Story*: As a dogen user, I want empty directories to be removed so
that I don't have to do it manually.
#+end_quote

When housekeeper finishes deleting all extra files, it should check
all of the processed directories to see if they are empty. If they
are, it should delete the directory.

We should probably have a command line option to control this
behaviour.

*** Header only models shall not generate projects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to generate models with just headers
that do not result in full blown projects.
#+end_quote

A project with just exceptions does not need a make file, and fails to
compile if a makefile is generated. We need a way to not generate a
makefile if there are no implementation files generated.

*** IO header could depend on domain forward declaration              :story:

At present we are depending on the domain header but it seems we could
depend only on the forward declarations.

*** Property types are always fully qualified                         :story:

When we code generate non-primitive properties we always fully qualify
them even if they are on the same namespace as the containing type.

** Maybe in current major release

If we have enough time and disposition, we may sneak some of these
in. This release is all about getting the architecture right.

*** Consider enabling =-Wshadow=                                      :story:

We make use of shadowing on occasion so maybe this is why we disabled
this warning. Enable it and check to see what breaks.

:    # definition shadows another
:    # FIXME: causes too many problems.
:    # set(warning_flags "${warning_flags} -Wshadow")

*** Enable =maybe-uninitialized= warning                              :story:

This warning caused build breaks. The main problem seems to come from
boost variants using model types, which then rely on the variant's
swap function. This uses the move constructor. For some reason, the
compiler does not think the default move constructor is initialising
the member variables correctly. Not obvious why that would be.

*** Consider supporting non-boost exceptions                          :story:

It should be fairly trivial to disable the use of boost exception when
generating exceptions. This would allow us to create a model that is
totally independent of boost.

*** Consider supporting user supplied exception base class            :story:

For models that interface with other models, it may make sense to have
an exception class that is derived from a user defined class. We could
easily support an exception base class supplied via meta-data. The
user would have to expose the type via JSON.

*** Consider computing hashes only once for immutable types           :story:

if a type is immutable the hash should be computed once at
construction and then cached. Not quite sure how this would be
implemented though since we have hashing as a totally separate aspect
from types.

*** Support for locations other than filesystems                      :story:

It would be nice to be able to point to a yarn intermediate model
anywhere, such as for example in github and generate it. For this we
just need to detect the protocol in the path and if supported, read in
the file; the frontend implementations should take in a stream rather
than a file path. For cases such as LibXML, we may need to read up all
of the data first and then create a stringstream for it.

We should not implement this story unless we need it such as for
example when we have a site/service. At that point it would be nice to
allow users to just point to their models in github.

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with [[*Build%20shared%20objects%20instead%20of%20dynamic%20libraries][using shared objects]], this
should improve build times.

In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

More thoughts on this:

- we don't really need to have different directories for this; we
  could just put all the include files in the same directory. At
  packaging time, we should only package the public files (this would
  have to be done using CPack).
- also the GCC/MSVC visibility pragmas should take into account these
  options and only export public types.
- the slight problem with this is that we need some tests to ensure
  the packages we create are actually exporting all public types; we
  could easily have a public type that depends on a private type
  etc. We should also validate yarn to ensure this does not happen.
- this could also just be a packaging artefact - we would only package
  public headers. Layout of source code would remain the same.

*** Control the emission of pragma once with dynamic extensions       :story:

At present we are always adding =#pragrma once= to the header guard:

: #if defined(_MSC_VER) && (_MSC_VER >= 1200)
: #pragma once
: #endif

This should really be optional and controlled via dynamic extensions,
probably at the c++ model level.

*** Add the the EMF purchase order examples to manual                 :story:

We should make use of the examples for eCore and EMF or perhaps
Microsoft Access Northwind to demo Dogen. We could make a similar
chapter flow in the manual, starting with a simpler model and then
evolving it to the most complicated one. It would require a little bit
of an adaptation since Dogen does not support all of the features that
EMF has, but it seems a better way to demo Dogen rather than just
talk about the code structure. We could add this after the section on
authoring diagrams.

- [[https://northwinddatabase.codeplex.com/][Northwind Database]]: may be a bit too complicated, but it is a good
  way to demo complex features.

*** File extension is hard-coded against file type                    :story:

At present we are choosing the C++ extension based on the file type:

:    if (ps.file_type() == formatters::file_types::cpp_header)
:       stream << dot << ps.header_file_extension();
:    else if (ps.file_type() == formatters::file_types::cpp_implementation)
:        stream << dot << ps.implementation_file_extension();

It would make more sense to have a formatter group - e.g. header or
implementation - and to associate the extension with the group.

*** Change stitch's standard control block start marker to match t4   :story:

For some reason we used =<#+= as the start marker for standard control
blocks; t4 uses =<#=. We should use the same as t4. One disadvantage
of t4's choice is that we now need to ensure we are not in the
presence of a expression block. We could check for expression blocks
first and if not, then check for standard control blocks.

*** Improve stitch's processing of inline statements                  :story:

At present we have very different handling for the different kinds of
inline statements:

- directives
- expression blocks
- standard control blocks

However, they follow the same pattern and could be implemented with
largely the same algorithm:

Generic processing of an inline statement:

- check start and end markers: =validate_start_and_end_markers=.
- strip start and end markers: =strip_start_and_end_markers=.
- check for any marker, if present error: has_markers=.
- if directive, check for kvp form.

There is no need for looping etc.

*** Add more validation to stitch                                     :story:

Missing validation:

- check that directive an has end marker.
- start control block marker inside of an inline control block

*** Clean up stitch terminology using markup fundamentals             :story:

We came up with a number of quick definitions for stitch because we
needed them for our use cases. However, the names we chose were fairly
random. We should look into the theory around markup languages to name
these things properly.

Links:

- [[https://en.wikipedia.org/wiki/Markup_language][Wikipedia's page on Markup Languages]]

*** Consider adding include directive to stitch                       :story:

T4 supports including templates from templates. At present we are
doing this via the helper methods. As these have all sorts of logic to
determine what gets included, it is not possible to directly replace
these with an include (we would need to recurse across all nested
types in a class to figure out if the inclusion is needed or not).

Nevertheless, this story is a placeholder for furhter investigation in
case we can find a use case for this.

*** Consider adding stitch support for class feature control blocks   :story:

T4 supports an additional type of control blocks called class
features. These permit declaring methods on the class (external
functions for us) that can then be called by the standard control
blocks.

From a stitch perspective we don't necessarily need these, because the
stitch template is not bound to a given function (such as the
transform function in the T4 case); rather, one has to declare the
function that wraps the template within the template itself.

At any rate, this story is a place holder for further analysis on this
in case there is a sensible way to use these class feature control
blocks within stitch.

One interesting twist to this is that class feature blocks can also
contain text blocks. This means we would have two separate cases of
mixed blocks.

Links:

- [[http://www.olegsych.com/2008/02/t4-class-feature-blocks/][Understanding T4: Class Feature Blocks]]
- [[https://msdn.microsoft.com/en-us/library/bb126478.aspx][Writing a t4 template]] (section Class feature control blocks).

*** Tidy-up test models                                               :story:

#+begin_quote
*Story*: As a dogen developer, I want to be able to understand the
test models quickly so that I know at which model to look at when
doing a change.
#+end_quote

We have a lot of fine grained test models for historic reasons. A lot
of these could be collapsed into a smaller number of models, focused
on testing a set of well defined features.

One thing to keep in mind is that when we do fundamental work (such as
moving from the old formatting architecture to the new one), it is
really useful to have small, incremental test models because we can
then get tests to pass, one by one, as we refactor. We don't want to
move to a world where you need to implement all the features in one
go.

Note: we should take this opportunity to fix naming inconsistencies in
test models such as some models having the word "model" such as
=std_model= and other not (=enumeration=, etc).

**** Models that need changing

Merge the following models into a =basic= or =trivial= model (no
aggregation, no association):

- classes_in_a_package
- classes_inout_package
- classes_without_package
- class_in_a_package
- class_without_attributes
- class_without_package
- stand_alone_class

We should also check the combined model has all the scenarios
described in [[*Cross%20package%20referencing%20tests][Cross package referencing tests]].

Merge the following models into stereotypes:

- enumeration
- exception

Consider deleting the comments model and make sure we have comments in
all models with the same features:

- top-level comment for the model
- package level comment
- notes

These models are at the right level of granularity but need renaming:

- all_primitives: primitives or primitives_model to line up with boost
  and std.
- trivial_association: association
- trivial_inheritance: inheritance

**** Models that do not need changing and why

These models test other models, and we cannot remove the postfix
=_model= to avoid clashes with namespaces:

- boost_model
- std_model

These models test command line options, which means they cannot be
merged:

- disable_cmakelists
- disable_facet_folders
- disable_full_ctor
- enable_facet_domain
- enable_facet_hash
- enable_facet_io
- enable_facet_serialization

These models test features which have enough scenarios to justify
keeping them in isolation:

- database

These models test dia features and must be kept isolated:

- compressed
- two_layers_with_objects

**** Add objects, enumerations and exceptions to comments model

At present we are only testing packages in comments.

**** Rename the =database= test model to =odb=

This name is a bit misleading, this is not a generic database model
but its designed to specifically test odb.

*** Improve formatters code generation marker                         :story:

Things the marker can/should have:

- model level version;
- the dogen version too. However, this will make all our tests break
  every time there is a new commit so perhaps we need to have this
  switched off by default.

*** Move some of the more verbose logging to trace                    :story:

We have a category for finer debug logging (=TRACE=) but we are not
making use of it. There is some rather verbose logging that could be
moved to it. Go through all the logging and move some to =TRACE=.

One strategy would be to put in the final object of each workflow as
=DEBUG= (say the expanded model, etc) but the intermediate steps as
=TRACE=. This mirrors the way we investigate the problem: we
could check if each sub-system has done it's job correctly, and spot
the one that didn't; we can then just enable that one sub-system's
=TRACE= (when that is supported).

We probably should only do this at the end, as we want to make sure
that the code generator is usable with full logging on. Or perhaps set
the default to =TRACE=. We should also add a command line option,
perhaps really verbose or extra verbose.

*** Inline comments in =comment_formatter= are a hack                 :story:

We need to tidy-up comment formatter. We had introduced
=documenting_previous_identifier= rather than "comment inline" but in
reality we do need to distinguish these two use cases. If there are
several lines, we want to finish each line with a new line. However,
if thee is just one line, as is the case with enumerations, we do not
want to add any new lines. This is because the stitch template will
add new lines so we end up with too many of them.

*** Consider adding a start and end dogen variable block in dia       :story:

At present we defined a special market to find dogen kvp's in dia's
comments: =#DOGEN=. The problem with this is that, as we start adding
more and more knobs to dynamic, we have to repeat it more and more:

: #DOGEN dia.comment=true
: #DOGEN licence_name=gpl_v3
: #DOGEN copyright_notice=Copyright (C) 2012 Kitanda <info@kitanda.co.uk>
: #DOGEN modeline_group_name=emacs

It would be nice to be able to create a block instead, maybe (first stab):

: #DOGEN_START
: dia.comment=true
: licence_name=gpl_v3
: copyright_notice=Copyright (C) 2012 Kitanda <info@kitanda.co.uk>
: modeline_group_name=emacs
: #DOGEN_END

*** Create a "utility" model like formatters for frontends            :story:

We have a number of utilities that are common to several backends,
similar to what happened to formatters. We should probably extract
those into a common model. At present we have:

- =identifier_parser=: in dia to sml but should also be used from JSON
when we support full models.
- "method identifier": this will be used by the merger to identify
methods and to link them back to language specific methods. Not
quite frontend, but not far.

*** Consider adding =with= support for fluent properties              :story:

It seems the java guys have decided to add the prefix =with= when
using fluent interfaces, e.g.:

: x.with_property_x(false).with_property_y(true);

We could easily add this via dynamic extensions.

*** Consider creating a netty like builder for fluency                :story:

An alternative to fluent properties is to have a fluent builder, as
used by Netty quite extensively. This allows the class itself to
remain immutable; the builder just calls the complete constructor at
the end. We could easily have a "buildable" stereotype that generates
a builder just like we do for visitor.

See [[https://github.com/netty/netty/blob/master/example/src/main/java/io/netty/example/echo/EchoServer.java][this example]] (ServerBootstrap in particular).

*** Consider using boost pointer container for formatters             :story:

At present we are using a container of shared pointers to house the
different formatter types. These are then encased on a "container"
class. However, in reality we are passing around references to that
container class; it seems we do not need shared pointers at all. We
should look into using a [[http://www.boost.org/doc/libs/1_57_0/libs/ptr_container/doc/ptr_container.html#motivation][boost pointer container]]. We do not have dogen
support for this so we would have to add it first.

*** Remove new lines from all text to be logged                       :story:

We should strive to write to the log one line per "record". This makes
grepping etc much easier. We should create a method to convert new
lines to a marker (say =<new_line>= or whatever we are already doing
for JSON output). This should be applied to all cases where there is a
potential to have new lines (comments, etc).

*** Add unit test benchmarking                                        :story:

#+begin_quote
*Story*: As a dogen developer, I would like to know if any of my
changes impact performance so that I can address these problems early.
#+end_quote

*New understanding*:

Create a set of performance specific tests. These wont get executed by
regular users (e.g. they are not part of =run_all_specs=) but they do
get executed in the build machine. These are selected tests with big
loops (say 1M times) doing things like reading dia diagrams etc. We
could chose a few key things just to give us some metrics around
performance.

In fact, we could create a set of colossi models: models with really
large number of classes (say 500), maybe 5 of these with
references. We could then use the diagrams to test the individual
workflows: dia, dia_to_sml, cpp and engine with no writing. We should
avoid writing files to filesystem to avoid number jitter caused by the
hard drive. There should be no comparisons between actual and expected
for the same reason.

We need to make sure the benchmark tests won't run on valgrind or else
the nightly builds will take over 24 hours. However, if we had it
running on continuous we'd spot regressions on every check-in. But we
don't want to delay continuous any more than necessary. Perhaps we
need a separate build called performance which is also continuous and
only runs these tests. We could pass in some kind of variable to CMake
so that if performance is on, it ignores all tests other than
performance and vice-versa. We'd also need a performance target that
only builds the performance binary, and a =run_performance= target
that executes it.

Perhaps we could use a ruby script to generate the test models?

Also, investigate nonius:

https://github.com/rmartinho/nonius

*Old understanding*:

[[https://svn.boost.org/trac/boost/ticket/7082][Raised ticket]]

- nightly builds should run all unit tests in "benchmarking mode";
- for each test we should find the sweet spot for N repetitions;
- when plugged into ctest, make sure the benchmark tests have
  different names from the main tests otherwise the timing history
  will be nonsense.
- [[http://lists.boost.org/boost-users/2011/01/65790.php][sent]] email to boost users mailing list asking for benchmarking
  support.
- some tips on using chrono to benchmark [[http://www.cookandcommit.eu/2014/11/simple-macro-for-algorithms-time.html][here]].

*** Attributes versus properties                                      :story:

At present we have assumed that all attributes in objects should be
generated as properties. This is not quite the right thing to do; one
may actually want to generate a member variable which is not a
property. One solution would be to create a dynamic extension at the
class level that defaults all attributes to properties (or to member
variables). This could be the default for objects but not for
services.

We would have to extend yarn to understand member variables as well as
properties.

*** Returning optional of base class results in invalid code          :story:

When defining a model with a type with a field of =boost::optional<x>=
where =x= is an abstract base class, we get compilation errors in test
data. The problem appears to be that our test data factories try to
instantiate =x= rather than go through the abstract base class
machinery. We need to build a test model for this and fix the code.

We should also question if this is a valid scenario - if not we must
add it to the validation rules.

*** Remove references to namespace when within namespace              :story:

Due to moving classes around, we seem to have lots of cases where code
in a namespace (say =sml=) refers to types in that namespace with
qualification (say =sml::qname=). We need to do a grep in each project
to look for instances of a namespace and ensure they are valid.

*** Investigate current support for =std::set= and =std::map=         :story:

It seems that we do not support sets and maps at present. When we
tried to use a set, we got errors in the guts of test data generation:

: /usr/bin/../lib/gcc/x86_64-linux-gnu/4.9/../../../../include/c++/4.9/bits/stl_function.h:371:20: error: invalid operands to binary expression ('const dogen::sml::qname' and 'const dogen::sml::qname')
:       { return __x < __y; }

This could be a bug (e.g. we are placing the =operator<= in the wrong
place etc). Or it could be that we just never needed ordered maps and
sets so we never added proper support.

*** Add support for =std::forward_list=                               :story:

We have been using =std::list= quite liberally. However, on hindsight,
for the vast majority of cases, we don't require a full blown list; a
simple forward list would do. Problem is Dogen does not support
forward lists just yet. We need to add support for these, including
solving the missing boost serialisation problem.

We seem to have partial support for this at present; the type is in
library.

*** Add support for boost and/or std tuple                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of tuples in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

It would be nice to be able to use =std::tuple= and/or =boost::tuple=
from dogen. The processing would be rather similar to containers. It
would be even nicer if one could associate an enumeration to a tuple
so that the gets would be more meaningful, e.g.:

: std::get<my_field>()

rather than

: std::get<0>()

Using =std::tuple= would mean we'd have to create our own serialisers
for it most likely.

*** Add support for posix_time_zone                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost posix_time_zone
so that I don't have to manually generate code for types that use it.
#+end_quote

At present we need to use std::string to convey time zone
information. We should be able to use the time zones available in
boost date time library.

See boost documentation: [[http://www.boost.org/doc/libs/1_53_0/doc/html/date_time/local_time.html#date_time.local_time.posix_time_zone][Posix Time Zone]]

*** Use diagram files to setup test models in cmakefile               :story:

In the CMakeLists for the test models we are already looping through
all the diagrams:

: foreach(dia_model ${all_dia_test_models})

We should take advantage of this to define =include_directories= and
=add_subdirectory=. At present we are doing these manually.

*** Persisters only support XML                                       :story:

Persister should support all archive types. At present it always
outputs in XML; it should respect the archive type requested by the
user.

*** Persisters should throw on invalid archive types                  :story:

At present we are checking to see if the archive type is invalid, and
if so ignoring it:

:     if (at == archive_types::invalid)
:        return; // FIXME: should we not throw?
:
:    const auto& dp(create_debug_file_path(at, p));
:    sml::persister persister;
:    persister.persist(m, dp);

We should:

- pass the archive type into persister;
- throw if the archive type is not supported.

*** Generalise persister and remove serialisation helpers             :story:

With the move of the knit persister into each model, it became obvious
that users need a way to hydrate and dehydrate certain types by just
supplying a path. The ideal setup would be where each supported
serialisation mechanism registers a number of extensions with an
hydrator / dehydrator and the user can supply a path; the path gets
dispatched to the correct serialisation. In this world we wouldn't
need the XML serialisation helper (in utilities) because we would code
generate a complete serialisation solution. This only works for files
(and not for streams) because we infer the format from the
extension. Having said that, if there was a way to supply an enum or
such-like with the stream, we could create a class for streams and
then implement the file one as an adaptor to the stream class.

*** Add support for ignoring types                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to ignore certain types I am working
on so that I can evolve my diagram over time, whilst still being able
to commit it.
#+end_quote

Sometimes when changing a diagram it may be useful to set some types
to "ignore", i.e. make dogen pretend they don't exist at all. For
instance one may want to introduce new types one at a time. It would
be nice to have a dynamic extension flag for ignoring.

We should probably have some kind of warning to ensure users are aware
of the types being ignored.

*** Setup containing module correctly in mock factory                 :story:

We did not update the yarn mock model factory to populate the
containing type. We also did not setup the members of the module.

*** Consider moving =add_model_module= to flags                       :story:

When we implemented support for =add_model_module= in yarn mock factory
we added the flag to all relevant methods. We could have added it to
the flags instead. The downside of this approach is that we have
static factories in specs, so all tests will have the same set of
flags. Still, intuitively it sounds like all tests should have it
either on or off for a given class being tested. Patch:

: @@ -82,7 +82,8 @@ public:
:              const bool resolved = false,
:              const bool concepts_indexed = false,
:              const bool properties_indexed = false,
: -            const bool associations_indexed = false);
: +            const bool associations_indexed = false,
: +            const bool add_model_module = false);
:
:      public:
:          /**
: @@ -139,6 +140,14 @@ public:
:          void associations_indexed(const bool v);
:          /**@}*/
:
: +        /**
: +         * @brief If true, adds a module for the model.
: +         */
: +        /**@{*/
: +        bool add_model_module() const;
: +        void add_model_module(const bool v);
: +        /**@}*/
: +
:      private:
:          bool tagged_;
:          bool merged_;
: @@ -146,6 +155,7 @@ public:
:          bool concepts_indexed_;
:          bool properties_indexed_;
:          bool associations_indexed_;
: +        bool add_model_module_;
:      };

*** Allow placing types in the global module in Dia                   :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate certain types in the
global namespace so that I don't have to manually code them.
#+end_quote

At present all types in a Dia diagram are placed in the model
module. However, there may be cases where one may wish to place types
in the global module. At present this is only done in the hardware
model, and that is supplied via JSON. However, we may need to do this
from Dia. Find example use cases for this first.

In terms of implementation, a trait could be added to dia
=dia.use_global_module=. This would force the type to be contained
directly in the global module rather than the model module. If the
trait is used in the model or a package, all types in the containing
scope will inherit it.

*** Make features optional at compile time                            :story:

#+begin_quote
*Story*: As a dogen user, I want to ignore all facets in a model that
I don't need so that I don't have to install unnecessary third-party
dependencies.
#+end_quote

One scenario we haven't accounted for is for compile time
optionality. For example, say we have several serialisation facets,
all of them useful to a general model; however, individual users of
that model may only be interested in one of the several
alternatives. In these cases, users should be able to opt out from
compiling some of the facets and only include those that they are
interested in. This is different from the current optionality we
support in that we allow the user to determine what to code
generate. In this case, the mainline project wants to code generate
all facets, but the users of the model may choose to compile only a
subset of the facets.

To implement this we need a trait - say =optional= - that when set
results in a set of macros that get defined to protect the facet. The
user can then pass in that macro to cmake to disable the facet. This
is not the same as the "feature" macros we use for ODB and EOS. These
are actually not Dogen macros, just hand-crafted macros we put in to
allow users to compile Dogen without support for EOS and ODB.

The macros should follow the standard notation of =MODEL.FACET= or
perhaps =MODEL.FACET.FEATURE=, e.g. =cpp.boost_serialization= to make
the whole of serialisation optional or
=cpp.boost_serialization.main_header= to make the header optional. Not
sure if the latter has any use.

*** Add a configuration class to yarn mock factory                    :story:

Every time we need to extend the mock factory we are finding we need
to modify every single function. This is particularly painful due to
the fact we rely on defaults. For example, we can't easily add an
external module path because we need to modify every single method. We
need to look into patterns for this. One option would be to create a
factory configuration class that has the super set of all parameters
required and pass that configuration to each function.

We did add the flags to the constructor, but it would be better if we
could pass in the configuration for each method invocation rather than
for the entire factory.

*** Add support for cross-model concept refinement                    :story:

We've implemented support for cross-model inheritance in sprint 87 but
we did not cover concepts. Most of the approach is the same, but
unfortunately we can't just reuse it.

Tasks:

- we need a refines field which is a text collection.
- we need refinement settings, factory etc.
- update parsing expander.

*** Concepts cannot be placed inside of packages                      :story:

#+begin_quote
*Story*: As a dogen user, I want to place concepts in packages so that
I can scope them when required.
#+end_quote

At present it is not possible to create a concept inside a
package. This is because the concept qualified name is assumed to be
at top-level. In the future it may be useful to use scoping for
concept names in the stereotype. We do not yet have a use case for
this.

*** Re-enable schema updates in database model                        :story:

We are deleting the entire DB schema and re-applying it for every
invocation of the tests. This does not work on a concurrent world. We
commented it out for now, but we need a proper solution for this.

*** Move test model diagrams into main diagrams directory             :story:

For some reason - lost in the mists of time - we decided to split the
test model diagrams from the main models; the first is in the =diagrams=
directory, the latter is in the rather non-obvious location of
=test_data/dia_sml/input/=. All source code goes into =projects=
though, so this seems like a spurious split. Also, the test data
directory should really only have data that we generate as part of
testing (e.g. where there is a pairing of expected and actual) and
the test model diagrams are not of this kind - we never output dia
diagrams, at least at present.

The right thing to do is to move them into the =diagrams=
directory. This is not an easy undertaking because:

- there is hard-coding in the test model sets pointing to these
- the CMake scripts rely on the location of the diagrams to copy them
  across

We should create =production= and =test= sub-directories for diagrams.

*** Investigate the integration of =boost::log= with throw exception  :story:

At present we write a lot of code like this:

: BOOST_LOG_SEV(lg, error) << object_not_found << qn;
: BOOST_THROW_EXCEPTION(indexing_error(object_not_found +
:     boost::lexical_cast<std::string>(qn)));

This is to ensure we log the fact that an exception occurred to make
debugging problems easier. However, it leads to a lot of duplicated
code. We need to figure out a way of simplifying this, most likely
through a macro.

*** Make test data generator more configurable                        :story:

#+begin_quote
*Story*: As a dogen user, I want to configure test data generation so
that I don't have to handle corner cases manually.
#+end_quote

One thing that would be useful is to have a way to attach lambdas to
test data generator. Let =a= be a class with a property =prop= of type
string. It would be nice to be able to do:

: a_generator g;
: g.prop([](const unsigned int seed) {
:     std::ostringstream s;
:     s << "my property " << seed * 10;
:     return s.str();
: });

And so on, for all member variables. The generators would have some
default behaviour, but it could be overridden at any point by the
user. With this, test data generator would be a great starting point
as a way of generating random data for test systems.

See also [[http://www.json-generator.com/][JSON generator]].

*** Forward declaration is not always correct for services            :story:

In cases where we used a service as a way of declaring a stand alone
function (such as the traversals in yarn), the forward declarations do
not match the header file at all. In this cases we should use
=nongeneratable= rather than =service= stereotypes, and perhaps when
that happens we should switch off forward declarations?

In addition, in some cases we may want to use a =struct= rather than a
=class=. At present we are always forward declaring as =class= but
sometimes declaring as =struct=.

*** Refactor node according to composite pattern in dia to sml        :story:

This is not required if we decide to [[*Add%20composite%20stereotype][implement]] the composite
pattern. We should just follow the composite pattern.

*** Use dogen models to test dogen                                    :story:

We should really use the dogen models in the dogen unit tests. The
rationale is as follows:

- if somebody changes a diagram but forgets to code generate, we want
  the build to break;
- if somebody changes the code generator but forgets to regenerate all
  the dogen models and verify that the code generator still works, we
  want the build to break.

This will cause some inconvenience during development because it will
mean that some tests will fail until a feature is finished (or that
the developer will have to continuously rebase the dogen models), but
the advantages are important.

*** Add test to check if we are writing when file contents haven't changed :story:

We broke the code that detected changes and did not notice because we
don't have any changes around it. A simple test would be to generate
code for a test model, read the timestamp of a file (or even all
files), then regenerate the model and compare the timestamps. If there
are changes, the test would fail.

*** Adding new knit tests is hard                                     :story:

In order to test models at the knit level one needs to first generate
the dia input. This can be done as follows:

: ./dogen_knitter --save-dia-model xml --stop-after-merging
: -t ../../../../dogen/test_data/dia_sml/input/boost_model.dia

From the bin directory. We need to make these steps a bit more
obvious. Why do we even need this?

*** Add test model for disabling XML                                  :story:

At present we are not testing model generation with XML disabled.

*** Format doubles, floats and bools properly                         :story:

At present we are using IO state savers but not actually setting the
formatting on the stream depending on the primitive type.

Ideally we should pass in some dynamic extensions to determine the
formatting. We should also consider using =boost::format= for this.

*** Check if we've replaced =assert_object= with =assert_file=        :story:

Assert file is now able to do intelligent comparisons based on the
extension of the file. From a cursory look, all the usages we have of
assert object can be replaced by assert file. If that's the case we
can also remove this function.

*** Add tests for disconnected connections                            :story:

We should throw if a diagram has a disconnected inheritance or
composition relationship.

At present the error message for an inheritance object in dia which
has less than two connections is less than helpful:

: 2013-06-26 22:58:50.236488 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2013-06-26 22:58:50.236917 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dia_to_sml::processing_error>
: std::exception::what: Expected 2 connections but found: 1
: [tag_workflow*] = Code generation failure.

We should really try to detail which object ID failed, as well as
details of the connected object if possible, etc.

*** Add tests for duplicate identifiers in Dia                        :story:

Detect if a diagram defines the same class or package multiple
times. Should throw an exception. We should also detect multiple
properties with the same name.

*** Test model sanity checks fail for enable facet serialisation      :story:

For some reason we are unable to compile the serialisation test for
the test model which focuses only on the serialisation facet. Test is
ignored for the moment.

*** Missing =enable_facet_XYZ= tests                                  :story:

- test data

*** Create model with invalid primitive type                          :story:

At present we are validating that all primitive types work but we
don't check that an invalid type doesn't work.

*** Private properties should be ignored                              :story:

At present we treat private properties as if they were public; we
should ignore them. We need to go through all the models and change
the private ones to public before we do this.

We should also log a warning.

*** Generator usage in template tests needs to be cleaned             :story:

At present some template tests in =utility/test= ask for a
generator, other for instances. We should only have one way of doing
this. We should probably always ask for generators as this means less
boiler plate code in tests. It does mean a fixed dependency on
generators.

*** Replace old style for iterations in IO                            :story:

At present we are still doing C++-03 iterations in the STL IO files
such as =vector_io=, =list_io=, etc. We should be using the new =for=
syntax for C++-11.

*** Add an includer for all includers                                 :story:

#+begin_quote
*Story*: As a dogen user, I need a quick and dirty way of including
all files in a model so that I can test them without having to
include every file manually.
#+end_quote

It would be nice to totally include a model. For that we need an
includer that includes all other includers. This should be as easy as
keeping track of the different includers for each facet in the map
inside of the includer service.

We need to find a good use case for this.

Taking into account the "[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-renaming-includers][master header]]" rename, this would be a
"master master include" file?

*** Add new equivalence operator to domain types                      :story:

#+begin_quote
*Story*: As a dogen user, I would like to know if two objects are
equal ignoring the version properties so that I can model my domain
more accurately.
#+end_quote

We should have an operator that compares the state of two objects
ignoring the version (for versioned objects).

*** Add run spec targets for each test                                :story:

We could piggy back on the ctest functionality and add a target for
each test so one could =make enable_facet_domain= and =make
run_enable_facet_domain=. The targets need to be prefixed with module
name and test suite.

*** Tests for error conditions in libxml                              :story:

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.

*** Add specification comments to tests                               :story:

We started off by adding a technical specification as a doxygen
comment for a test but forgot to keep on doing it. Example:

: /**
:  * @brief It shall not be possible to create more terms than those
:  * supported by a finite sequence, using std::generate_n.
:  */

This helps make the purpose of the test clearer when the name is not
sufficient.

This may not be required once we move over to Catch since the specs
become very readable.

** In next major release

These stories are good candidates for the subsequent release. This
release will be all about adding new features.

*** Concepts, immutability and fluency                                :story:

At present we allow the object to determine if any attributes obtained
from modeling a concept are immutable and/or fluent. This seemed
logical at the time, but its actually not a good idea: this means that
two objects modeling a concept may not actually implement the same
interface, thus meaning that they are not really modeling the
concept.

Instead if this cleverness, users should be allowed to mark the
concept itself as immutable/fluent and we should simply throw if there
is an incompatibility between concept and object.

*** Improve handling of defaulting for enumeration types              :story:

At present we check for a primitive market as the default type for
enumerations, and apply this type to all enumerations that have no
underlying type. The problem is that if a user creates a type for
primitives (or if we use =std::string= as a primitive type, say) we
still require loading the primitive model to locate a default type. We
should probably only locate this type once we have got the first
enumeration that needs it.

*** Create an interface for the text reader                           :story:

In order to do performance testing of the dia model we should create
an interface for text reader and implement it as a mock. This will
avoid the overhead of reading stuff from the hard drive.

*** Allow multiple types to go into a single formatter                :story:

We have found a number of cases where it may be useful to have more
than one type going into a formatter:

- [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-inner-classes][inner classes]];
- declaring all/some of the following in a single header: exceptions,
  enumerations, primitives.
- typedefs ([[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#manual-typedef-generation][manual]], [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#automatic-typedef-generation][automatic]])
- [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#include-groups][include groups]] (and to be fair, [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-renaming-includers]["master" headers]] too).
- grouping a number of forward declarations into a file.

There are probably a few more in the backlog. What all these use cases
share in common is that in some cases we want to be able to send
several types into a given formatter. This is actually not that hard
to do:

- find a way to "label" types in yarn, perhaps for a given formatter;
- transfer those labels across to CPP's formattables;
- group formattables by label;
- have a separate interface for formatters that take multiple
  formattables; one of the methods of this interface is the label;
- for each formatter, find all types with matching label and pass them
  on.

One thing to bear in mind though is that the labeling is done at the
yarn level; and for a given yarn entity, we may have a number of
formattables. Should all be passed in?

This work must be integrated with the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].

Merged stories:

Types that share one file

#+begin_quote
*Story*: As a dogen user, I want to generate a single file for a
number of related classes so that I don't have to deal with lots of
files when they are not needed.
#+end_quote

At present we force all types etc to have their own file. However, in
cases it may be useful to have multiple types sharing the same
file. For instance, one may want to have all enumerations in one file,
or all exceptions, etc.

We could implement this using dynamic extensions.

*** Adding types to package namespace                                 :story:

Whilst it is possible to document a namespace, it is not possible to
add any classes etc to that namespace. For example, it may make sense
to add some constants at the namespace level. This is not possible
with the current setup.

This is related to merging multiple types in a single formatter.

*** Add support for inner classes                                     :story:

Inner classes could be expressed in the same way as the short-hand for
namespaces. For example, given a class =a=, an inner class =b= could
be declared as =a::b=. The system would have to recognise that =a= is
a class and then treat it accordingly in yarn. The formatters would
have to be taught to express inner classes when formatting the main
class. This probably requires merging two yarn entities into a single
cpp entity. Finally some dynamic extension support would be required
to determine if the inner class is public or private.

We just need a use case for where inner classes would be useful.

*** Consider adding support for inline hashing                         :epic:

At present we have an entire facet for hashing. However, it is
conceivable that some users may find it a bit of an overkill and would
rather have it added to the types facet instead. We already do
something similar for the =operator<<=. We need to consider a more
generic mechanism for allowing the inlining of certain features when
they are more core.

In fact, one wonders if we are not getting towards a multi-option
implementation for certain things:

- as a facet;
- as a formatter;
- as a toggleable aspect of an existing formatter.

Where the user gets to choose one of the three possible
implementations. In some cases we may not support all of these
options; for example, for operators we could support the second and
the third option and so on.

See also Support "cross-facet interference".

*** Consider making header guards configurable                        :story:

It may be nice to be able to switch parts of the header guards off (do
not use namespaces, etc) or even to have a completely different policy
to generate header guards. We do not have a use case for this yet, but
this story is a place-holder for it.

The end-game for this is to have a set of "variables" that can be used
to express the header guards:

: $(namespaces)_$(class_name)_$(extension)

And so forth.

*** Code generation of registrars for static registration             :story:

We are using a lot of static registration and we have converged into
what appears to be a useful pattern. It would be nice to be able to
mark a type as registrable and to have the registrar automatically
generated for it, with a name configurable via dynamic extensions. We
would also need to configure the registration method (name,
arguments - we may want to register against a string or just have a
list of registered types).

The next logical step would be to code-generate the static
initialisers too. For this we would have to be aware of all types to
register in a given model (perhaps by looking at inheritance across
models) and for each of these generate the appropriate initialiser
code. This is more tricky but it would be really useful. Actually
given the templatisation of formatters, this is not useful any longer.

Tasks:

- create a registrar type in yarn and associated stereotype.
- add keys to link types to a yarn type. This should be generic so
  that we can use it for other model elements. We could either do it
  the hard (static) way where we manually list the types, or in a more
  dynamic way by providing a base class and letting yarn determine all
  of the descendants.
- add formatter interfaces and a stitch template for registrar.

Note: the sample principles probably apply for code generating the
container.

*** Load system models intelligently                                   :epic:

#+begin_quote
*Story*: As a dogen user, I want to load only the system models
required for the model I want to generate so that generation is as
quick as possible.
#+end_quote

At present we are loading all library models. This is not a problem
because they are small and there are only a few of them. However, in a
distant future, one can imagine a very large number of system models,
each of which with large number of types (say the C# system models,
the C++ system models, etc). In this world we may need to disable the
loading of some system models: either by programming language or more
explicitly by choosing individual models in a given language.

It may even make more sense to load just what is required: load the
target model, infer all of its dependencies (including at the
programming language level) and then load only the system models that
are required for those languages.

This may not be as hard as it seems: we already infer that all models
the target depends on are present by looking at the list of distinct
model names required by the target qualified names. We could use the
same logic to determine what system models to load. The only exception
is the hardware model, which must always be loaded (or we need some
kind of mapping between "empty" model name and the hardware model).

We should keep in mind the model groups too; not all models are
applicable to all model groups. We should only consider compatible
models.

*** Consider adding facet specific types                               :epic:

#+begin_quote
*Story*: As a dogen user, I want to code-generate simple types for
facets other than =types= so that I don't have to create them manually.
#+end_quote

Types in dogen are somewhat "uni-dimensional"; that is, the main focus
of all work is types and the other facets are thought to either be
code generated in total (serialisation, hashing, etc) or manually
generated in total (test for mock factories). However, in some cases
it may make sense to add a type directly to a facet. For example, we
may want to add simple value objects to the mock factory. We don't
want to pollute =types= with these classes, but at the same time we'd
rather not have to manually generate them. It would be nice to be able
to associate a type with just a facet via dynamic extensions. Of
course, this does mean we would not be able to rely on all other
facets such as serialisation and even streaming or else things would
get a bit confusing. But it would still be useful.

Another possible (less clean) approach is this:

#+begin_quote
It would be great if we could use dynamic extensions to enable and
disable facets (there probably already is a story for this). But in
addition to this, it would also be great if one could override the
default name for an object in a facet; for instance: one could add an
object called =serialization_manager=, disable all facets bar
serialisation, disable the serialisation postfix of this file and
disable code generation. This way one could add manual code to any of
the facets, independently.

At present we support this, but only for types as it is hard-coded.
#+end_quote

*** Add support for deprecation                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to mark certain properties, classes
or methods as deprecated so that I can tell my users to stop using
them.
#+end_quote

We should be able to mark classes and properties as deprecated and
have that reflected in both doxygen and C++-11 deprecated attributes.

Note that at present nothing stops the users from adding the marker
themselves.

Perhaps we should add general support for attributes. This would be
useful for languages like C# and Java, to control serialisation, etc.

*** Control JSON output via traits                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the output JSON so that
I get exactly what the external users are looking for.
#+end_quote

Once we add support for JSON we will face the same sort of problems
that Json.net has already solved: we may want to have keys that do not
match the property names (for instance we may want to use human
readable names in the json), we may want to translate enumerations to
numbers or to human readable descriptions, we may want to collapse a
class into some less verbose JSON, etc. Some of these are describable
via traits, very much like Json.Net uses C# attributes. We should look
into the available attributes and see if they make sense as dogen
traits to control JSON. Some of these may have wider application and
be used to control other serialisation formats.

*** Special purpose formatters                                         :epic:

In the future, when the creation of formatters is made easier, we may
start designing formatters that are totally a application specific and
may not have any particular use for any other application. They should
be accepted in mainline Dogen:

- to make sure we don't break this code;
- to allow other people to copy and paste to generate their own
  formatters;
- because sometimes what one thinks is special purpose actually much
  more general.

However, we need to make sure we don't start cluttering the code base
with these formatters. We will also have to start to worry about
things like defining stable interfaces:

- at which point do we decide that some code has bitrot and
  deprecated, so will have to be removed?
- what happens when a formatter moves from version 1 to version 2 of
  some dependent library, must we create a version 1 and version 2
  formatter or just update the existing one? what if it breaks code
  for people using version 1 that do not wish to move to version 2?
- do we mandate compilation tests for all formatters? This would mean
  our build machine would be full of third-party libraries (some
  potentially not available in Debian), and quite hard to
  maintain. Alternatively we could mandate that if you have a
  formatter you must setup a CTest agent with a compilation for that
  formatter and publish the results of the build to dashboard; if your
  build becomes consistently red we are allowed to remove the
  formatter.
- for the diff tests, is it acceptable if someone refactors the code?
  Once "your" formatter is merged in it is now owned by the community
  and it is entirely possible that someone will improve it/extend it,
  etc. In order for this to work they need to be very sure they have
  not broken the original use case.

We probably just need to setup a very simple policy to start off with,
but its best to keep track of these potential pitfalls.

Merged with this story:

*Private formatters*

We should look into code we do in dogen that is highly repetitive and
create "private formatters" for it. For example, field definitions are
more or less exclusive to dogen so it doesn't make it any sense to add
it to the "public" side of dogen; but it would be nice to create a
formatter to generate them so that we don't have to do it
manually. For these "private formatters" we would need to load a SO
with them into a dogen binary.

*** Add support for user defined literals                             :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of literals so that I can
make my code more type safe.
#+end_quote

With user defined literals in C++11, defining one's own numeric types
became more convenient. We should look into adding support for this in
dogen.

See [[http://www.codeproject.com/Articles/447922/Application-of-Cplusplus11-User-Defined-Literals-t][Application of C++11 User-Defined Literals to Handling Scientific
Quantities, Number Representation and String Manipulation]]

One interesting way of doing this is as per [[http://researcher.ibm.com/researcher/files/zurich-jku/mdse-08.pdf][MDSD]] book (p111): to
create meta-entities for the quantities. This needs a bit more
analysis but the gist of it is that we could then map quantities to
the most appropriate platform specific technology - in Java/C# create
a type or use a int, in c++ use literals, etc.

*** Add export macros support                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to export types selectively so that I
can control what external users can depend on.
#+end_quote

We should add export macros for shared objects/DLLs:

: #ifdef ExportDeclaration
:    #undef ExportDeclaration
: #endif
:
: #ifdef
:    #define ExportMacro __declspec(dllexport)
: #else
:    #define ExportMacro __declspec(dllimport)
: #endif

There is also a GCC equivalent explained [[http://pic.dhe.ibm.com/infocenter/tpfhelp/current/index.jsp?topic%3D%252Fcom.ibm.ztpf-ztpfdf.doc_put.cur%252Fgtpl2%252Fexport.html][here]].

We should have some dynamic extensions to control the outputting of
these.

*** Improve cross model visitation support                            :story:

One of the problems we have at present is that its not possible to
define a base class in a model with a visitor and then extend it in
other models. The visitor needs to be aware of all the leaves in order
to dispatch. There seem to be some ideas in this space which may
provide a solution:

- [[http://stackoverflow.com/questions/11796121/implementing-the-visitor-pattern-using-c-templates][Implementing the visitor pattern using C++ Templates]]

One simpler but hacky way of solving this problem is perhaps to have
"model specific" visitors in each model, and have them extend the base
visitor. Clients can then decide which visitor to use. This does mean
that if two models are extending the base visitor, you will need to
visit twice, but at least for the most common case (one model
extending another) it provides a workable solution.

*** Modeling of visitors in =cpp= can be improved                     :story:

In the =cpp= model we are assuming that if the original parent was
visitable, then the visitor was named after it:

:             } else if (c.is_original_parent_visitable() && !c.is_parent()) {
: #>
: public:
:    virtual void accept(const <#= c.original_parent_name() #>_visitor& v) const override {

The right thing to do here is to have a =visitor_info= attached to the
=class_info= that is generated during transformation and deals with
all such rules. The template should just loop through the visitor
infos. In addition, the visitor infos should tell the template if they
are abstract or implemented. We may be able to reuse the existing
=visitor_info= class for this.

*** Allow renaming of visitor                                         :story:

At present the visitor is named by dogen. There is nothing stopping us
from allowing users to rename it via meta-data. We don't have a use
case yet.

*** Disable =invalid= value in enumerations                           :story:

#+begin_quote
*Story*: As a dogen user, I may not want to allow invalid values in
enumerations because they do not model my problem domain accurately.
#+end_quote

At present all enumerations must have an invalid value. One can
conceive cases where that is not a useful thing. We should have a
dynamic extension flag that disables it.

*** Move =invalid= value to a different value                         :story:

Value zero should always be assigned to the most used enumeration
value because it is very efficient to compare against zero. We should
use a different value for invalid such as =0xFF...= etc.

*** Bitmask enumeration                                               :story:

#+begin_quote
*Story*: As a dogen user, I want to define a bitmask enumeration in
dogen so that I don't have to create it manually.
#+end_quote

We should have a dynamic extension flag that generates enumerators
with values that are powers of two. These can then be used for flags,
as per the [[*Add%20support%20for%20bitsets][bitset story]].

*** Support for file level comments via dynamic extensions            :story:

#+begin_quote
*Story*: As a dogen user, I may want to add comments at the file level
so that I can provide documentation to the model users.
#+end_quote

We could easily have a tag for file level comments and transport that
all the way to the output. The only problem is that it would be a one
liner only so it may not be that useful.

Multi-line support could be simulated by concatenating multiple
entries - cumbersome but workable...

*** Add getter and setter prefixes                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to change the default getter and
setter conventions so that I can integrate my code with
dogen-generated code.
#+end_quote

External users may have getter and setter prefix conventions such as
=set_prop= or =SetProp=. It would be nice if we could pass in a
getter/setting prefix and then dogen would append them when converting
the diagram, e.g. =--getter-prefix=set_=.

We should check what ODB has done for this and implement the same
pattern.

We should also look for some support in clang format, although it
seems very unlikely.

*** Add support for qualified class names in dia                      :story:

#+begin_quote
*Story*: As a dogen user, I don't want to have to define packages in
certain cases.
#+end_quote

It has become apparent that creating large packages in dia and placing
all classes in a large package is cumbersome:

- there are issues with the large package implementation in dia,
  making copying and pasting a dark art; its not very obvious how one
  copies into a package (e.g. populating the child node id correctly).
- models do not always have a neat division between packages; in
  dogen, where packages would be useful, there are all sorts of
  connections (e.g. inheritance, association) between the package and
  the model "package" or other packages. Thus is very difficult to
  produce a representative diagram.

A solution to this problem would be to support qualified names in
class names; these would be interpreted as being part of the current
model. One would still have to define a large package, but it could be
empty, or contain only the types which only have connections inside
the package, plus comments for the package, etc.

*** Add support for boost concept                                     :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate boost concepts so
that I don't have to manually create them.
#+end_quote

Now dogen supports concepts, the natural thing to do is to express
them in C++ code. This could easily be done using boost concept, or
the C++-14 concepts light.

See [[http://www.boost.org/doc/libs/1_53_0/libs/concept_check/creating_concepts.htm][Creating Concepts]].

It is important to be able to switch the concept generation off
too. We may want to create concepts for internal purposes but have no
need to actually express it in code. This could be suppressed via
meta-data.

*** Add documentation for concepts                                    :story:

It seems it is possible to document a concept in doxygen:

[[http://stackoverflow.com/questions/10087171/documenting-a-c-concept-using-doxygen][Documenting a C++ concept using doxygen?]]

*** Add support for object cloning                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to be able to clone object state so
that I don't have to do this manually.
#+end_quote

We should have a clone method which copy constructs all non-pointer
types, and then creates new objects for pointer types.

*** Consider adding support for keys                                   :epic:

We had originally added some half-baked support for key
generation. The basic idea is that users could mark certain properties
in a class as being part of a key and dogen would automatically create
a key. The key could be versioned or unversioned. However, we never
had a use case for it and the feature was not implemented in a
convincing way. Having said that, it seems like a useful feature for
things such as caches etc so we may want to resurrect it as some point
in the future.

This was removed in 4ed3fbd.

Merged stories:

*Definition of Identity Attribute*

:    {
:        "name" : {
:            "simple" : "identity_attribute",
:            "qualified" : "yarn.dia.identity_attribute"
:        },
:        "ownership_hierarchy" : {
:            "model_name" : "yarn.dia"
:        },
:        "value_type" : "boolean",
:        "definition_type" : "instance",
:        "scope" : "property"
:    }

This was removed in commit =5e80256=:

yarn.dia: remove support for identity attribute

*Create a =key_extractor= service*

We need a way to automatically extract a key for a =keyed_entity=.
The right solution is to create a service to represent this
concept.

Injector creates objects for these just like it does with keys; the
C++ transformer intercepts them and generates the correct view models.

*Use explicit casting for versioned to unversioned conversions*

At present we have to_versioned; in reality this would be dealt much
better using explicit casts:

: explicit operator std::string() { return "explicit"; }

Actually the real solution for this is to make the versioned key
contain the unversioned key; then dogen will generate all the
required code.

At this point in time we do not have enough use cases to make the
correct design decisions in this area. We need to wait until we start
using keys in anger in Creris and then design the API around the use
cases.

It is not possible to use global cast operators so we need to
introduce a dependency between versioned and unversioned keys in order
for this to work.

*Consider not creating unversioned keys for single property*

If a key is made up of a single property, its a bit nonsensical to
create an unversioned key. We should only generate the versioned
key. However, it does make life easier. Wait for real world use cases
to decide.

*** Add versioning support                                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make changes to diagrams in a
backwards compatible way so that I can upgrade the users of my code
incrementally.
#+end_quote

We had some basic support for versioning but it was half-baked. It was
removed in 4ed3fbd.

*New understanding*:

- Add versioning support by adding versions at the object level and at
  the property level. Properties with 0 version will have no special
  handling. Properties with non-zero version (V) will have the
  following code added in serialisation:

: if (version > V)
:    // read or write property

- If a number of consecutive properties all share the same version,
  dogen will group them under the same version if. There will be no
  other special grouping or otherwise changing of order of properties.
- The object version will be max(version) of all properties for that
  class, excluding inherited properties.
- The object version will be stamped using boost serialisation class
  version macro, unless the object version is zero.
- Dogen will make no validation or otherwise dictate the management of
  version numbers; its up to the users to ensure they make sensible
  backwards compatibility decisions such as adding only new properties
  and always adding to the end.
- The model version is a human level concept and has no direct
  relation to class versioning. It will be implemented as an
  implementation specific parameter in the Dia model and as a string
  in the yarn model class. See [[*Improve%20OM's%20code%20generation%20marker][this story]].
- Model version will be used for the following:
  - stamped on doxygen documentation for the model namespace;
  - stamped on DLLs, etc.
  - used by humans to convey the "type" of changes made to the
    diagram/model (e.g. a minor version bump is a small change, etc).

Previous understanding:

Versioning support is now available in yarn, so we need to apply it to
yarn itself. That is, we need a way of having two versions of an yarn
model coexist, and allow Dogen to diff those two versions to make code
generation decisions so that we can add basic backwards compatibility
support.

Before we can do this, we need a way of stamping a model version into
models. This can easily be done via implementation specific
parameters. See [[*Improve%20OM's%20code%20generation%20marker][this story]].

We then need to create some kind of strategy for version number
management:

- minor bumps are backwards compatible; e.g. only adding new fields.
- major bumps are not backwards compatible: e.g. deleting fields,
  classes, etc.

However, at present we only support a single version number. Perhaps
we should just declare which versions are backwards compatible and
which ones are not.

Once all of these are in place we should add versioning support to
dogen:

- add a new command line argument: =--previous-version= or something
  of the kind.
- the model supplied by this argument must have the same name as the
  model supplied by =--target=.
- change all yarn types to be versioned.
- dogen will load up both models, and stamp the versions in each
  type. Merger will then be responsible for stamping the versions on
  each property, taking previous and new as input.
- for every field which is in new model but not in previous, add boost
  serialisation code to handle that.
- add unit tests with v1, v2 models.
- in order for dia diagrams with multiple versions to coexist in the
  same directory we will probably need to add the version to the
  diagram name, e.g. =sml_1.dia= or =sml_v1.dia=. We probably need
  some parsing code that looks for the last few characters of the file
  name and if it obeys a simple convention such as =_v= followed by a
  number, it ignores these for the model name and uses it for the
  version.

With this in place, when rebasing we can now do a proper comparison
between expected and actual.

Potential future feature: to put the files of different versions in
separate folders. This would allow the creation of "conversion" apps
which take types for one version and transform them into the next
version.

*** Add support for boost parameter                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost parameter so
that I can generate classes with named parameters without having to
manually create this code.
#+end_quote

It would be nice to have boost parameter support. [[http://www.boost.org/doc/libs/1_53_0/libs/parameter/doc/html/index.html#named-function-parameters][Documentation here]].

Ideally one would mark a type with a stereotype such as =named
parameter= and this would result in a full constructor with named
parameters. However since it seems one has to add a lot of boiler
plate code, perhaps its better to have a create function on a separate
header which internally calls the appropriate setters.

*** Add composite stereotype                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate composite objects so
that I don't have to do it manually.
#+end_quote

It would be nice if one could just mark a object as =composite= and dogen
automatically created the composite structure. As we only support
boost shared pointer that's what we'd use. We have a few use cases for
this (node, nested qname, etc).

This would be part of the injection framework.

*** Add support for bitsets                                           :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of bitsets in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

We are using a lot of boolean variables in yarn. In reality, these all
could be implemented with =std::bitset=, plus an enumeration. One
possible implementation is:

- add =std::bitset= to std model.
- create a new stereotype of bitset.
- classes with stereotype bitset are like enumerations, e.g. users are
  expected to add a list of names to the class.
- dogen will then implement the properties of type bitset as a
  =std::bitset= of the appropriate size, and also generate an
  enumeration which can be used for indexing the bitset. This may need
  to be a C++-03 enumeration, due to type safety in C++-11
  enumerations.
- we should also implement default bitsets with values corresponding
  to the flags.

Example usage:

#+begin_src c++
const unsigned int my_bitset_size(10);
std::bitset<my_bitset_size> bs;

bs[first_flag_index] = 1;
bs = first_flag_value;
#+end_src

Links:

- [[http://www.java2s.com/Tutorial/Cpp/0360__bitset/Usebitsetwithenumtogether.htm][Use bitset with enum together]]
- [[http://stackoverflow.com/questions/9857239/c11-and-17-5-2-1-3-bitmask-types][C++11 and {17.5.2.1.3} Bitmask Types]]
- [[https://www.justsoftwaresolutions.co.uk/cplusplus/using-enum-classes-as-bitfields.html][Using Enum Classes as Bitfields]]

*** Add string table support                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate "enumerations" of
strings so that I don't have to create them manually.
#+end_quote

We need a way of creating "tables" of strings such as for example for
listing all the valid values for dia field names, etc. We could
implement this by creating a new stereotype where the name is the
string name and the default value is the string value. All strings
would be static public members of a class.

We should also add a validate method which checks to see if a string
is a valid value according to the string table. We could have a "case
insensitive" validate too.

*** Enumeration string conversion could be configurable               :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the input strings that
get converted into enumerations so that I can adapt it to my
requirements.
#+end_quote

It should be possible to pass in one or more string values as implementation
specific parameters that tells dogen what valid values an enumerator
can have. We can then generate a from string method that does the
appropriate conversions.

These should be passed in as dynamic extensions. At present
enumerators do not have dynamic extensions support so we need to add
it too (e.g. add the concept to them).

*** Enumeration string dumps could be configurable                    :story:

#+begin_quote
*Story*: As a dogen user, I want to output user defined strings from
enumerations so that I can adapt it to my requirements..
#+end_quote

It should be possible to pass in a string value as a dynamic extension
that tells dogen what string to use for debug dumping. At present
enumerators do not have dynamic extensions support so we need to add
it too (e.g. add the concept to them).

*** Add is comparable to yarn                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to define types as comparable so that
I can use them in ordered containers.
#+end_quote

A object can have a stereotype of comparable. If so, then at least one
property must be marked as comparable. Properties are marked as
comparable if they have an implementation specific parameter called
=comparison_order=. =comparison_order= is a sequence starting at 0 and
incrementing by 1; it determines the order in which properties are
compared between two objects of the same type.

In order for a property to qualify as a comparison candidate its type
must be:

- primitive;
- =std::string=;
- a object marked as comparable.

Some facts about comparable objects:

- they generate =operator<= as a global operator in the type
  header file.
- they can be keys in =std::map= and =std::set=.

Relation to keys:

- If all properties that are part of a key are also comparable then
  the key will be comparable.
- comparable versioned keys always compare the version after all other
  comparable properties.

If an object itself is marked as comparable, then it is equivalent to mark
all properties as comparable using their relative position as the
comparison order.

*Merged with ordered containers story:*

In order to provide support for ordered containers such as maps and
sets we need to define =operator<=. However, it makes no sense to code
generate this operator as its unlikely we'll get it right. We could
assume the user wants to always sort by key, but that seems like a bad
assumption. The alternatives are:

- to expect a user-defined =entity_name_less_than.hpp= in domain. we'd
  automatically ignore any files matching this patter so the user can
  create them and not lose it. The problem with this approach is that
  we may have different sort criteria. This is a good YAGNI start.
- to provide the =Compare= parameter in the template and then expect a
  user-defined =entity_name_Compare.hpp=. The same ignore
  applies. This would allow users to provide any number of comparison
  operations.

Either approach requires [[Ignore%20files%20and%20folders%20based%20on%20regex][Ignore files and folders based on regex]].

Another way of generating comparators is explained here:

[[http://playfulprogramming.blogspot.se/2016/01/a-flexible-lexicographical-comparator.html][A flexible lexicographical comparator for C++ structs]]

*** Add support for type framing                                      :story:

#+begin_quote
*Story*: As a dogen user, I want to send model types over the wire so
that I can build networked applications.
#+end_quote

In places such as a cache or a socket, it may be useful to create a
basic "frame" around serialised types. The minimum requirements for a
frame would be a model ID, a type ID, a "format" (i.e. xml, text, etc)
and potentially a size, depending on the medium. The remainder of the
frame would be the payload - i.e. the serialised object.

In order for this to work we probably need the concept of a "model
group"; the type frame would be done for a group of models.

This can be done with or without boost fusion. See this presentation
for details on a boost fusion approach:

- [[https://www.youtube.com/watch?v%3DwbZdZKpUVeg][CppCon 2014: Thomas Rodgers "Implementing Wire Protocols with Boost Fusion"]]

*** Add pimpl support                                                 :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate PIMPL objects so
that I don't have to do it manually.
#+end_quote

It may be useful to mark classes as pimpl and generate a private
implementation. On the public header we could forward declare all
types.

*** Add camel case option                                             :story:

#+begin_quote
*Story*: As a dogen user, I want my models to use camel case so that I
can integrate dogen code with my code base.
#+end_quote

It would be nice to have a command line option that switches names
from underscores into camel case. The default convention would be that
diagrams are always with underscores and then you can convert them at
generation time. There should be a regex for this conversion.

*** Manual typedef generation                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of manually generated
typedefs in automated code so that I don't have to manually generate
all code that makes use of these typedefs when when its trivial code.
#+end_quote

- We should be able to create a stereotype of =typedef group=. This is
  a object type with lots of attributes. The code generator will take
  the name and type of each attribute and generate a file with the
  name of the group and all the typedefs inside.
- We should be able to create a forward declarations like header that
  defines typedefs for =shared_ptr= etc at the users choosing. This
  could be implemented as a tag. We could create a =memory_fwd= header
  to avoid cluttering the main =fwd= file for the type. We will need
  another type of relationship to model this, as well as another type
  of file in tags; the file would then have several Boolean flags one
  can tick such as =std_shared_ptr=, =boost_shared_ptr= and so on.
- it should also be possible to add some dynamic extensions to an
  attribute and get it to generate a typedef, e.g. cpp.typedef = "xyz"
  would result in the creation of typedef xyz using the type of the
  attribute; getters, setters and property would then be declared with
  the typedef.

*** Automatic typedef generation                                      :story:

#+begin_quote
*Story*: As a dogen user, I want dogen to generate typedefs so that I
don't have to create them manually.
#+end_quote

We should generate typedefs for all smart pointers, containers, etc -
basically anything that has template arguments. This would make
generated code much more readable and could also be used by client
code. In theory all we need is:

1. determine if the property has type arguments;
2. if so, construct the typedef name by adding =_type= to the property
   name, e.g. =attribute_value= becomes =attribute_value_type=, etc;
3. create a typedef section at the top of the class declaring all
   typedefs;
4. add a property to the property view model containing the typedef
   name and use it instead of the fully qualified type name.
5. we should also generate a typedef for the key if the class is an
   entity. See Typedef keys for each type.

We could also always generate a typedef for smart pointers in the
class that uses the smart pointer, with a simple convention such as
=attribute_value_ptr_type= or =shared_attribute_value_type=.

*** Add support for iterable enumerations                             :story:

#+begin_quote
*Story*: As a dogen user, I want enumerations to be iterable so that I
don't have to manually create code to do this.
#+end_quote

We should create an additional aspect for each enumerations which
creates a =std::array= with the enumerators (excluding invalid). This
would allow plugging the enumerations into for loops, boost ranges,
etc. The CPP should contain a static array; The HPP contains a method
which returns it, e.g. =my_enumeration_to_array.hpp=:

: std::array<my_enumeration, 5> my_enumeration_to_array();

We could make this slightly more generic by adding the notion of
enumeration groups. Out of the box we have:

- all: includes invalid;
- valid: excludes invalid

Users could then add implementation specific properties to create
other groups if needed.

*** Add support for user supplied test data sets            :reviewing:story:

#+begin_quote
*Story*: As a dogen user, I want to make use of test data without
having to manually add code for it.
#+end_quote

*New understanding*:

We need to create a test data sets model. it should have an
enumeration for all of the available test data sets, and an
enumeration for the valid file formats. we should be able to pass in a
pair of file formats (input, actual/expected) and out should come a
triplet of directories. This would make maintenance really easy as
we'd only need to ad new strings to a string table. the service would
also handle things like the actual and expected directories, etc.

It should fix the following issues:

- [[*Adding%20new%20engine%20spec%20tests%20is%20hard][Adding new engine spec tests is hard]]
- [[*Naming%20of%20saved%20yarn/Dia%20files%20is%20incorrect][Naming of saved yarn/Dia files is incorrect]]

*Old understanding*:

The correct solution for test data and test data sets is as follows:

- the code generated by dogen in the test data directory is one of
  many possible ways of instantiating a model with test data.
- there are two types of instantiations: code and data. code is like
  dogen =test_data=; data is XML, text or binary - or any other
  supported boost archive; it also includes other external formats
  such as dia diagrams.
- a model should have a default enum with all the available test data
  sets: =test_data::sets=. If left to its default state it has only one
  entry (say =dogen=). The use is free to declare an enumeration on a
  diagram with the name test_data_sets and add other values to it.
- there must be a set of folders under test_data which match the
  enumerators of =test_data::sets=. Under each folder there must be an
  entry point such as =ENUMERATOR_generator=. Dogen will automatically
  ignore these folders via regular expressions.
- a factory will be created by dogen which will automatically include
  all such =ENUMERATOR_generator=. It will use static methods on the
  generator to determine what sort of capabilities the generator has
  (file, code, which formats supported, etc.) and throw if the user
  attempts to misuse it.
- all models must have a repository. Perhaps we need a stereotype of
  =repository= to identify it. This is what the factory will create.
- users will instantiate the factory and call =make=:

: my_model::test_data::factory f1;
: auto r = f1.make(my_model::test_data::sets::dogen);
:
: my_model::test_data::factory f2(expected_dir, actual_dir);
: auto r = f2.make(my_model::test_data_sets::some_set,
:   my_model::test_data::file_formats::boost_xml, file_locations::expected);

- if the user requires parsing a non-boost serialisation file then it
  should be make clear on the enum: =std_model, std_model_dia=. The
  second enumerator will read dia files. It will not support any file
  formats. The file must exist on either the expected or actual
  directory as per =file_locations= parameter.

Another topic which is also part of test data is the generation of
data for specific tests. At present we have lots of ad-hoc functions
scattered around different places. They should all live under test
data and be part of a test data set. The test data set should probably
be the spec name.

*** Add support for template classes                                  :story:

At present we can create classes in dia that require template
parameters; however, when we try to create member variables that
instantiate that class the parser fails to parse. For example,
=expansion::inclusion_dependencies_provider= has a template parameter:

: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>

The parser fails:

: 2015-04-02 15:55:02.084366 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>
: 2015-04-02 15:55:02.086929 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(197): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string: std::forward_list<boost::shared_ptr<expansion::inclusion_dependencies_provider<sml::module>>
: [P12tag_workflow] = Code generation failure.

In addition, even if the parser succeeded, we still need a way to tell
dogen that the class has a template parameter; this is hard-coded at
the moment for containers (we determine if the type is an associative
container, etc). We need it to be dynamically determined when
inspecting the yarn type. For example, we could have a "number of
template parameters" in the type. This could be set in the meta-data
for STL containers. Or we could actually specify the template
parameters as "type arguments" just like we do with operations and
properties (preferred). The parser would then use this info.

*** Add support for =std::function=                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of =std::function= so
that I don't have to manually generate code for types that use it.
#+end_quote

At present its not possible to declare an attribute of type
=std::function= anywhere in a diagram. It won't really be possible to
do so for entities and values because boost serialisation will always
be a problem. If this was really a requirement, we could look into
serialising functions:

- [[https://groups.google.com/forum/?fromgroups%3D#!topic/boost-list/sHWRPlpPsf4][how to serialize boost::function]]

However we don't seem to need this quite just yet. What we do need is
a way of having attributes in services and that is slightly easier:

- the parser needs to be able to understand the function template
  syntax (e.g. =void(int)=). It seems this could be hacked easily
  enough into the parser.
- Nested qualified names need to be able to remember that in the case
  of a function, the first argument is a return type (they also need
  to know they represent a function). MC: is this actually necessary?
  all we need is to be able to reconstruct this syntax at format time.
- we need a =void= type in the primitives model. This is a bit more
  complicated since this type can't have values, only pointers, and we
  don't really support raw pointers at the moment. Adding the type
  blindly would open up all sorts of compilation errors.

This should be sufficient for services. At present we have a hack that
allows functions without any template arguments, e.g. =std::function=,
in services.

*** Add support for references and pointers to types                  :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of references and
pointers so that I don't have to manually generate code for types that
use it.
#+end_quote

At present its not possible to create a type that has a reference to
another type. This should be a case of updating the parser to cope
with references and adding reference to property or nested type
name. This would be a good time to inspect our support for raw
pointers, it probably suffers from exactly the same problem and
requires the same solution.

In addition we should also bear in mind moving. Ideally one should be
able to declare moveable attributes and the end result should be a
setter that takes the type by =&&=. The question then is should we
also move on the getter? Sometimes it may not be a copyable type
(e.g. asio's =socket=).

It seems we can't also cope with =const= or pointers. To be fair we
only need const for shared pointer for now. On all cases we need to
make the parser more clever:

: boost::shared_ptr<const my_type>
: std::string&

We should try to create tests for all the cases we consider important
and mark them as ignore until we can find a spirit expert to help out.

*** Add support for default values                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to set default values for certain
primitives so that I can model my domain more accurately.
#+end_quote

It would be nice to be able to add a default value in Dia and have it
set on the default constructor, if the type is a primitive or a
=std::string=.

These could be added as dynamic extensions; however, they should not
be model specific - the same defaults should apply to all
languages. However, note that in certain cases the default values may
be language specific (e.g. =0.0f= for float, etc).

Note also that there are two levels of default values: the type-level
(all primitives of type =x= default to =y=) and the property level
(instances of type =x= in class =z= default to =y=). The meta-data can
be applied at each level to achieve the desired effect.

*** Add support for interfaces                                         :epic:

It would be great to be able to mark a type as an interface, add all
methods that the interface has, and have dogen fully code-generate the
interface, marking all methods as pure virtual, override, etc. We
would want to distinguish this from the abstract base class case,
because we may want to add one or more concrete methods to the ABC.

We need support for operations for this to work. We also need support
for pointers, references and const.

*** Use error codes in exceptions                                     :story:

Avoid breaking tests every time the exception text changes by creating
a error code property in kitanda exceptions.

After some investigation it was found that boost already supports this
approach in =system=, as per [[http://en.highscore.de/cpp/boost/errorhandling.html][boost book]]. We could define a new
category per model and then create an enumeration of all error codes
in dia, for which the values would be the strings to use for the
error. The user could then create an exception and pass in the error
code in the constructor.

We should also make use of string tables to define all the error
messages.

Could we just have an exception factory that handles all of the
machinery of creating an exception with the right code, message etc?
it could also be responsible for appending more content to an existing
exception so that we'd have the tags all in one place.

Alternatively we could have each exception define the supported error
codes. This would allow us to code generate them. The only problem is
if multiple exceptions share an error code, but this should probably
not happen?

*** Dry-run option to just diff with existing generated code           :epic:

#+begin_quote
*Story*: As a dogen user, I want to know what has changed with the
next code generation so that I can evaluate if the changes are as
expected or not.
#+end_quote

It would be useful to have an option that would do everything except
writing the files to disk; instead, it would diff them with the
existing files and report if there are any differences. This would be
useful to make sure the source code matches the latest version of the
diagram.

We could use something like the [[https://code.google.com/p/dtl-cpp/wiki/Tutorial][DTL library]].

*** Add support for configurable enumerations types                   :story:

#+begin_quote
*Story*: As a dogen user, I need to configure the primitive type of my
enumerations so that I model my domain accurately.
#+end_quote

We've updated the =hardware= model with a "default enumeration value"
field. This allows us to dynamically determine which primitive to use
as the type of enumerations. However:

- we didn't follow it through in the formatters; we are hard-coding
  this at present in C++. In a cross-language world, we should
  dynamically detect the default enumeration type. This is not quite
  as trivial as it seems (what would happen if we loaded multiple
  programming languages?). Supporting this properly may require adding
  a programming language to the model.
- it is not possible to override this from JSON/Dia. We could do this
  by supplying a type via dynamic extensions.

*** Add support for configurable enumeration values                   :story:

At present we generate the enumeration value as part of the
transformation process in =yarn.dia=, based on the relative position
of the enumerator. This is not ideal:

- it won't work for JSON;
- it does not allow users to supply their own values;
- it does not allow users to disable enumeration values altogether and
  rely on language defaults instead.

We could:

- move it to =yarn= so that JSON and Dia share the same code;
- add meta-data for users to supply their own values;
- add meta-data to disable setting the enumerator value altogether.

*** Exception classes should allow inheritance                        :story:

#+begin_quote
*Story*: As a dogen user, I need to generate object graphs for my
exception classes so that I can model my domain better.
#+end_quote

We need to have a form of inheriting from a base exception for a given
model. We also need to be able to inherit from other exceptions in a
model. At present exceptions are not objects so the dependency graph
support is not there.

When we do this we need to split relatable into "associatable" and
"generalisable" and get exceptions to model generalisable.

*** Add support for private and protected attributes                  :story:

#+begin_quote
*Story*: As a dogen user, I would like to define properties as
protected so that I only expose them to derived types.
#+end_quote

We need to distinguish between public and protected attributes when in
the presence of inheritance. If not, issue a warning.

Another interesting use case for protected and private attributes is
extensions. This is not yet supported by C++, but one can envision a
time where it will be possible to extend the code generated classes
with manually crafted code; these extensions may have access to
protected/private state, thus allowing for encapsulation.

*** Shared pointers as keys in associative containers                 :story:

This is not supported; it would require generating the
hashing/comparison infrastructure for shared pointers. Further, as it
has been pointed out, keys should be immutable; having pointers as
keys opens the doors to all sorts of problems. We need to throw an
error at model building time if an user tries to do this.

*** Add support for error info augmenting of exceptions               :story:

When we declare a boost exception, we some times need to augment it
with additional data. This is explained [[http://www.boost.org/doc/libs/1_58_0/libs/exception/doc/tutorial_transporting_data.html][here]]. We should be able to add
these tags to the exception class and have dogen generate the
=error_info= boilerplate in the exception header.

Note that we can have many tags associated with a given
exception. They all need to have meaningful names. They should all
have globally unique names, unless the name makes sense in the context
of more than one exception. We could make the tags a property of the
exception (e.g. inner class). Or we could simply have a global class
that declares all of the application tags in one go. If most tags are
exception specific, then the first approach is better. If many tags
are shared, the latter is preferable.

In terms of dia/frontend, we can simply add these as attributes of the
exception and have the code generator do the right thing for
C++. Other languages would/could have different implementations.

Once we implement this we need to go through all exceptions and add
all the needed attributes; in many cases we are being lazy and logging
one message but throwing a less informative one. This should really be
captured in the exception type.

We should probably support some meta-data to enable/disable this
behaviour, in case someone wants real attributes in an exception.

*** Ensure an error info tag is not set already before we set it      :story:

Now that we will start making use of error info tags in anger, one
problem we have spotted is that some times we reset the same tag:
i.e. when an exception has been thrown, we do not want to set the same
tag multiple times because it overwrites it. There should be a way to
either allow multiple bits of information to be associated with the
same tag, or a way to check if the tag has been already set. But if it
is, what then? We can't throw.

** Tooling, infrastructure, blog posts, etc.

Stuff that is not directly related to the problem domain but its
needed by Dogen.

*** Reduce build times to avoid timeouts                              :story:

At present we get random build time violations on travis due to builds
taking longer than 50 mins. We need to think of ways to reduce the
build time. Things to try:

- remove all of the hashing etc for the types we don't need to hash.
- get rid of the warnings for boost.

*** Add scripts to sanity check packages                               :epic:

Create a set of scripts that validate packages:

- install packages on all supported platforms.
- sanity check that package installed correctly, e.g. check for a few
  key files.
- run sanity tests, e.g. create a dogen model and validate the results
- run uninstaller and sanity check that files are gone
  - this should actually be a build agent so we can see that deployment
    is green. We should create a deployment CMake script that does this.

We had some initial support for this in commit ba54859.

*** Finish setting up coveralls                                       :story:

Remaining issues:

- we are generating far too much output. We need to keep it quieter or
  we will break travis.
- we are not filtering out non-project files from initial
  processing. There must be a gcov option to ignore files.

: Process: /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc-5/Debug/projects/quilt/spec/CMakeFiles/quilt.spec.dir/main.cpp.gcda
: ------------------------------------------------------------------------------
: File '../../../../projects/quilt/spec/main.cpp'
: Lines executed:62.50% of 8
: Creating '^#^#^#^#projects#quilt#spec#main.cpp.gcov'
:
: File '/usr/local/personal/include/boost/smart_ptr/detail/sp_counted_impl.hpp'
: Lines executed:60.00% of 20
: Creating '#usr#local#personal#include#boost#smart_ptr#detail#sp_counted_impl.hpp.gcov'

See also:

- [[https://github.com/JoakimSoderberg/coveralls-cmake-example/blob/master/CMakeLists.txt][example use of coveralls-cmake]]
- [[https://github.com/SpinWaveGenie/SpinWaveGenie/blob/master/libSpinWaveGenie/CMakeLists.txt][SpinWaveGenie's support for Coveralls]]

Previous story [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_84.org#add-initial-support-for-coveralls][here]].

*** Consider adding profiling support via build type                  :story:

At present we used a dogen specific variable for profiling:
=WITH_PROFILING=. However, we could extend the cmake build type
mechanism with a =Profiling= build type.

See:

- [[https://github.com/Milkyway-at-home/milkywayathome_client/blob/master/cmake_modules/Profiling.cmake][milkywayathome_client's profiling for CMake]]

*** Build boost for MinGW                                             :story:

At present our MinGW build is borked because we do not have a boost
build. Create one and add it to DropBox or Conan.

*** Add support for coverity analysis                                 :story:

We've created a project and added the badge but we have not yet setup
travis for this.

- [[https://scan.coverity.com/projects/domaindrivenconsulting-dogen/builds/new?tab%3Dtravis_ci][Automate Analysis with Travis CI]]
- [[https://github.com/d-led/picojson_serializer/blob/master/.travis.yml][example .travis.yml]]

We should upload a build whenever we tag a release.

*** Link against boost dynamic libraries                              :story:

At present we are statically linking against Boost just to make our
life easier. This was done mainly to fix the problems we had with
Travis red builds but had the side effect of fixing our Debian
packages. However, as soon as boost 1.60 is available on Travis we
probably should go back to dynamic linking. Removed code:

:    # elseif (UNIX)
:    #     if (library_path MATCHES "boost")
:    #         set(version "${Boost_MAJOR_VERSION}")
:    #         set(version "${version}.${Boost_MINOR_VERSION}")
:    #         set(version "${version}.${Boost_SUBMINOR_VERSION}")
:
:    #         get_filename_component(library_name ${library_path} NAME_WE)
:    #         string(REGEX REPLACE "_" "-" library_name "${library_name}")
:
:    #         # FIXME: boost 1.55 is not the default in unstable yet.
:    #         # set(library_name "${library_name}-dev")
:    #         set(library_name "${library_name}${Boost_MAJOR_VERSION}.")
:    #         set(library_name "${library_name}${Boost_MINOR_VERSION}-dev")
:    #         if ("${boost_deps}" STREQUAL "")
:    #             set(boost_deps "${library_name} (>= ${version})")
:    #         else()
:    #             set(boost_deps "${boost_deps}, ${library_name} (>= ${version})")
:    #         endif()
:    #     endif()

See also main CMakeLists.txt around =6b837334c3247bee2f6be8099cd5288a284fa36c=.

Or is the best practice that the user should choose how to link
against boost in the same way it chooses to generate static or dynamic
libraries internally for Dogen?

*** Resurrect CDash agents                                            :story:

CDash has bitrotted and is no longer working.

- we need to get the build green on the Windows agent again.
- we need to get the linux agent up and running again.

*** Get OSX build to compile code                                     :story:

We've added the initial support for OSX. However, it still needs a lot
of work:

- we can't install the conan package because we don't know how to
  install pkg files. We should raise a ticket on conan for this.
- Alternatively we could build boost ourselves and upload it to
  DropBox.

Notes:

- [[http://www.mactech.com/articles/mactech/Vol.26/26.02/TheFlatPackage/index.html][The Flat Package]]
- [[https://docs.travis-ci.com/user/multi-os/][Matrix with multiple OSs]]

*** Assorted improvements to CMake files                               :epic:

It seems we are not using proper CMake idioms to pick up compiler
features, as explained here:

- [[http://www.slideshare.net/DanielPfeifer1/cmake-48475415][CMake - Introduction and best practices]]
- [[https://datascience.lanl.gov/data/151208-LANL-Hoffman-Science.pdf][Building Science with CMake]]
- [[http://voices.canonical.com/jussi.pakkanen/2013/03/26/a-list-of-common-cmake-antipatterns/][A list of common CMake antipatterns]]

We need to implement this using proper CMake idioms.

Notes:

- Add version and language to project.
- start using [[https://cmake.org/cmake/help/v3.3/command/target_compile_options.html][target compile options]] for each target. We will have to
  repeat the same flags; this could be avoided by passing in a
  variable. See also [[http://stackoverflow.com/questions/23995019/what-is-the-modern-method-for-setting-general-compile-flags-in-cmake][What is the modern method for setting general
  compile flags in CMake?]]
- define qualified aliases for all libraries, including nested
  aliasing for =dogen::test_models=. Ensure all linking is done
  against qualified names.
- use target include directories for each target and only add the
  required include directories to each target. Mark them with the
  appropriate visibility, including using =interface=. We should then
  remove all duplication of libraries in the specs.
- try replacing calls to =-std=c++-14= with compiler feature
  detection. We need to create a list of all C++-14 features we're
  using.
- remove all of the debug/release compilation options and start using
  =CMAKE_BUILD_TYPE= instead. See [[http://pastebin.com/jCDW5Aa9][this]] example. We added build type
  support to our builds, but as a result, the binaries moved from
  =stage/bin= to =bin=. There is no obvious explanation for this.
- remove =STATIC= on all libraries and let users specify which linkage
  to use. We already have a story to capture this work.
- remove the stage folder and use the traditional CMake
  directories. This will also fix the problems we have with
  BUILD_TYPE.

Merged stories:

*Usage of external module path in cmakelists*                       :story:

It seems like we are not populating the target names
properly. Originally the target name for test model all primitives was:

: dogen_all_primitives

When we moved the test models into =test_models= the target name did
not change. It should have changed to:

: dogen_test_models_all_primitives

*** Check that custom targets in CMake have correct dependencies      :story:

At present we have a number of custom targets, which create a new Make
target. These are good because they do not require re-running CMake to
manage the files in the output directory; however, we do not have the
correct dependencies between the targets and the target
dependencies. For example, create_scripts should check to see if any
script has changed before re-generating the tarball; it seems to have
no dependencies so it will always regenerate the tarball. We need to:

- check all custom targets and see what their current behaviour is:
  a) change a dependency and rebuild the target and see if the
  change is picked up or not; b) change no dependencies and re-run the
  target and ensure that nothing happens.
- add dependencies as required.

*** Enable doxygen warnings for all undocumented code                 :story:

At present doxygen only warns about undocumented parameters when a
function already has documented parameters. We should consider
enabling warnings for all undocumented code. We also need to figure
out how to mark code as ignored (for example serialisation helpers,
etc won't require documentation).

*** Add WinSock definition in CMakeLists for ODB support              :story:

We did a crude implementation of finding WinSock just to get windows
to build. There should be a FindWinSock somewhere. If not create one.

Do we need this anymore? we probably need it for linking the database
model, but we should check - maybe ODB has some magic around this.

Actually this was commented out in code so removed it. Was:

: # WinSock (for database)
: # if (WIN32)
: #     find_library(WSOCK_LIB NAMES wsock32 DOC "The winsock library")
: #     if(WSOCK_LIB)
: #         list(APPEND CMAKE_REQUIRED_LIBRARIES wsock32)
: #     else()
: #         message(FATAL_ERROR "winsock not found.")
: #     endif()
:
: #     find_library(WSOCK2_LIB NAMES ws2_32 DOC "The winsock 2 library")
: #     if(WSOCK2_LIB)
: #         list(APPEND CMAKE_REQUIRED_LIBRARIES ws2_32)
: #     else()
: #         message(FATAL_ERROR "winsock2 not found.")
: #     endif()
:
: #     find_library(MSWSOCK_LIB NAMES mswsock DOC "The winsock 2 library")
: #     if(MSWSOCK_LIB)
: #         list(APPEND CMAKE_REQUIRED_LIBRARIES mswsock)
: #     else()
: #         message(FATAL_ERROR "mswsock not found.")
: #     endif()
: # endif()

*** Support for cmake components and groups                           :story:

#+begin_quote
*Story*: As a dogen user, I need to integrate the generated models
with my existing packaging code.
#+end_quote

We recently added support for creating multiple packages from a single
source tree. We need generated models to have a new top-level cmake file:

: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/src)
: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/tests)
:
: install(
:     DIRECTORY include/
:     DESTINATION include
:     COMPONENT headers
:     FILES_MATCHING PATTERN "*.hpp")

And the =src= cmake file:

: install(TARGETS dia ARCHIVE DESTINATION lib COMPONENT libraries)

*** Self-contained build files                                        :story:

#+begin_quote
*Story*: As a dogen user, I would like to build models without having
to add any code so that I use dogen without needing to learn lots
about build systems.
#+end_quote

It would be nice to be able to generate a complete application from a
given model, or a library. At present there is an expectation that the
user will slot in the generated CMake files into a larger, more
comprehensive CMake build. All we need is:

- some kind of binary type: e.g. executable or library. we should have
  this anyway. meta data at the model level could be used to convey
  this.
- if executable, we should automatically ignore a main.cpp in the
  source directory.
- generate a stand-alone CMake template.

The idea is that with this the user could immediately generate a
binary without any further configuration required.

*** Remove calls to unix utilities directly                           :story:

We have a couple of cases where we are calling UNIX utilities when
there are equivalent CMake calls. For example
=test_data/CMakeLists.txt=:

:        set(target_name "rebase_${model_name}")
:        if(EXISTS "${git_expected}/")
:            add_custom_target(${target_name}
:                COMMAND rm -rf ${expected}/*
:                COMMAND cp ${actual}/* ${expected}
:                COMMAND cp ${actual}/* ${git_expected}/
:                WORKING_DIRECTORY ${stage_bin_dir})

These can be replaced with:

:        file(REMOVE_RECURSE ${stage_test_data_dir}/${dir}/expected)
:        file(COPY ${curdir}/${dir} DESTINATION ${stage_dir}/test_data)
:        file(MAKE_DIRECTORY ${stage_test_data_dir}/${dir}/actual)

etc.

*** Detect the presence of the diff command                           :story:

The diff targets is dependent on the presence of the diff command, but
we are not checking to see if it exists:

:        set(target_name "diff_${model_name}")
:        add_custom_target(${target_name}
:            COMMAND diff -r -u ${expected} ${actual}
:            WORKING_DIRECTORY ${stage_bin_dir})
:        add_dependencies(diff_dataset ${target_name})

We need to detect it and only add this targets if diff has been
found. We probably should look for a =FindDiff.cmake= script.

*** Consider creating an Emacs mode for stitch                        :story:

It would be nice to have syntax highlighting for stitch templates. We
have a [[https://github.com/mcraveiro/cunene/blob/master/lisp/other/utils/t4-mode.el][mumamo-based version]] in cunene - originally done for t4 - but
which is rather unusable.

See also [[https://github.com/fxbois/web-mode][web-mode]].

Merged stories:

*Investigate adding polymode support for stitch templates*

We need a way to visualise stitch templates that is a bit more
readable than fundamental mode. One option is [[https://github.com/vspinu/polymode/tree/master/modes][polymode]].

*** Installable package names should follow a well-known convention   :story:

We need to make sure our package names are consistent with the
platform conventions.

- [[http://pastebin.com/TR17TUy9][Example of platform IFs]]
- [[http://libdivsufsort.googlecode.com/svn-history/r6/trunk/CMakeModules/ProjectCPack.cmake][Example CPack]]
- [[http://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.htmlhttp://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.html][Automatically add a revision number to the CPack installer name]]
- [[http://www.cmake.org/Wiki/CMake:CPackConfiguration][CPack Configuration]]

There are some known limitations in package naming:

- [[http://public.kitware.com/Bug/view.php?id%3D12997][0012997: Provide a more flexible way to name package produced by CPack]]

*** Allow user to choose whether to build shared or static libraries  :story:

#+begin_quote
*Story*: As a dogen user, I want to generate models as shared objects
so that I don't have to statically link models all the time.
#+end_quote

With the increase in tests build speeds have started to suffer,
especially on low hardware. One potential way to mitigate this is to
avoid unnecessary linking. The problem we have at present is that
every time something changes in any model we have to relink all the
binaries that use that model as it is consumed as a static library. If
all the static libraries were converted to shared objects this would
no longer be necessary.

We probably need a dogen command line option to determine what to
build so that users are not forced to always build static / shared
libraries. We should make sure one of the tests is using a static
library to make sure this scenario doesn't get borked.

Actually we need to figure out how to pass this as a parameter into
CMake to let the user configure how the libraries are generated
(static or dynamic). Look for a best practices presentation on CMake
that talks about this.

*** Migrate from Boost Test to Catch                                  :story:

The [[https://github.com/philsquared/Catch][Catch]] testing framework appears to be very nice:

[[https://vimeo.com/131632252][Testdriven C++ with Catch - Phil Nash]]

We should look into moving from Boost test to Catch. We should also
take the opportunity to extend catch with a few [[https://github.com/philsquared/Catch/tree/master/include/reporters][reporters]]:

- boost log reporter such that all the logging from unit testing goes
  into the log file and the log file is setup automatically for each
  scenario.
- "emacs" reporter such that we get compilation error like
  messages. This may already be in Catch.
- CDash reporter so that we don't have to do all of the CMake magic in
  order to see the unit tests in CDash

Merged Stories:

We should check the Martin Fowler article on [[http://martinfowler.com/bliki/GivenWhenThen.html][Given-When-Then]] as a way
of specifying unit tests, to see if it would make our tests
clearer. We are already following some kind of Given and Then, but we
should consider making it explicit.

The best approach may be to move all unit tests to [[https://github.com/philsquared/Catch][Catch]], since it
natively supports GWT.

*** Enable package installation tests for Linux                        :epic:

Now that we will be using docker, we could create a simple =systemd=
ctest script that runs as root in a docker container:

- build package and drop them on a well known location;
- Create a batch script that polls this location for new packages;
  when one is found run package installer. Looks for files that match
  a given regular expression (e.g. we need to make sure we match the
  bitness and the platform)
- if it finds one, it installs it and runs sanity scripts (see story
  for sanity scripts).
- it then uninstalls it and makes sure the docker image is identical
  to how we started (however that is done in docker)

*** Add support for uploading packages in cloud storage               :story:

We need to upload the packages created by each build to a public
Google Drive (GDrive) location or to DropBox

- Google drive folder created [[https://drive.google.com/folderview?id%3D0B4sIAJ9bC4XecFBOTE1LZEpINUE&usp%3Dsharing][here]].
- See [[https://developers.google.com/drive/quickstart-ruby][this article]].
- [[http://stackoverflow.com/questions/15798141/create-folder-in-google-drive-with-google-drive-ruby-gem][Create folders]] to represent the different types of uploads:
  =tag_x.y.z=, =last=, =previous=. maybe we should only have latest
  and tag as this would require no complex logic: if tag create new
  folder, if latest, delete then create.

We are uploading the tag packages to GitHub already, but ideally we
should test all packages for all commits.

*** Consider moving dia diagrams to their own project                 :story:

We originally create a diagrams top-level directory for all of the
diagrams that generate dogen models. However it may make more sense to
have the diagrams closer to the project they generate; for example a
folder within the project. The downside of this approach is that we
now have to make references across projects; we could use relative
paths for this but the targets will still look verbose.

*** Consider renaming =diagrams= to =models=                          :story:

The name "diagrams" is a bit misleading. These are not just diagrams,
they are specifically Dogen inputs. We need a name that is a bit more
meaningful. For example, if we start creating models in JSON, it would
render the directory name meaningless. We should call the directory
=models=.

*** Add Travis support for 32-bits                                    :story:

It seems its fairly straightforward to add 32-bit support to Travis,
as per RapidJSON:

https://github.com/miloyip/rapidjson/blob/master/.travis.yml

*** Add a GitHub page for Dogen                                       :story:

We should be able to create a simple project page for Dogen by using
the automatic project generator:

https://pages.github.com/

However, to do so we probably should convert the README into MarkDown
first. It is used to generate the project page's contents.

*** Add Doxygen docs to GitHub page                                   :story:

Once we setup the Dogen page, we should automatically upload the
Doxygen documentation there. This should be done from Travis:

- setup a Doxygen build in Travis that builds the docs into staging,
  then changes branch into =gh-pages=, then =rm -rf= the previous docs
  and copies across the new docs then commits and pushes.
- we need to make sure the docs are not very large.

Example from RapidJSON available [[https://github.com/miloyip/rapidjson/blob/master/travis-doxygen.sh][here]].

This can be done by adding a build to the matrix with a
=DOCUMENTATION= env variable. When this is on, the travis build builds
and deploys documentation. We should only do this when tagging.

Merged Stories:

*Add doxygen build in travis using GitHub pages*

It seems is pretty straightforward to add a doxygen build to travis:

- [[http://blog.gockelhut.com/2014/09/automatic-documentation-publishing-with.html][Automatic Documentation Publishing with GitHub and TravisCI]]

See also [[https://github.com/tgockel/nginxconfig/blob/master/config/publish-doxygen][this script]].

*** Add vagrant and docker support                                    :story:

It would be nice to provide vagrant and docker support to dogen in
terms of development. The idea is that you can =git clone= the repo
and then =vagrant up= it and you would be ready to start coding. This
would make drive-by patches much easier.

*** Use clang format to format the code base                          :story:

It seems clang-format is being used by quite a lot of people to save
time with the formatting of the code. More info:

http://clang.llvm.org/docs/ClangFormat.html

Emacs support:

https://llvm.org/svn/llvm-project/cfe/trunk/tools/clang-format/clang-format.el

*** Consider adding an emblem for clang's analysis checks             :story:

NeoVim appears to have emblems for waffle as well as clang's analysis
checks:

https://github.com/neovim/neovim

*** Consider adding NuGet support                                     :story:

Seems like AppVeyor supports deployment into nugget. Example:

http://www.nuget.org/packages/RxCpp/

It also comes with a couple of useful emblems:

https://github.com/Reactive-Extensions/RxCpp

We should push both the C++ libraries as well as the dogen binary.

*** Travis deployment of tags fails                                   :story:

As per [[https://github.com/travis-ci/travis-ci/issues/2577][issue 2577]] in travis, it does not support wildcards at the
moment. We need to find another way to upload packages into GitHub
without using wildcards.

Error:

: dpl.1
: Installing deploy dependencies
: Fetching: addressable-2.3.6.gem (100%)
: Successfully installed addressable-2.3.6
: Fetching: multipart-post-2.0.0.gem (100%)
: Successfully installed multipart-post-2.0.0
: Fetching: faraday-0.9.0.gem (100%)
: Successfully installed faraday-0.9.0
: Fetching: sawyer-0.5.5.gem (100%)
: Successfully installed sawyer-0.5.5
: Fetching: octokit-3.5.2.gem (100%)
: Successfully installed octokit-3.5.2
: 5 gems installed
: Fetching: mime-types-2.4.3.gem (100%)
: Successfully installed mime-types-2.4.3
: 1 gem installed
: error: could not lock config file .git/config: No such file or directory
: error: could not lock config file .git/config: No such file or directory
: dpl.2
: Preparing deploy
: Logged in as Marco Craveiro
: Deploying to repo: DomainDrivenConsulting/dogen
: Current tag is: v0.56.2767
: dpl.3
: Deploying application
: /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `initialize': No such file or directory - stage/pkg/*.deb (Errno::ENOENT)
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `new'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/octokit-3.5.2/lib/octokit/client/releases.rb:86:in `upload_asset'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:118:in `block in push_app'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:102:in `each'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider/releases.rb:102:in `push_app'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider.rb:122:in `block in deploy'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:41:in `fold'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/provider.rb:122:in `deploy'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:32:in `run'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/lib/dpl/cli.rb:7:in `run'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/gems/dpl-1.7.6/bin/dpl:5:in `<top (required)>'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/bin/dpl:23:in `load'
:     from /home/travis/.rvm/gems/ruby-1.9.3-p550/bin/dpl:23:in `<main>'
: failed to deploy

We solved this for now by just uploading the application packages.

*** Consider hosting documentation in read the docs                   :story:

We should consider hosting the documentation here:

- https://readthedocs.org/

This probably means we need to move to Mark Down.

*** Create a demo of installing dogen and generating hello world      :story:

#+begin_quote
*Story*: As a dogen user, I want basic instructions on using the
program so that I don't have to figure it all out by myself.
#+end_quote

We need to start creating a series of quick videos demoing dogen. The
script for the first video of the series is as follows:

- download packages from GitHub and install them.
- obtain the hello world model from git.
- generate the hello world model.
- create a hello world main with make files and compile it.
- give a quick overview of the available facilities.

*** Add a CMake module for git                                        :story:

We are finding git manually at the moment, which means we are probably
not doing it right. It appears there is a CMake script for it:

- [[%20http://gnuradio.org/redmine/projects/gnuradio/repository/revisions/accb9f2fe8fd8f6a1e114adac5b15304b0e0012d/entry/cmake/Modules/FindGit.cmake][FindGIT.cmake]]

*** Consider adding support for clang-tidy                            :story:

As [[http://clang.llvm.org/extra/clang-tidy/][per docs]]:

#+begin_quote
clang-tidy is a clang-based C++ “linter” tool. Its purpose is to
provide an extensible framework for diagnosing and fixing typical
programming errors, like style violations, interface misuse, or bugs
that can be deduced via static analysis. clang-tidy is modular and
provides a convenient interface for writing new checks.
#+end_quote

See also:

- [[https://github.com/polysquare/clang-tidy-target-cmake][clang-tidy-target-cmake]]

*** Add support for the =scan-build= static analyser                  :story:

=scan-build= is a command line utility that enables a user to run the
static analyzer over their codebase as part of performing a regular
build.

- [[%20http://clang-analyzer.llvm.org/scan-build.html][scan-build]] project page

*** Add support for iwyu                                              :story:

There is a clang based tool that checks if which includes are actually
used by the translation unit. We should have a build for this that
breaks whenever one includes something which is not required.

- [[http://code.google.com/p/include-what-you-use/][iwyu project page]]
- [[http://mpd.jinr.ru/svn/mpdroot/trunk/cmake/modules/FindIWYU.cmake][FindIWYU.cmake]]
- [[https://github.com/christophgysin/addp/blob/master/cmake/iwyu.cmake][iwyu.cmake]]

*** Add support for Address Sanitizer (ASan)                          :story:

This seems like another interest dynamic analysis tool:

[[https://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction][Address Sanitizer]]

*** Add support for CPPCheck                                          :story:

Seems like CPPCheck has a different take on dynamic analysis when
compared to Valgrind. We should look into how hard it is to integrate
it with CTest, Travis, etc.

Links:

- [[http://cmake.3232098.n2.nabble.com/Static-code-analysis-with-CDash-td6079787.html][CMake and CPPCheck]]

** Visionary work and random ideas

These stories are far in the future or just plain crazy.

*** Consider using the fmt library instead of iostreams               :story:

It would be interesting to benchmark dogen using iostreams vs dogen
using the fmt library:

https://github.com/fmtlib/fmt

*** Add support for element renaming                                  :story:

This story is still in its early days. We could probably create a
simple tool that renames types. There are several things we may want
to achieve from this:

- dia updating tool that receives new and old name and updates the
  diagram. This makes renames easier because we keep missing places
  where types are used.
- dogen support: with access to new name and old name we can now
  rename files which are not code generated. This avoids the usual
  loss of work, git reverts etc.

The problem we have is that if we do not update the diagram
automatically, the rename is not that useful. However, updating the
diagram is not entirely trivial. We probably could record XPaths
against elements and then use those XPaths to update the XML
tree. This would be a multi-step process:

- first we'd generate the final Yarn model;
- then use the old ID to locate the element to rename and all of its
  usages;
- then update the XML based on the XPaths obtained from the renamed
  elements and save it to file (maybe as =.new=?).
- we can then reload the yarn model from the new and generate from
  there.

- *Automatic Renames*

This could be potentially done automatically if we could access say
the existing version of the diagram and the new version; we could load
both models and then do a diff. This could be a bit dangerous though
if there are other changes, etc.

*** Generate tests skeleton                                           :story:

When we create a dogen project for the first time, we should be able
to, optionally, add the tests directory with skeleton code and a
sample test. If the directory already exists (or if the option is off)
we do nothing.

*** Allow generating executables from dogen                           :story:

At present dogen always assumes we want to generate a static
library. It would be nice to be able to generate an executable too,
with all of the cmake infrastructure generated.

*** Consider generating program options code                          :story:

If there was a syntax to describe boost program options, we should be
able to generate most of the code for it:

- the code that initialises the options;
- the domain objects that will store the options;
- the copying of values from program options objects into domain
  objects.

This would mean that creating a command line tool would be a matter of
just supplying an options file. We could then have a stereotype for
this (name to be yet identified). Marking a type with this stereotype
and supplying the appropriate meta-data so one could locate the
options file would cause dogen to emit the program options binding
code.

A similar concept seems to exist for python: [[http://docopt.org/][docopt]]. We should keep
the same syntax. We just need to have a well defined domain object for
these. The aim would be to replace config.

For models such as these, the dia representation is just overhead. It
would be great if we could do it using just JSON.

Actually even better would be if we could have a text file in docopt
format and parse it and then use it to generate the code described
above.

*** Create a tool to generate product skeletons                       :story:

Now that dogen is evolving to a MDSD tool, it would be great to be
able to create a complete product skeleton from a tool. This would
entail:

- directory structure. We should document our standard product
  directory structure as part of this exercise.
- licence: user can choose one.
- copyright: input by user, used in CMakeFiles, etc. added to the
  licence.
- CI support: travis, appveyor
- EDE support:
- CMake support: top-level CMakefiles, CPack. versioning
  templates, valgrind, doxygen. For CTest we should also generate a
  "setup cron" and "setup windows scheduler" scripts. User can just
  run these from the build machine and it will start running CTest.
- conan support: perhaps with just boost for now
- agile with first sprint
- README with emblems.

Name for the tool: dart.

Tool should have different "template sets" so that we could have a
"standard dogen product" but users can come up with other project
structures.

Tool should add FindODB if user wants ODB support. Similar for EOS
when we support it again. We should probably have HTTP links to the
sources of these packages and download them on the fly.

Tool should also create git repo and do first commit (optional).

For extra bonus points, we should create a project in GitHub, Travis
and AppVeyor from dart.

We should also generate a RPM/Deb installation script for at least
boost, doxygen, build essentials, clang.

We should also consider a "refresh" or "force" statement, perhaps on a
file-by-file basis, which would allow one to regenerate all of these
files. This would be useful to pick-up changes in travis files, etc.

One problem with travis files is that each project has its own
dependencies. We should move these over to a shell script and call
these. The script is not generated or perhaps we just generate a
skeleton. This also highlights the issue that we have different kinds
of files:

- files that we generate and expect the user to modify;
- files that we generate but don't expect user modifications;
- files that the user generates.

We need a way to classify these.

Dart should use stitch templates to generate files.

We may need some options such as "generate boost test ctest
integration", etc.

Notes:

- [[https://github.com/elbeno/skeleton][Skeleton]]: project to generate c++ project skeletons.

*** Easy addition of facets and formatters                             :epic:

The ideal state of the world is one where:

- the facet directory contains a small JSON file with the fields
  specific to a facet, including defaults, etc.
- the facet directory is made up of a number of stitch templates and
  their expansion into c++ (e.g. no separation of template and
  formatter).
- the backend model has an entity marked as =Stitch= or =Stitchable=
  and linked to a stitch meta-template. Ideally one should be able to
  create a concept for it so that we can define these properties only
  once.
- the template should have all of the parameters required such as
  types of variables.

*** Overuse of =std::set=, =std::map=, etc                            :story:

According to this paper by Matt Austern we should see if we really
need sets/etc or if there are other alternatives:

[[http://lafstern.org/matt/col1.pdf][Why you shouldn't use set (and what you should use instead)]]

In cases where we know upfront the number of elements we require and
we already have them sorted, we may not need a set.

*** Remove unused features                                             :epic:

This story captures any features that we no longer require and will
remove at some point. We have already removed most of the unused
features, but the story keeps track of any remnants.

At the very start of dogen we added a number of features that we
thought were useful such as suppressing model directory, facet
directories etc. We should look at all the features and make a list of
all features that we are not currently making use of and create
stories to remove them.

We may have to split this story into several but we should at least
trim down the obvious ones:

- disable facet folders: no use case.
- delete extra files: we always do so why make it optional.
- force write: we never do.
- etc.

Basically any feature which we are not using at present and cannot
think of an obvious use case.

*** Thoughts around multi-kernel support                               :epic:

Dogen only has a single backend kernel: quilt. This story looks into
how the world looks like if we want to support multiple of these.

*More recent ideas on this space*

- it may be possible for simpler languages to have an outputting model
  that has only formatters and uses yarn directly. For example for C#
  and Java it is likely we could get away with a simple utility layer.
- this implies that we may either end up with outputting models that
  are nothing but just stitch templates.
- this being the case, we could perhaps have two kernels: the cpp
  kernel (quilt) which requires lots of hand-crafting to generate the
  output and this more generic kernel which is just formatters per
  language (pleat?). Quilt/cpp kernel could also be transformed into a
  C/cpp/glib kernel, where the services we have at present for C++ are
  shared between these different languages (they all need includes,
  etc). Other languages may also have similar requirements (D?).

*Previous notes, slightly bit-rotted*

At present we have hard-coded knit to support a single C++ model,
cpp. However, in reality the world looks more like this:

- there are "groups of models" that have models that target specific
  languages. We need to give a name to the "default" model group in
  dogen. We should choose something from the [[http://en.wikipedia.org/wiki/Glossary_of_sewing_terms][sewing terms]]; for now
  lets call it =quilt=. =quilt= contains a number of languages such as
  =cpp=. A user can only generate one model group at a time. Users can
  generate one or more languages within a group (depending on what the
  group supports).
- we should have a top-level folder to house all model groups:
  =backends=. The existing =backend= model becomes =backends::core=.
- there may be facilities that are language specific, shared by model
  groups. These can be housed in language specific folders:
  =backends::cpp= and so on. For instance, the language specific stuff
  now in =formatters= should move here.
- different groups may express yarn models differently; almost by
  definition, they will, or else there is little purpose in having
  multiple groups. For example, one can imagine a model group (say
  =pleat=) which expresses [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-language-agnostic-models-lam][LAM]] as a model that is identical on every
  supported language, ignoring native types; that is, expresses LAM as
  a LAM model. However, =quilt= would still express LAM as a mapping
  between generic LAM types and concrete native types (e.g. LAM
  dictionary is a C++ unordered map). A good candidate for =pleat=
  would be [[http://www.eclipse.org/modeling/emf/][eCore]].
- if one was to try to generate code that is identical to =protobuf=,
  the xsd tool, =odb=, etc one would generate model groups for these.
- we may need multiple "needles" for each model group. For example,
  the supporting libraries for =quilt= may be (and almost certainly,
  will be) totally different than that of those in =pleat=. And of
  course, needle would have different expressions in each programming
  language. So perhaps needle is more of a concept than a physical
  thing. We should rename it to something meaningful that represents
  "a library with supporting code for a given model group". However,
  it does make sense to have a top-level folder to house all of the
  supporting libraries, so maybe needle does exist physically as the
  namespace to house all of the different supporting libraries. For
  example: =dogen::needle::quilt=, etc.
- the different needle libraries should be pushed to the appropriate
  repositories (e.g. nuget for C# and maybe C++, biicode for C++,
  maven for Java, etc).
- in the model groups world, each model most likely will only support
  a single model group: for example either quilt or pleat, etc. This
  is because some types only make sense with a given model group (say
  for example a cross platform =String= type in pleat won't exist in
  quilt and so forth). This means one must filter the models one is
  loading depending on the model group. This applies to both internal
  and external models. Also a model group may support a different
  subset of programming languages compared to another model group.
- we need a better name than "model group". word-storming: dimension,
  universe, space, package, module, ensemble, generation unit,
  assembly.
- Another way to think about it is that model groups are really
  backends. Backends support one or more "languages" (we need a word
  to reflect variations such as XML). Only one backend can be enabled
  at one time. One or more languages can be enabled, depending on what
  the backend supports. The options that configure languages and
  backends are in the meta-data; it does not make sense to supply
  these in the command-line because the model is coupled with the
  backend to a large extent (for example, native types are only
  supported in the native backend and so on).
- model groups and type support: some types will only make sense with
  certain model groups. For example, if one were to create a "cross
  platform string type", say String, for =pleat= which would then be
  implemented in =needle::pleat= for all languages, it would not make
  sense to try to use this type from =quilt=. This means that we need
  some kind of way to associate types with a model group. In terms of
  code generation, the formatter "enabled/disabled" logic will kick
  in, and if the type has no formatters in a given backend, then it is
  effectively disabled. But one wonders if this is a sensible way to
  figure out what types are available to which model groups. Seems
  like one would have to spend a lot of time looking into the
  meta-data to determine whats available.
- we probably need to add the model group to the ownership hierarchy,
  but at present we cannot think of a use case for it; we never enable
  anything across languages in a model group. In the same vein, we
  would also need the language. Fields would then be
  =quilt.cpp.enabled= and so on.

Merged with other stories:

*Multi-purpose models per language*

#+begin_quote
This story is a very vague story that keeps track of ideas on making
dogen useful for code generators of other kinds.
#+end_quote

One of the stories in the backlog covers other targets of code generation:

[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-thrift-and-protocol-buffers][Add support for thrift and protocol buffers]]

Originally we thought about adding support for these within a model;
that is to say, one would have additional serialisation "kinds"
available with a given dogen model. However, there is another way to
look at this; one could make other kinds of code generators using the
dogen infrastructure.

That is, contrary to it's name, dogen isn't just for "domain model
generation". Nothing stops one from building a protocol buffers or
thrift compiler using dogen infrastructure that outputs *exactly* the
same code as the original tools. All that would be required to do so
is:

- create a front-end that reads in their specification;
- to ensure yarn is expressive enough to cover all of the aspects of
  the code that needs to be generated;
- to create the formatters.

In this view of the world, we have two options:

- create groups of facets within the =cpp= model; for example,
  the thrift group, the domain generation group etc. These are
  mutually incompatible sets of formatters and only one of them can be
  enabled for a given execution.
- create models at the same level of the =cpp= model. We could group
  them by language (e.g. the =cpp= namespace). However, this seems
  less practical because these models would probably have a lot in
  common. This is yet to be seen as we need to finish the large
  formatters refactor before we can answer this question.

Taking this to its logical consequence, even a tool as complex as ODB
could potentially be implementable in this way: one can conceive a
clang front-end that reads in source code and generates an yarn model;
this model then can be used to generate C++ code that is identical to
the code produced by ODB (again assuming that yarn is extended to be
expressive enough to represent all the constructs required by ODB).

This would be a compelling proposition if we had =stitch= because it
would make the generation of formatters quite trivial and would also
mean that people that want to create code generators don't have to
worry about a lot of the boilerplate code. However, the biggest
problem is that we'd be imposing a large and complex "framework" on
them with all the evilness that that entails.

Food for thought:

- in this light, a better name for dogen would be =codegen= (or =cogen=
  to make it a bit more unique in google). The tag line is then The
  Generic Code Generator. Unfortunately there are already a few
  projects with the name =cogen= so we may need to find a better
  name. Alternatively we can maintain the name dogen, but take away
  its meaning (i.e. no longer "The Domain Generator").
- the merge of =cpp= and =cpp_formatters= may not have been for the
  best in this case; it would make more sense to have a =cpp::dogen=
  where we collect all of the formatters related to domain
  generation - after the =cogen= rename; if no rename then we need
  some other name to imply domain generation. At this level we could
  then have =cogen::cpp::odb=, =cogen::cpp::protobuf= and so on. They
  all make use of the core types defined in =cogen::cpp=. The problem
  with this approach is that dogen is not really designed to share a
  namespace in this way. We won't be able to have a =cpp= project as
  well as placing other projects inside of the =cpp= namespace. We can
  have one or the other in the current setup, but not both. We could
  take the same approach as we did for test models: create a cpp
  folder and then put the model under a different name such as =model=
  or =domain= etc. Note that we still have to define all of the
  formatter interfaces in the "main" model, as well as workflows
  etc. However, some interfaces may not make sense for other models:
  what is a registrar in protocol buffers? If it exists at all, its
  probably something very different from boost serialisation and as
  such will require other data.
- note that this kind of grouping is not necessarily at the language
  level. For example, domain generation should be common to a set of
  languages, and so would protocol buffers. This means that rather
  than a facet or formatter grouping, we need a higher level construct
  to aggregate things; "domain generation" is made up of languages,
  languages are made of of facets, facets have formatters. We need a
  name/classification for "domain generation" in this context.

We should bear in mind [[http://st-www.cs.illinois.edu/users/droberts/evolve.html][this quote]]:

#+begin_quote
People develop abstractions by generalizing from concrete
examples. Every attempt to determine the correct abstractions on paper
without actually developing a running system is doomed to failure. No
one is that smart. A framework is a reusable design, so you develop it
by looking at the things it is supposed to be a design of. The more
examples you look at, the more general your framework will be.
#+end_quote

*** Consider adding a meta-meta-model                                 :story:

This story is just a very vague placeholder for ideas around the DSL
space.

We could create a meta-meta-model that would allow us to describe
meta-models in general. The concepts of the meta-meta-model would be
more or less those defined in here:

- [[http://www2.informatik.hu-berlin.de/sam/lehre/MDA-UML/UML-Infra-03-09-15.pdf][UML 2.0 Infrastructure Specification]]

With a meta-meta-model we could then allow users to describe their own
meta-models, which are DSLs. This could be done graphically (say using
Dia). The GME story explains what the end-game of such an approach
would be and its more or less realised by GME in its current form.

The meta-meta-model would also allow us to think about model-to-model
transformations using a language such as [[https://eclipse.org/atl/][ATL]]. This would then mean
that we could transform a meta-model created by the user into one of
our meta-models used for code generation.

A proposed name for the meta-meta-model is =seam= for no good reason;
it means:

A seam, in sewing, is the line where two pieces of fabric are held
together by thread.

Meta-meta-models are vaguely related to binding.

On further thought, we should probably make a clear separation of
responsibilities. A few notes on this:

- there are two separate, but interrelated problem domains:
  meta-modeling and modeling.
- UML is a tool for modeling. It sits in the M2 to M1 space. M2
  because its meta-model is extensible, M1 because we spend most of
  our time defining user models.
- Dogen also sits in the M2 to M1 space. M2 because it defines its own
  meta-model, not entirely MOF compatible. It also allows user
  extensions with share some very basic similarities with profiles. M1
  is the core of the work.
- modeling is a huge domain. Dogen only concerns itself with the code
  generation aspects of modeling; as such it provides a set of
  "adapters" that convert models from other technologies (one could
  say DSLs) into its meta-models. These adapters are
  hand-crafted/hard-coded and they will always remain that way.
- Dogen does not and will not concern itself with: a) UIs for modeling
  (we will always rely on the existing tools) b) automated model
  transformation via some transformation language (we will always
  hand-craft them) c) management of the model life-cycle (these will
  be left to the users).
- there is a need for a separate product that lives a layer up from
  Dogen: M3 to M2. This is equivalent to GME. The idea is that we need
  to define a meta-meta-model and a constraint language. We can then
  allow users to create their own meta-models.
- lets call the tool Memod (Meta-modeler).
- the job of Memod is to do meta-meta-modeling and then modeling
  according to the defined meta-model. One can imagine Dogen as a
  special case of this, where we hard-coded the meta-model. One could
  of course describe Dogen's meta-model in Memod. In fact one could
  describe all of Dogen's meta-models in Memod (including the ones we
  adapt). However this is of limited use because there already is a
  good language to perform modeling in (UML). One vaguely useful use
  case is to automate the model transformations, but the effort
  required in describing every single detail of the models and the
  mapping is equivalent (if not greater) than hand-crafting the
  model. However, it is useful to have a Memod description of just the
  Dogen main meta-model (yarn).
- for other domains Memod would be useful though. Examples: a)
  finance: we could model all structured products using a financial
  products meta-model. The rules that describe how each composite
  product work must be described via the constraint language (is it
  sufficient?). For example, one could state that a Risk Reversal is
  made up of two Vanilla options, and describe the constraints in
  terms of Expiry Date, Strike etc via the language. With this one can
  create a Finance DSL. The canonical user for this tool would be the
  structuring desk. b) Computational Neuroscience: NeuroML is a DSL
  that describes neurons, topologies, etc. One can imagine a UI,
  similar to NEURON, which allows one to describe specific neurons and
  neuronal networks.
- Once we have a Memod meta-model, we could define transformation
  rules into Dogen's meta-model. Taking finance as an example, if we
  had a) a Memod representation of the Dogen meta-model b) an instance
  of the Dogen meta-model with the domain specific concepts
  (e.g. Financial Products) c) a Memod meta-model for finance and d) a
  user defined instance of the Memod meta-model for finance,
  describing structured products we could then code-generate
  user-defined structured products. Having said that, this still seems
  like an extremely large amount of work for something that does not
  change that frequently and could be spec'd and passed over to
  developers rather than automated. Finally, it would be quite tricky
  to get it right to the point one could put the output of this
  process into production.

*** Investigate the Generic Modeling Environment                      :story:

[[http://www.isis.vanderbilt.edu/projects/gme/][GME]] - Generic Modeling Environment - is a complete environment for
meta-modeling. It seems that they have already dealt with a lot of the
problems we are now facing. However, note that this is not a modeling
environment - it is a meta-modeling environment.

#+begin_quote
The Generic Modeling Environment is a configurable toolkit for
creating domain-specific modeling and program synthesis
environments. The configuration is accomplished through metamodels
specifying the modeling paradigm (modeling language) of the
application domain. The modeling paradigm contains all the syntactic,
semantic, and presentation information regarding the domain; which
concepts will be used to construct models, what relationships may
exist among those concepts, how the concepts may be organized and
viewed by the modeler, and rules governing the construction of
models. The modeling paradigm defines the family of models that can be
created using the resultant modeling environment
#+end_quote

Source code is available [[http://repo.isis.vanderbilt.edu/GME/old/15.5.8/][here]].

*** Consider introducing formatter "location strings"                 :story:

In MDSD, we have the notion of "location strngs":

#+begin_quote
A third and very useful technique is the application of location
strings that identify the transformation or the template used, as well
as the underlying model elements in the gen- erated code. A location
string might look like this:

: [2003-10-04 17:05:36]
: GENERATED FROM TEMPLATE SomeTemplate
: MODEL ELEMENT aPackage::aClass::SomeOperation().
#+end_quote

This may be a useful thing. However, adding dates and dogen version
etc will cause spurious diffs.

*** Consider adding support for OCL                                   :story:

UML supports adding constraints via [[http://www.omg.org/spec/OCL/2.4/][OCL]], the Object Constraint
Language. We could generate code from it.

*** Code generation and system configuration                          :story:

This is a very sketchy story. We probably should look into any
relationships between system configuration and code generation. A
starting point on this are these papers:

- [[http://www.infosun.fim.uni-passau.de/publications/docs/PTD%2B14.pdf][Coevolution of Variability Models and Related Software Artifacts]]
- [[http://homepages.dcc.ufmg.br/~mtov/pub/2015_modularity.pdf][Feature Scattering in the Large: A Longitudinal Study of Linux
  Kernel Device Drivers]]

*** Support for transactional writes                                  :story:

It would be nice if dogen either generated all files or didn't touch
the directory at all, at least as an option. We could simply generate
into a temporary directory and then swap them at the end.

*** Supporting user defined generic types                             :story:

At present we have done a bit of a hack to support templates. However,
all that is required to allow users to create their own template types
is:

- parse dia information for type arguments;
- change object to have type arguments;
- change merger to allow variables of the type of the type argument;
- change view model to propagate type arguments;
- change formatter to create template class if type arguments are
  present.

However this would then mean that IO and serialisation would fail
since they are implemented on the cpp. As there is no need for
template types, this seems like an ok limitation.

*** Visitor adaptor for usage in ranges                               :story:

It would be great if we automatically generated an adaptor to visitors
which could be plugged into a range. Internally the adaptor would
perform the accept on its =operator()=. We could also have an adaptor
for a =std::pair= which would be templatised on the first member of
the pair. Or should one just use a keys or values range iterator.

*** Visitor with =std::function= for each =visit= method              :story:

#+begin_quote
*Story*: As a dogen user, I want an extensible visitor so that I don't
have to manually generate one.
#+end_quote

It would be nice if the code generator created a visitor which has as
its properties a set of =std::function= which match the signature of
the visit functions; then the visit functions would just check that
the functions have been assigned and call them. If not, throw.

*** Create a visitor interface with multiple implementations          :story:

#+begin_quote
*Story*: As a dogen user, I need multiple visitor interfaces so that I
can decide which one is best for my requirements.
#+end_quote

We decided to use a base class for visitor; it would have been better
to create an interface, with multiple implementations:

- "negative" visitor: any unimplemented methods throw;
- default visitor: all methods do nothing;
- [[*Visitor%20with%20%3Dstd::function%3D%20for%20each%20%3Dvisit%3D%20method][std::function visitor]]: usr supplies lambdas to visitor.

Users can then inherit from these visitors where appropriate
(e.g. negative and default visitors). Each of these would have their
own formatter.

Actually we don't necessarily need visitor interfaces, but we need
some knobs to control the kind of visitor we generate. We should also
have a way to control the number of visitable methods generated (const
visitor, mutable visitor etc).

*** Add targets to output manual in downloadable formats              :story:

#+begin_quote
*Story*: As a dogen user, I would like to read the manual offline and
have the ability to print them.
#+end_quote

We should build HTML and PDF representations of the manual. When we
start using worktrees we could also have a simple script that copies
across these into the website. This can be done manually for now at
the end of each script (running the script).

*** Investigate the possibility of creating a mock facet              :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate mocks for interfaces
so that I don't have to generate this code manually.
#+end_quote

This is straight out of left-field (pardon the pun), but may actually
be a good idea. One annoying thing with mocking frameworks such as
[[http://turtle.sourceforge.net/index.html][turtle]] is the amount of macros. However, =dogen= already has all the
required information needed to create an expectation based mock - the
meta-model. We could mimic the turtle API with a mock facet that is
made up of real C++ objects. When a class is marked as an interface,
we could automatically generate its mock in a mock facet and allow
users to supply lambdas for the expectations.

This will require proper operations support.

*** Serialisable and ioable exceptions                                 :epic:

#+begin_quote
*Story*: As a dogen user, I want to send exceptions across the wire so
that I can report errors to remote users. I also want to dump
exceptions to the log file.
#+end_quote

At present we only generate the types facet for exceptions. However,
there is nothing stopping us from adding serialisation support for
exceptions. This would be useful for example for services to convey
errors on the remote end point. The same logic applies to io.

This should be fairly straightforward since exceptions are simple
types. We haven't got a use case for it yet though.

*** Consider representing namespaces in file names rather than directories :story:

Languages like .Net represent namespacing using dots rather than
separate folders. Perhaps we should support a mode of operation where
all files are placed in a single folder but have the namespacing
encoded in the file name. For example:

: /a_project/types/a.cpp
: /a_project/io/a_io.cpp

would become:

: /a_project/types_a.cpp
: /a_project/io_a_io.cpp

or, using dot notation, so we can distinguish namespaces from
"composite" names:

: /a_project/types.a.cpp
: /a_project/io.a_io.cpp

We do not have a use case for this yet, but it should be fairly
straight forward to add it. We just need meta-data support to enable
the feature and then take it into account when generating the file
names (e.g. instead of using =/= as a separator, use =.=).

*** Allow manual overrides to facets                                  :story:

#+begin_quote
*Story*: As a dogen user, I sometimes want to provide my own
implementation for a given facet.
#+end_quote

Sometimes it may make sense to provide a user-supplied implementation
for a given facet. For example for qname it may make sense to use the
string converter approach to do the actual JSON serialisation. However
its not possible to just disable code generation of a given type's
inserters and then provide a manual implementation. This could easily
be achieved via dynamic extensions: we could provide a meta-data
parameter for each facet such as "generation type", manual or
automatic. This can be done once we get rid of the current use of
generation types for full/partial/no generation.

*** Investigate ribosome for ideas                                    :story:

[[http://ribosome.ch/][Ribosome]] seems like a templating tool with a simple syntax. May have
some ideas useful for stitch.

*** Generate skeleton for "non-generatable" files                      :epic:

We are using non-generatable files quite a lot (at present called
"services"). Every time we do this we end up copying manually the
contents of the forward declarations to setup the skeleton of the
file. Since we already have all of the boiler plate code such as
licence, header guards, etc, we could just create a skeleton to stop
us from having to copy and paste it.

In addition to the class definition, it should also define all of the
automatic constructors, and add a private section at the
bottom. Ideally we should also generate stubs for all methods - adding
a blank implementation with return types where required and commented
out parameters, to make the code compilable out of the box. This will
be possible once we start supporting operations.

*** Add merging code generation support                                :epic:

#+begin_quote
*Story*: As a dogen user, I want to manually change some code in
generated files so that I can add functionality that is missing in
dogen.
#+end_quote

At present it is not possible to manually add methods to a class that
was code generated; one must stop code generating the class and
maintain the whole class manually. This is made even more painful by
the fact that one cannot add support for IO etc for these types
manually.

However, in a lot of cases it makes sense to have a combination of
manually generated and code generated code:

- value objects need helper methods such as for example boolean
  properties (e.g. =is_empty=) that make use of other properties, or
  simple methods such as population etc that really belong in the
  object rather than an external service;
- services sometimes need state and it would be good if we could
  manage that via code generation.

For this we need a merging code generator: that is, a code generator
that is aware of code that was crafted manually and does not overwrite
it - but instead "intelligently" merges manual with code generated
code.

From the beginning we avoided this because we thought it would be too
complicated for dogen. However, its becoming apparent that this is a
needed feature for the real world - there are many cases where we are
working around this deficiency. A few solutions are possible:

- let the code generator manage the header file and create two types
  of CPP files, one which includes the other: a manual and an
  "automatic" one. This would effectively separate the two types of
  code. For this dogen would have to be able to generate complex types
  in operations (e.g. we'd have to solve the lack of support for
  =const std::string&=).
- use clang to do the merging. this probably means adding some kind of
  attribute to every method - possibly using C++ attribute support
  (e.g. =[ [generated ] ]= and/or =[ [ manual ] ]= (spaces due to org
  mode). We could then say to clang: read current state of the file,
  grab every non-generated method and copy them across to the newly
  code generated file. Merging could be the final stage before
  writing. In addition, we should also have some dynamic extensions to
  determine which files require merging. The dynamic extension could
  be populated automatically (e.g. grep for the manual attribute) or
  manually. Note that using clang to do merging will make things a lot
  slower so we probably want to know up front which files need to be
  merged to avoid doing spurious work.

Notes:

- include management would be a mix of manual versus automatically
  generated. This is not possible because there will be no way to
  determine which one is which. To solve this problem we need to allow
  users to add include files from the dynamic extensions and get those
  processed like all the other includes. In the new world this means
  adding includes to the formatter settings. These are local
  settings. As at present, we cannot identify a use case for adding an
  include file for all types, so there is no need to support this
  feature at the global settings level. Thus this fits nicely with the
  existing settings infrastructure.
- merging could be done without needing clang, which would also make
  it cross language. All that is required is for the language to
  support some kind of meta-data to mark a method as "manually
  generated". This could even be done using comments but this is not
  ideal. The process would then be: dogen would open up an existing
  source file and locate the attribute; then look for a open brackets
  to indicate the start of the method (={=) and then find the matching
  close brackets (=}=). We could keep a counter and increment it when
  a new open bracket is found and decrement it when a close bracket is
  found. When its zero we are done. All the code from the attribute to
  the close brackets would be lifted. A very simple regex matching
  would be done to find the method name - or perhaps some trivial
  parsing could be done, but it should be kept as simple as
  possible. The objective is simply to figure out the method name. The
  method is copied across and stored in the =cpp= model, in the
  correct method. When code generating, if a method is marked as
  "manually generated" and if there is implementation content, we dump
  that; otherwise we generate the skeleton of the method as if it was
  not "manually generated". We could also create a very simple spirit
  parser that only knows of comments, function names and function
  bodies.
- merging could be done as part of yarn, in meta-data. That is, we
  could annotate the merged method into language specific properties
  in the meta-data and then query those in the language specific model
  generation. We could have another yarn workflow to look for files;
  it could use the meta-data for file path. The extension will tell it
  what "function parser" to use. We could literally look through the
  meta-data extensions looking for file path, and for each run the
  "function parser"; it will return a set of "manual" functions. These
  we can then slot into the meta-data and reuse later on. Actually, we
  can't use meta-data for this given the existing convention that
  meta-data is constant. However, nothing stops us from adding the
  required properties to yarn directly (e.g. we could have an
  =operation= which has a language specific container of
  implementations), to be used by formatters. Interestingly, this then
  seems to share some logic with method helpers. That is, if we could
  supply the stitch template as an operation (per facet, per
  language) and if we could annotate the operation somehow as
  "external" to the object, helper methods could use the same
  infrastructure. But perhaps this doesn't make a lot of sense since
  for helper methods we need to run the formatter whereas for merging
  we already have the final form of the code and we just need to carry
  it along to dump it in the formatter.
- the proper technical name for methods that can be manually edited is
  "protected regions". Merging is not a technical term according to
  MDSD at least.

We should also address the MDSD comment:

#+begin_quote
If, for performance reasons, or because the target language doesn’t
offer any options for consolidating different artifacts, handwritten
code must be inserted into generated code directly, the introduction
of protected areas is inevitable. Please do this only if such
exceptional conditions require this approach!
#+end_quote

*** Add support for languages available in Dia2Code                    :epic:

Dia already has a default code generator: [[http://dia2code.sourceforge.net/][Dia2Code]]. At present it
supports the following outputs (as per [[http://dia2code.sourceforge.net/features.html][features]] page):

- Ada, C, C++, C#, CORBA IDL, Java, PHP, PHP5, Python, Ruby,
  shapefile, and SQL.

We could probably add support for these by creating a test Dia
diagram, running it through Dia2Code an then making sure we get a
binary identical output. This would be a good way to bootstrap
multi-language support.

We couldn't find simple test diagrams in Dia2Code so perhaps we could
also contribute these to the project. It does have a set of [[http://dia2code.sourceforge.net/examples.html][examples]],
which could be used as a starting point for the tests. They are a bit
complex though.

They also appear to have support for functions etc. We should see what
they've done as this would also be a good way to evolve Dogen.

*** Generate model from database tables                               :story:

Insane idea: one could point knit to a database from a supported
vendor and generate the C++ code needed to represent those
tables. This could be filtered by the user. Idea came from here:

[[https://msdn.microsoft.com/en-us/data/gg558520.aspx][T4 Templates and the Entity Framework]]

All we need is a "database frontend" responsible for connecting to the
database and performing the mapping between the database types and the
language specific types. The model would then have ODB support to read
and write these tables.

We should have a specific tool to do this. Ideally the tool itself
should use ODB or at least the ODB database specific libraries to
access the DML. We should also investigate existing ODB support for
this.

*** Generate model from CSV files                                     :story:

Following on from the idea of using database tables, another
interesting use case are CSV files. It is common that one has a number
of CSV files that one needs to process - for example, import to a
database to perform further comparisons. It would be great if one
could just point knitter at a set of CSV files with headers and have
it:

- create a class for each file type.
- given the data in the file, find the most appropriate type for the
  field. Note when there are more than one file of the same type, and
  instead of generating multiple classes, just use the data for type
  inference.
- generate ODB support if requested by the user. If more than one file
  have the same field with the same type, create foreign keys.

With [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-csv-serialisation][support for CSV serialisation]], this means one could build a CSV
importer and processor very quickly. With support for diffs, this
would mean one could create a CSV file differ easily.

We should probably add this to the same tool that supports JSON/XML
imports.

*** Generate model from JSON or XML instance files                    :story:

It would be great if one could point dogen at a given JSON or XML file
and have it infer the model. For XML, if the schema is available, one
should just use the XSD tool of course. This approach is just for
quick and dirty modeling where one needs to build a tool to process a
set of existing documents.

We would have to have the ability to use more than one "source
document" and to "merge" properties across these. For example, if 3
JSON documents are supplied, we should be able to create a type that
is the superset of these.

To be clear, this story is not about using a JSON or XML schema to
generate a yarn model, but to use instances of a given schema in JSON
or XML to infer a possible yarn model that could read that instance
document. This is useful for example to consume a web service from
C++.

We should have a separate tool to do this work.

*** Investigate GDB visualisers for generated models                  :epic:

#+begin_quote
*Story*: As a dogen user, I would like pretty-printing for my types in
GDB so that I debug more easily programs using dogen models.
#+end_quote

It would be great if the code generator created GDB visualisers for
the types in a generated models such that one could inspect values of
STL containers with types of that model.

- [[http://sourceware.org/gdb/onlinedocs/gdb/Pretty-Printing.html][Pretty printing]]
- [[https://github.com/ruediger/Boost-Pretty-Printer][Boost pretty printer]]
- [[https://groups.google.com/group/boost-list/browse_thread/thread/ff232ac626bf41cf/18fbf516ceb091da?pli%3D1][Example for multi-index]]

*** yarn models could have a model classification                       :epic:

Consider creating an enumeration for model classification (e.g. type
of the model):

- relational model
- core domain model
- generic sub-domain model
- segregated core model

This still requires a lot of analysis work. This is kind of a model
level stereotype which can be used by the code generator for example
to determine which models are compatible. It could also be used to
determine what facets can be enabled/disabled.

*Merged with modes of operation story:*

Create "modes" of operation: relational, object-oriented and
procedural. they limit the types available in yarn. relational only
allows primitives plus relational commands (FK and PK; FK is when
using a model type, PK is a marker on a property). procedural only
allows primitives plus model types. we will need pointer support for
this. object oriented is the current mode. the modes are validated in
the middle end.

*** Consider creating a UI for editing type libraries                 :story:

At present we have to edit the JSON files by hand; this is becoming
increasingly painful as we rely more and more on the meta-data. It
would be great to be able to edit these files in some sort of UI that
would make repetitive operations quicker. This story captures all of
the use cases for the UI.

- there are many cases of types that require an inclusion directive
  for the types facet but none for all other facets. It is really
  painful to set each of the other facets to =inclusion_required=
  false. However, perhaps a more sensible way to handle this is to
  default inclusion required to false on all cases other than those
  provided. Story will be raised for this.

*** Add memory measurements support                                   :story:

Firefox has an interesting approach to estimating memory usage:

[[http://njn.valgrind.org/measuring.html][Measuring data structures: Firefox (C++) vs. Servo (Rust)]]

The gist of it is to add code like this:

: struct CookieDomainTuple
: {
:   nsCookieKey key;
:   nsRefPtr<nsCookie> cookie;
:
:   size_t SizeOfExcludingThis(mozilla::MallocSizeOf aMallocSizeOf) const;
: };

Where:

: size_t
: CookieDomainTuple::SizeOfExcludingThis(MallocSizeOf aMallocSizeOf) const
: {
:   size_t amount = 0;
:   amount += key.SizeOfExcludingThis(aMallocSizeOf);
:   amount += cookie->SizeOfIncludingThis(aMallocSizeOf);
:   return amount;
: }

We could add something similar to Dogen. This should be an optional
aspect.

Once we got the size information, the next thing should be a way to
dump a size report and make it visualisable in a tool such as baobab
(or in a manner similar to baobab) so that users can, at any time, get
a break down of where the objects are. Of course this would imply we
need a top-level object that contains all objects.

*** Add support for Microsoft Bond                                     :epic:

More of a placeholder - will require further investigation. Seems like
Microsoft has their own Thrift/ProtoBuf-like implementation called
[[https://github.com/Microsoft/bond/][Bond]]. It supports [[https://microsoft.github.io/bond/manual/bond_cpp.html][C++]]. Once could conceive Dogen support for it.

*** Add cling support                                                 :story:

One thing that would be really cool is if one could use generated code
from [[https://github.com/vgvassilev/cling][cling]]. For each project we generate, we could also generate a
"setup cling script" as described [[https://github.com/vgvassilev/cling/issues/67#issuecomment-107004157][here]].

The script simply loads up the SO/static library and all of the
includes of the SO. It also loads up all of the scripts of its
dependencies.

*** Detect moved files                                                :story:

It would be nice if we could detect files that are non-generatable and
have been moved, so we could move them across. For this we need to
know:

- that the file is not generatable: e.g. service, etc.
- that the new file name is equal to the previous file name, just in a
  different directory.

We could then just replace the empty file with the contents of the
previous file. Of course, we would still need updating namespaces,
etc.

Another way of doing this is to have UUIDs associated with each
type. The UUID is preserved into the file (ideally into a language
attribute, queryable by clang but could also be a comment). Before we
write the file we check to see if a file already exists with the same
UUID. If it does, we simply rename it to the new name matching the
UUID.

We should also issue an error if a file hasn't got a matching UUID and
at least to start off with force users to manually update it. Ideally
we should be able to use clang to update the UUID but for this we need
merging code generation support.

*** Consider offering identifier suggestions for misspelling           :story:

We just bumped into an error where a type was not spelled correctly:

: Error: Object has property with undefined type: <cpp><expansion><inlcusion_directives_repository>

We meant to say =inclusion= rather than =inlcusion=. We should
investigate if the clang "misspell" matching is available to use from
the outside word; it would be useful in cases such as this. We should
be able to just push all of the available qnames in yarn and then,
given a name, see if it matches.

See Spell Checker section of [[http://blog.llvm.org/2010/04/amazing-feats-of-clang-error-recovery.html][this]] post.

Seems like the name of the algorithm used is [[http://en.wikipedia.org/wiki/Levenshtein_distance][Levenshtein
distance]]. Still haven't found clang's API for this but there are
several implementations available.

- https://github.com/cdmh/algorithms

We could compute the Levenshtein distance as part of resolution, in
cases where we fail to resolve - e.g. second pass after all has
failed.

*** Consider using a graph in yarn for indexing and other tasks        :story:

To keep things simple we created a number of specialised indexers,
each performing a complete loop, recursion, etc over the merged
model. A better way of doing things would be to do a DAG of the model
that includes both concepts and objects and then DFS it; at each
vertex we could plug in a set of indexers, each acting on the
vertex. We could also have dependencies between the indexers (for
example concept indexing must take place before property indexing and
so on). This could also be extended to the quilt models, provided we
could express dependencies. We just need a simple interface that any
indexer can implement. Of course we also have to worry about indexers
which need to see the intermediate results of other indexers.

In addition, for this to work properly we need to remove all of the
frontend workflow processing and make these work off of the merged
model. Property expansion for example could be done by splitting
modules by model. The good thing about this approach is that we could
setup the graph in such a way that any type that does not link back to
the target model can be dropped so we would do very little work for
these - other than the original front end loading. Its not easy to
avoid loading models which we will not use because we only know which
models we need after resolution, and that requires having the model
loaded.

We did something similar with the consumer interface, which was never
used. The graph could probably be reused as is. See:

: 5db6524 * sml: remove consumer workflow and associated classes

One possible approach is to use [[http://www.nuget.org/packages/RxCpp/][Rx]]. Each of the indexers is a stream
which does some processing. Streams are linked to each other based on
the indexer dependencies, such that we pass on the processed types
once we are finished with it. They are passed on up all the way to
quilt indexers. We need to come up with a streams architecture linking
all indexers. We then use the graph to determine the order in which
yarn type are being passed in to the stream pipeline.

More thoughts: what we really want is to have a "transfomers" pipeline
("indexers" seems too limited a name) that is designed to generate the
inputs for the formatters. This means that by the end of the pipeline
we end up with properties, settings and the yarn type. And of course
we could take this one step further and then say that the formatters
themselves are also in the pipeline and the ultimate result of the
pipeline is a file.

We should not tackle this task until we have support for a few
languages other than C++ because we may be hacking things for C++
which wont work for those languages. It will be much harder to change
the code once we have a graph + pipeline.

*** Consider c++ itself as a front-end                                 :epic:

One can imagine a clang-based front-end that reads C++ code suitably
annotated, perhaps with =Generatable= or some such attributes -
basically all attributes required to build a sensible yarn model. The
frontend will parse the code and generate yarn. We can then generate
serialisation, hashing etc for the hand-crafted code.

One very good use case for this is for legacy code bases. One could
benefit from adding serialisation and IO support to an existing legacy
code base. This raises the interesting problem that we would need to:

- disable the types facet;
- inject types created from the c++ code with the types facet
  properties populated (namely include and file path).
- to be totally non-intrusive: generate a shared library that provides
  dogen code and requires the presence of the user code library.
- we may also need to somehow add a whole load of supporting types in
  order to be able to resolve references. So there would be a "target
  model" that we need to somehow identify but there would also be the
  supporting libraries (all other types the model references that we
  do not wish to code generate). This will probably be tricky. If the
  user needs to add these types manually it will not be
  practical. However, it may not be trivial to distinguish between the
  two.

This feature requires a bit of thinking as the architecture does not
support any of these.

The c++ frontend would also open some interesting possibilities:

- *read c++ and generate JSON*. This is a great way to import types into
  dogen; given a set of libraries returns a basic JSON model for dogen
  for them, with as much filled in as possible such as include
  directories, etc. This would save us a lot of time instead of
  manually adding these. The story for this was: As a dogen user, I
  want to generate system models automatically so that I don't have to
  create them manually.
- *read c++ and generate dia*. This would allow creating diagrams of
  existing code bases. Not trivial because the layout of the diagram
  would be quite hard to get right. We should create a separate tool
  for this.
- *update existing diagrams*. See [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#investigate-support-for-automatic-diagram-updates][this story]].

*** Consider compiling knit output with clang                         :story:

It make sense to perform some kind of minimal sanity checks on the
code generated by =knit= to provide a heads up to users. A full
compilation may be beyond the scope (database compilation may not be
setup, etc) but it would be nice to pickup basic syntax errors.

*** Compile stitch template output with clang                         :story:

Once we have integrated clang for formatting stitch output, it would
be great to also integrate it for compilation. It would work as
follows:

- start creating compilation databases when we do a regular build with
  clang;
- within stitch, get clang to detect the compilation database "near"
  the template; presumably clang has some infrastructure to do so
  already - if not we could perform a file look up. Actually we should
  just have a stitch command line parameter for the compilation
  database as well as for the clang-format settings.
- create a mapping of template lines to output lines during output
  generation.
- run clang with the compilation database options against the template
  output. Use the mapping to provide a cross reference between the
  template output and the template. We could use a scheme similar to
  GCC (included from, etc).
- apparently its possible to compile from a memory buffer. See [[http://lists.cs.uiuc.edu/pipermail/cfe-dev/2015-July/044029.html][this]]
  thread.

*** Add support for type "labels" or "tags"                           :story:

It may be useful to "tag" a number of types with a "label", and to
allow users to access these at run time. This only makes sense in the
context of reflection. This story needs a bit more fleshing out. We
don't yet have a use case for this.

This would allow for example to list all objects with a given tag.

*** Splitting facets into their own projects                          :story:

It is not always desirable to generate a facet as part of the main
model. For example, say we want to support coherence. We don't want to
generate a single shared object with the model and also include
coherence in it because this means everyone will have to link against
the coherence libraries. Instead, one would like to create "project
facets"; that is, to be able to somehow create a top-level project
just for that facet (or perhaps group of facets) such that they would
end up in a different project and thus different shared object. For
example:

: project_a/cpp/types
: project_a/cpp/coherence

would become

: project_a/cpp/types
: project_a.coherence/cpp/coherence

Or some such approach. Now that we have support for dot model names,
this is easier to do. We should have a meta-data element that
determines if a given facet is "built-in" ("shared"?) or if it is
"external".

*** Support for platform specific code                                :story:

There are some features which may only make sense on a given platform,
or may have different expressions depending on a platform. For
example, [[https://msdn.microsoft.com/en-us/library/aa370448%2528v%3Dvs.85%2529.aspx][DLL Main]] is required on Windows but not on UNIX. These files
must be correctly handled by CMake such that they are excluded on UNIX
and added on Windows. Same with [[http://en.wikipedia.org/wiki/Precompiled_header][StdAfx.h]] and cpp, which will require
looking into pre-compiled headers support in CMake.

*** Add support for plugins                                           :story:

An interesting idea is to generate a model that contains formatters,
create a dynamic library and then have some kind of loading mechanism
in Dogen. The interesting thing is that with static factories, dogen
could make use of this without any code changes at all (e.g. loading
the library into the process is sufficient to trigger registration,
and then its up to the dynamic extensions to decide whether to use the
formatters or not). So a user could create a model with formatters,
add its own text templates, compile and link it and then add it to
Dogen and then make use of the new formatters. The usual constraints
apply such as ABI (ensure one is using the same compiler as used to
compile Dogen, flags, etc).

This may also make sense for the front-end, but less so. At present we
already have support for registering both frontends and backends so
this should work out of the box. We just need a way to load DLLs. For
this we can use  [[http://apolukhin.github.io/Boost.DLL/index.html][Boost.DLL]]. We can add an additional command line
argument so that the user can supply the plugins; before doing
anything we must load all DLLs.

An interesting logical conclusion for this would be that - if stitch
was able to generate all of the infrastructure required to create a
new formatter - we should have a tool that creates all of the
infrastructure to build the stitch template into a shared object. This
would mean generating CMakeLists with right targets, conan file to
pull in dogen dependencies, etc and possibly even having a target to
call dogen loading the SO. In terms of conan - since we need to have
the dogen package installed, we probably should just require users to
install the dev package too. Or perhaps dogen as a tool (e.g. the
EXEs) could also be provided via conan? If we go for packages, we
should generate an "install script" to install all the required
dependencies, including compiler and so on.

We can either extend dogen or create a new utility for this.

*** Improving test data generation via yarn "reflection"              :story:

A really random but perhaps implementable idea: to create a
description of test data as JSON objects. For example, one could
supply a "yarn instantiation description language" (YIDL?) for a user
model, such as:

: {
:    "__type__" : {
:        "model_name": "my_model",
:        "external_module_path": []
:        "module_path": [],
:        "simple_name": "my_type"
:    },
:    "property_0" : true
: }

Where =__type__= is the "meta-type" for the object we want to
instantiate, allowing us to locate it in the yarn model, and the
remaining properties are as per user yarn model. Once the type is
located, one could then iterate through the properties in yarn and
locate their instance values in the JSON.

With this JSON and an yarn instance of =my_model= we could generate
code that looks like so:

: my_model m;
: m.property_0(true);

This would allow users to provide JSON descriptions of the test data
factories. If we took this approach we should consider renaming test
data to something a bit better (sample data?).

In this light, the current test data is akin to a "generic
instantiation language" in that all strings are instantiated with the
same values (or algorithm of value generation), all ints etc.

This is almost like reflection, in that if one had a strongly typed
model for the instantiation description language, it would look like a
reflected yarn model. The problem, of course, is that we do not code
generate yarn; we code generate in language specific models such as
=cpp=, etc. We would also have to have some kind of reflection support
for these models, and a transformation layer between the yarn
reflection and the implementation model. This is what the formatter
would then use to output.

Actually, this is more like compile-time reflection. We do not need to
know at run time what the model looks like, we just need to code
generate instantiations of the objects at compile time. So in theory,
loading yarn into memory is sufficient for this. In addition, we could
simplify "YIDL" by using yarn qualified names. This can be done for
both types and properties:

: {
:    "type_id" : "<my_model><my_type>",
:    "<my_model><my_type><property_0>" : true
: }

This is sufficient to resolve this to a specific yarn type and its
property. The syntax would get a bit more complicated in the presence
of composition, arrays etc but its more or less usable. One could even
conceivably extend it to collections by supporting some additional
properties (collection size for example) and allowing users to assume
the existence of counters (special values for strings perhaps such as
=%count%=?).

*Notes*

- See also [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#make-test-data-generator-more-configurable][Make test data generator more configurable]]
- this idea stems from the need to generate [[http://www.neuron.yale.edu/neuron/static/docs/refman/hoc.html][Hoc]] files from a neuron
  model described yarn. For that particular use case we would have a
  language model (=hoc=) that uses the instantiation description
  language to generate hoc files. Actually this is really not an
  appropriate solution for this use case; it would be far too limited
  to deal with this problem in general. We need to find a use case for
  this other than just generalising test data.
- We need a better name than YIDL ("yarn instantiation description
  language"). Perhaps something that reflects the nature of this being
  a test data templating engine.

*** Consider using bounded integers                                   :story:

This library seems to improve static error checking with integers,
etc:

https://bitbucket.org/davidstone/bounded_integer

However, off the top of my head, there are no cases for bounded
integers in dogen yet.

*** Investigate ModelQ for ideas                                      :story:

We need to mine this project for potential ideas - see how their
approach compares to ours, see if we can learn any lessons from it:

*ModelQ*: ModelQ is a code generator for creating Golang codes/models
to access RDBMS database/tables (only MySQL supported for now).

ModelQ: https://github.com/mijia/modelq

*** Evaluate all of our data structures based on usage                 :epic:

This presentation is very interesting:

[[https://www.youtube.com/watch?v%3DrX0ItVEVjHc][CppCon 2014: Mike Acton "Data-Oriented Design and C++"]]

The presenter makes a lot of points that are directly applicable to
Dogen. The main one is that we need to look at all the data structures
to see how they are used, and to try to extract deeply nested if's
that in many cases can easily be extracted from the bottom and moved
to the top. There are many other excellent points, we probably need to
watch the presentation again and write each of them down.

The key point though is that the re-engineering exercise should only
be done after we finished all of the current refactorings - we must
make sure the code does all that it is intended to do first and then
tackle the Acton's suggestions. This is to ensure that we have
captured all the main use cases. Data analysis can be done after this.

*** Consider generating dependency injection code                      :epic:

If one could mark constructors as =injectable= in a diagram, we could
then generate something like a castle windsor container and do all of
the management of dependency injection from generated code. We also
have access to all interfaces and their implementations so a lot of
the clever logic done at run time by castle/guice etc could be done in
the generated code.

*** Consider adding XML schema as a front-end                          :epic:

Tools like =Xsd= and =ejc= use the XML schema to provide the input to
the code generator. There is a simple mapping between the XML schema
and the language. [[http://en.wikipedia.org/wiki/Java_Architecture_for_XML_Binding][JAXB]] is a good example of this. As we already have a
dependency on libxml, we can load XML schemas without any additional
dependencies.

Note though that the idea is not to generate another xsd tool since we
already have a good [[http://www.codesynthesis.com/products/xsd/][C++ xsd tool]]. The point is to figure out if a XML
schema based front-end to Dogen is more convenient than Dia XML or
JSON. This may be appealing for a certain class of users: those using
the XSD tool or JAXB to generate POJOs or their C# equivalent.

It may also be worth looking at the [[https://jaxb.java.net/2.2.4/docs/xjc.html][xjc]] command line tool interface to
pick up ideas for Dogen.

We should also support the annotations used by JAXB such as
inheritance, etc.

*** XML serialisation for interoperability                             :epic:

A very good point raised by Boris on his [[https://www.youtube.com/watch?v%3DAuamDUrG5ZU][XML talk]] is that XML is
useful as an interchange format, mainly for interoperability and
future-proofing. It would be nice if we could use dogen to generate a
"controlled" XML, using a well defined schema (also
code-generated). This would be a stable format that could allow
third-parties to hook into the serialisation - as opposed to boost
serialisation's XML.

*** Add support for CSV serialisation                                 :story:

It would be nice to be able to serialise some simpler types into CSV
files, and to be able to read them from CSV files. We should use a
third-party library to perform the serialisation. We should check that
the types only have simple types. We should also provide some dynamic
extensions for options such as "use quoted strings", etc.

*** Consider generation of validators                                  :epic:

It would be nice if we could constraint the domain of a type via some
kind of rules; for example, provide a regular expression or an EBNF
definition with a string that tells dogen how to validate it. We could
then construct simple validators. As usual these expressions can be
supplied via dynamic extensions.

This is in effect an attempt to add OCL (object constraint language)
support. This would be extremely difficult, but we capture the story
nonetheless.

*** Providing view model hints                                         :epic:

Once we start supporting view models, it would be nice to be able to
take an inheritance tree of objects and collapse it into a list view,
handling all of the use cases (reorder columns, remove columns, add
columns, updating rows, etc). All of this code can be inferred from
the type hierarchy.

*** Consider replacing out libxml bindings with RapidXML              :story:

We rolled our own libxml bindings for reading the dia XML. However, it
may make more sense to use [[https://github.com/dwd/rapidxml][RapidXML]] instead. It seems basic but our
needs are also very basic.

*** Add support for BSON serialisation                                :story:

It would be useful to support Mongo DB's BSON. There is a C++ stand
alone library for this:

https://github.com/jbenet/bson-cpp

For examples on how to use the C++ API see the tutorial:

https://github.com/mongodb/mongo-cxx-driver/wiki/Tutorial

*** Consider adding SWIG and Boost.Python support                      :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose models to other languages
so that I can make use of them from there.
#+end_quote

We could generate the code required to expose the C++ types into ruby
or python by creating a formatter for it. Boost.Python would be more
straightforward as it is plain C++ code; SWIG would require generating
an interface file (IDL-like) and as such is closer to [[*Add%20support%20for%20thrift%20and%20protocol%20buffers][this]] story.

See also this article: [[https://www.softwariness.com/articles/api-design-for-swig/][C++ API Design for SWIG]]

*** Consider adding support for Boost.Operators                       :story:

It may or may not make sense to add support for [[http://www.boost.org/doc/libs/1_58_0/libs/utility/operators.htm][Boost.Operators]]. There
is a bit of boilerplate which seems easy to code-generate; however,
the point of adding operators is surely that one needs some
hand-crafted functionality. At any rate, this story keeps track of the
thinking on this topic. Articles:

- [[http://arne-mertz.de/2015/01/operator-overloading-the-basics/][Operator Overloading: The Basics]]
- [[http://arne-mertz.de/2015/01/operator-overloading-common-practice/][Operator Overloading: Common Practice]]
- [[http://arne-mertz.de/2015/02/operator-overloading-introduction-to-boost-operators-part-1/][Operator Overloading – Introduction to Boost.Operators, Part 1]]
- [[http://arne-mertz.de/2015/02/operator-overloading-introduction-to-boost-operators-part-2/][Operator Overloading – Introduction to Boost.Operators, Part 2]]
- [[http://arne-mertz.de/2015/03/operator-overloading-introduction-to-boost-operators-part-3/][Operator Overloading – Introduction to Boost.Operators, Part 3]]

*** Add yuml markup language support                                  :story:

#+begin_quote
*Story*: As a dogen user, I want to generate diagrams using yuml so
that I don't have to install Dia.
#+end_quote

It should be fairly straightforward to add a yuml front end that reads
a file using their markup language and generates an yarn model from it.

*** Add support for thrift and protocol buffers                        :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose dogen models to other
languages so that I can make use of them on these languages.
#+end_quote

Amongst other things, these technologies provide cross-language
support, allowing one to create c++ services and consume them from say
ruby, python, etc. At their heart they are simplified versions of
CORBA/DCOM, with IDL equivalents, IDL compilers, specification for
wire formats, etc. As they all share a number of commonalities, we
shall refer to these technologies in general as Distributed Services
Technologies (DST). We could integrate DST's with Dogen in two
ways. First approach A:

- generate the IDL for a model; we have enough information to produce
  something that is very close to it's Dogen representation,
  translated to the type system of the IDL; e.g. map =std::string=,
  =std::vector=, etc to their types. This IDL is then compiled by the
  DST's IDL to C++ compiler. Note: we could use LAM for this, but the
  problem is if one starts with a C++ model, one would have to convert
  it into LAM just to be able to do the mappings. A solution for this
  problem would be to "reverse map" LAM from C++ and get to the
  generic type this way.
- possibly generate the transformation code that takes a C++ object
  generated by Dogen and converts it into the C++ object generated by
  the DST's C++ compiler and vice-versa. We probably have enough
  information to generate these transformers automatically, after some
  analysis of the code generated by the DST's C++ compiler.

In order for this to work we need to have the ability to understand
function signatures for services so that we can generate the correct
service IDL for the DST. In fact, we should be able to mark certain
services as DST-only so that we do not generate a Dogen representation
for them. The DST service then internally uses the transformer to take
the DST's domain types and convert them into Dogen domain types, and
then uses the Dogen object model to implement the guts of the
service. When shipping data out, the reverse process takes place.

Approach A works really well when a service has a very narrow
interface, and performs most of it's work internally without exposing
it via the interface. Once the service requires the input (and/or
output) of a large number of domain types, we hit a cost limitation;
we may end up defining as many types in Dogen as there are in the IDL,
thus resulting in a large amount of transformations between the two
object models.

In these cases one may be tempted in ignoring Dogen and implementing
the service directly in terms of the DST's object model. This is not
very convenient as the type system is not as expressive as regular
C++ - there are a number of conventions that must be adopted, and
limitations imposed too due to the expressiveness of the IDL. We'd
also loose all the services provided by Dogen, which was the main
reason why we created it in the first place.

Approach B is more difficult. We could look into the wire format of
each DST and implement it as serialisation mechanism. For this to
work, the DST must:

- provide some kind of raw interface that allows one to plug in types
  serialisation manually. Ideally we wouldn't have to do this for
  services, just for domain types, but it depends on the low-level
  facilities available. A cursory look at both thrift and protocol
  buffers does not reveal easy access to such an interface
- provide either a low-level wire format library (e.g. =std::string=
  to =string=, etc) or a well specified wire format that we could
  easily implement from scratch.

This approach is the cleaner technically, but its a lot of work, and
very hard to get right. We would have to have a lot of round-trip
tests. In addition, DST's such as thrift provide a wealth of wire
formats, so if there is no easy-access low-level wire format library,
it would be very difficult to get this right.

*** IOable services                                                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to output all the state of my
services without having to manually create code for it.
#+end_quote

Even though we do not code generate services, it would be nice if we
could still setup their IO infrastructure - something basic just
outputting the type and taking inheritance into account. We end up
doing a lot of this manually anyway.

Also, if a service has a bunch of attributes that are IOable, we
should set them up too.

*** Code generation as a service                                       :epic:

*Latest Understanding*

- create a website using Wt: single screen with a few radio buttons to
  upload files or edit/paste JSON input; command line options
  available as text boxes.
- create a service that receives requests for code generation and
  returns a tarball/zip with the generated code: =jersey=.
- consider creating a simple HTTP wrapper around =jersey= just to see
  how it works. =hem=.
- two modes of operation: 1) upload a set of files, marking one as the
  target 2) simple input box where user can type in JSON and code
  generate.
- tick boxes for main options.
- potentially display a sample of one of the types.
- =jersey= receives a code generation request with the files as part of
  the payload. This means we need to either save them down to a temp
  directory and then pass paths over to knitter or we should have
  support for different kinds of inputs such as streams in knitter.
- save all requests and responses to a Postgres database for analysis
  later (performance, bugs, etc). It would also be nice to have user
  comments/complaints, perhaps linked to a user voice-like interface.
- a more useful version of this service would allow: 1) ability add
  types to existing models such as std and boost, or even create new
  system models altogether. Some minimal UI for model editing would be
  required, including meta-data support (includes, etc). Ideally, each
  of these models should have an associated test model which would
  automatically be updated with the new types so users could get
  compilation feedback on their changes. 2) the ability to edit stitch
  templates on the site or even the ability to add new facets or new
  features for existing facets, and to code generate using those. Also
  the ability to add helper methods for a given facet to a given
  type. The testing for this could be achieved via a docker container
  with an incremental build and simple validation (time out, etc). If
  the user submits dodgy changes only the container would be
  affected. Once built, the container is used for code generation. It
  would be really nice to be able to test the template as one is
  editing it, perhaps by choosing a type in a model; the user could
  press "generate" at any time and see the file for that feature and
  facet. Behind the scenes this is stitching, building, code
  generating and then displaying the result for that one type. 3) the
  ability to submit PR from within the site. Users log in using their
  github accounts. They make their changes to a remote git repo. Using
  github magic - in a similar way to how gitter forks and creates
  PRs - when the user is happy enough with its changes, their are
  submitted as a PR.
- note that this advanced version would probably require having the
  test models in JSON so that we could edit them via the site; it
  would be cumbersome to add new types to a dia model
  programmatically.
- for extra bonus points, we should integrate with clang-format
  generation sites such as [[http://zed0.co.uk/clang-format-configurator/][this one]] and allow users to add their own
  styles dynamically.
- it should be possible to add more modelines/modeline groups,
  licences, and even fields. This only makes sense if the fields are
  totally dynamic such that the formatter could make use of the new
  field directly. For this we would have to supply the ownership
  hierarchy from within the stitch template.
- we should create interesting end points such as: dia to json
  conversion; merged model generation; merged model to code
  generation, with language options; formatting; etc. Each is a
  distinct end point.

*Previous Understanding*

One way of testing new functionality added to dogen is to try to
exercise it as part of the code generation itself. We have been doing
this with the bootstrapping, but there were limitations on
functionality such as ODB and EOS where we couldn't see any obvious
use for it in code generation. However, there is one way of exercising
this and a lot more of these sort of features: to create a Web-based code
generation service, along the lines of Web Sequence Diagrams or
YUML. We could create a simple bootstrap based website that forwards
requests to a set of end-points, all done within the dogen project.

We'd create a casablanca REST layer with a simple interface, with
functionality such:

- create workspace: returns a UUID and creates some kind of internal
  storage area.
- upload target: uploads a Dia or JSON model to be used as the code
  generation target.
- upload reference: uploads a Dia or JSON model to be used as a
  reference.
- set options: which facets to generate, which languages, etc.
- codegen: runs the code generation and returns a tarball with
  generated files and the log file; or returns a set of code
  generation errors.
- we could integrate with google drive to load the files from there.

As a further layer we could create an ASIO service that is queried by
the casablanca REST. This would exercise all of the messaging
infrastructure. Internally it would create the engine and run code
generation. It could also exercise ODB by writing session information
to a database and keeping track of the historical usage of the
service, log files etc.

This stack would allow us to continuously exercise pretty much every
feature we need out of dogen. As an added bonus, when we get to the UI
we could also exercise that (Wt, GTK).

Finally, this would also allow us to play with Docker, and place each
service in their own container, create load balancing etc.

Links:

- [[http://codeplanet.io/principles-good-restful-api-design/][REST API Design]]
- [[http://www.drdobbs.com/tools/json-and-the-microsoft-c-rest-sdk/240164821][Using Microsoft REST SDK]]

*** Automatic generation of C interfaces for C++ code                  :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of c++ models from C so
that I can create "bridge" APIs.
#+end_quote

Once we have proper C support, it should be doable to have a C++ facet
that automatically exposes a C interface.

*** Support for COM/CORBA                                              :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of COM/CORBA so that I
can create code to interface with legacy systems.
#+end_quote

We should investigate how hard it is to add support for these IDLs.

*** Add a utility that converts a dia model into JSON                  :epic:

#+begin_quote
*Story*: As a dogen user, I want to convert some Dia models into JSON
documents whenever I don't require UML and diagram formatting, so that
I don't have to generate the documents manually.
#+end_quote

It would be great if one could take a dia model and convert it into a
JSON representation. This would allow users to take models that are
not particularly useful in UML and convert them into JSON.

Name according to convention: tailor. General coversion tool.

*** Investigate support for automatic diagram updates                  :epic:

For classes that are manually generated, it would be really nice if we
could update the properties of the class in the diagram from the
source code. This would work as follows:

- user creates a class =x= and marks it as non-generatable; executes
  dogen.
- dogen creates the initial file and adds as much boilerplate as
  possible. For instance if the user manually added properties or
  operations to the class, dogen generates skeletons for these.
- once the file exists, dogen will no longer touch it (see also the
  merging code generation story, for a different take on this).
- the user runs a second tool (the diagram updator, in need of a name)
  which uses clang internally; it reads the diagram and looks for all
  of the non-generatable classes; for each of these, it updates the
  dia class with the properties found in the source file. Everything
  else is left untouched.

This feature would be extremely powerful when in presence of many
other features such as mocking, remote method invocation, etc - the
user would have no effort at all in generating the
code. Implementation-wise we'd have to:

- create an XML writer;
- add write support for the dia model and ensure we generate valid dia
  models;
- integrate clang libraries with dogen;
- create tool - or perhaps we should just have an "update diagram"
  mode in dogen?

*** Add POF serialisation support                                      :epic:

If coherence has open source C++ libraries, we should add support for
serialisation to and from POF.

Links:

- [[http://docs.oracle.com/cd/E24290_01/coh.371/e22839/cpp_api.htm][Understanding the Coherence for C++ API]]

*** Generate Visual Studio solutions                                   :epic:

#+begin_quote
*Story*: As a dogen user, I want to use visual studio solutions
directly so that I don't have to rely on CMake.
#+end_quote

At present we rely on CMake as the C++ meta-build system. There is
nothing stopping us from supporting more native build systems such as
Visual Studio. Consider adding direct support for Visual Studio.

*** Improve the integration of dogen with dia                          :epic:

It would be great if the model generation in dia was slightly more
interactive:

- dia could have a button to run/configure an external tool, where the
  setup for dogen would be kept.
- pushing an execute button would code generate.
- pushing a validate button would validate the current diagram, taking
  into account declared references. references to types that are not
  resolved could make the class or function go red.

The idea is to do the least intrusive changes in dia that would
provide us with this support. In order to access dogen, instead of
running the executable and parsing the command line output, it would
make more sense to create a C interface that supports these specific
use cases (and nothing else).

Dia already has a plugin interface, so we should bind to that rather
than require dia to compile against dogen.

*** Dia limitations that impact dogen usage                            :epic:

Collection of limitations we found in Dia that are annoying when using
it in anger with dogen:

- moving types in and out of packages does not work very well.
- comments for packages are missing.
- cannot wrap attributes; this is a problem when we have attributes
  with very long types.
- changing a diagram in the filesystem does not trigger any alerts
  etc - its very easy to loose changes because one updates the files
  from git but the diagram was opened in dia, and did not refresh.
- crossing lines (associations, etc) should "curve" up so that one can
  still follow the relationship.

It seems like dia is also using GitHub these days:

https://github.com/GNOME/dia

If we have a go at creating any patches for the above ideas we should
submit a PR.

*** Consider adding YQL support                                        :epic:

YQL offers a REST based API with lots of interesting information; an
example of the information provided is available [[https://github.com/yql/yql-tables/blob/master/yahoo/finance/yahoo.finance.quant.xml][here]]. There should be
somewhere a matching XML schema for each of these queries, at least
for the end points that return XML. It would be great if one could
take one of those schemas and generate an yarn representation for them.

More generally, it would be great if dogen was able to create a domain
model off of an XML schema. However, we already have the Code
Synthesis [[http://www.codesynthesis.com/products/xsd/][XSD tool]] for that, so maybe this is just scope creep.

*** Add support for GtkBuilder / Glade XML files                       :epic:

There is nothing stopping us from using a GtkBuilder / Glade XML file
to do the boiler plate setup of the UI. With a bit more work one could
potentially even generate the bindings for a presentation model.

*** Remote method invocation                                           :epic:

See [[*Type%20framing][type framing]], [[*Model%20and%20type%20enums][Model and type enums]].

It seems fairly straightforward to add remote method invocation to a
few select types. The following would have to be done:

- create a new stereotype like =dispatchable=, =remotable= or suchlike
- for languages which support this natively, we could map to their
  technology (e.g. Beans in Java, etc).
- create a new stereotype: interface.
- add support for interface code generation.
- validation: model must have a model ID, thought to be unique across
  models.
- validation: types must be marked as both =remotable= and
  =interface= and have a unique type ID in the model.
- validation: types must have at least one public method
- injector: if at least one type is =remotable=, a new facet is
  created: =rmi=.
- injector: a system enumeration will be created with all the
  supported serialisation types. actually, we should create this
  anyway in serialisation or reflection.
- rmi will contain one class that represents a "frame". this
  frame will be composed as follows: model ID, type ID, serialisation
  type, raw buffer. we need to look at RMI terminology to come up with
  a good name for this frame.
- messages: for each method that exists in each dispatchable service,
  a message class will be created with a name following some well
  defined convention such as =CLASS_NAME_METHOD_NAME=. we need
  examples to make up a sensible convention. or perhaps an
  implementation specific parameter can override the class name. the
  message class is a data object and has as attributes all of the
  parameters of the method.
- a dispatcher class will be created in dispatching. it will have as
  constructor arguments references to all the dispatchable
  services. when passed in a frame, it will hydrate it and dispatch it
  to the correct service.
- a "framer" class will be created in dispatching. it will be
  configured for a given serialisation type. it will take a message
  object, serialise it and frame it.
- we could support the notion of callbacks. for this we need to be
  able to serialise stubs as references such that when the other end
  receives it, it calls a registrar to activate a client stub.

Now we just need a way of creating some generic interfaces that take a
wire client and a wire service and plug the framer and the dispatcher
into it.

*** Generate state diagrams                                            :epic:

There is nothing stopping us from reading the UML State Chart objects
in Dia and generating an FSM off of it, using one (or both) of boost's
state machines. We could make the state machine contain inheritable
methods which could be overridden by the user manually.

These seem to be vaguely related to workflows. Some interesting posts
in dia mailinglist:

- https://mail.gnome.org/archives/dia-list/2015-June/msg00013.html
- https://mail.gnome.org/archives/dia-list/2015-June/msg00014.html

*** Add reflection support by using model and type enums               :epic:

#+begin_quote
*Story*: As a dogen user, I want to use reflection on generated models
so that I can do run-time introspection.
#+end_quote

It may be useful to create enumerations for models, types and
properties within objects. This would in the future form the basis of
reflection. One could use implementation specific properties to set
the model ID and objects IDs.

A more advanced version would allow you to invoke methods via
reflection. However, since some languages support this natively, and
since it may even be part of C++ in the future, we probably should not
spend a huge amount of effort on this.

Use cases:

- diff support

*** Add C++-03 mode                                                    :epic:

#+begin_quote
*Story*: As a dogen user, I want to create models in C++ 03 so that I
can interface with legacy code.
#+end_quote

It shouldn't be too hard to generate C++-03 code in addition to
C++-14. We could follow the gcc/odb convention and have a =-std=
option for this in meta-data. The only problem would be testing - at
present the language settings comes from cmake, and we'd have to make
sure the compiler is not in C++-14 mode when compiling test models
in 03. Also, the mixing and matching of 03 with 14 may not be
trivial. We should wait for a use case.

*** Add diff support                                                   :epic:

One very useful feature we need to implement is the ability to compare
objects (even composite objects such as an entire model) and to obtain
a list of differences. This has several use cases:

- when something breaks we want to know the before and after (or the
  actual and expected) but we also want to know why they are
  different, in particular for complex object graphs. This is the case
  with unit tests and with changes to code when we have A/B paths.
- in general, users of a system want to explain differences. For
  example, if we had a report that was generating one set of numbers S
  but is now generating another set of numbers S' we need to know
  what contributes to this change.
- for efficiency purposes, we may want to only send the differences
  between two very large objects over the wire rather than the
  entirety of the new version.

In theory nothing stops us from code generating all of the required
classes needed to diff objects in a model. There are two possible
approaches:

*Simple approach*:

Just create a new facet call diff and make these classes generate a
simple textual representation of differences, inspired in
=diff=. Where the object is an entity provide its ID. In general just
provide some "path" to the difference, e.g. model/object/member
variable/etc.

This is mainly useful for the first use case and it is simpler to
implement.

*Complex Approach*:

Models can have a =diff= option. When switched on, knit generates a
set of =diff= classes. These are system types like keys and live in a
sub-folder of =types=. They have full serialisation, hashing etc
support like any other model type. The generated classes are:

- =differ=: for each model type a differ gets generated. this is a
  top-level class that diffs two objects of the same type.
- =changeset=: for each model type a changeset gets generated. it has
  a variant called =changeset_types=, made up of all the types of all
  properties in the model. if a model property has a model type then
  it uses the changeset for that type rather than the type itself; for
  all other cases, including containers, it uses the type itself.

In addition, we need set of enumerations in =reflection=. To start off
with, all it contains is a list of classes in the model and a list of
fields in each class.

The =changeset= then has a container of =changeset_types= against a
reflection class and field.

Diff support is injected into the model just like keys. It also
requires that basic reflection support gets injected too.

With this approach we can satisfy all use cases.

*** Add support for object caches                                      :epic:

It would be good to have meta-model knowledge of "cacheability". This
is done by marking objects with a stereotype of =Cacheable=. It then
could translate to:

- adding a serialisation like interface with gets, puts, etc. We need
  to bind this to a specific cache such as memcache, coherence, etc.
- create a type to string which converts a key made up of primitives
  into a underscore delimited string, used as a key in the cache.
- we should also consider external libraries like [[https://github.com/cripplet/cachepp][cachepp]].

*** Add SQL support to Dogen                                           :epic:

It would be nice to generate all of the tables required to store a
model as well as the stored procs to read and write instances of the
model.

Notes:

- use an attribute with the type to determine if we want only the ID
  of the foreign key in C++ code or if we want a whole type.
- file names are: "qualified name-entity", e.g. model_type.
- drop/create statements: schema, table, load, save, erase, test data
  generators, test.
- ability to drop/create all: table, procs, etc.
- CMake deployment support: targets such as deploy_database,
  undeploy_database
- testing/CI: target that deploys all the SQL to a clean database,
  runs all SQL tests and un-deploys all the SQL.
- database connection settings: use pgpass.
- must cope with the test/development database separation. At present
  there is a massive hack required to populate both databases
  (changing makefile manually and then reverting the change).
- there should be a way of passing in the database name as an
  environment variable into the CMake
** Other Stories

Stories we just don't quite know where to bucket them.

*** Mine MongoDB Smasher for test data ideas                          :story:

There is a project to generate large data sets for MongoDB:

https://github.com/duckie/mongo_smasher

We should have a look at their feature set and see if any of the
features make sense for dogen.

*** Unmaintained emblem                                               :story:

If we ever reach a point where Dogen is complete according to the
vision, we should add this emblem:

http://unmaintained.tech/

*** Keep track of sewing terms allocation                              :epic:

This story just keeps track of how we are using the different sewing
terms in Dogen. We are only tracking terms which are not yet
incorporated into the product. It also keeps track of ideas that have
not yet allocated a term.

| Term   | Meaning | Dogen usage                                               |
|--------+---------+-----------------------------------------------------------|
| weave  |         | Reserved for AOP support?                                 |
| [[https://en.wikipedia.org/wiki/Glossary_of_sewing_terms#D][dart]]   |         | Will be used for the skeleton generator tool.             |
| [[https://en.wikipedia.org/wiki/Yoke_(clothing)][yoke]]   |         |                                                           |
| tailor |         | Format converter. e.g. Dia to JSON, etc.                  |
| jersey |         | Code generation service.                                  |
| hem    |         | HTTP Wrapper around jersey.                               |
| twine  |         | Tool to infer model from XML/JSON/CSV instance documents. |
|        |         | Tool to infer model from SQL database schemas.            |

*** Feature toggles and code generation                               :story:

This article by Fowler has some interesting ideas:

[[http://martinfowler.com/articles/feature-toggles.html][Feature Toggles]]

It is not clear how it applies to code generation, if at all. This
story is to bookmark the article so we can mine it later for ideas.
*** Write blog post on reflexive programming                          :story:

Basic idea is that domain expertise tightens the agile circle,
allowing for much faster progress. Check Soros definition of
reflexive.

*** Investigate the emblems used by Bit7z                             :story:

This project uses emblems for version, platform and compiler:

https://github.com/rikyoz/Bit7z

This may or may not be useful to dogen.

*** Mine =common-universal-cmake= for ideas                            :epic:

This project seems interesting:

- [[https://github.com/polysquare/common-universal-cmake][common-universal-cmake]]

They seem to have added support for conan, appveyor, etc in an
extensible way. We need to figure out if anything can be nicked for
our infrastructure.
*** Read A Concept Design for the STL                                 :story:

It seems this paper has a lot of useful information for meta-modeling:

[[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3351.pdf][A Concept Design for the STL]]
