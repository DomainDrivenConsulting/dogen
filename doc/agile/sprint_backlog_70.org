#+title: Sprint Backlog 70
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) spike(p) }

* Mission Statement

- Finish implementing all formatters using the new cpp architecture.
- Remove legacy cpp architecture.
- Remove unused features.
- Start SML refactor.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75
#+CAPTION: Clock summary at [2015-07-09 Thu 21:49]
| <75>                                                                        |         |       |      |
| Headline                                                                    | Time    |       |      |
|-----------------------------------------------------------------------------+---------+-------+------|
| *Total time*                                                                | *16:03* |       |      |
|-----------------------------------------------------------------------------+---------+-------+------|
| Stories                                                                     | 16:03   |       |      |
| Active                                                                      |         | 16:03 |      |
| STARTED Sprint and product backlog grooming                                 |         |       | 0:46 |
| COMPLETED Rename top-level registrars to avoid clashes with serialisation   |         |       | 0:10 |
| COMPLETED Implement serialisation registrar implementation formatter        |         |       | 4:20 |
| COMPLETED Add boilerplate for cmakelists formatters                         |         |       | 0:52 |
| COMPLETED Implement include cmakelists formatter                            |         |       | 0:54 |
| COMPLETED Implement source cmakelists formatter                             |         |       | 0:46 |
| COMPLETED Implement includer formatter                                      |         |       | 3:35 |
| COMPLETED Implement serialisation formatter for enumerations                |         |       | 0:47 |
| COMPLETED Implement odb pragma formatter for enumerations                   |         |       | 0:28 |
| COMPLETED Generate empty files for non-generatable types                    |         |       | 0:56 |
| COMPLETED Switch off overrides for legacy formatters                        |         |       | 0:45 |
| COMPLETED Switch off SML to CPP transformation                              |         |       | 0:07 |
| STARTED Remove knit backends and use backend model instead                  |         |       | 1:37 |
#+end:

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2015-07-09 Thu 21:21]--[2015-07-09 Thu 21:49] =>  0:28
    CLOCK: [2015-07-07 Tue 23:15]--[2015-07-07 Tue 23:21] =>  0:06
    CLOCK: [2015-07-06 Mon 15:01]--[2015-07-06 Mon 15:13] =>  0:12


Updates to sprint and product backlog.

*** COMPLETED Rename top-level registrars to avoid clashes with serialisation :story:
    CLOSED: [2015-07-07 Tue 16:11]
    CLOCK: [2015-07-07 Tue 16:01]--[2015-07-07 Tue 16:11] =>  0:10

Due to the hacks we put in that generate serialisation registrar's
qnames, we now are no longer able to have domain types called
=registrar= in the top-level namespace. For now we can just rename
these and fix it properly later.

*** COMPLETED Implement serialisation registrar implementation formatter :story:
    CLOSED: [2015-07-07 Tue 16:13]
    CLOCK: [2015-07-07 Tue 15:51]--[2015-07-07 Tue 16:00] =>  0:09
    CLOCK: [2015-07-07 Tue 13:31]--[2015-07-07 Tue 15:00] =>  1:29
    CLOCK: [2015-07-06 Mon 17:30]--[2015-07-06 Mon 17:57] =>  0:27
    CLOCK: [2015-07-06 Mon 15:14]--[2015-07-06 Mon 17:29] =>  2:15

Create the formatter, stitch template and all associated
machinery. Fix all differences.

Problems:

- references are not handled correctly.

Problems solved:

- includes for leaves are missing. We need to somehow get the include
  generation to work for registrar and also the aspect settings
  generation. Lots of hackery required to achieved this.
- arguments passed in to eos serialisation were hard coded in legacy.
- no newline at the end of the file.
- missing includes. Factory not populating includes either.
- eos check is not yielding expected result.

*** COMPLETED Add boilerplate for cmakelists formatters               :story:
    CLOSED: [2015-07-07 Tue 17:04]
    CLOCK: [2015-07-07 Tue 16:12]--[2015-07-07 Tue 17:04] =>  0:52

Add skeletons for interfaces, stitch templates, etc and hook it all
together with the initialiser.

*** COMPLETED Implement include cmakelists formatter                  :story:
    CLOSED: [2015-07-07 Tue 17:55]
    CLOCK: [2015-07-07 Tue 17:05]--[2015-07-07 Tue 17:59] =>  0:54

Create the formatter, stitch template and all associated
machinery. Fix all differences.

*** COMPLETED Implement source cmakelists formatter                   :story:
    CLOSED: [2015-07-07 Tue 22:26]
    CLOCK: [2015-07-07 Tue 21:40]--[2015-07-07 Tue 22:26] =>  0:46

Create the formatter, stitch template and all associated
machinery. Fix all differences.

*** COMPLETED Implement includer formatter                            :story:
    CLOSED: [2015-07-08 Wed 16:08]
    CLOCK: [2015-07-08 Wed 16:09]--[2015-07-08 Wed 16:31] =>  0:22
    CLOCK: [2015-07-08 Wed 14:29]--[2015-07-08 Wed 16:08] =>  1:39
    CLOCK: [2015-07-08 Wed 14:03]--[2015-07-08 Wed 14:28] =>  0:25
    CLOCK: [2015-07-08 Wed 13:39]--[2015-07-08 Wed 14:02] =>  0:23
    CLOCK: [2015-07-07 Tue 22:28]--[2015-07-07 Tue 23:14] =>  0:46

Create the formatter, stitch template and all associated
machinery. Fix all differences.

Problems:

- no includes.
- need to add guard to legacy.

Problems Solved:

- no general settings.
- missing machinery to hookup includers.
- generating includers for disabled facets.

*** COMPLETED Implement serialisation formatter for enumerations      :story:
    CLOSED: [2015-07-08 Wed 17:48]
    CLOCK: [2015-07-08 Wed 17:34]--[2015-07-08 Wed 17:48] =>  0:14
    CLOCK: [2015-07-08 Wed 16:57]--[2015-07-08 Wed 17:30] =>  0:33

Create the formatter, stitch template and all associated
machinery. Fix all differences.

*** COMPLETED Implement odb pragma formatter for enumerations         :story:
    CLOSED: [2015-07-08 Wed 20:45]
    CLOCK: [2015-07-08 Wed 20:33]--[2015-07-08 Wed 20:45] =>  0:12
    CLOCK: [2015-07-08 Wed 17:49]--[2015-07-08 Wed 18:05] =>  0:16

Create the formatter, stitch template and all associated
machinery. Fix all differences.

*** COMPLETED Generate empty files for non-generatable types          :story:
    CLOSED: [2015-07-08 Wed 21:43]
    CLOCK: [2015-07-08 Wed 21:40]--[2015-07-08 Wed 21:43] =>  0:03
    CLOCK: [2015-07-08 Wed 20:46]--[2015-07-08 Wed 21:39] =>  0:53

At present we are ignoring all types that are non-generatable or
partially generatable. We are also not generating forward
declarations.

*** COMPLETED Switch off overrides for legacy formatters              :story:
    CLOSED: [2015-07-08 Wed 22:06]
    CLOCK: [2015-07-08 Wed 21:44]--[2015-07-08 Wed 22:05] =>  0:21
    CLOCK: [2015-07-08 Wed 16:32]--[2015-07-08 Wed 16:56] =>  0:24

We need to switch off the legacy formatters and deal with the fallout.

Problems:

- generating empty file names.
- not generating empty files for non-generatable types.
- not generating odb pragmas or serialisation for enumerations.

*** COMPLETED Switch off SML to CPP transformation                    :story:
    CLOSED: [2015-07-08 Wed 22:14]
    CLOCK: [2015-07-08 Wed 22:07]--[2015-07-08 Wed 22:14] =>  0:07

Switch the legacy transformation and formatting.

*** COMPLETED Remove knit backends and use backend model instead      :story:
    CLOSED: [2015-07-09 Thu 21:51]
    CLOCK: [2015-07-09 Thu 21:50]--[2015-07-09 Thu 21:55] =>  0:05
    CLOCK: [2015-07-09 Thu 20:56]--[2015-07-09 Thu 21:15] =>  0:09
    CLOCK: [2015-07-09 Thu 07:36]--[2015-07-09 Thu 08:22] =>  0:46
    CLOCK: [2015-07-09 Thu 07:22]--[2015-07-09 Thu 07:33] =>  0:11
    CLOCK: [2015-07-08 Wed 22:32]--[2015-07-08 Wed 22:53] =>  0:21

We need to stop using the knit version of the backends. This will
probably require fixing the managed directories problem.

*** Handling of managed directories is incorrect                      :story:

At present we are querying the dia to sml transformer to figure out
what the managed directories are. These are basically the top-level
directories from where we want the housekeeper to operate. In reality
this is (or can be placed) in the meta-data. We should be able to
extract the managed directories from the meta-data as a step in one of
the workflows.

This can be done by the backend. It does mean that we should be
returning a composite type from generation:

- list of files;
- list of managed directories.

Alternatively we could have a =managed_directories= method that takes
in an SML model and then internally reads in the meta-data for a given
model to produce the list.

*Merged with previous story*

Compute managed directories from knitting options

At present the backend is returning empty managed directories. This
means housekeeping will fail in the new world. We need to change the
interface of this method to take in the knitting options and return
the managed directories.

This is not entirely trivial. At present the managed directories are
computed in the locator. It takes into account split project, etc to
come up with all the directories used by the backend. We need to make
these decisions during path expansion, expect we only need manged
directories for the root object. However we do not know which object
is the root object at present, during the expansion. We could identify
it via the QName and the SML model in context thought. We could then
populate the managed directories as a text collection. We then need
some settings and a factory to pull out the managed directories from
the root object. This could be done in =managed_directories=, by
having an SML model as input.

*** Consider creating a "locator" like class for path management      :story:

At present we are using path settings to compute paths in several
places. Most of these exist because of hacks but it still seems that
it needs to be done in more than one place. We should consider
something like =sml_to_cpp::locator= that is initialised with the path
settings and can then be used to create paths.

*** Thoughts on cpp refactoring                                       :story:

We haven't quite arrived at the ideal configuration for the cpp
model. We are close, but not there yet. The problem we have at the
moment is that the formatters drive a lot of the work in
formattables, resulting in a circular dependency. This is happening
because we are missing some entities. This story is just a random set
of thoughts in this space, trying to clear up the terminology across
the board.

*Random thoughts*

What is probably needed is to have facets, aspects and "file kinds" as
top-level concepts rather than just strings with which we label
formatters. In addition, we need a good name for "file kinds". This is
a meta-concept, something akin to a file template. The formatter
produces a physical representation of that meta-concept. As part of
the formatter registration, we can also register this meta-concept
(provided it relies on an existing formattable). And in effect, these
are the pieces of the puzzle:

- you define a "file kind".
- a facet and a model are groupings of "file kinds". These happen to
  be hierarchical groupings. There are others: header and
  implementation, or class header formatter. Those are
  non-hierarchical.
- you bind a transformer to a SML type to generate a formattable.
- a formattable is associated with one or more "file kinds" or better
  yet a file kind is associated with a formattable. It is also
  associated with formatting properties and settings. It is those
  tuples that we pass to the formatters.
- you bind a formatter to a "file" and process the associated
  formattable.

Perhaps we can call these "file kinds" file archetypes or just
archetypes.

What can be said about an archetype:

- conceptual notion of something we want to generate.
- one SML entity can map to zero or many archetypes. Concept at
  present maps to zero. Object maps to many.
- a representation of the archetype as source code is done by the
  formatter. It uses a template to help it generate that
  representation.
- a given archetype maps to one and only one SML entity.
- a given archetype maps to one and only one CPP entity.
- archetypes can be grouped in many ways. One way is facets and
  models.
- archetypes have definitions: name of the archetype, what groups it
  belongs to.
- archetypes have associated data: formattables, settings,
  properties. This is an entity and needs a name.
- formatters work on one and only one archetype.
- archetypes have qualified names; this is (mostly) what we called
  ownership hierarchy. Qualified names can be represented as separate
  fields or using the dot notation.
- archetypes have labels: this is what we called groups.
- dynamic is a model designed to augment SML with some archetype
  data. This is not true in the dia case. Check all fields to see if
  it is true everywhere else.
- an aspect is a property of one or more archetypes; it is a knob that
  affects the generation of the source code representation.
- an archetype instance belongs to an archetype.
- we should remove the concept of "integrated facets". It just happens
  that a facet such as types may have aspects that enable features
  similar to aspects in other facets. There may be rules that
  determine that when certain aspects are enabled, certain facets must
  be switched off because they are incompatible.
- facet is a good name for grouping archetypes, but model isn't. We
  need a better name for a set of facets. Aspect is also a good
  name. In addition, a model group is also a bad name. A "model" is a
  cohesive group of archetypes that are meant to be used together. A
  "model group" is a cohesive group of models that provide the same
  conceptual representations in different programming languages. Maybe
  we should use a more "random" name such as: pod. Then perhaps a
  model group could become a "pod family": a family of related pods. A
  given model can be represented by one pod family or another - they
  are mutually exclusive. Of course, from a command line perspective,
  its better to think of "modes". Each mode corresponds to choosing
  one "pod family" over another. This does not map very cleanly.
- archetypes have an associated programming language - a grammar.
- a facet may exist in more than one programming language and an
  aspect too.
- pods are programming language specific.
- formattables are kind of like an archetype friendly representation
  of the domain types. We need a good name for this.
- internal and external now make slightly more sense, at least once we
  got a good name for formatters. We still need a good name for it
  though. If the archetype instance is generated because of the
  presence of the domain type, it is external. If the archetype has no
  sensitivity to domain types (but may have sensitivity to other
  things such as options) it is internal. The naming around this is
  not totally clear.
- internal formatters may not be allowed to be disabled. For example,
  if serialisation is on, registrar must be generated. With
  CMakeLists, we may want do disable them altogether.
- in the thrift story in the backlog we mention the existence of
  mutually exclusive groups of facets. We should also come up with a
  name for these.
- archetype may not quite be the right name. See [[http://www.pearsonhighered.com/samplechapter/032111230X.pdf][Archetypes and
  archetype patterns]]. See also:
  - [[http://www.step-10.com/SoftwareDesign/ModellingInColour/ColourCoding.html][Class Archetypes, UML and Colour]]
  - [[http://www.step-10.com/SoftwareDesign/ModellingInColour/index.html][Peter Coad's 'Modeling in Color']]
  - [[http://www.step-10.com/Books/JMCUBook.html][Java Modeling in Color with UML]]
- the process of mapping domain types to archetypes could be called
  "expansion" because its a one to many relationship in most cases.
- its not quite correct to call CPP types "formattables". The
  archetype has to have an ordered container of inputs to the
  formatter. This is sort of the "payload" for formatting; the
  archetype is a container of such entities. Taking into account the
  cases where more than one type is placed in the same file, this
  would result in the includes being merged. Or perhaps these things
  are really formattables, but then we need a way to distinguish
  between "top-level formatters" that generate archetypes from
  "partial" formatters that can be combined.
- with "facet specific types" we go one level deeper: it should be
  possible to add an enumeration definition to say test data. This
  would mean that archetypes and facets are not quite so aligned as we
  first thought. Potentially, one should be able to ask for say a
  formattable at facet X in an artchetype at facet Y.
- One way to look at it is as follows: there is the modeling
  dimension, in which we have an entity, say entity =A=; and there is
  the implementation dimension, in which =a= can be represented by
  =A1, A2, ..., An= archetypes. In effect, the implementation
  dimension has multiple dimensions, one for each pod (and of course
  the pod families would be an extra dimension and so on). Actually,
  we probably have 3 steps: the modeling dimension, the translation of
  that into a language-specific representation and then finally the
  archetype dimension.
- a good name for the top-level container of archetypes is
  "kernel". This was inspired (loosely) in some ideas from EMF. So
  we'd have say the "quilt kernel", with support for multiple
  programming languages such as cpp, java etc. We we'd have the "pleat
  kernel" and so forth. Each kernel has a set of languages and the
  languages have archetypes. Archetypes have a collection of
  properties such as the formattables they need, the formatters and so
  on. The job of a model such as =quilt::cpp= is to implement this
  binding.
- dynamic fields can be owned by archetypes or by other types of
  owners (e.g. dia). We should have a way of expressing this
  ownership.
- we haven't used the word "feature" anywhere yet (properly; we
  mentioned it in the manual and so on, but not given it any good
  meaning).
- we created a split between "internal" and "external" formatters, but
  its interesting to notice that we have "internal" formatters that
  are "regular" formatters - in that we need to create a qname for
  them and the formatter properties will work correctly; whereas some
  others are "irregular" formatters - they have strange filenames that
  cannot be generated without some fiddling. Actually, ODB options is
  the main problematic one. If we could place it in a sensible
  location we could probably get rid of irregular formatters
  altogether.
- we need to have "special" facets; cmake files for example should not
  really have a facet but it seems having an empty facet name breaks a
  lot of stuff.
- we need a map between types/states in SML and enablement. For
  example, if a type is "non-generatable" that is taken to mean
  "generate types if file does not exist, default all else to
  disabled". We need a way to express this sort of logic. This is akin
  to an "enablement map". For example, users could define these maps
  somewhere, given them a name and then assign a type to a map. In
  addition, we need a way to express "generate but don't override" and
  "generate and override".

*** Thoughts on simplifying the formattables generation               :story:

We have a problem in the way which we are doing the formattables:
because we are doing model traversals for each of the factories, we
cannot easily introduce a set of manually generated qnames such as the
registrar and includers. However, if we started off the main workflow
by creating a structure like so:

- qname
- optional entity (new base class in SML); if null we need to create
  extensions as an empty object.

We then need a list of these that get passed in to all repository
factories. These use a visitor of entity to resolve to a type (where
required).

We can inject types to this list that have a qname but no entity. For
these we generate some parts of the formatter properties. Actually, we
still need to generate inclusion lists even when there is no
entity. Perhaps we need to create a new method in the provider that
does not take an SML entity but still generates the inclusion list.

Actually this should all be done in SML. We should have zero qname
look-ups coming out of SML, just follow references. This story is a
variation of the split between "partial" models and "full" models.

Well not everything should be done in SML. We still need to create a
structure with the properties above, but that is done by iterating
through a list in the SML model.

This work is dependent on [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#split-a-fully-formed-model-from-partial-models][this]] story.

One slight problem with this approach: sometimes we need to preserve
some relationships in the newly generated objects. For registrar we
need to preserve the model leaves. For the includers / master headers
we need to express somehow the inclusion relationship at the formatter
level. The latter is definitely a special case because it is a pure
C++ concept: include files cannot be modeled in SML. However,
registrar is slightly different because we still need to compute the
includes based on the leaves. This means that the above approach will
not provide a clean solution, unless we synthesise an SML object when
providing the includes. And of course we need to be careful taking
that route or else we will end up generating the object across all
facets.

It is important not to confuse formatters with archetypes. A formatter
(or at least, a "top-level formatter"; those that generate files) is
in a sense a "category" of archetypes. In other words, for a given
formatter many archetypes will be generated. This may mean that the
"archetype" is not a very good choice because it may imply some kind
of meta-class-ness. In a sense, we are dealing with arch-entities
("entity" being SML's base class for all modeled domain types). So
fundamentally, the correct workflow is vaguely like this:

- we create a model for some problem domain. We represent this model
  in SML. All objects are identifiable by a qname.
- we apply a transformation of this model into something which is
  closer to the programming language that we wish to generate; these
  we choose to call formattables.
- we may also inject some formattables which do not have a mapping to
  the original domain objects. These have synthetic qnames.
- we apply a function that takes the qname, the SML entity, the
  formattable and generates an archetype skeleton. To start off with,
  this is made up of only a file name and a top-level formatter. The
  structure exists in memory as a map of qnames to formatter names to
  archetypes.
- we then fill in the blanks: compute includes, enablement, etc. The
  final blank that needs to be filled in is the generation of the
  file, which is done by applying a formatter to a number of the
  archetype properties.

Another point of interest is that we may be able to move some of the
archetype processing to common code. For example, file name
generation, enablement, and so on are not language specific. However,
we need to have a representation of the archetype which is specific to
a model (e.g. =quilt::cpp= say) because not all properties will be
common. We could, possibly, have an archetype base class, which then
would imply a formatter's base class and so on - but then we hit the
visitor across models problem.

In this approach we do have an advantage which is we can parallelise a
lot of work across each stage in the "pipeline". For instance we can
run transformation from SML to formattables in parallel. We could
conceivably even have futures for each of the archetype
properties. None of this is a concern for the foreseable future, of
course.

FIXME: improve references by having models inside of models; we should
be able to keep only the types that we refer in the final model.

*** Add include providers for all types                               :story:

We need to implement the provider container support for primitives,
modules and concepts.

Update:

- inclusion dependencies factory
- provider container

*** Implement all formatter interfaces                                :story:

We still have a couple of skeleton interfaces:

- primitve
- concepts

*** Remove unused features                                             :epic:

At the very start of dogen we added a number of features that we
thought were useful such as suppressing model directory, facet
directories etc. We should look at all the features and make a list of
all features that we are not currently making use of and create
stories to remove them.

We may have to split this story into several but we should at least
trim down the obvious ones:

- empty model name
- split project
- output to stdout
- remove repository, factory, stereotypes
- keys, entities
- removing cpp backend?
- etc.

Basically any feature which we are not using at present and cannot
think of an obvious use case.

*** Model groups and multi-language support                           :story:

At present we have hard-coded knit to support a single C++ model,
cpp. However, in reality the world looks more like this:

- there are "groups of models" that have models that target specific
  languages. We need to give a name to the "default" model group in
  dogen. We should choose something from the [[http://en.wikipedia.org/wiki/Glossary_of_sewing_terms][sewing terms]]; for now
  lets call it =quilt=. =quilt= contains a number of languages such as
  =cpp=. A user can only generate one model group at a time. Users can
  generate one or more languages within a group (depending on what the
  group supports).
- we should have a top-level folder to house all model groups:
  =backends=. The existing =backend= model becomes =backends::core=.
- there may be facilities that are language specific, shared by model
  groups. These can be housed in language specific folders:
  =backends::cpp= and so on. For instance, the language specific stuff
  now in =formatters= should move here.
- different groups may express SML models differently; almost by
  definition, they will, or else there is little purpose in having
  multiple groups. For example, one can imagine a model group (say
  =pleat=) which expresses [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-language-agnostic-models-lam][LAM]] as a model that is identical on every
  supported language, ignoring native types; that is, expresses LAM as
  a LAM model. However, =quilt= would still express LAM as a mapping
  between generic LAM types and concrete native types (e.g. LAM
  dictionary is a C++ unordered map). A good candidate for =pleat=
  would be [[http://www.eclipse.org/modeling/emf/][eCore]].
- if one was to try to generate code that is identical to =protobuf=,
  the xsd tool, =odb=, etc one would generate model groups for these.
- we may need multiple "needles" for each model group. For example,
  the supporting libraries for =quilt= may be (and almost certainly,
  will be) totally different than that of those in =pleat=. And of
  course, needle would have different expressions in each programming
  language. So perhaps needle is more of a concept than a physical
  thing. We should rename it to something meaningful that represents
  "a library with supporting code for a given model group". However,
  it does make sense to have a top-level folder to house all of the
  supporting libraries, so maybe needle does exist physically as the
  namespace to house all of the different supporting libraries. For
  example: =dogen::needle::quilt=, etc.
- the different needle libraries should be pushed to the appropriate
  repositories (e.g. nuget for C# and maybe C++, biicode for C++,
  maven for Java, etc).
- in the model groups world, each model most likely will only support
  a single model group: for example either quilt or pleat, etc. This
  is because some types only make sense with a given model group (say
  for example a cross platform =String= type in pleat won't exist in
  quilt and so forth). This means one must filter the models one is
  loading depending on the model group. This applies to both internal
  and external models. Also a model group may support a different
  subset of programming languages compared to another model group.
- we need a better name than "model group". word-storming: dimension,
  universe, space, package, module, ensemble, generation unit,
  assembly.
- Another way to think about it is that model groups are really
  backends. Backends support one or more "languages" (we need a word
  to reflect variations such as XML). Only one backend can be enabled
  at one time. One or more languages can be enabled, depending on what
  the backend supports. The options that configure languages and
  backends are in the meta-data; it does not make sense to supply
  these in the command-line because the model is coupled with the
  backend to a large extent (for example, native types are only
  supported in the native backend and so on).
- model groups and type support: some types will only make sense with
  certain model groups. For example, if one were to create a "cross
  platform string type", say String, for =pleat= which would then be
  implemented in =needle::pleat= for all languages, it would not make
  sense to try to use this type from =quilt=. This means that we need
  some kind of way to associate types with a model group. In terms of
  code generation, the formatter "enabled/disabled" logic will kick
  in, and if the type has no formatters in a given backend, then it is
  effectively disabled. But one wonders if this is a sensible way to
  figure out what types are available to which model groups. Seems
  like one would have to spend a lot of time looking into the
  meta-data to determine whats available.
- we probably need to add the model group to the ownership hierarchy,
  but at present we cannot think of a use case for it; we never enable
  anything across languages in a model group. In the same vein, we
  would also need the language. Fields would then be
  =quilt.cpp.enabled= and so on.

This work must be integrated with the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].

*** Multi-purpose models per language                                  :epic:

#+begin_quote
This story is a very vague story that keeps track of ideas on making
dogen useful for code generators of other kinds.
#+end_quote

One of the stories in the backlog covers other targets of code generation:

[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-thrift-and-protocol-buffers][Add support for thrift and protocol buffers]]

Originally we thought about adding support for these within a model;
that is to say, one would have additional serialisation "kinds"
available with a given dogen model. However, there is another way to
look at this; one could make other kinds of code generators using the
dogen infrastructure.

That is, contrary to it's name, dogen isn't just for "domain model
generation". Nothing stops one from building a protocol buffers or
thrift compiler using dogen infrastructure that outputs *exactly* the
same code as the original tools. All that would be required to do so
is:

- create a front-end that reads in their specification;
- to ensure SML is expressive enough to cover all of the aspects of
  the code that needs to be generated;
- to create the formatters.

In this view of the world, we have two options:

- create groups of facets within the =cpp= model; for example,
  the thrift group, the domain generation group etc. These are
  mutually incompatible sets of formatters and only one of them can be
  enabled for a given execution.
- create models at the same level of the =cpp= model. We could group
  them by language (e.g. the =cpp= namespace). However, this seems
  less practical because these models would probably have a lot in
  common. This is yet to be seen as we need to finish the large
  formatters refactor before we can answer this question.

Taking this to its logical consequence, even a tool as complex as ODB
could potentially be implementable in this way: one can conceive a
clang front-end that reads in source code and generates an SML model;
this model then can be used to generate C++ code that is identical to
the code produced by ODB (again assuming that SML is extended to be
expressive enough to represent all the constructs required by ODB).

This would be a compelling proposition if we had =stitch= because it
would make the generation of formatters quite trivial and would also
mean that people that want to create code generators don't have to
worry about a lot of the boilerplate code. However, the biggest
problem is that we'd be imposing a large and complex "framework" on
them with all the evilness that that entails.

Food for thought:

- in this light, a better name for dogen would be =codegen= (or =cogen=
  to make it a bit more unique in google). The tag line is then The
  Generic Code Generator. Unfortunately there are already a few
  projects with the name =cogen= so we may need to find a better
  name. Alternatively we can maintain the name dogen, but take away
  its meaning (i.e. no longer "The Domain Generator").
- the merge of =cpp= and =cpp_formatters= may not have been for the
  best in this case; it would make more sense to have a =cpp::dogen=
  where we collect all of the formatters related to domain
  generation - after the =cogen= rename; if no rename then we need
  some other name to imply domain generation. At this level we could
  then have =cogen::cpp::odb=, =cogen::cpp::protobuf= and so on. They
  all make use of the core types defined in =cogen::cpp=. The problem
  with this approach is that dogen is not really designed to share a
  namespace in this way. We won't be able to have a =cpp= project as
  well as placing other projects inside of the =cpp= namespace. We can
  have one or the other in the current setup, but not both. We could
  take the same approach as we did for test models: create a cpp
  folder and then put the model under a different name such as =model=
  or =domain= etc. Note that we still have to define all of the
  formatter interfaces in the "main" model, as well as workflows
  etc. However, some interfaces may not make sense for other models:
  what is a registrar in protocol buffers? If it exists at all, its
  probably something very different from boost serialisation and as
  such will require other data.
- note that this kind of grouping is not necessarily at the language
  level. For example, domain generation should be common to a set of
  languages, and so would protocol buffers. This means that rather
  than a facet or formatter grouping, we need a higher level construct
  to aggregate things; "domain generation" is made up of languages,
  languages are made of of facets, facets have formatters. We need a
  name/classification for "domain generation" in this context.

We should bear in mind [[http://st-www.cs.illinois.edu/users/droberts/evolve.html][this quote]]:

#+begin_quote
People develop abstractions by generalizing from concrete
examples. Every attempt to determine the correct abstractions on paper
without actually developing a running system is doomed to failure. No
one is that smart. A framework is a reusable design, so you develop it
by looking at the things it is supposed to be a design of. The more
examples you look at, the more general your framework will be.
#+end_quote

*** Include groups                                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to create includers for user defined
groups of files so that I don't have to do it manually.
#+end_quote

One of my personal preferences has always been to group includes by
"library". Normally first come the C includes, then the standard
library ones, then boost, then utilities and finally types of the same
model. Each of these can be thought of as a group. Inside each group
the file names are normally ordered by size, smallest first. It would
be nice to have support for such a feature in Dogen.

Formatters would then push their includes into the correct
group. Group names could be the model name (=std=, etc).

A bit of a nitpick but nice nonetheless.

*** Models should have an associated language                          :epic:

#+begin_quote
*Story*: As a dogen user, I want to make sure I only use valid system
models so that I don't generate models that code generate but do not
compile.
#+end_quote

Certain models (e.g. system / library models) can only be used in a
give language; for example =boost= and =std= only make sense in C++. A
.Net library model would only make sense in .Net, etc. These are
Language Specific Models (LSM). Once a model depends on a LSM it
itself becomes an LSM and it should not be able to then make use of
models of other languages nor should one be able to request a code
generation for other languages.

However, one day we will have a system model which is a Language
Agnostic Model (LAM). The system model will provide a base set of
functionality across languages such as containers, and for each type
it will have mappings to language specific types. The mapping is
declared as dynamic extensions in the appropriate section
(i.e. =tags::cpp::mapped_type= or something of that ilk). If a model
depends only on LAMs, it is itself a LAM and can be used to generate
code on any supported language (presumably a supported language is
defined to be that for which we have both mappings and a code
generation backend).

A first step for this would be to have a language enumeration in SML
which is a property of the model, and one entry of which is "language
agnostic".

See also the model groups work.

See also the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_69.org#thoughts-on-cpp-refactoring][archetype work]].


** Deprecated

*** CANCELLED Injection framework                                      :epic:
    CLOSED: [2015-07-09 Thu 21:26]

*Rationale*: We should inject types on a case-by-case basis. At
present we have covered all use cases in a fairly sensible way.

We need a more generic way of handling system types injection into
models. This is because there is a number of things that can be
derived from the existing model types:

- keys
- diff support
- reflection
- cache code
- etc.

So we need to:

- make injector a composite of injectors that do the real work such as
  =key_injector=. internally =injector= just delegates the work to
  these classes.
- injector decides which internal injectors to use based on options
  passed in.
- in the IoC spirit, we should probably create a =injector_interface=.

*** CANCELLED IoC work                                                 :epic:
    CLOSED: [2015-07-09 Thu 21:28]

*Rationale*: this is a lot of work and does not buy us much. We should
tackle each IoC requirement at a time.

All stories related to IoC work are tracked here.

*New Understanding*:

in reality, there is really only one place where IoC makes sense: in
the workflows. It would be great if one could pass in something akin
to a IoC container into the workflow's constructor and then use the
container to obtain access to all services via interfaces. Using
sml::workflow as an example, one could have:

- container_interface which returns grapher_interface,
  processor_interface, etc.
- the container could even return references to the these interfaces
  and own the lifetime of the objects.
- this would then allow us to provide mock container interface
  implementations returning mock services.

However:

- it seems like a lot of moving parts just to allow testing the
  workflow in isolation. this is particularly more so in the case of
  the workflows we have, which are fairly trivial. perhaps we should
  consider this approach when dogen is generating the interfaces
  automatically as this would require a lot of manual work for little
  gain.

*Old understanding*:

- add workflow_interface to SML.
- we should be doing a bit more IoC, particularly with inclusion
  manager, location manager etc. In order to do so we could define
  interfaces for these classes and provide mocks for the tests. This
  would make the tests considerably smaller.
*** CANCELLED Log analysis tool                                       :story:
    CLOSED: [2015-07-09 Thu 21:35]

*Rationale*: we will incubate these ideas on its own project.

We should create a log analyser tool (=logan=?), as follows:

- separate repo. it could be incubated in dogen to start off with
  though.
- use a dogen model to describe the tool's domain. Very simple domain.
- use the dogen version line to determine the application, the version
  and the run time. All other entries are foreign-keyed against this
  entry.
- use JSON object markers to extract JSON objects from the log line
  into a postgres JSON field.
- use ODB to create the database schema.
- create a simple parser that is hard-coded to the log lines in dogen,
  with perhaps an addition for threads.
- when profiling is present, have a way to split profiling information
  from the rest.
- create some simple stored procs that compare two runs from a
  performance perspective.
- create a stored proc to list all errors and all warnings, with
  perhaps some lines around it.
- create a stored proc that does a text search using postgres text
  search facilities.
- we need to figure out how splunk decides to start loading the log
  files (only after roll, incrementally - and if so, how does it keep
  track, etc).

*** CANCELLED Create a trivial Linux gcc script                       :story:
    CLOSED: [2015-07-09 Thu 21:35]

*Rationale*: we want to move away from complicated CDash scripts. We
should stick with what we got for valgrind, and use travis etc for any
new developments.

The previous attempts to clean up the build environment were too
elaborate given the available time. We need to go back to basics with
a trivial script that works for Linux 32-bit and 64-bit with gcc.

*** CANCELLED Create a trivial Linux clang script                     :story:
    CLOSED: [2015-07-09 Thu 21:35]

*Rationale*: we want to move away from complicated CDash scripts. We
should stick with what we got for valgrind, and use travis etc for any
new developments.

We need to be able to build Linux clang 32-bit and 64-bit again.
