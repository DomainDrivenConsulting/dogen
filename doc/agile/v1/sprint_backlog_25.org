#+title: Sprint Backlog 25
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Sprint Goals

- Complete the work on the physical meta-model and model.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2020-05-05 Tue 22:01]
| <75>                                   |        |      |      |       |
| Headline                               | Time   |      |      |     % |
|----------------------------------------+--------+------+------+-------|
| *Total time*                           | *3:59* |      |      | 100.0 |
|----------------------------------------+--------+------+------+-------|
| Stories                                | 3:59   |      |      | 100.0 |
| Active                                 |        | 3:59 |      | 100.0 |
| Edit release notes for previous sprint |        |      | 3:59 | 100.0 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2020-05-05 Tue 22:01]
    :LOGBOOK:
    CLOCK: [2020-05-05 Tue 21:45]--[2020-05-05 Tue 22:00] =>  0:15
    CLOCK: [2020-05-05 Tue 19:03]--[2020-05-05 Tue 21:44] =>  2:34
    CLOCK: [2020-05-04 Mon 21:02]--[2020-05-04 Mon 22:05] =>  1:03
    :END:

Add github release notes for previous sprint.

Release Announcements:

- [[https://twitter.com/MarcoCraveiro/status/1248358530245148677][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6646494675207278592/][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

**** Dogen v1.0.24, "Imbondeiro no Iona"

*NOTE: Release notes under construction*

#+caption: Baobab tree in Iona
[[https://pbs.twimg.com/media/CpAcgYpWIAEGmCF?format=jpg]]
/A baobab tree in Iona national park, Namib, Angola. (C) 2011 [[https://commons.wikimedia.org/wiki/File:Imbondeiro_Tree.jpg][Alfred Weidinger]]/

**NOTE: Release notes under construction**

***** Introduction

Welcome to the second release of Dogen under quarantine. As with most
people, we have now converged to the new normal - or at least adjusted
best one can to these sorts of
world-changing-types-of-circumstances. Development continued to
proceed at a steady clip, if somewhat slower than the previous
sprint's, and delivered a fair bit of internal changes. Most
significantly, with this release we may have finally broken the back
of the fabled generation model refactor - though, to be fair, we'll
only know for sure next sprint. We've also used some of our [[http://www.catb.org/~esr/jargon/html/C/copious-free-time.html][copious
free time]] to make key improvements to infrastructure, fixing a number
of long-standing annoyances. So, grab yourself a hot cup of coffee and
get ready for yet another exciting Dogen sprint review!

***** User visible changes

This section covers stories that affect end users, with the video
providing a quick demonstration of the new features, and the sections
below describing them in more detail. As there have only been a small
number of user facing changes, we've also used the video to discuss
the internal work.

[![Sprint 1.0.24 Demo](https://img.youtube.com/vi/RysjvA2eZ4o/0.jpg)](https://youtu.be/RysjvA2eZ4o)
_Video 1: Sprint 24 Demo._

****** Add model name to tracing dumps

Though mainly useful for Dogen developers, the tracing subsystem can
be used by end users as well. As before, it can be enabled via the
usual flags:

#+begin_example
Tracing:
  --tracing-enabled              Generate metrics about executed transforms.
  --tracing-level arg            Level at which to trace.Valid values: detail,
                                 summary. Defaults to summary.
  --tracing-guids-enabled        Use guids in tracing metrics, Not  recommended
                                 when making comparisons between runs.
  --tracing-format arg           Format to use for tracing metrics. Valid
                                 values: plain, org-mode, graphviz. Defaults to
                                 org-mode.
  --tracing-backend arg          Backend to use for tracing. Valid values:
                                 file, relational.
  --tracing-run-id arg           Run ID to use to identify the tracing session.
#+end_example

With this release, we fixed a long standing annoyance with the file
backend, which is to name the trace files according to the model the
transform is operating on. This is best demonstrated by means of an
example. Say we take an arbitrary file from a tracing dump of the
injection subsystem. Previously, the file names were named like so:

#+begin_example
000-injection.dia.decoding_transform-c040099b-858a-4a3d-af5b-df74f1c7f52c-input.json
...
#+end_example

This made it quite difficult to find out which model was being
processed with this transform, particularly when there are large
numbers of similar files. With this release we've added the model name
to the tracing file name for the transform (/e.g./, =dogen.logical=):

#+begin_example
000-injection.dia.decoding_transform-dogen.logical-c040099b-858a-4a3d-af5b-df74f1c7f52c-input.json
...
#+end_example

This makes locating the tracing files much easier, and we've already
made extensive use of this feature whilst troubleshooting during
development.

****** Primitives use compiler generated default constructors

A bug was detected in generated code by two sharp-eyed individuals -
Indranil and Ian - and a lifesaving tool - valgrind. Up to know our
valgrind output had been so noisy that we weren't really paying too
much attention to it. However, with this release we finally tidied it
up - as we shall see later on in these release notes.. Lo-and-behold,
as soon as we did that, an obvious bug was spotted the generated
code. Turns out we were generating primitives that used the compiler
generated default constructor even when the underlying type was a
primitive type. Taking an example for the [[https://github.com/MASD-Project/cpp_ref_impl][C++ reference model]]:

#+begin_src cpp
class bool_primitive final {
public:
    bool_primitive() = default;
...
private:
    bool value_;
#+end_src

This of course resulted in uninitialised member variables. With this
release the generated code now creates a manual default constructor:

#+begin_src cpp
class bool_primitive final {
...
public:
    bool_primitive();
...
#+end_src

Which does the appropriate initialisation (do forgive the
=static_cast=, these will be cleaned up at some point in the future):

#+begin_src cpp
bool_primitive::bool_primitive()
    : value_(static_cast<bool>(0)) { }
#+end_src

This fix illustrates the importance of static and dynamic analysis
tools, forcing us to refresh [[https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-clang-sanitizers][the story on the missing LLVM/Clang
tools]]. Sadly there aren't enough hours of the day to tackle all of
these but we must get to them sooner rather than later.

****** Allow creating models with no decorations

Since we're on the topic of [[http://www.catb.org/~esr/jargon/html/B/brown-paper-bag-bug.html][brown-paper-bag bugs]], another interesting
one was fix this sprint: our "sanity check model" which we use to make
sure our packages generate a minimally usable Dogen binary was causing
Dogen to segfault. This is in truth a veritable comedy of errors. The
problem started with the fact that our [[https://github.com/MASD-Project/dogen/blob/master/build/scripts/test_package.linux.sh][test packaging script]] needs to
know the version of the compiler for which the package was built in
order that we can find the files in the filesystem. This is of course
not ideal, but it is what it is and we have more pressing matters to
look at, sadly. The code is like so:

#+begin_src sh
#
# Compiler
#
compiler="$1"
shift
if [[ "x${compiler}" = "x" ]]; then
    compiler="gcc8";
    echo "* Compiler: ${compiler} (default)"
...
elif [ "${compiler}" = "clang8" ]; then
    echo "* Compiler: ${compiler}"
elif [ "${compiler}" = "clang9" ]; then
    echo "* Compiler: ${compiler}"
else
    echo "* Unrecognised compiler: ${compiler}"
    exit
fi
#+end_src

However, we forgot to update the script when we moved to
=clang-9=. Now, normally this would have been picked up by travis as a
red build, /except/ we decided to return a non-error-error-code (see
above). This meant that packages had not been tested for quite a
while. To make matters interesting, we did introduce a bad bug over
time; we changed the handling of default decorations. The problem is
that all test models use the test profile, and the test profile
contains decorations. The only model that did not contain any
decorations was, you guessed it, the hello world model. So once we
fixed the package testing script we then had to fix the code that
handles default decorations.

***** Development Matters

In this section we cover topics that are mainly of interest if you
follow Dogen development, such as details on internal stories that
consumed significant resources, important events, etc. As usual, for
all the gory details of the work carried out this sprint, see the
[[https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_24.org][sprint log]].

****** Ephemerides

The 11,111th commit was reached during this release.

![11111th commit](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/dogen_11111_commits.png)
_Figure 1: 11,111th commit in the Dogen git repository._

****** Milestones

The first set of completely green builds have been obtained for
Dogen - both nightlies and continuous builds. This includes tests,
dynamic analysis and code coverage.

![Dogen CDash](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_dogen_green_build.png)
_Figure 2: Builds for Dogen in CDash's dashboard._

The first set of completely green nightly builds have been obtained
for the C++ Reference Model. Work still remains on continuous builds
for OSX and Windows, with 4 and 2 test failures respectively.

![C++ Reference Implementation CDash](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_cpp_ref_impl_green_build.png)
_Figure 3: Builds for C++ reference model in CDash's dashboard._

****** Significant Internal Stories

The sprint was mostly dominated by a large number of small refactors
that changed the internals of Dogen dramatically - though in many
cases, mainly with regards to naming and location of classes. We've
aggregated all of these stories under two themes.

******* Generation model refactor

There were several stories connected to the generation model refactor,
which we have aggregated under the sundry umbrella of the current
section. In all honesty, we did not do a brilliant job of sizing tasks
this sprint. Instead, we ended up with a couple of gigantic,
_epic-like_ stories - XXXL? - rather than a number of small, focused
and roughly equally sized stories that we prefer - L and X, in [[https://www.c-sharpcorner.com/article/agile-story-point-estimation-techniques-t-shirt-sizing/][t-shirt
sizes]]. Yet another great opportunity for improvement is clearly
presenting itself here. To make things more understandable for this
/post-mortem/, we decided to paper over the cracks and provide a
slightly more granular view - rather than the coarse-grained way in
which it was originally recorded on the sprint backlog.

The core of the work was divided as follows:

- *Adding physical entities to the logical model*: this story was
  continued from the previous sprint. The entities themselves had
  already been added to the logical model, so the work was mainly on
  creating the required transforms to ensure they had the right data
  by the time we hit the M2T (Model-to-Text) transforms.
- *Generating physical model entities from =m2t= classes*: we finally
  go to the point where the top-level M2T transforms are generating
  the physical archetypes, which means the complete generation of the
  physical meta-model is not far now. The remaining physical
  meta-model entities (backend, facet, parts) are not quite as fiddly,
  hopefully.
- *Bootstrapping of physical entities*: we continued the work on
  generation of physical entities via the logical model elements that
  represent them. This is very fiddly work because we are trying to
  bootstrap the existing templates - that is, generate code that
  resembles the existing generators - and therefore requires a great
  deal of concentration; its very easy to lose track of where we are
  and break everything, and we done so a few times this sprint,
  costing us a fair bit of time in tracking back the errors. There is
  hope that this work is almost complete though.
- *Add T2T (Text-to-Text) transforms*: As usual, a great deal of
  effort was spent on making sure that the code is consistent with the
  current understanding of the conceptual model. One aspect that had
  been rather illusive is the handling of templates; these are in
  effect not M2T transforms, because we've already discarded the model
  representation. With this sprint we arrived at T2T (Text-to-Text)
  transforms, which are a surprisingly good fit for both types of
  logic-less templates we have in Dogen (stitch and wale) but also
  have the potential to model _cartridges_ such as [[https://www.codesynthesis.com/products/odb/][ODB]], [[https://www.codesynthesis.com/products/xsd/][XSD tool]] and
  many other types of code generators. More work on this remains next
  sprint, but the direction of travel is very promising.
- *Rename the =m2t= model to =text=*: following on from the previous
  entry, given that we now had two different types of transforms in
  this model (_e.g._, M2T and T2T) we could not longer call it the
  =m2t= model, and thus decided to rename it to just =text=. As it
  turns out, this is a much better fit for the conceptual model and
  prepares ourselves for the coming work on cartridges, which now have
  a very suitable location in which to be placed.

As you can probably gather from what is written on these topics [in
the [[https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_24.org#add-physical-entities-to-logical-model][sprint backlog]], these few bullet points do little justice to the
immense amount of mental effort that was spent on them. Sadly, we do
not have the time - and I dare say, the inclination - to explain in
the required detail how all of these issues contribute to the overall
picture we are trying to form. Hopefully when the generation refactor
is completed and all the fuzziness is taken away, a blog post can be
produced summarising all of the moving parts in a concise narrative.

******* Code Coverage

Code coverage is important to us, for very much the same reason it is
important to any software project: you want to make sure your unit
tests are exercising as much of the code as possible. However, in
addition to this, we also need to make sure the generated code is
being adequately tested by the generated tests, both for Dogen as well
as the Reference Implementation models. Historically, C++ has had good
code coverage tools and services but they haven't been the
most... user friendly, shall we say, pieces of software ever made. So,
since Dogen's early days, I've been very eager to experiment the new
wave of code coverage cloud services such as [[https://coveralls.io/github/MASD-Project/dogen][Coverals]] and [[https://codecov.io/gh/MASD-Project/dogen][Codecov]] and
tools such as [[https://github.com/SimonKagstrom/kcov][kcov]] to track code coverage. The experiment was [[https://github.com/MASD-Project/dogen/blob/master/doc/agile/v0/sprint_backlog_57.org#add-support-for-coveralls][long
running]] but has now run its course, I am sorry to report, as we just
faced too many problems for my liking. Now, in the interest of
fairness, its not entirely clear if /some/ of the problems we
experienced are related to =kcov= rather than the cloud services; but
other issues such as troubles with API keys and so forth were
/definitely/ related to the services themselves. Given we don't have
the time to troubleshoot every problem, and we must be able to rely on
the code coverage numbers to make important decisions, I had no option
but to move back to good old [[https://blog.kitware.com/additional-coverage-features-in-cdash/][CDash]] - a tool that had proven reliable
in the past for this.

![CDash continuous coverage](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_continuous_code_coverage.png)
_Figure 4: Code coverage for Dogen, continuous builds, after moving back to CDash._

I must confess that it was with a heavy heart that I even begun to
contemplate moving away from =kcov=, as I quite like the tool;
compared to the pain of setting up =gcov= or even =llvm-cov=, I think
=kcov= is a work of art and a master of delightful user
experience. Also, the maintainer is very friendly and responsive, as
[[https://github.com/SimonKagstrom/kcov/issues/272][previous communications]] attest. Alas, as far as I could see, there was
no easy way to connect the output of =kcov= with CDash, so back to the
drawing board we went. I shan't bother you with graphic descriptions
of the trials and tribulations of setting up =gcov= and =llvm-cov= - I
presume any Linux C/C++ developer is far too battle-scarred to find
any such tales interesting - but it suffices to say that, after a
great deal of pain and [[https://github.com/MASD-Project/dogen/commits/master?after=074076edbb18cbcbf5ab4179edd40beb19edfd0b+69][many, many failed builds]] later we eventually
managed to get =gcov= to produce the desired information.

![CDash nightly coverage](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_dogen_nightly_coverage.png)
_Figure 5: Code coverage for Dogen, nightly builds, after moving back to CDash._

Figure 4 illustrates the progress of code coverage on Dogen's
continuous builds over time, whereas Figure 5 looks at coverage in
nightlies. As we [[https://github.com/MASD-Project/dogen/releases/tag/v1.0.19][explained previously]], we have different uses for
coverage depending on which build we use. Nightly builds run all
generated tests, and as such they produce code coverage that takes
into account the generated tests. This is useful, but its important
not to confuse it with manually generated tests, which provide us with
"real" coverage; that is, coverage that emerged as a result of
"real" - /i.e./, domain - use of the types. We need both of these
measurements in order to make sense of what areas are lacking. With
CDash we now seem to have a reliable source of information for both of
these measurements. As you can see from these charts, the coverage is
not oscillating through time as it did previously when we used the
coverage services (possibly due to kcov problems, but I personally
doubt it). As an added bonus, we no longer have red builds due to
"failed checks" in GitHub due to [[https://coveralls.io/builds/30280785][stochastic decreases in coverage]], as
we had far too many times in the past.

![Nightly build duration](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_nightly_build_time.png)
_Figure 6: Dogen nightly build duration over time._

A very important aspect when adding code coverage to already busy
nightlies was the impact on build duration. We first started by trying
to use clang and =llvm-cov= but we found that the nightlies started to
take far too long to complete. This is possibly something to do with
our settings - perhaps valgrind was not happy with the new coverage
profiling parameters? - but given we didn't have a lot of time to
experiment, we decided instead to move over to =gcov= and gcc debug
builds. Figures 6 and 7 show the impact to the build time to both
Dogen and the C++ Reference Model. These were deemed acceptable.

![Nightly build duration](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_cpp_ref_impl_nightly_build_time.png)
_Figure 7: C++ reference model build duration over time._

******* Dynamic Analysis

As with code coverage, we've been making use of CDash to keep track of
data produced by [[https://valgrind.org][valgrind]]. However, we let the reports bit-rot
somewhat, with lots of false positives clouding the view (or at least
we hope they are false positives). With this release we took the time
to update our suppression files, removing the majority of false
positives. We then immediately located a couple of issues in the code
(circular use of =boost::shared_ptr= and the problems with
uninitialised member variables explained above.

![Valgrind errors over time](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_dogen_dynamic_analysis.png)
_Figure 8: Valgrind errors over time in CDash._

I don't think we need any additional incentives to keep the board nice
and clean as far as dynamic analysis is concerned. Figure 8 shows the
current state of zero warnings, which is a joy to behold.

******* MDE Paper of the Week (PofW)

This sprint we started another experiment with YouTube and video
recording: a sort of "self-journal club". For those not from a
research background, many research labs organise a weekly (insert your
frequency here, I guess) meeting where the participants discuss a
scientific paper. The idea is that everyone reads the paper, but the
chosen presenter will go through it in depth, and the audience can ask
questions and so forth. Normally, this is a great forum to discuss
papers that you are reading as part of your research and get some help
to understand more difficult parts. Its also a place where you can see
what everybody else is up to across your lab. At any rate, with the
move back to gainful employment I no longer get the chance to
participate in my lab's journal club. In addition, I found that many
of the papers I had read during the years had lots of useful
information that now makes a lot more sense so a re-read was required.

So I combined these two ideas and come up with the somewhat sad idea
of a "self-journal club", where I read and discuss the papers of
interest . These are available in YouTube, should you for whatever
unfathomable reason find these interesting. Four papers have been read
thus far:

- [[https://www.youtube.com/watch?v=SRnQgrvq7Cg][MDE PotW 01: Systems Variability Modeling: A Textual Model Mixing Class and Feature Concepts]]
- [[https://www.youtube.com/watch?v=cJ1J5Evz3mg][MDE PotW 02:A Code Generation Metamodel for ULF-Ware Generating Code for SDL]]
- [[https://www.youtube.com/watch?v=QFlnn4Mbchs][MDE PotW 03: A Lightweight MDSD Process Applied in Small Projects]]
- [[https://www.youtube.com/watch?v=Z24mT64j0po][MDE PotW 04: Un estudio comparativo de dos herramientas MDA: OptimalJ y ArcStyler]]

The last paper was more experimental than usual, what with it being in
Spanish, but it worked better than we expected, so from now on we
shall consider papers on other languages we can parse.

As with coding videos, the most significant advantage of this approach
is motivational; I now find that I must re-read a paper a week even
when I don't feel like it purely because of the fact that I publish
them online. Lets see how long the YouTube effect will last though...

****** Resourcing

Weighing in at around 280 commits and with 83 hours of commitment,
this sprint was by traditional measurements a success. To be fair, we
did return to the more regular duration of around four weeks rather
than the three of the previous sprint, resulting in a utilisation rate
of precisely 50% -a decrease of 16% from the previous sprint. On the
other hand, this slower velocity seems far more sustainable than the
break neck pace we attempted previously; our aim will continue to be
around 50%, which effectively means part-time work.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_24_pie_chart.jpg)
_Figure 9: Cost of stories for sprint 24._

Where things become a bit muddier is when we break down the stories by
"type". We spent around 56% of the overall ask on stories directly
connected to the sprint goal, which may appear to be a bit low. The
bulk of the remaining 44% were spent largely on process (24.5%), and
infrastructure (11.5%) with a notable mention for the almost 6% spent
moving code coverage into CDash. Another 6.6% was spent on reading MDE
papers, which is of course time well spent from a strategic
perspective but it does eat into the coding time. Of the 24.5% spent
on process, a notable mention is the 11.3% spent editing the release
notes. These are becoming a bit too expensive for our liking so next
sprint we need to speed these along.

****** Roadmap

The roadmap remains more or less unchanged, except our goal continues
to be finishing the generation refactor. But now that the problem is
understood a bit better, there is some faint hope that next sprint
could bring it to a close.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_24_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_24_resource_allocation_graph.png)

****** Binaries

You can download binaries from either [[https://bintray.com/masd-project/main/dogen/1.0.23][Bintray]] or GitHub, as per
Table 2. All binaries are 64-bit. For all other architectures and/or
operative systems, you will need to build Dogen from source. Source
downloads are available in [[https://github.com/MASD-Project/dogen/archive/v1.0.23.zip][zip]] or [[https://github.com/MASD-Project/dogen/archive/v1.0.23.tar.gz][tar.gz]] format.

| Operative System    | Format | BinTray                             | GitHub                              |
|---------------------+--------+-------------------------------------+-------------------------------------|
| Linux Debian/Ubuntu | Deb    | [[https://dl.bintray.com/masd-project/main/1.0.23/dogen_1.0.23_amd64-applications.deb][dogen_1.0.23_amd64-applications.deb]] | [[https://github.com/MASD-Project/dogen/releases/download/v1.0.23/dogen_1.0.23_amd64-applications.deb][dogen_1.0.23_amd64-applications.deb]] |
| OSX                 | DMG    | [[https://dl.bintray.com/masd-project/main/1.0.23/DOGEN-1.0.23-Darwin-x86_64.dmg][DOGEN-1.0.23-Darwin-x86_64.dmg]]      | [[https://github.com/MASD-Project/dogen/releases/download/v1.0.23/DOGEN-1.0.23-Darwin-x86_64.dmg][DOGEN-1.0.23-Darwin-x86_64.dmg]]      |
| Windows             | MSI    | [[https://dl.bintray.com/masd-project/main/DOGEN-1.0.23-Windows-AMD64.msi][DOGEN-1.0.23-Windows-AMD64.msi]]      | [[https://github.com/MASD-Project/dogen/releases/download/v1.0.23/DOGEN-1.0.23-Windows-AMD64.msi][DOGEN-1.0.23-Windows-AMD64.msi]]      |

/Table 2: Binary packages for Dogen./

*Note:* The OSX and Linux binaries are not stripped at present and so
are larger than they should be. We have [[https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped][an outstanding story]] to
address this issue, but sadly CMake does not make this a trivial
undertaking.

****** Next Sprint

The goal for the next sprint is to complete most of the work on the
generation refactor. It is unlikely we shall finish it in its entirety
as they are quite a few fiddly bits, but we shall aim to get most of
it out of the way.

That's all for this release. Happy Modeling!

*** Wale templates must be bound to logical archetypes                :story:

A stitch template may make use of a wale template. At present we are
loading these from the file system, thus requiring the
locator. However, since we already have the templates in memory, we
could model these a bit better.

Notes:

- we could easily add a wale template meta-data parameter to the
  logical archetype. The problem is, at this point all we are saying
  is that there are logical associations between elements. We then
  need to somehow load up the artefact corresponding to the wale
  template into the element artefacts of all logical archetypes which
  refer to that template. We could have a =text= transform that does
  this. Finally we could add a dependency between the stitch artefact
  and the wale artefact. However, for this to work, we need to supply
  the entire =element_artefacts= into the text transform and let the
  stitch transform locate whatever it needs.
- a second problem is that we need to load the wale templates from the
  file system before we reach the physical model. This could be done
  as part of the wale template. We already do something similar for
  stitch; if it exists load it, if not create it. We need a similar
  logic.

*** =templating= should not depend on =physical=                      :story:

For some random reason we implemented the =templating= model in terms
of artefacts of the physical model. There is no need for this in the
new world, so we should try to decouple these models. Templating
should not even know of files; it should receive a string and return a
string.

*** Extend tracing to M2T transforms                                  :story:

There is nothing stopping us from having a context with the tracer,
and doing a dump of the artefact before and after a M2T transform.

*** Rename "model-to-X" to TLAs                                       :story:

Given that model-to-text and text-to-model (to a lesser extent) are
well known TLAs in MDE we should make use of these in class names. The
names we have at present are very long. The additional size is not
providing any benefits.

*** Replace initialisers with facet-based initialisation              :story:

Now that we have facets, archetypes, etc as proper meta-model
elements, it is becoming clear that the initialiser is just a facet in
disguise. We have enough information to generate all initialisers as
part of the code generation of facets and backends. Once we do this,
we have reached the point where it is possible to create a new
meta-model element and add a formatter for it and code will be
automatically generated without any manual intervention. Similarly,
deleting formatters will delete all traces of it from the code
generator.

*** Consider making technical spaces a core concept                   :story:

At present we are trying to instantiate a stitch template. It requires
knowing the technical space that the original archetype belongs to so
that we can locate the appropriate decoration. However, because we are
in the text model and the archetype is a physical concept, we have no
way of knowing what the original TS was for a given archetype. We
could of course locate the associated M2T transform etc - but this
perhaps hints at a bigger problem with the conceptual model: technical
spaces are much more of a pervasive concept than just logical model:

- injectors belong to technical spaces.
- logical model entities belong to technical spaces.
- text transforms belong to technical spaces.
- physical model entities such as archetypes and artefacts belong to
  technical spaces.

It would be nice if we could have this modeled correctly, in some kind
of shared model. At present, the only model which does this is
=variability= but it does not make a lot of sense to put TS'
there. Perhaps we should wait until we have enough entities to see
what the name of this "core" model should be.

*** Inject backend, facets and archetypes into PMM                    :story:

At present we only have artefacts in the PMM. We need to inject all
other missing elements. We also need to create a transform which
builds the PMM. Finally while we're at it we should add enablement
properties and associated transform.

Notes:

- we should also change template instantiation code to use the PMM.
- once we have a flag, we can detect disabled backends before any work
  is carried out. The cost should be very close to zero. We don't need
  to do any checks for this afterwards.
- we need to add a list of archetypes that each archetype depends
  on. We need to update the formatters to return archetypes rather
  than names and have the dependencies there.

Merged stories:

*Implement archetype locations from physical meta-model*

We need to use the new physical meta-model to obtain information about
the layout of physical space, replacing the archetype locations.

Tasks:

- make the existing backend interface return the layout of physical
  space.
- create a transform that populates all of the data structures needed
  by the current code base (archetype locations).
- replace the existing archetype locations with a physical meta-model.
- remove all the archetype locations data structures.

Notes:

- template instantiation domains should be a part of the physical
  meta-model. Create a transform to compute these. *done*
- remove Locatable from Element? *done*

Merged stories:

*Clean-up archetype locations modeling*

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*Implement the physical meta-model*

We need to replace the existing classes around archetype locations
with the new meta-model types.

Notes:

- formatters should add their data to a registrar that lives in the
  physical model rather than expose it via an interface.

*** Split enablement features                                         :story:

At present we are instantiating the =enabled= feature across the
entire =masd= template instantiation domain. This is a very
"efficient" way to do it because we only define one feature. However,
it also means its now possible to disable a facet or backend at the
element level. And worse, the binding point is global:

: #DOGEN masd.variability.default_binding_point=any
: #DOGEN masd.variability.generate_static_configuration=false
: #DOGEN masd.variability.instantiation_domain_name=masd

The right thing to do is to create four separate features, one for
the backend, one for the features and one for the archetype
(global). Then another one for the archetype, locally. Each with the
correct binding point.

*** Add PMM enablement transform                                      :story:

This transform reads the global enablement flags for backend, facet
and archetype. It is done as part of the chain to produce the PMM.

*** Add a PMM enablement satisfiability transform                     :story:

For now this transform can simply check that there are no enabled
archetypes that depend on disabled archetypes. In the future we could
have a flag that enables archetypes as required.

*** Add =is_generatable= to logical model                             :story:

Logical types which cannot be generated should be removed prior to
physical expansion. There are two types:

- intrinsically non-generatable types such as object templates, etc.
- types that may not be generated depending on state: modules.

In the future, when we support the static / dynamic pattern,

Tasks:

- add a generatable flag in logical model elements with associated
  transform.
- add a pruning transform that filters out all non-generatable types
  from logical model.

Merged stories:

*Intrinsic non-generatable types

In the decoration transform we have this hack:

: bool decoration_transform::
: is_generatable(const assets::meta_model::name& meta_name) {
:     // FIXME: massive hack for now.
:     using mnf = assets::helpers::meta_name_factory;
:     static const auto otn(mnf::make_object_template_name());
:     static const auto ln(mnf::make_licence_name());
:     static const auto mln(mnf::make_modeline_name());
:     static const auto mgn(mnf::make_modeline_group_name());
:     static const auto gmn(mnf::make_generation_marker_name());
:
:     const auto id(meta_name.qualified().dot());
:     return
:         id != otn.qualified().dot() &&
:         id != ln.qualified().dot() &&
:         id != mln.qualified().dot() &&
:         id != mgn.qualified().dot() &&
:         id != gmn.qualified().dot();
: }

This is done because we know up front that some elements in the assets
meta-model cannot be generated. We need a way to tag this elements
statically. This should be done when the elements are code
generated. It is not yet clear how this should be done though.

Notes:

- one possible approach is to have a constant that is code generated
  which states if a type is meant for generation or not.
- however, it would be even better if we could determine if a type has
  formatters or not. This would mean we would cover two possible
  scenarios: types that are intrinsically non-generatable and types
  that are not yet generatable. It may be that there is no need to
  distinguish between these two.
- when we have meta-model elements for logical meta-elements we just
  need to add this as a property (e.g. generatable). If a user tries
  to add a formatter to a non-generatable type we error.

*** Create a physical ID in logical-physical space                    :story:

Artefacts are points in logical-physical space. They should have an ID
which is composed by both logical and physical location. We could
create a very simple builder that concatenates both, for example:

: <dogen><variability><entities><default_value_override>|<masd><cpp><types><class_header>

The use of =|= would make it really easy to split out IDs as required,
and to visually figure out which part is which. Note though that the
ID is an opaque identifier and the splitting happens for
troubleshooting purposes only, not in the code. With the physical
model, all references are done using these IDs. So for example, if an
artefact =a0= depends on artefact =a1=, the dependency is recorded as
the ID of =a1=. The physical model should also be indexed by ID
instead of being a list of artefacts.

*** Make physical model name a qualified name                         :story:

At present we are setting up the extraction model name from the simple
name of the model. It should really be the qualified name. Hopefully
this will only affect tracing and diffing.

*** Add enablement test in C#                                         :story:

At present we have probably broken enablement in C# due to the hackery
around physical space expansion. However all tests are green. We need
to define a profile in C# that disables a facet in order to ensure we
test enablement before we start hacking around with the enablement
transforms. It will most likely be red - we need to add the pruning
hack to get rid of disabled artefacts as we do in C++.

*** Add dependencies to artefacts                                     :story:

We need to propagate the dependencies between logical model elements
into the physical model. We still need to distinguish between "types"
of dependencies:

- transparent_associations
- opaque_associations
- associative_container_keys
- parents

Basically, anything which we refer to when we are building the
dependencies for inclusion needs to be represented. We could create a
data structure for this purpose such as "dependencies". We should also
include "namespace" dependencies. These can be obtained by =sort |
uniq= of all of the namespaces for which there are dependencies. These
are then used for C#.

Note however that all dependencies are recorded as logical-physical
IDs.

We also need a way to populate the dependencies as a transform. This
must be done in =m2t= because we need the formatters. We can rely on
the same approach as =inclusion_dependencies= but instead of creating
/inclusion dependencies/, we are just creating /dependencies/.

*** Consider allowing representation of namespaces in file names      :story:

Languages like .Net represent namespacing using dots rather than
separate folders. Perhaps we should support a mode of operation where
all files are placed in a single folder but have the namespacing
encoded in the file name. For example:

: /a_project/types/a.cpp
: /a_project/io/a_io.cpp

would become:

: /a_project/types_a.cpp
: /a_project/io_a_io.cpp

or, using dot notation, so we can distinguish namespaces from
"composite" names:

: /a_project/types.a.cpp
: /a_project/io.a_io.cpp

We do not have a use case for this yet, but it should be fairly
straight forward to add it. We just need meta-data support to enable
the feature and then take it into account when generating the file
names (e.g. instead of using =/= as a separator, use =.=).

Actually this is _almost_ already possible: we provide a facet folder
meta-data that is always used to generate a new folder. If however
there was a way for it to not generate a folder we could achieve
this. For example, say we had to supply:

: /types/

as the facet folder. Then the user could simply supply instead:

: types_
: types.

And no folder would be created.

Notes:

- see also the story on destinations.
- consider splitting this story into two: one is about how folder
  layout (physical) may need to match namespace layout (logical);
  another is related to allowing users to flatten facet
  directories. They have some connection, but its not obvious how much
  they overlap.

*** Add a PM enablement and overwrite transform                       :story:

This relies on PMM enablement flags. Also, it reads the local
archetype enablement and overwrite flags and has the logic to set it
as per current enablement transform.

Once this transform is implemented, we should try disabling the
existing enablement transform and see what breaks.

*** Add a PM enablement satisfiability transform                      :story:

To start with, this should just check to see if any of the
dependencies are disabled. If so it throws. In the future we can add
solving.

*** Add a PM transform to prune disabled artefacts                    :story:

We must first start by expanding the physical space into all possible
points. Once enablement is performed though we can prune all artefacts
that are disabled. Note that we cannot prune based on global
information because archetypes may be enabled locally. However, once
all of the local information has been processed and the enabled flag
has been set, we can then remove all of those with the flag set to
false.

In a world with solving, we just need to make sure solving is slotted
in after enablement and before pruning. It should just work.

This transform is done within the =m2t= model, not the =physical=
model, because we need to remove the artefacts from the =m2t=
collection.

*** Implement formatting styles in physical model                     :story:

We need to move the types related to formatting styles into physical
model, and transfors as well. WE should also address formatting input.

Merged stories:

*Move formatting styles into generation*

We need to support the formatting styles at the meta-model level.

*Replace all formatting styles with the ones in physical model*

We still have a number of copies of this enumeration.

*** Implement locator in physical model                               :story:

Use PMM entities to generate artefact paths, within =m2t=.

Merged stories:

*Create a archetypes locator*

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present;
they should be a transform inside generation.

*Create the notion of project destinations*

At present we have conflated the notion of a facet, which is a logical
concept, with the notion of the folders in which files are placed - a
physical concept. We started thinking about addressing this problem by
adding the "intra-backend segment properties", but as the name
indicates, we were not thinking about this the right way. In truth,
what we really need is to map facets (better: archetype locations) to
"destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly
  if this is done at the element level. That is, you can easily define
  a global mapping for all formatters, and then override it
  locally. This solves the long standing problem of creating say types
  in tests and so forth. With this approach you can create anything
  anywhere.
- we need to have some tests that ensure we don't end up with multiple
  files with the same name at the same destination. This is a
  particular problem for CMake. One alternative is to allow the
  merging of CMake files, but we don't yet have a use case for
  this. The solution would be to have a "merged file flag" and then
  disable all other facets.
- this will work very nicely with profiles: we can create a few out of
  the box profiles for users such as flat project, common facets and
  so on. Users can simply apply the stereotype to their models. These
  are akin to "destination themes". However, we will also need some
  kind of "variable replacement" so we can support cases like
  =include/masd.cpp_ref_impl.northwind/types=. In fact, we also have
  the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder
  structure that depends on package names etc or choose to flatten it
  and can do so for one or all facets. This means for example that we
  could use nested folders for =include=, not use model modules for
  =src= and then flatten it all for =tests=.
- actually it is a bit of a mistake to think of these destinations as
  purely physical. In reality, we may also need them to contribute to
  namespaces. For example, in java the folders and namespaces must
  match. We could solve this by having a "module contribution" in the
  destination. These would then be used to construct the namespace for
  a given facet. Look for java story on backlog for this.
- this also addresses the issue of having multiple serialisation
  formats and choosing one, but having sensible folder names. For
  example, we could have boost serialisation mapped to a destination
  called =serialisation=. Or we could map it to say RapidJSON
  serialisation. Or we could support two methods of serialisation for
  the same project. The user chooses where to place them.

*** Implement dependencies in terms of new physical types             :story:

- add dependency types to physical model.
- add dependency types to logical model, as required.
- compute dependencies in generation. We need a way to express
  dependencies as a file dependency as well as a model
  dependency. This caters for both C++ and C#/Java.
- remove dependency code from C++ and C# model.

Notes:

- in light of the new physical model, we need a transform that calls
  the formatter to obtain dependencies. The right way to do this is to
  have another registrar (=dependencies_transform=?) and to have the
  formatters implement both interfaces. This means we can simply not
  implement the interface (and not register) when we have no
  dependencies - though of course given the existing wale
  infrastructure, we will then need yet another template for
  formatters which do not need d

Merged stories:

*Formatter dependencies and model processing*

At present we are manually adding the includes required by a formatter
as part of the "inclusion_dependencies" building. There are several
disadvantages to this approach:

- we are quite far down the pipeline. We've already passed all the
  model building checks, etc. Thus, there is no way of knowing what
  the formatter dependencies are. At present this is not a huge
  problem because we have so few formatters and their dependencies are
  mainly on the standard library and a few core boost models. However,
  as we add more formatters this will become a bigger problem. For
  example, we've added formatters now that require access to
  variability headers; in an ideal world, we should now need to have a
  reference to this model (for example, so that when we integrate
  package management we get the right dependencies, etc).
- we are hard-coding the header files. At present this is not a big
  problem. To be honest, we can't see when this would be a big
  problem, short of models changing their file names and/or
  locations. Nonetheless, it seems "unclean" to depend on the header
  file directly.
- the dependency is on c++ code rather than expressed via a model.

In an ideal world, we would have some kind of way of declaring a
formatter meta-model element, with a set of dependencies declared via
meta-data. These are on the model itself. They must be declared
against a specific archetype. We then would process these as part of
resolution. We would then map the header files as part of the existing
machinery for header files.

However one problem with this approach is that we are generating the
formatter code using stitch at present. For this to work we would need
to inject a fragment of code into the stitch template somehow with the
dependencies. Whilst this is not exactly ideal, the advantage is that
we could piggy-back on this mechanism to inject the postfix fields as
well, so that we don't need to define these manually in each
model. However, this needs some thinking because the complexity of
defining a formatter will increase yet again. When there are problems,
it will be hard to troubleshoot.

*Move dependencies into archetypes*

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*Tidy-up of inclusion terminology*

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Merge C++ and C# model into =m2t=                                 :story:

Once we remove all of formatables and helpers from each technical
space and once we remove all of the transforms in =m2t= that don't
really belong there, we can probably merge all of these models into
one. We would then have a =transforms= namespace, with sub-namespaces
per language. Each of the namespaces is declared as a backend.

*** Top-level "inclusion required" should be "tribool"                :story:

One of the most common use cases for inclusion required is to have it
set to true for all types where we provide an override, but false for
all other cases. This makes sense in terms of use cases:

- either we need to supply some includes; in which case where we do
  not supply includes we do not want the system to automatically
  compute include paths;
- or we don't supply any includes, in which case:
  - we either don't require any includes at all (hardware built-ins);
  - or we want all includes to be computed by the system.

The problem is that we do not have a way to express this logic in the
meta-data. The only way would be to convert the top-level
=requires_includes= to an enumeration:

- yes, compute them
- yes, where supplied
- no

We need to figure out how to implement this. For now we are manually
adding flags.

*** Add the notion of a major and a minor technical space             :story:

When we move visual studio and other elements out of the current
technical spaces, we will need some way of distinguishing between a
"primary" technical space (e.g. C++, C# etc) and a "secondary"
technical space (e.g. visual studio, etc). We could use emacs'
convention and call these major and minor technical spaces.

This should be a property of the backend.

*** Move decorations to their "final" resting place                   :story:

At present we are handling decorations in the generation model but
these are really logical concerns. The main reason why is because we
are not expanding the decoration across physical space, but instead we
expand them depending on the used technical spaces. However, since the
technical spaces are obtained from the formatters, there is an
argument to say that archetypes should have an associated technical
space. We need to decouple these concepts in order to figure out where
they belong.

*** Create a common formatter interface                               :story:

Once all language specific properties have been moved into their
rightful places, we should be able to define a formatter interface
that is suitable for both c++ and c# in generation. We should then
also be able to move all of the registration code into generation. We
then need to look at all containers of formatters etc to see what
should be done at generation level.

Once we have a common formatter interface, we can add the formatters
themselves to the =element_artefacts= tuple. Then we can just iterate
through the tuples and call the formatter instead having to do
look-ups.

Also, at this point we can then update the physical elements generated
code to generate the transform code for backend and facet
(e.g. delegation and aggregation of the result).

*** Stitch formatter updates                                          :story:

There are a number of issues with stitch formatters at present:

- stitch transform is still generating its own artefact.

*** Order of headers is hard-coded                                    :story:

In inclusion expander, we have hacked the sorting:

:        // FIXME: hacks for headers that must be last
:        const bool lhs_is_gregorian(
:            lhs.find_first_of(boost_serialization_gregorian) != npos);
:        const bool rhs_is_gregorian(
:            rhs.find_first_of(boost_serialization_gregorian) != npos);
:        if (lhs_is_gregorian && !rhs_is_gregorian)
:            return true;

This could be handled via meta-data, supplying some kind of flag (sort
last?). We should try to generate the code in the "natural order" and
see if the code compiles with latest boost.

*** Move technical space and generability transforms                  :story:

At present these transforms are in generation, but we don't think
that's the right place. We need some analysis to understand what they
do and why they are not in the logical model.

*** Consider bucketing elements by meta-type in generation model      :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

This can be done in the generation model, as part of the generation
clean up.

*** Dimension vs view vs perspective                                  :story:

We need to find the definition for how these terms are used within
UML and see which one is more appropriate for MASD.

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with using shared objects, this
should improve build times.

In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

More thoughts on this:

- we don't really need to have different directories for this; we
  could just put all the include files in the same directory. At
  packaging time, we should only package the public files (this would
  have to be done using CPack).
- also the GCC/MSVC visibility pragmas should take into account these
  options and only export public types.
- the slight problem with this is that we need some tests to ensure
  the packages we create are actually exporting all public types; we
  could easily have a public type that depends on a private type
  etc. We should also validate yarn to ensure this does not
  happen. This can be done by ensuring that a type marked as external
  only depends on types also marked as external and so forth.
- this could also just be a packaging artefact - we would only package
  public headers. Layout of source code would remain the same.
- when module support is available, we could use this to determine
  what is exported on the module interfaces.

*** Replace traits with calls to the PMM elements                     :story:

Where we are using these traits classes, we should really be including
the formatter and calling for its static name - at least within each
backend.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

** Deprecated
