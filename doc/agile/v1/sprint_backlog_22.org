#+title: Sprint Backlog 22
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Start the variability model refactor, removing all direct
  dependencies to the physical model.
- Get a good grasp of the physical domain model.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2020-03-11 Wed 12:24]
| <75>                                                    |         |       |      |       |
| Headline                                                | Time    |       |      |     % |
|---------------------------------------------------------+---------+-------+------+-------|
| *Total time*                                            | *46:09* |       |      | 100.0 |
|---------------------------------------------------------+---------+-------+------+-------|
| Stories                                                 | 46:09   |       |      | 100.0 |
| Active                                                  |         | 46:09 |      | 100.0 |
| Edit release notes for previous sprint                  |         |       | 4:11 |   9.1 |
| Create a demo and presentation for previous sprint      |         |       | 0:25 |   0.9 |
| Sprint and product backlog grooming                     |         |       | 5:40 |  12.3 |
| Analysis on defining a combined logical-physical space  |         |       | 5:55 |  12.8 |
| Analysis on removing physical references in variability |         |       | 5:27 |  11.8 |
| Remove =rapidjson= formatters                           |         |       | 0:03 |   0.1 |
| Separate feature templates from bundles                 |         |       | 9:28 |  20.5 |
| Move enabled to generation                              |         |       | 0:04 |   0.1 |
| Create a chain to encapsulate variability transforms    |         |       | 0:37 |   1.3 |
| Move variability chain into assets                      |         |       | 2:22 |   5.1 |
| Move profile meta-data into profile element             |         |       | 7:12 |  15.6 |
| Split profiles and profile templates in variability     |         |       | 4:02 |   8.7 |
| Add location and modules to variability                 |         |       | 0:43 |   1.6 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2020-02-18 Tue 20:35]
    :LOGBOOK:
    CLOCK: [2020-02-18 Tue 20:30]--[2020-02-18 Tue 20:44] =>  0:14
    CLOCK: [2020-02-18 Tue 19:04]--[2020-02-18 Tue 19:34] =>  0:30
    CLOCK: [2020-02-18 Tue 18:02]--[2020-02-18 Tue 18:37] =>  0:35
    CLOCK: [2020-02-17 Mon 23:16]--[2020-02-18 Tue 00:23] =>  1:07
    CLOCK: [2020-02-17 Mon 22:44]--[2020-02-17 Mon 23:15] =>  0:31
    CLOCK: [2020-02-17 Mon 20:00]--[2020-02-17 Mon 20:10] =>  0:10
    CLOCK: [2020-02-17 Mon 19:51]--[2020-02-17 Mon 19:59] =>  0:08
    CLOCK: [2020-02-17 Mon 19:02]--[2020-02-17 Mon 19:58] =>  0:56
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.21, "Nossa Senhora do Rosário"

#+BEGIN_SRC markdown
![Igreja de Nossa Senhora do Rosário](
https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Church_in_Tombua%2C_Namibe%2C_Angola.JPG/1280px-Church_in_Tombua%2C_Namibe%2C_Angola.JPG)
_Igreja de Nossa Senhora do Rosário, Tombwa, Namibe, Angola. (C) 2010 Paulo César Santos._

# Introduction

Very much like an iceberg, this sprint was deceptively small on user features but big on internal changes: after several sprints of desperate chasing, we finally completed the mythical "fabric refactor". The coding work was not exactly glamorous, as we engaged on a frontal attack on all "quasi-meta-types" we had previously scattered across the codebase. One by one, each type was polished and moved into the assets meta-model, to be reborn anew as a fully-fledged modeling element. All the while, we tried to avoid breaking the world - but nevertheless did so, frequently. It was grueling work. Having said that, the end of the refactor made for a very exciting sprint, and though the war remains long, we can't help but feel an important battle was won.

So let's have a look at how it all went down.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. All features this sprint are related to the addition of new meta-model types, which resulted in a number of breaking changes. These we have highlighted with :warning:.

[![Sprint 1.0.21 Demo](https://img.youtube.com/vi/J5duq-gw-nI/0.jpg)](https://youtu.be/J5duq-gw-nI)
_Video 1: Sprint 21 Demo._

## New meta-model elements

As we explored the lay of the land of our problem domain, we inadvertently found ourselves allowing Dogen to evolve a "special" set of meta-types. These we used to model files deemed inferior in stature to _real source code_: mostly build-related material, but also some more "regular" source code which could be derived from existing elements - _e.g._ visitors, serialisation registrars and the like. Due to its second-class-citizen nature, these  "special types" were controlled via variability in haphazard ways. Over the years, a plethora of meta-data switches was introduced at the model level but, in the absence of a coherent overall plan, these were _ad-hoc_ and inconsistent. On the main, the switches were used to enable or disable the emission of these "special types", as well as to configure some of their properties. Table 1 provides a listing of these switches.

|Meta-data key|Description|
|-------------------------------------------------------------------------------------|---------------------------------------------------------|
|```masd.generation.cpp.cmake.enabled```|Enable the CMake facet.|
|```masd.generation.cpp.cmake.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.cmake.source_cmakelists.enabled```|Enable the CMakeLists file in ```src``` directory.|
|```masd.generation.cpp.cmake.source_cmakelists.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.cmake.include_cmakelists.enabled```|Enable the CMakeLists file in ```include``` directory.|
|```masd.generation.cpp.cmake.include_cmakelists.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.msbuild.enabled```|Enable the MSBuild facet.|
|```masd.generation.cpp.msbuild.targets.enabled```|Enable the MSBuild formatter for ODB targets.|
|```masd.generation.cpp.msbuild.targets.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.enabled```|Enable the Visual Studio facet.|
|```masd.generation.cpp.visual_studio.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.project_solution_guid```|GUID for the Visual studio solution.|
|```masd.generation.cpp.visual_studio.project_guid```|GUID for the Visual studio project.|
|```masd.generation.cpp.visual_studio.solution.enabled```|Enables a Visual Studio solution for C++.|
|```masd.generation.cpp.visual_studio.solution.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.project.enabled```|Enables a Visual Studio solution for C++.|
|```masd.generation.cpp.visual_studio.project.postfix```|Postfix to use for filename.|
|```masd.generation.csharp.visual_studio.project_solution_guid```|GUID for the Visual studio solution.|
|```masd.generation.csharp.visual_studio.project_guid```|GUID for the Visual studio project.|
|```masd.generation.csharp.visual_studio.solution.enabled```|Enables a Visual Studio solution for C#.|
|```masd.generation.csharp.visual_studio.solution.postfix```|Postfix to use for filename.|
|```masd.generation.csharp.visual_studio.project.enabled```|Enables a Visual Studio project for C#.|
|```masd.generation.csharp.visual_studio.project.postfix```|Postfix to use for filename.|

_Table 1: Meta-data switches related to "special" types._

The meta-data was then latched on to model properties, like so:

```
#DOGEN masd.generation.cpp.msbuild.enabled=true
#DOGEN masd.generation.csharp.visual_studio.project_guid=9E645ACD-C04A-4734-AB23-C3FCC0F7981B
#DOGEN masd.generation.csharp.visual_studio.project_solution_guid=FAE04EC0-301F-11D3-BF4B-00C04F79EFBC
#DOGEN masd.generation.cpp.cmake.enabled=true
...
```

As we continued to mull over the problem across sprints, the entire idea of "implicit" element types - injected into the model and treated differently from regular elements - was [ultimately understood to be harmful](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_11.org#analyse-the-state-of-the-mess-of-refactors). The approach is in fact an _abuse_ of variability, due to how these elements had been (_mis-_)modeled. And it had consequences:

- **Invisibility**: it was not possible to manage variability of the injected types in the same fashion as for all other elements because they were "invisible" to the modeler.
- **Difficulty in troubleshooting**: it was hard to diagnose when something didn't work as expected, because all of the magic was internal to the code generator.
- **Inconsistency in generation**: we had a rather inconsistent way of handling different element types; some "just appeared" due to the state of the model (like ```registrar```); others were a consequence of enabling formatters (_e.g._ ```CMakeLists.txt```); still others required the presence of stereotypes (_e.g._ ```visitor```). It was very hard to explain the rationale for each of these to an unsuspecting user.
- **Inconsistency in population**: properties that were common to other elements had to be handled specially via meta-data. For example, adding comments or changing decoration for these elements required bespoke meta-data and associated transforms, even though we already had a pipeline which operated on "regular" elements.
- **Inconsistencies in facet spaces**: the types did not follow the existing facet conventions - _i.e._, to be placed on a folder named after the facet, _etc_. Even in that they were "special".

Programming is nothing if not a quest for the generalisation and removal of special cases, and these types had been a major thorn in the design. Thus the idea of refactoring fabric out of existence was born. With this release we finally removed all of the above meta-data keys, and replaced them with regular meta-model elements, instantiable via the appropriate stereotypes (Table 2). Sadly, a single use case was left, due to the specificity of its implementation: visitors. These shall be addressed on a future release.

> :warning: **Breaking change**: Users need to update any models which make use of the meta-data in Table 1 and replace them with the corresponding elements and stereotypes.

|Stereotype|Description|
|--------------|---------------|
|```masd::serialization::type_registrar```|The serialisation type registrar used mainly for boost serialisation support.|
|```masd::visual_studio::solution```|Visual Studio solution support.|
|```masd::visual_studio::project```|Visual Studio solution support.|
|```masd::entry_point```| Provides an entry point to a component, _e.g._ ```main```.|
|```masd::orm::common_odb_options```|Element modeling the common arguments for ODB.|
|```masd::visual_studio::msbuild_targets```|Element modeling ODB targets using MSBuild.|
|```masd::build::cmakelists```|Element modeling build files using CMake.|
|```masd::assistant```|C# helper type.|

_Table 2: Stereotypes for the new meta-model elements ._

Now, the observant reader won't fail to notice that _the generated code has not changed_ in any way - well, at least not intentionally. All of these new meta-model elements already existed, but in their previous incantation variability was used to trigger them (mostly). With this release they are modeled as proper meta-model elements, controlled by the user, and processed in the exact same way as all other elements. This means we can make use of all of the existing machinery in Dogen such as profiles.

![Use of new meta-elements in C++](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cpp_new_meta_elements.png)
_Figure 1: Use of new meta-elements in a C++ model._

Whilst this is a big improvement in usability, there are still a number of pitfalls:

- users now need to remember to add types where Dogen used to inject them automatically. This is the case with ```registrar```, which was generated automatically when a model made use of inheritance.
- there are no errors or warnings when a diagram is on an inconsistent state due to the choice of elements used. For example, one can add a solution without a project.

![Use of new meta-elements in C#](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/csharp_new_meta_elements.png)
_Figure 2: Use of new meta-elements in a C# model._

These are problems that will hopefully be looked into once we eventually reach the validation work, in a few sprints time.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_21.org).

## Significant Internal Stories

The sprint was mostly dominated by a number of stories dealing with moving fabric types, but since they have user visible consequences, they have been dealt with in _User visible changes_. The only other story of note is described below.

### Remove support for element extensions

In the past we created another "special" concept: element extensions. These allowed two meta-model elements to share the same position in modeling space, _i.e._ two elements sharing the same name. Whilst this may sound crazy on first sight, the initial idea behind it was more or less sound. Files such as forward declarations in C++, or ODB options, were better modeled when using "lightweight" meta-model elements which provided the specific data needed. In order for this to work, we needed to have some kind of way of containing meta-elements within meta-elements, and thus "element extensions" were born. As with fabric types, element extensions did not stood the test of time and added a lot of complexity and special cases. Now that the last fabric types that made use of element extensions were removed, we managed to remove the extensions themselves from the meta-model, greatly simplifying things.

## Resourcing

This sprint was a "model" sprint (if you pardon the pun) in terms of Dogen development. At an overall elapsed time of four weeks, our utilisation rate improved significantly from 23% to an amazing 56%. In other words, we managed to maintain a steady pace and clocked around 20 hours every week. Furthermore, a staggering _75.5%_ of the overall ask was spent on stories directly related to the sprint's mission of refactoring fabric. This is quite possibly the highest in Dogen's eight-year history, as far as I can recall. We spent 19.6% on process related activities, which whilst not the smallest amount ever, its also in line with recent sprints - particularly when we have video recording activities. The remainder of the sprint was used chasing minor spikes such as problems with the setup (1.4%), errors in tests (0.8%), issues with coveralls (0.3%) and so on. Overall, from a resource management perspective, this was a very successful sprint.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_pie_chart.jpg)
_Figure 3: Cost of stories for sprint 21._

## Roadmap

Two very minor changes were made to the road map this sprint. First and foremost, we finally removed the fabric refactor from the roadmap, which is extremely pleasing. Secondly, we bumped up resource usage by a fair (if somewhat random) amount, which projected timescales in time somewhat, in a more realistic manner. How realistic is up to debate, but at least it is hopefully slightly less wrong.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_resource_allocation_graph.png)

# Next Sprint

The next great big refactoring battle is with the generation model. We need to move all concepts that had been incorrectly placed in generation to the meta-model, and, with it, reduce the huge code duplication we have between backends.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.21_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.21/dogen_1.0.21_amd64-applications.deb)
- [dogen-1.0.21-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.21/DOGEN-1.0.21-Darwin-x86_64.dmg)
- [dogen-1.0.21-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.21-Windows-AMD64.msi)

**Note:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+END_SRC markdown

- [[https://twitter.com/MarcoCraveiro/status/1229849866416816129][twitter]]
- [[https://www.linkedin.com/posts/marco-craveiro-31558919_masd-projectdogen-activity-6635632094846476289-oXZM][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    CLOSED: [2020-02-18 Tue 19:03]
    :LOGBOOK:
    CLOCK: [2020-02-18 Tue 18:38]--[2020-02-18 Tue 19:03] =>  0:25
    :END:

Time spent creating the demo and presentation. Use the demo project:

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2020-03-11 Wed 08:40]--[2020-03-11 Wed 08:50] =>  0:10
    CLOCK: [2020-03-11 Wed 08:14]--[2020-03-11 Wed 08:39] =>  0:25
    CLOCK: [2020-03-10 Tue 21:40]--[2020-03-10 Tue 21:49] =>  0:09
    CLOCK: [2020-03-09 Mon 16:08]--[2020-03-09 Mon 16:39] =>  0:31
    CLOCK: [2020-03-05 Thu 14:48]--[2020-03-05 Thu 14:53] =>  0:05
    CLOCK: [2020-03-05 Thu 14:20]--[2020-03-05 Thu 14:47] =>  0:27
    CLOCK: [2020-03-05 Thu 14:04]--[2020-03-05 Thu 14:19] =>  0:15
    CLOCK: [2020-03-05 Thu 10:50]--[2020-03-05 Thu 11:13] =>  0:23
    CLOCK: [2020-03-03 Tue 12:37]--[2020-03-03 Tue 12:51] =>  0:14
    CLOCK: [2020-03-03 Tue 11:01]--[2020-03-03 Tue 11:12] =>  0:11
    CLOCK: [2020-03-02 Mon 17:24]--[2020-03-02 Mon 18:02] =>  0:38
    CLOCK: [2020-03-02 Mon 15:20]--[2020-03-02 Mon 15:40] =>  0:20
    CLOCK: [2020-02-28 Fri 17:21]--[2020-02-28 Fri 18:10] =>  0:49
    CLOCK: [2020-02-17 Mon 19:31]--[2020-02-17 Mon 19:50] =>  0:19
    CLOCK: [2020-02-17 Mon 17:43]--[2020-02-17 Mon 18:27] =>  0:44
    :END:

Updates to sprint and product backlog.

*** COMPLETED Adding reference to itself results in resolution errors :story:
    CLOSED: [2020-02-17 Mon 18:17]

*Rationale*: this was fixed a few sprints ago.

Whilst trying to fix the JSON models we inadvertently added a
self-reference in =dogen.generation.json=:

:    "yarn.reference": "dogen.generation.json",

This resulted in some puzzling errors:

: 2018-10-18 19:15:00.861210 [ERROR] [yarn.transforms.enablement_transform] Duplicate element archetype: quilt.cpp.serialization.registrar_implementation <dogen><generation><registrar>

Ideally we should either warn and ignore or fail to process models
with self-references.

*** COMPLETED Registrar assumes references have serialisation enabled :story:
    CLOSED: [2020-03-02 Mon 15:30]

*Rationale*: this problem was addressed with the new implementation of
registrar, which only adds registrars if the user created them
manually.

At present we are assuming that all references that are nom-proxy
references have serialisation enabled. This is a problem because:

- we are now disabling serialisation where possible unless we need it
- as we move from the data directory into real models we will have a
  number of models that will not require generation and so will not
  have a registrar.

We need to figure out a way to obtain an enablement map of referenced
types. In theory we already have this because we do not add includes
when a facet is off. However, something is not working quite right
with registrar because we are including this file. Sample diff:

: diff -u src/serialization/registrar_ser.cpp src/serialization/registrar_ser.cpp
: Reason: Changed generated file.
: ---  src/serialization/registrar_ser.cpp
: +++  src/serialization/registrar_ser.cpp
: @@ -26,6 +26,7 @@
:  #include <boost/archive/binary_oarchive.hpp>
:  #include <boost/archive/polymorphic_iarchive.hpp>
:  #include <boost/archive/polymorphic_oarchive.hpp>
: +#include "masd/serialization/registrar_ser.hpp"
:  #include "masd.cpp_ref_impl.two_layers_with_objects/serialization/registrar_ser.hpp"
:
:  namespace masd::cpp_ref_impl::two_layers_with_objects {
: @@ -32,18 +33,19 @@
:
:  template<typename Archive>
: -void register_types(Archive&) {
: +void register_types(Archive& ar) {
: +    masd::register_types(ar);
:  }
:

For now we did a quick hack to solve this problem and marked the MASD
model as proxy:

:    "masd.injection.is_proxy_model": true,

*** COMPLETED Analysis on defining a combined logical-physical space  :story:
    CLOSED: [2020-03-03 Tue 12:43]
    :LOGBOOK:
    CLOCK: [2020-03-03 Tue 11:13]--[2020-03-03 Tue 12:36] =>  1:23
    CLOCK: [2020-03-02 Mon 15:41]--[2020-03-02 Mon 15:59] =>  0:18
    CLOCK: [2020-03-02 Mon 13:25]--[2020-03-02 Mon 15:19] =>  1:54
    CLOCK: [2020-02-19 Wed 18:01]--[2020-02-19 Wed 19:01] =>  1:00
    CLOCK: [2020-02-19 Wed 08:02]--[2020-02-19 Wed 08:54] =>  0:52
    CLOCK: [2020-02-19 Wed 07:02]--[2020-02-19 Wed 07:30] =>  0:28
    :END:

*Rationale*: we understand the problem well enough to start moving
forwards a bit with coding. This is too complex to design it all up
front.

It now seems that we have been searching for a meta-model that
combines both aspects of logical modeling as well as physical
modeling. Facets, archetypes etc are all parts of the physical
dimension of this space. We need to find all stories on this topic and
organise them to see if we can come up with a consistent system of
meaning.

Notes:

- archetypes must support a notion of "kind". This is so we can have
  public include headers, private include headers and implementation
  files. This "kind" affects the topology of the physical dimension.
- the locator is a function that takes points in the logical-physical
  space and maps them to filesystem locations. It uses properties of
  those meta-model elements to configure the mapping.
- actually, the separation of technical spaces and backends is
  somewhat artificial. In reality, if we were to clean up all backends
  such that they only contain a single technical space then we
  wouldn't have this distinction. However, there are problems with
  this approach. Some features span across multiple technical spaces,
  such as ODB. It requires:

  - c++ support in generating the pragmas,
  - ODB options files.
  - msbuild for odb targets
  - cmake for odb targets

  It would be tempting to say that ODB is not a technical space, but
  just a feature. In which case we need options files to be a
  technical space not solely connected to ODB. This is possible,
  provided we can find evidence of other systems using options
  files. If we could generalise this then the problem would be
  solved. However, it is not yet clear if ODB is a special case or an
  indicator of a pattern which we are ignoring.
- in this world, we would have a top-level =techspace= model,
  equivalent to generation at present. It would be responsible for
  knowing about all available technical spaces. Component models would
  have one or more representations. A representation can have one
  primary technical space and zero or many secondary technical
  spaces.
- input and output technical spaces are modeling errors. In reality,
  models have types: they are either PIMs or PSMs. If a model is a
  PIM, he must only refer to other PIMs. However, an additional
  wrinkle is that in order to load the mappings, we need to have
  access to the references (as these contain the mappings). Perhaps we
  can allow any reference, but then when resolving, we need to ensure
  that the types are all consistent.
- perhaps we are looking at this in the wrong way. In reality, there
  are only the following permutations:

  - if a model has a single representation, then either a) the input
    technical space is the same as the output (e.g. PSM) or b) its a
    PIM in which case we need to perform the mapping.
  - if a model has more than one representation, then it must be a
    PIM.

  If a model had a way to declare itself PIM, then in resolver we
  could ensure that all types are referencing only other PIM
  types. However, it would still be possible for a C++ model to
  reference a C# model. For this validation to take place, we would
  need a way to associate a technical space to an element and then
  check that on reference resolution. Actually, if we ensure we map
  before we resolve (which we probably already do) then we can rely on
  the fact that only PSM types will exist. If we had a way of knowing
  which types in a PIM need mapping, then we could detect which ones
  did not map. Then we could issue a mapping error. This way the world
  would be cleanly divided between PIM and PSM, and we could ignore
  technical spaces for PIMs. We cannot know at mapping time
- one aspect that is not very clean is that we should only allow more
  than one representation on a model prior to mapping. After mapping,
  there can only be one representation (the technical space we have
  mapped to).

Merged stories:

*Create meta-model elements for location*

We need to factor out all meta-model elements we have scattered around
the generation models which model concepts related to physical
locations, and move them into assets. We then need to create classes
to instantiate these model elements as part of kernel registration, as
well as the associated overrides. Finally, we need a way to compute
paths using these new meta-model elements.

Notes:

- at present, we are relying on archetype location in the variability
  model. The main reason why is template expansion. We have a small
  number of features that are templated, and need to expand across
  physical space (e.g. for each facet, for each archetype, for each
  kernel etc). These we inject into variability by reading them from
  the backend. Thing is, variability is not really connected directly
  to the physical space. That is to say, these archetype locations are
  not points in variability space where we find these features - just
  like when we are using features in the logical model, we are not
  stating that points in logical space "have" features. Instead, we
  have points in feature space that happen to be mapped to points in
  other spaces. We need a clear cut separation between variability
  space and all other spaces to avoid confusion. We could say that
  variability space is hierarchical, and features can live at
  "levels". These levels can then be mapped to the hierarchy of
  physical space or modeling space as required.
- up to know we have assumed that physical space was somehow connected
  to logical space. An alternative way of looking at this is to see
  them as completely separate dimensions. It just so happens that on
  very few occasions, we need to refer to physical concepts in logical
  space, but this is just an implementation detail and should be kept
  to the minimum.
- when we finally enter generation, we require points in the
  logical-physical space in order to resolve them to concrete
  artefacts

*** COMPLETED Analysis on removing physical references in variability :story:
    CLOSED: [2020-03-05 Thu 14:19]
    :LOGBOOK:
    CLOCK: [2020-03-05 Thu 11:24]--[2020-03-05 Thu 13:03] =>  1:39
    CLOCK: [2020-03-04 Wed 17:00]--[2020-03-04 Wed 17:44] =>  0:44
    CLOCK: [2020-03-04 Wed 10:41]--[2020-03-04 Wed 12:38] =>  1:57
    CLOCK: [2020-03-03 Tue 17:26]--[2020-03-03 Tue 17:47] =>  0:21
    CLOCK: [2020-03-03 Tue 15:15]--[2020-03-03 Tue 16:01] =>  0:46
    :END:

By the end of this story there should be no dependencies between the
archetype location model and the variability model.

Notes:

- in reality we only need the archetype location in order to perform
  the template expansion. Which leads to the obvious conclusion that
  we probably should have a type to be used as input for that, and not
  associated with every instance.
- since all we need is a three-tiered hierarchy, we could use the
  terminology of group, subgroup and element, or even set, subset and
  element - both loosely borrowed from maths.
- we can clean up a number of related issues in one go if we shift our
  approach slightly. First, let us posit that we only need template
  instantiation for three cases: kernel, facet, archetype. Then, let
  us redefine facet bundles as facet groups (with potentially the
  ability to rename the facet group name's contribution to the feature
  full name). Let us also redefine facet template initialisers as
  "owners" (the actual name needs some work). In this scenario, there
  is no longer the need to have fully qualified names, as these are
  computed on the basis of ownership: on feature initialisation, the
  owner supplies its name as it registers the feature. In addition, we
  also know the name of the facet group statically. This addresses the
  needs for all non-templatised features. For the templatised
  features, we need to have a way to "inject" owners and groups which
  are "instantiable". For owners we can simply have meta-data for
  this. For groups we could also have meta-data, and allow "empty
  groups" (e.g. groups with no attributes. we can validate that such
  groups must be "abstract" or "instantiable"). However, given that in
  the future we shall introduce the notion of "facets" in the
  meta-model, it may be wiser to allow the creation of groups via
  meta-data. Or maybe this can wait until they are introduced. At any
  rate, finally we need a third concept, which is really only required
  for formatters. We can call these "subgroups". These should be added
  via meta-data since we do not want to have to add a new entity for
  each formatter. Meta-data could look like so:

: #DOGEN masd.variability.subgroup.name=x
: #DOGEN masd.variability.subgroup.description=y

  Of course one needs to associate the new subgroup to an existing
  owner and group.
- in fact there is yet another way of looking at this, and it appears
  to be the best. There are three types of variability elements:

  - feature groups. These map to UML packages. They are containers and
    merely add to path. they support "overrides" such that the
    physical directory may differ from the "logical" directory. For
    example, we want to place features on a folder called =features=
    but we don't want to have to have a feature name =feature.x.y=.
  - feature bundles. These map to UML classes, in terms of how we
    access the features. Note that these do not exist within
    variability itself, only in the logical meta-model. Note also that
    with this we no longer need feature templates everywhere - the
    bundles should contain only features (see next section).
  - feature template bundles: these only contain feature templates,
    not features themselves. Feature templates do not respect
    containment in groups etc. They behave quite differently:
    templates are instantiated against a "range". Ranges are mapped to
    "tags". Any meta-model element can introduce tags. These are
    simple KVPs, of the form:

: #DOGEN masd.variability.tag=X,a.b.c

    Where =X= is the tag, and =a.b.c= is an element in the range. Many
    such elements can be added to the range. For example, let =kernel=
    be the tag and =masd.generation.cpp= be an element in the
    range. Then, in instantiation, any template for tag =X= is
    instantiated for each element in the range. Note that the range is
    a qualified path. It will give rise to the groups
    (e.g. =x.y.z=). Note that these groups are expected to already
    exist, created via the usual meta-model formalism of
    =feature_group=. That will contain the documentation for the
    group.

    With this infrastructure, we can now dump all the features to the
    console, organised by groups, very easily. We just need a
    container which keeps track of this hierarchical structure and
    associated descriptions; we can iterate through it and generate
    text to output in the console. We should also allow for a
    grep-friendly mode where we just simply list all the
    features. This must be done after template instantiation.

    Note that this approach still requires the mapping in order to
    solve the "directory and prefix" duplication issue.

Previous understanding:

Tasks:

- create the notion of "levels".
- replace archetype location with a variability location based on
  levels.
- inject these locations by transforming archetype locations into
  variability locations within engine.

*** COMPLETED Remove =rapidjson= formatters                           :story:
    CLOSED: [2020-03-06 Fri 12:42]
    :LOGBOOK:
    CLOCK: [2020-03-06 Fri 12:39]--[2020-03-06 Fri 12:42] =>  0:03
    :END:

We never finished implementing these and now they are just adding to
the cognitive load. Remove them for now, re-add them properly later.

*** COMPLETED Separate feature templates from bundles                 :story:
    CLOSED: [2020-03-09 Mon 11:46]
    :LOGBOOK:
    CLOCK: [2020-03-09 Mon 11:51]--[2020-03-09 Mon 11:58] =>  0:07
    CLOCK: [2020-03-09 Mon 10:05]--[2020-03-09 Mon 11:46] =>  1:41
    CLOCK: [2020-03-07 Sat 19:04]--[2020-03-07 Sat 19:15] =>  0:11
    CLOCK: [2020-03-07 Sat 17:37]--[2020-03-07 Sat 18:17] =>  0:40
    CLOCK: [2020-03-06 Fri 20:52]--[2020-03-06 Fri 21:59] =>  1:07
    CLOCK: [2020-03-06 Fri 17:16]--[2020-03-06 Fri 18:28] =>  1:12
    CLOCK: [2020-03-06 Fri 15:48]--[2020-03-06 Fri 17:15] =>  1:27
    CLOCK: [2020-03-06 Fri 14:26]--[2020-03-06 Fri 14:59] =>  0:33
    CLOCK: [2020-03-06 Fri 12:43]--[2020-03-06 Fri 13:29] =>  0:46
    CLOCK: [2020-03-06 Fri 10:55]--[2020-03-06 Fri 12:39] =>  1:44
    :END:

At present we have the notion of feature templates where the template
instantiation type is "instance". This is, in effect, the vast
majority of the features. We then have some 5 or less cases where we
use the feature templates proper. We should not have to instantiate
templates for the cases where the instantiation is just the identity
of the feature. We need a way to model these directly as features. We
then need to introduce "feature template bundles", which are marked as
abstract and can only contain feature templates.

Notes:

- need to rename method in registrar, at present its called register
  templates but it also registers features.
- we should create an abstract feature base class to simplify
  code. Given the implementation is trivial we can do it now instead
  of waiting until all of the refactor is finished.

*** COMPLETED Move enabled to generation                              :story:
    CLOSED: [2020-03-09 Mon 11:50]
    :LOGBOOK:
    CLOCK: [2020-03-09 Mon 11:46]--[2020-03-09 Mon 11:50] =>  0:04
    :END:

For some reason we have placed this in the archetypes model.

*** COMPLETED Create a chain to encapsulate variability transforms    :story:
    CLOSED: [2020-03-09 Mon 23:11]
    :LOGBOOK:
    CLOCK: [2020-03-09 Mon 22:47]--[2020-03-09 Mon 23:11] =>  0:24
    CLOCK: [2020-03-09 Mon 20:01]--[2020-03-09 Mon 20:14] =>  0:13
    :END:

At present we are using individual variability transforms in the
engine, and interspersing those with other transforms. A nicer way is
to have a chain in variability that takes in a configuration model set
and runs a chain against it.

Actually we can only encapsulate two transforms:

- profile_binding_transform
- profile_repository_transform

Still, its worthwhile doing it.

*** COMPLETED Move variability chain into assets                      :story:
    CLOSED: [2020-03-10 Tue 18:33]
    :LOGBOOK:
    CLOCK: [2020-03-10 Tue 18:13]--[2020-03-10 Tue 18:33] =>  0:20
    CLOCK: [2020-03-10 Tue 16:10]--[2020-03-10 Tue 18:12] =>  2:02
    :END:

We originally placed this chain in engine because it went across
models (e.g. variability and assets). In reality, the assets model is
already composed of the variability model and they are closely
intertwined so it doesn't make a lot of sense to separate them on that
ground. And now that we are adding proper support for profiles at the
assets level, this is more of a requirement because there are
dependencies between transforms within the assets pipeline and the
variability pipeline. We should just move the chain into assets if
there are no dependencies in engine.

*** COMPLETED Move profile meta-data into profile element             :story:
    CLOSED: [2020-03-11 Wed 10:00]
    :LOGBOOK:
    CLOCK: [2020-03-11 Wed 10:00]--[2020-03-11 Wed 10:22] =>  0:22
    CLOCK: [2020-03-11 Wed 08:51]--[2020-03-11 Wed 09:59] =>  1:08
    CLOCK: [2020-03-10 Tue 21:10]--[2020-03-10 Tue 21:39] =>  0:29
    CLOCK: [2020-03-10 Tue 19:01]--[2020-03-10 Tue 19:16] =>  0:15
    CLOCK: [2020-03-10 Tue 14:16]--[2020-03-10 Tue 16:08] =>  1:52
    CLOCK: [2020-03-10 Tue 09:05]--[2020-03-10 Tue 12:04] =>  2:59
    CLOCK: [2020-03-09 Mon 23:12]--[2020-03-09 Mon 23:19] =>  0:07
    :END:

At present we have left some of the profile meta-data in the
configuration of the profile element. However, for feature templates
and feature bundles, we read out the meta-data in assets and then make
use of it when transforming the element to its variability
representation. We should use the same approach for profiles - if
nothing else at least the approach is consistent.

Notes:

- at present we always declare the features and create the static
  configuration in the same place. This has worked thus far because we
  tend to declare the features where we consume them. Profiles are
  different: a profile is making use of a feature declared for a
  feature. That is, a profile is the instantiation of a feature
  defined elsewhere; remember that features are nothing more than a
  type system designed to give a "strongly typed" feel to the
  meta-data. Profiles are just an instantiation of those strong
  types. In theory, profile meta-data should already exist and match
  exactly what was defined for features. In practice there is a
  mismatch, and this is due to how we modeled features and feature
  bundles: to avoid repetition, we placed some features at the
  top-level and others in the features themselves. This approach does
  not match the shape required for profiles, so we need to redefine
  the bundle. However, of course, we do not want to register the
  features this time around (after all, they already exist) so we need
  to disable feature registration.
- "adaption" is not a word, rename to "adaptation" (in
  =profile_template_adaption_transform=).

*** STARTED Split profiles and profile templates in variability       :story:
    :LOGBOOK:
    CLOCK: [2020-03-11 Wed 10:23]--[2020-03-11 Wed 12:24] =>  2:01
    CLOCK: [2020-03-09 Mon 19:36]--[2020-03-09 Mon 20:00] =>  0:24
    CLOCK: [2020-03-09 Mon 18:49]--[2020-03-09 Mon 19:05] =>  0:16
    CLOCK: [2020-03-09 Mon 18:41]--[2020-03-09 Mon 18:48] =>  0:07
    CLOCK: [2020-03-09 Mon 16:40]--[2020-03-09 Mon 17:54] =>  1:14
    :END:

As with features, we seem to have conflated the profiles with profile
templates. Given we use templates in a very small number of cases, it
should really be two separate concepts to avoid confusing end users
and ourselves.

Notes:

- we should create a chain in engine to gather all of the transforms
  related to variability.
- rename "entities transform" to "features transform" and "application
  transform" to profiles transform". Add lots of commentary explaining
  the three distinct phases of variability processing (feature model
  generation, profile processing and feature processing for code
  generation). Copy across the profile processing from "entities
  transform" into "application transform". Update profile adapter from
  stash (and consider merging it into the application
  transform?). Profiles transform is really a chain as it is calling
  transforms.

*** Invalid stereotypes outside of objects are not detected           :story:

At present we are only checking for invalid stereotypes (e.g. those
which are neither static stereotypes, nor profiles nor object
templates) on objects and object templates. We need to add a validator
that checks all other element types. This should be easy, if we ended
up with any dynamic stereotypes we should error.

In fact we should generalise this processing: object templates should
mark their stereotypes as bound and then we will check every element
for any unbound stereotypes using the traversal.

*** STARTED Add location and modules to variability                   :story:
    :LOGBOOK:
    CLOCK: [2020-03-09 Mon 15:25]--[2020-03-09 Mon 16:08] =>  0:43
    :END:

We need to create a location class in variability, based on a simple
list of strings. It should probably have an attribute called "modules"
in order to follow the same approach as assets. We then need to
replace the existing uses of archetype location with this new,
variability-only, location. However, note that this is only done for
the features, not for the feature templates.

We need to introduce "feature groups". This is a stereotype applied to
packages which generates a feature group with documentation. Note that
these groups do not contribute to the path of physical elements, only
to the path of features - note, specifically *not* to feature
templates. Groups can nest arbitrarily.

Notes:

- we need to also figure out how to code-generate infrastructure for
  templates that allows us to retrieve features for a tag. We may not
  need code generation in fact, given we deal with them
  generically. We should just retrieve them manually. See
  =archetype_location_properties_transform=.
- check what the labels in variability are used for. If nothing,
  remove them. Actually, these are used by profiles (e.g. the
  stereotype to bind with). We probably should consider renaming
  these. If its function is just to name the stereotype for a profile,
  it should be called "stereotype" and there should be just one per
  profile.
- actually we have exactly the same problem with profiles and profile
  templates. We used the approach of "template kind instance" for
  profile templates. This is very confusing. We need to split these
  two types prior to refactoring this code.

*** Replace variability enum mapper with lexical casts                :story:

Its not clear what value the mapper adds now we can just lexical cast
enums from strings.

*** Name all transform exceptions consistently                        :story:

It seems on engine we call them "transform exception" but on assets we
call them "transformation error". Check all other models and them
these consistently.

*** Consider renaming labels to stereotypes in variability            :story:

It appears we use labels just to store the stereotype of the
profile. If so rename the attribute and associated feature and make it
just a string.

Merged stories:

*Make labels a plain text field not a collection*

At present it is possible to label a profile with multiple
labels. This is not a good idea. Make it a plain text field so we can
only apply a single label.

*** Implement template instantiation in terms of tags                 :story:

At present we are instantiating templates over a physical space
(e.g. kernel, facet, archetypes). We need to replace this with
tags. The idea is that we can inject tags (which are qualified names)
into the model via meta-data:

: #DOGEN masd.variability.tag=X,a.b.c

We can then create feature templates that are instantiated on a range
given by the set of all values for a given tag, for example =X=
(e.g. =kernel=, =facet=, =archetype=). Tags can be introduced by any
model which has a configuration point. However, elements in ranges are
expected to be unique and to point to existing groups.

*** Add command line option to dump all features                      :story:

We need a way to access a text description of all features in the
command line. The variability model should expose this, and then the
CLI model should use it to print a human readable version of the
features. We should allow for two modes: human friendly (e.g. wrap at
a column, indent by groups) and grep friendly (e.g. no indent, long
lines).

We should probably create a new command called info and then have
options of what to dump:

- features
- frontends
- backends
- stereotypes

And so forth.

Links:

- https://www.rosettacode.org/wiki/Word_wrap#C.2B.2B

Merged stories:

*Add annotation types description*

It would be useful to have a description of the purpose of the field
so that we could print it to the command line. We could simply add a
JSON attribute to the field called description to start off with. But
ideally we need a command line argument to dump all fields and their
descriptions so that users know what's available.

This should be sorted by qualified name.

Notes:

- we already added comments to many features. This seems to be the
  right place in the model to record this information. We just need to
  propagate it into the feature template and then into the feature.
- context is already doing all of the hard work for feature
  instantiation. We just need to create a transform that calls the
  context factory, retrieves all of the descriptions as strings
  somehow, and then get the command line to print them out. This can
  then be extended in the future to include backends, etc.

*** Postfix and directory fields should be templates                  :story:

We need to understand why we didn't templatise these fields. It is
very painful to have to add these manually for each facet and
formatter.

Most likely it is because each formatter/facet needs to "override" a
base value with its own value. For example, we almost always want a
blank postfix, but occasionally need to set it (=fwd= for forward
declarations and so forth). Our variability implementation does not
cope with this type of overrides. We would have to have some kind of
way of allowing instance templates even though a facet/archetype
template already exists, and then use the instance template as the
override. Alternatively, we could simply check for postfix/directory;
if not present default to empty string.

For extra bonus points, we could allow variables: =${facet.name}=
could expand to the current facet name on the facet template.

The right solution for this is to allow users to supply a map with
KEY, VALUE on a field:

: #DOGEN masd.variability.template_kind=archetype_template
: #DOGEN masd.variability.mapped_default=forward_declaration,fwd

In this case, any archetypes (e.g. "elements" in the new world)
matching the KEY =forward_declaration= would have a default value of
=fwd=.

Merged stories:

*Field definition templates do not support facet specific defaults*

At present we cannot use field definition templates for fields that
require facet specific default values such as =directory=. We could
either support something like a "variable", e.g. "find facet simple
name" or we could do overrides - the field definition is defined as a
template but then overriden at a facet level. Or we could handle
default values in a totally separate way - maybe a file with just the
default values.

In addition, we have the case where at the facet level we may have a
default value for a field but not at the formatter level - =postfix=.

For variables, the simple way is to have some "special names". For
example =$(facet_name)= could be made to mean the facet name. With
just support for this we could probably handle all of the use cases
except for =postfix=.

*Use templates for directory and prefix fields*

At present we have a lot of duplication on the annotations for certain
fields. This is because we need different defaults depending on the
facet etc. A different approach would be to use the appropriate
template (without default values) and then using profiles to default
those that need defaulting.

Other fields may also need a similar clean up:

- overwrite

In addition, we could add support for "default value variables". These
are useful for directories. They work as follows: the default value is
something like =${facet.simple_name}= or perhaps just
=${simple_name}=, in which case we assume the template kind determines
the target. Say the target is the kernel:

:      "family": "quilt",
:      "kernel": "quilt.cpp",

The simple name is then =kernel - family=, e.g. =cpp=. Unfortunately
this does not work for prefix.

Tasks:

- make prefix a recursive field at archetype level, adding default
  values to profiles.
- make directory a recursive field at facet level,  adding default
  values to profiles.

*Postfix and directory fields in annotations look weird*

Why are we manually instantiating postfix and directory for each
formatter/facet instead of using templates? This is one of the main
reasons for breaks/errors when adding a new formatter.

*** Detect non-configurable fields                                    :story:

Some stereotypes cannot be placed in a configuration. Placing them
there will only cause confusion and hard to debug errors. List:

- =injection.dia.comment=: this is only applicable to the UML note in
  dia.
- =injection.reference=: as we need these to load models, it would
  cause cycles if placed in configuration.
- =injection.input_technical_space=
- all fields needed to load the configuration itself, as it would
  cause cycles.

We should have a property in the field such as "supports
configuration" or configurable or some such. When reading the
configuration, we need to validate that none of the entry templates
contain fields with this value set to false.

Its not quite "supporting configuration", more like " supporting
unbound configurations". All features are by definition configurable.

A related problem is the converse: some fields _can_ be placed on a
configuration. In this case, we should not read the fields prior to
performing configuration expansion. This can probably be detected
quite easily: say we can have a flag that tells us if we have expanded
the configuration. If the flag is false, we should throw when we
attempt to read fields that can be placed in profiles. In effect we
are saying configurations exist in one of two states:

- pre-expansion, in which only fields that are "pre-expansion" can be
  read;
- post-expansion, in which only fields that are "post-expansion" can
  be read.

*** Allow dropping facet postfix for an element                       :story:

We sometimes need to suppress the facet postfix. For example, when
outputting tests, at present we have:

: cpp_ref_impl.boost_model/generated_tests/main_tests.cpp

We may want it just to be called:

: cpp_ref_impl.boost_model/generated_tests/main.cpp

However, we don't want all files on that facet to drop their postfix,
just main.

*** Refactor archetype model                                          :story:

- rename model to =physical=.
- create meta-model namespace.
- add missing meta-types from generation (parts, etc).
- remove all types from generation which are not yet used.
- add concept of artefact types (e.g. c++ public header, c++ private
  header, etc). Associate extensions with artefact types (and perhaps
  other properties?).

*** Rename assets model to logical model                              :story:

- rename all references to archetypes to "physical", e.g.:
  =artefact_properties= should be renamed, etc.

*** Consider renaming "meta-model" namespace                          :story:

Originally we created a number of namespaces in models called
"meta-model". It started with assets, where it really was the
meta-model, but we now have meta-models on pretty much all models
(injection, extraction, etc). Its no longer clear what value this
prefix adds. In addition its a technical word, so it seems to imply
there is some meaning to it, but since pretty much we have in dogen is
a meta-model of something, its not exactly useful. We need a term that
is more neutral.

Ideas:

- elements
- entities

Notes:

- look for ideas on other projects.

*** Implement locator in terms of new physical types                  :story:

- get kernels to export the new information.
- using the information compute the paths. Create a new field so that
  we can diff new and old paths.
- once there are no differences, remove all locator related legacy code.

*** Implement dependencies in terms of new physical types             :story:

- add dependency types to physical model.
- add dependency types to logical model, as required.
- compute dependencies in generation. We need a way to express
  dependencies as a file dependency as well as a model
  dependency. This caters for both C++ and C#/Java.
- remove dependency code from C++ and C# model.

*** Remove =element= from the modeling location                       :story:

We introduced this for inner classes, but its (probably) not being
used. If so, remove it and add a story for inner classes, if one does
not yet exist.

*** Add primitives to the archetypes model                            :story:

Instead of using strings we should use primitives for:

- facets
- formatters
- backends
- simple and qualified names.
- etc.

*** Enablement problem is in the variability domain                   :story:

Up to now we have considered the enablement problem as a generation
model problem, but this is incorrect. The enablement problem is
basically the idea that if I set a type to be hashable (for example),
the system should implicitly determine all other types that need to be
hashable too. This means that if I have descendants, they should also
be hashable, and if I have properties, the type of those properties
must also be hashable. In reality this is just a variability
problem. We need to tell the variability model about:

- features that require "propagation across model elements". We need a
  good name for this, without referencing model elements.
- the relationship between bound configurations. This can be copied
  from the model element (the bound configuration has the exact same
  name as the model element).

Then, we can simply build a DAG for the feature model using only bound
configurations (e.g. at present, binding type of "not applicable") and
then DFS the DAG setting properties across this relationship. Call the
relationship R between a and b, where a and b are configurations; all
properties that have the "propagate" flag on will be copied across
from a to b as is (due to R). If done after building the merged model
and after stereotype expansion this will work really well:

- we don't really care how a got into the state it is at present, we
  just copy the relevant properties across.
- there is no solving, BDD, etc. However, R must not have cycles. We
  probably need to first see how many cycles we find with inheritance
  and associations.
- we may need a way to switch this off. Say we really want to
  introduce a cycle; in that case, the bound configurations should be
  ignored.

Note that we will probably need to store pointers to the configuration
in order for this to work, or else we'll end up doing a lot of lookups
and copying around (to get the configurations from the model elements
into variability, the DAG etc and then back into the model at the
end).

Interestingly, this also means that we should not move the
global/local enablement computations into archetypes as we had planned
earlier. Instead, we need to explore if it is possible to generalise
the notion of "local" and "global" configurations, with overrides and
default values. This would work as part of the configuration binding
via implicit relationships - its just that the global configuration is
not really a relationship inferred from the underlying model. We then
need to look at the cleverness that we are using for overwrite as
well. Whilst we only need this logic for enablement, it may be useful
for other fields as well in the future. We also need some kind of way
of declaring certain fields as "cloneable" (for want of a better
term). In this case, we start off with a list of these fields, and if
there is no configuration point for them locally, we take the global
configuration point; if none exists, we take the default value.

Actually its more like "hierarchical copy" because we need to take
into account the hierarchy. In addition, we don't particularly care
about say backend, facet, etc at the element level, we just want the
archetype. So we need to encode these rules as a type of bind. It can
even be hacked as a bind "special" just for this purpose, its still a
better approach.

Another interesting issue is that of "reverse references". That is,
the fact that a model m is referenced by a set of models S; each of
these models may enable facets on elements that are associated with
elements from model m. On a first pass, we need to be able to consider
the configuration requirements as "non-satisfiable". The user
requested a configuration on the target model which cannot be
satisfied unless we alter the configuration of a referenced model. On
a second pass, when we have product level support, we could consider
adding "referenced" models to each model. This means that when we are
building m we have visibility of how m is used in the product and we
can take those uses into account when building the DAG.

We should really read up on OMG's CVL and associated technologies, as
it seems they have done much of the analysis required here.

Merged stories:

*Propagate =fluent= stereotype*

It would be nice to be able to mark an object template called say
Message with =dogen::fluent= and then have all of the classes that
instantiate that template set to fluent.

This is a variation on the general problem of feature propagation
(e.g. hashing, etc).

It would also be nice to have a meta-data parameter to determine if
the "auto-propagation" is on or off.

*Computation of enablement values*

Note: this story is still *very* sketchy.

At present we have a very simple way of determining what formatters
are enabled: if a facet has been enabled by the user then all
formatters on that facet are enabled. This is a good starting point
but results in a lot of manual work:

- if we add a type which does not support all facets, we will generate
  invalid code. Users should be able to mark which facets are
  supported and then the graph of dependencies should do the right
  thing, propagating the disabled status.
- we are enabling all formatters in a facet. For hashing and forward
  declarations, it would make more sense to have a "dependency based
  enablement": if we determine that someone in the model needs that
  feature, we enable it, if not its disabled. Users can always
  override this and force it to be globally enabled.
- if a user creates a "service", all facets other than types are
  disabled. Ideally we should be able to define "enablement profiles"
  and then set an element's enablement profile. Each enablement
  profile is made up of a set of enabled facets. They could be
  supplied as a KVP. In fairness we probably just need "types and io"
  or "default".

One way to think of this problem is to imagine a matrix for each
element in element space. Each matrix is two-dimensional: one
dimension is the facets and the other are "dependent elements". These
are effectively made up of all attributes for each element, with a
name tree expansion. Each value of the matrix can either be 0
(disabled), 1 (enabled) or 2 (not computable). Not computable is a
hack to cope with cycles in the graph of dependencies.

Each value is computed by looking up an element's matrix and looking
for zeros. If there is one or more zero against a facet, the element's
value for that facet is zero. If there is a two we need to do a
two-pass whereby we first compute the matrix ignoring all the two's;
then, for each cycle we create a list of all the elements on that path
and the pair of elements that causes the cycle. We then compute the
enablement for this pair with a simple table (OR the computed
enablement values). We then traverse the cycle in reverse, updating
the twos to real values.

We could start with one large matrix with rows by element and columns
by feature. All values on this matrix are set to 1. We would then
multiply it against the global enablement matrix. We would then
multiply it by the local enablement matrix, for each element. We would
then compute the dependency matrices for all elements only taking into
account facets that are still enabled. We need to find the linear
algebra operation that takes a column with zeros and ones and returns
one if all rows are one and zero otherwise.

This produces the enabled facets. We then need to worry about the
formatters. There are a few sources of information:

- the facet enablement.
- the user local or global decision for that formatter.
- some kind of default formatter property (e.g. disabled by default).
- dependencies.

For these we need to create a "get dependencies" method in
each formatter which returns dependent formatters. For example, the
visitor formatter depends on the forward declarations formatter. This
is a static dependency. The more complex case is where there are
dynamic dependencies. For example, if hashing is detected for a given
type, we then need to enable the hashing facet for the containee. We
should probably hard-code this scenario for now.

We may want to make these computations disableable. For example: a)
all: no computation, everything is enabled b) all supported: all that
is supported is enabled c) by dependencies.

Requested help from FB. Core of the email:

#+begin_quote
Lets start with the simple case. Let G be a DAG. For each vertex of G
there is an associated vector over a field F. Now I would say F is
GF(2), which suits my needs (as you will see below). The objective is
to compute, for each vertex, the value of its associated vector, as
follows:

- first we go through the vertices in any order and setup its initial
  values according to a predetermined heuristic. Different nodes will
  have different values, and the heuristic has no dependency on G.
- then we iterate through G using DFS. If a vertex has no children
  then the final value of its vertex is the initial value. If a vertex
  has children, the value of its vector is obtained by multiplying the
  initial value against the values of the vectors of its child
  vertices. Multiplication under GF(2) is just a logical AND which is
  great for my purposes.

Just to make sure I'm explaining my self correctly, lets look at it in
layperson's terms: if a vertex has a 1 at position zero of its vector,
and all of its children also have a 1 at position zero, then the final
value for position zero will be 1. If there is a 0 anywhere at
position zero then the value is 0. So far so good, this works as
expected.

However! The problem is, G is actually not always a DAG. Sometimes
there may be cycles, which are detectable during DFS. My question is:
is there anything I can do to still perform this heuristic (or some
approximation of it) with a graph that has cycles? For example:

- record the path to the cycle and perform several passes. This seems
  to breakdown when there are several cycles because I seem to hit
  some kind of recursive problem.
- ignore the cycle. Of course, the problem with this approach is that
  if there was a zero at either side of the cycle, I would be
  incorrectly computing the node, but maybe that's the best one can
  do?
#+end_quote

Actually maybe we are looking at this the wrong way. Lets imagine that
for each element there is a vector v in GF(2) called the initial
vector. The objective is to compute u, the output vector. The output
vector is made up of the initial vector of the element, times the
output vectors of all the elements the element depends on. However,
these can be formulated in terms of initial vectors too (e.g. the
initial vector of the depended element times the initial vectors of
the elements it depends on times the initial vectors of the elements
they depend on and so forth). Thus for each element there is an
expansion that just relies on initial vectors. For the cases where
there are cycles: its not a problem since multiplying n times by
the same vector (in GF(2)) produces the same result as multiplying
just once.

It would still be useful to have a graph though, to find all of the
initial vectors for a given element. We just need to stop DFS'ing when
we find a cycle. We can also cache the initial vectors for each
element.

Notes:

- we can greatly simplify this story if we do not allow for cycles. We
  can simply create a graph of all dependencies and then iterate the
  graph from the leaves. Call Ev the enablement vector for each
  element; we can descend the graph and perform an OR of Ev at each
  level. Consider element e0, which is a child of a set of elements E;
  for each entry in the set, we'd OR the element vector of e0 (and of
  all of its descendants). As a result, its values would be the
  superset of all of the enabled values on each leaf element.
- since we do not allow cycles, we should detect them and break with
  an error. We should provide the cycle path to the user and then
  allow users to remove certain types from this computation via
  meta-data. If a type is set not to contribute to the graph, we can
  simply skip it. The user is then responsible for manually setting
  that type.
- since we can only alter generatable types, we should detect when we
  reach an element which is not generatable. If the OR'ing of that
  element does not produce its current enablement vector we should
  simply error and tell the user the current enablement requirements
  are not satisfiable. The user is then responsible for addressing the
  issue by either changing enablement requirements, ignoring types,
  updating reference models manually or providing helper types. To
  make life easier we could state what are the enablement requirements
  that have not been met so that users can quickly decide what to do.
- once we compute the dependency graph we can also check to see what
  types are on it. Any type which is absent can be removed from the
  model. We could also compute the models that are on the graph and
  compare them to the list of references. If the list of references at
  present only includes references of the target model, we can figure
  out any unnecessary references. Sadly we cannot do the opposite:
  (lost the train of thought).
- it would be nice to have "enablement requirements". For example, if
  the user used =std::unordered_map= against a dogen type, it should
  trigger the generation of hash for that type (and all dependent
  types). Similarly, for =std::map= it should trigger the creation of
  =operator<=. If we could declare upfront that a type's types facet
  depends on another facet, this could be computed.

*Formatters need different =enabled= defaults*

We should be able to disable some formatters such as forward
declarations. Some users may not require them. We can do this using
dynamic extensions. We can either implement it in the backend or make
all the formatters return an =std::optional<dogen::formatters::file>=
and internally look for a =enabled= trait.

We need to be able to distinguish "optional" formatters - those that
can be disabled - and "mandatory" formatters - those that cannot. If a
user requests the disabling of a mandatory formatter, we must
throw. This must be handled in enabler.

This story was merged with a previous one: Parameter to disable cpp
file.

#+begin_quote
*Story*: As a dogen user, I want to disable cpp files so that I don't
generate files with dummy content when I'm not using them.
#+end_quote

It would be really useful to define a implementation specific
parameter which disables the generation of a cpp file for a
service. This would stop us from having to create noddy translation
units with dummy functions just to avoid having to define exclusion
regexes.

In some cases we may need a "enable by usage". For example,
it would be great to be able to enable forward declarations only for
those types for which we required them. Same with hash. We can detect
this by looking at the generated include dependencies. However,
because the include dependency only has a directive, we cannot tell
which formatter it belonged to. This would require some augmenting of
the directive to record the "origination" formatter.

*Disable facets on element state*

In certain cases it may not make sense to enable a facet. The main use
case is for testing: we should not bother testing an object if there
are no attributes. This can be achieved with a small hack: add a
container in archetype repository of all archetypes that require
objects to have properties. Then, augment =is_element_disabled= to
perform this check. We just need formatters to supply this information
when building the repository.

A much more robust version would be to have formatters return a
function that takes in the element and returns true or false. We could
default all formatters to just return true. However, we do not have
support for boost/std function so this would mean manually coding the
repository. We'd have a similar problem if we add an interface.

*Add support for facet dependencies*

At present we left it as an exercise to the user to ensure facets are
enabled to meet dependencies. In reality we need a solver for
this. Look for other solver story in backlog. In addition, we also
need to have a way to declare facet dependencies:

- all facets other than types depend on types.
- tests depends on at least types and test data.

Actually what we really need is a model to declare all entities in the
archetype space and their relationships:

- archetypes
- facets
- formatters
- kernels

The annotations model can then depend on this model. It should have
facilities for registration of kerneles, etc. However, note that this
has nothing to do with model to text transforms - its just declaring
the lay of the land for the archetype space. We called this generation
space up to know but generation is concerned with the mapping of
coding entities into archetype space, not with defining the geometry
of that space. We need a good name for this model:

- =masd.dogen.archetypes=

This also makes it clear why annotations had a need for locations in
archetype space: its because the configuration is the configuration of
formatting functions which are responsible for mapping coding elements
into archetypes. Of course we have configuration that is not related
to archetypes as well. We need some kind of way of stating this at the
archetype model level so that we don't have to associate all features
with a location on archetype space when none exists.

*Add support for formatter and facet dependencies*

Once we are finished with the refactoring of the C++ model, we should
add a way of declaring dependencies between facets and between
formatters. We may not need dependencies between facets as these are
actually a manifestation of the formatter dependencies.

These are required to ensure users have not chosen some invalid
combination of formatters (for example disable serialisation when a
formatter requires it). It is also required when a given
facet/formatter is not supported (for example when an STL type does
not support serialisation out of the box).

Note that the dependencies are not just static. For example, the types
facet depends on the hash facet if the user decides to add a
=std::unordered_map= of a user defined type to another user defined
type. We need to make sure we take these run-time dependencies into
account too.

*** Consider using a primitive for qualified representations          :story:

At present we have a number of maps with =string= as their key. We
can't tell what that string means. It would be better to have a
primitive to represent the different kinds of qualified id's we
have. This would also stop us from making mistakes such as using dot
notation in a container where we expected colon notation, or just
using any random string.

*** Add support for product skeleton generation                       :story:

Now that dogen is evolving to a MDSD tool, it would be great to be
able to create a complete product skeleton from a tool. This would
entail:

- directory structure. We should document our standard product
  directory structure as part of this exercise. Initial document added
  to manual as "project_structure.org".
- licence: user can choose one.
- copyright: input by user, used in CMakeFiles, etc. added to the
  licence.
- CI support: travis, appveyor
- CMake support: top-level CMakefiles, CPack. versioning
  templates, valgrind, doxygen. For CTest we should also generate a
  "setup cron" and "setup windows scheduler" scripts. User can just
  run these from the build machine and it will start running CTest.
- vcpkg support: add "ports" code? user could point to vcpkg directory
  and a ports directory is created.
- agile with first sprint
- README with emblems.

Name for the tool: dart.

Tool should have different "template sets" so that we could have a
"standard dogen product" but users can come up with other project
structures.

Tool should add FindODB if user wants ODB support. Similar for EOS
when we support it again. We should probably have HTTP links to the
sources of these packages and download them on the fly.

Tool should also create git repo and do first commit (optional).

For extra bonus points, we should create a project in GitHub, Travis
and AppVeyor from dart.

We should also generate a RPM/Deb installation script for at least
boost, doxygen, build essentials, clang.

We should also consider a "refresh" or "force" statement, perhaps on a
file-by-file basis, which would allow one to regenerate all of these
files. This would be useful to pick-up changes in travis files, etc.

One problem with travis files is that each project has its own
dependencies. We should move these over to a shell script and call
these. The script is not generated or perhaps we just generate a
skeleton. This also highlights the issue that we have different kinds
of files:

- files that we generate and expect the user to modify;
- files that we generate but don't expect user modifications;
- files that the user generates.

We need a way to classify these.

Dart should use stitch templates to generate files.

We may need some options such as "generate boost test ctest
integration", etc.

Notes:

- [[https://github.com/elbeno/skeleton][Skeleton]]: project to generate c++ project skeletons.
- split all of the configuration of CMake dependencies from main CMake
  file. Possible name: ConfigureX? ConfigureODB, etc. See how find_X
  is implemented.
- detect all projects by looping through directories.
- fix CMake generation so that most projects are generated by Dogen.
- add option to Dogen to generate test skeleton.
- detect all input models and generate targets by looping through
  them.
- add CMake file to find knitter etc and include those files in
  package. We probably should install dogen now and have dogen rely on
  installed dogen first, with an option to switch to "built" dogen.
- generate git ignore files with common regexes. See [[https://github.com/github/gitignore][A collection of
  useful .gitignore templates]]. We could also model it as a meta-model
  object with associated options so that the user does not have to
  manually edit the file.
- generate top-level CMake, allowing user to enter dependencies and
  their versions (e.g. Boost 1.62 etc) and CMake version.
- inject dogen support automatically to CMake (on a feature switch).
- determine the list of projects by looking at the contents of the
  input models directory.
- user to enter copyright, github URL.
- we probably need to create a kernel for dart due to the
  peculiarities of the directory structure.

*Directory Themes*

It seems obvious no one in C++ will agree with a single way of
structuring projects. The best way out is to start a taxonomy of these
project layouts (directory structure themes?) and add this to the
project generator as a theme. At present there are several already
available:

- [[https://github.com/vector-of-bool/vector-of-bool.github.io/blob/master/_drafts/project-layout.md][Project Layout]]: see also discussion in [[https://old.reddit.com/r/cpp/comments/996q8o/prepare_thy_pitchforks_a_de_facto_standard/][reddit]]. Also: [[https://vector-of-bool.github.io/2018/09/16/layout-survey.html][Project
  Layout - Survey Results and Updates]]
- [[https://build2.org/][Build2]]: the packaging system seems to have a preferred directory
  layout. In particular, see [[https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml#proj-struct][Canonical Project Structure]].
- GNU: gnu projects seem to have a well-defined structure, if not the
  most sensible.
- [[https://www.reddit.com/r/cpp/comments/cvuywh/structuring_your_code_in_directories/][Structuring your code in directories]]
- [[https://api.csswg.org/bikeshed/?force=1&url=https://raw.githubusercontent.com/vector-of-bool/pitchfork/develop/data/spec.bs#src.layout][The Pitchfork Layout (PFL)]]
- [[https://www.boost.org/development/requirements.html#Organization][Boost: Organization]]
- [[https://hiltmon.com/blog/2013/07/03/a-simple-c-plus-plus-project-structure/][A Simple C++ Project Structure]]

*Product Model*

Actually we have been going about this all wrong. What we've called
"orchestration" is in fact the product model. It is just lacking all
other entities in the product meta-model such as:

- injection/coding models: injection/coding models are themselves
  modeling elements within the product meta-model. However, to avoid
  having to load an entire coding/injection model, a product coding
  model can contain only the key aspects of the injection/coding
  models we're interested in: a) file or path to the model b)
  references c) labels: these allow us to group models easily such as
  say "pipeline" or "injection" etc. d) references: with this we can
  make a product graph of model dependencies. We can also avoid
  rereading models. we can also figure out what packages needed by the
  model graph.
- build systems: visual studio, msbuild, cmake
- ctest
- CI: travis, appveyor.
- kubernetes support, docker support.
- valgrind
- compiler: clang, gcc, msvc, clang-cl. Version of the compiler. This
  is used in several places such as the scripts, CI, etc.
- operative system: windows, linux. used in installation scripts, CI,
  etc.
- dependencies for install scripts; these are sourced from the
  component models.
- manual: org mode, latex
- org agile: product backlog, sprints, vision, etc.

Notes:

- a product may be associated with one or more primary technical
  spaces (e.g. support for say C# and C++ in the same model). This
  would have an impact at the product level.
- a product could have some simple wale templates so that when you
  initialise a product you would get a trivial dia model with a simple
  entry point (for executables) or a library with maybe no types.
- when generating a product we can generate all models (product and
  component), generate just the product, generate a specific component
  or generate a label (which groups components).
- we need a "init" command that initialises a product. It needs a
  product name and maybe some other parameters to determine what to
  add. Maybe it just makes a product model and asks the user to fill
  it in instead.
- there are several types of component models: 1) models that do not
  generate anything at all. these are useful for defining templates,
  configurations, etc. 2) regular component models 3) product
  models. 4) platform definition models that are used to adapt
  existing libraries into MASD.
- in this sense, we have two different models: product and
  component. Both of these need to be projected into artefact space
  (because we have multiple facets in products as well). This means we
  somehow need to use archetypes from both models.
- the product model should have meta-elements describing the component
  models (perhaps =masd::component_model::target=, with a matching
  =masd::component_model::reference= in the component models).
- See aslo the story about directories in dogen: [[*Move models into the project directory][Move models into the
  project directory]].
- we could create separate chains for product and component
  model. This would imply a need for distinct model types. On the
  product model, we would locate all of the meta-elements representing
  a component model, and for each of these, run the product model
  chain. For other meta-model elements we just run their associated
  transforms - hopefully not many as these are expected to be very
  simple elements. We should also make use of injection model caching
  to avoid reloading models.
- as with component models, we should also have templates for product
  models so that we could simply do a "dogen new product" or some such
  incantation and that would result in the creation of a dogen product
  model and possibly its initial generation. One slight problem is
  that if we do a "dogen new component" we still have to manually add
  the component to the product model.
- we need to have a separate injection adapter for product models so
  that we filter out "invalid" meta-elements for the model
  type. Similarly, in the component injection adapter, we should
  filter out product model meta-elements (travis build files, etc).

Links:

- [[https://github.com/bkaradzic/GENie][GENie - Project generator tool]]
- see [[https://github.com/cginternals/cmake-init][cmake-init]] for ideas.
- [[https://github.com/premake/premake-core][Premake: powerfully simple build configuration.]]
- [[https://jgcoded.github.io/CMakeStarter/][CMake Starter]]: "This website is a simple tool to help C++ developers
  quickly start new CMake-based projects. The tool generates an entire
  C++ project with boiler-plate CMake files and source code, and the
  generated project can be downloaded as a zip file."
- [[https://awfulcode.io/2019/04/13/professional-zero-cost-setup-for-c-projects-part-1-of-n/][Professional, zero-cost setup for C++ projects (Part 1 of N)]]:

*** Formatter meta-model elements                                     :story:

A second approach is to leave this work until we have a way to code
generate meta-model elements. Then we could have a way to supply this
information as meta-data - or perhaps it is derived from the position
of the element in modeling space? The key thing is we need a static
method to determine the meta-name, and a virtual method to allow
access to it via inheritance. Perhaps we need to capture this pattern
in a more generic way. It may even already exist in the patterns
book. Then the elements would become an instance of the pattern. We
should also validate that all descendants provide a value for this
argument (e.g. an element descendant must have the meta-name set). We
could also use this for stereotypes.

The binding of the formatter against the meta-type is interesting, in
this light. The formatter has a type parameter - the type it is
formatting. In fact the formatter may have a number of type
parameters - we need to look at the stitch templates to itemise them
all - and these are then used to generate the formatter's template. We
could take this a level up and say that, at least conceptually, there
is a meta-meta-type for formatters, which is made up of a
parameterisable type. Then we could declare the formatter as an
instance of this meta-meta-type with a well-defined set of
parameters. Then, when a user instantiates a formatter, we can check
that all of the mandatory parameters have been filled in and error if
not. In this case we have something like:

- =masd::structural::parameterisable_type=. This is a meta-type that
  has a list of KVPs. Some are mandatory, some are optional.
- =masd::codegen::meta_formatter=. This defines the parameters needed
  for the formatter, with default values etc.
- =masd::codegen::formatter=. This is the actual formatters. They must
  supply values for the parameters defined by the meta-formatter.

Of course, we do not need a three-level hierarchy for this, and if
this is the only case where these parameters are used, we could just
hard-code the formatter as a meta-element and treat it like we do with
all other meta-types. Interestingly, we could bind formatters to
stereotypes rather than meta-elements. This would allow us to avoid
binding into the dogen implementation, and instead think at the MASD
level (e.g. =dogen::assets::meta_model::structural::enumeration= is a
lot less elegant than =masd::enumeration= or even
=masd::structural::enumeration=).

We could also validate that the wale template exists. In fact, if the
wale template is a meta-model element, we can check for consistency
within resolution. However, we need a generic way to associate a wale
template with any facet. The ideal setup would be for users to define
wale templates as instances of a meta-model element which is
parameterisable (see above). In reality, what we have found here is
another pattern:

- there are templates as model elements. When we create a template we
  are instantiating a template's template.
- we can then constrain the world of possibilities in to a
  well-defined set of parameters which are needed for the specific
  template that we are working on. This has a meta-model element
  associated with it, and a file.
- the file is the template file. In the case of wale, the template
  file is then instantiated. This is done by associating facets with
  the wale templates, and for each facet, supplying the arguments to
  instantiate the template. We then end up with a number of actual
  CPP/HPP files.
- for stitch the process is a bit different. The main problem is
  because we incorrectly "weaved" the arguments into the stitch
  template. It made sense at the time purely because we don't really
  expect to instantiate a given stitch template N times; it is really
  only done once. This was slightly misleading. Because of this we
  hard-coded the behaviour related to certain keys (e.g. includes,
  etc). If instead we somehow handle stitch in exactly the same way as
  we handle wale, we can keep the templates in a common template
  directory; then associate them to specific facets via meta-data, and
  supply the arguments as part of the same meta-data. The template
  would then just contain the code that would be weaved. A formatter
  is then a meta-model element associated with a wale template for the
  header file and - very interestingly - a wale template for the cpp
  file _which generates stitch templates_. The user then manually
  fills in the stitch template, but supplies any parameters (remember
  these are fixed) in the meta-model element. Generation will then
  produce the CPP
- the logical consequence of this approach is that we must reference
  the c++ generation model in order to create new formatters, because
  it will contain the templates. However, because the wale content of
  the template is located in the filesystem, it will not be possible
  to instantiate the template. We need instead to find a way to embed
  the content of the template into the model element itself. Then the
  reference would be sufficient. The downside is that, in the absence
  of org-mode injectors, these templates will be extremely difficult
  to manage (imagine having to update a dia comment with a wale
  template every time you need to change the template). On the plus
  side, we wouldn't have to have a set of files in the filesystem,
  which would make things a bit "neater".
- in fact, we have two use cases: the templates which generate
  generators (e.g. stitch) and so must be loaded into the code
  generator and the templates which are a DSL and so can be
  interpreted. Ultimately these should have a JSON object as
  input. Ultimately there should be a JSON representation of instances
  of the meta-model that can be used as input. However, what we are
  saying is that there is a ladder of flexibility and each has its own
  use cases:

  - code generated;
  - code generated with overrides;
  - DSL templates;
  - generator templates;
  - handcrafted

  Each of these has a role to play.

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with [[*Build%20shared%20objects%20instead%20of%20dynamic%20libraries][using shared objects]], this
should improve build times.

In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

More thoughts on this:

- we don't really need to have different directories for this; we
  could just put all the include files in the same directory. At
  packaging time, we should only package the public files (this would
  have to be done using CPack).
- also the GCC/MSVC visibility pragmas should take into account these
  options and only export public types.
- the slight problem with this is that we need some tests to ensure
  the packages we create are actually exporting all public types; we
  could easily have a public type that depends on a private type
  etc. We should also validate yarn to ensure this does not
  happen. This can be done by ensuring that a type marked as external
  only depends on types also marked as external and so forth.
- this could also just be a packaging artefact - we would only package
  public headers. Layout of source code would remain the same.
- when module support is available, we could use this to determine
  what is exported on the module interfaces.

*** Integration of archetypes into assets                             :story:

Up to recently, there was a belief that the archetypes model was
distinct from the assets model. The idea was that the projection of
assets into archetype space could be done without knowledge of the
things we are projecting. However, that is demonstrably false: n order
to project we need a name. That name contains a location. The location
is a point on a one-dimensional asset space.

In reality, what we always had is:

- a first dimension within assets space: "modeling dimension",
  "logical dimension"? It has an associated location.
- a second dimension within assets space: "physical dimension", with
  an associated location. Actually we cannot call it physical because
  physical is understood to mean the filesystem.

So it is that concepts such as archetype, facet and technical space
are all part of assets - they just happen to be part of the
two-dimensional projection. Generation is in effect a collection of
model to text transforms that adapts the two-dimensional element
representation into the extraction meta-model. Formatters are model to
text transforms which bind to locations in the physical dimension.

In this view of the world, we have meta-model elements to declare
archetypes, with their associated physical locations. This then
results in the injection of these meta-elements. Formatters bind to
these locations.

However, note that formatters provide dependencies. This is because
these are implementation dependent. This means we still need some
transforms to occur at the generation level. However, all of the
dependencies which are modeling related should happen within
assets. Only those which are formatter specific should happen in
generation. The problem though is that at present we deem all
dependencies to be formatter specific and each formatter explicitly
names its dependencies against which facets. It does make sense for
these to be together.

Perhaps what we are trying to say is that there are 3 distinct
concepts:

- modeling locations;
- logical locations;
- physical locations.

The first two are within the domain of assets. The last one is in the
domain of generation and extraction. Assets should make the required
data structures available, but it is the job of generation to populate
this information. Thus directory themes, locator, etc are all
generation concepts.

One could, with a hint of humour, call the "logical dimension" the
meta-physical dimension. This is because it provides the meta-concepts
for the physical dimension.

A backend provides a translation into a representation considered
valid according to the rules of a technical space. A backend can be
the primary or secondary backend for a technical space. A component
can only have a primary backend, and any number of secondary
backends. Artefacts produced by a backend must have a unique physical
location. In LAM mode, the component is split into multiple
components, each with their own primary technical space.

*** Create a archetypes locator                                       :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present;
they should be a transform inside generation.

*** Clean-up archetype locations modeling                             :story:

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*** Move dependencies into archetypes                                 :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Move formatting styles into generation                            :story:

We need to support the formatting styles at the meta-model level.

*** Make creating new facets easier                                   :story:

For types that are stitchable such as formatters, we need to always
copy and paste the template form another formatter and then update
values. It would be great if we could have dogen generate a bare-bones
stitch template. This is pretty crazy so it requires a bit of
concentration to understand what we're doing here:

- detect that the =yarn::object= is annotated as
  =quilt.cpp.types.class_implementation.formatting_style= =stitch=.
- find the corresponding expected stitch file. If none is available,
  /dynamically/ change the =formatting_style= to =stock= and locate a
  well-known stitch formatter.
- the stitch formatter uses a stitch template that generates stitch
  templates. Since we cannot escape stitch markup, we will have to use
  the assistant. One problem we have is that the formatter does not
  state all of the required information such as what yarn types does
  it format and so forth. We probably need a meta-model concept to
  capture the idea of formatters - and this could be in yarn - and
  make sure it has all of this information. This also has the
  advantage of making traits, initialisers etc easier. We can do the
  same for helpers too.
- an additional wrinkle is that we need different templates for
  different languages. However, perhaps these are just wale templates
  in disguise rather than stitch templates? Then we can have the
  associated default wale templates, very much in the same way we have
  wale templates for the header files. They just happen to have stitch
  markup rather than say C++ code.

This is a radically different way from looking at the code. We are now
saying that yarn should have concepts for:

- facets: specialisation of modules with meta-data such as facet name
  etc. This can be done via composition to make our life easier.
- formatters and helpers: elements which belong to a facet and know of
  their archetype, wale templates, associated yarn element and so
  forth.

We then create stereotypes for these just like we did for
=enumeration=. As part of the yarn parsing we instantiate these
meta-objects with all of their required information. In addition, we
need to create what we are calling at present "profiles" to define
their enablement and to default some of its meta-data.

When time comes for code-generation, these new meta-types behave in a
more interesting way:

- if there is no stitch template, we use wale to generate it.
- once we have a stitch template, we use stitch to generate the c++
  code. From then on, we do not touch the stitch template. This
  happens because overwrite is set to false on the enablement
  "profile".

Merged stories:

*Code generate initialisers and traits*

If we could mark the modules containing facets with a stereotype
somehow - say =facet= for example, we could automatically inject two
meta-types:

- =initialzer=: for each type marked as =requires_initialisation=,
  register the formatter. Register the types as a formatter or as a
  helper.
- =traits=: for each formatter in this module (e.g. classes with the
  stereotype of =C++ Artefact Formatter= or =C# Artefact Formatter=),
  ask for their archetype. The formatters would have a meta-data
  parameter to set their archetype. In fact we probably should have a
  separate meta-data parameter (archetype source? archetype?).

We may need to solve the stereotype registration problem though, since
only C++ would know of this facet. Or we could hard-code it in yarn
for now.

Notes:

- how does the initialiser know the formatter is a =quilt.cpp=
  formatter rather than say a C# formatter? this could be done via the
  formatter's archetype - its the kernel.
- users can make use of this very same mechanism to generate their own
  formatters. We can then load up the DLL with boost plugin. Note that
  users are not constrained by the assets meta-model. That is to say,
  they can create new meta-types and inject them into assets. Whilst
  we don't support this use case at present, we should make sure the
  framework does not preclude it. Their DLL then defines the
  formatters which are able to process those meta-types. The only snag
  in all of this is the expansion machinery. We use static visitors
  all over the place, and without somehow dynamically knowing about
  the new types, they will not get expanded. We need to revisit
  expansion in this light to see if there is a way to make it more
  dynamic somehow, or at least have a "default" behaviour for all
  unknown types where we do the generic things to them such as
  computing the file path, etc. This is probably sufficient for the
  vast majority of use cases. The other wrinkle is also locator. We
  are hard-coding paths. If the users limit themselves to creating
  "regular" entities rather than say CMakeLists/msbuild like entities
  which have some special way to compute their names, then we don't
  have a problem. But there should be a generic way to obtain all path
  elements apart from the file name from locator. And also perhaps
  have facets that do not have a facet directory so that we can place
  types above the facet directories such as SLNs, CMakeLists, etc.

*** Create the notion of project destinations                         :story:

At present we have conflated the notion of a facet, which is a logical
concept, with the notion of the folders in which files are placed - a
physical concept. We started thinking about addressing this problem by
adding the "intra-backend segment properties", but as the name
indicates, we were not thinking about this the right way. In truth,
what we really need is to map facets (better: archetype locations) to
"destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly
  if this is done at the element level. That is, you can easily define
  a global mapping for all formatters, and then override it
  locally. This solves the long standing problem of creating say types
  in tests and so forth. With this approach you can create anything
  anywhere.
- we need to have some tests that ensure we don't end up with multiple
  files with the same name at the same destination. This is a
  particular problem for CMake. One alternative is to allow the
  merging of CMake files, but we don't yet have a use case for
  this. The solution would be to have a "merged file flag" and then
  disable all other facets.
- this will work very nicely with profiles: we can create a few out of
  the box profiles for users such as flat project, common facets and
  so on. Users can simply apply the stereotype to their models. These
  are akin to "destination themes". However, we will also need some
  kind of "variable replacement" so we can support cases like
  =include/masd.cpp_ref_impl.northwind/types=. In fact, we also have
  the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder
  structure that depends on package names etc or choose to flatten it
  and can do so for one or all facets. This means for example that we
  could use nested folders for =include=, not use model modules for
  =src= and then flatten it all for =tests=.
- actually it is a bit of a mistake to think of these destinations as
  purely physical. In reality, we may also need them to contribute to
  namespaces. For example, in java the folders and namespaces must
  match. We could solve this by having a "module contribution" in the
  destination. These would then be used to construct the namespace for
  a given facet. Look for java story on backlog for this.
- this also addresses the issue of having multiple serialisation
  formats and choosing one, but having sensible folder names. For
  example, we could have boost serialisation mapped to a destination
  called =serialisation=. Or we could map it to say RapidJSON
  serialisation. Or we could support two methods of serialisation for
  the same project. The user chooses where to place them.

*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe
  a given component type such as a library or an executable. Thus,
  they themselves have sub-types.
- profile models: useful to keep the configuration separate. However,
  it may make more sense to place them in the product model, since its
  shared across components?
- PDMs: these describe platforms.

At present there is no concept of model types, so any meta-model
element can be placed in any model. This is convenient, but in the
future it may make things too complicated: users may end up placing
types in PDMs when they didn't meant to do so, etc. What seems to
emerge from here is that, just as with variability, there is a concept
of a binding point at the model level too. That is, meta-model
elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that
represents each model type. We then instantiate this class within one
of the dogen models to register the different model types. Its
code-generation representation is the registration. It also binds to
all the meta-model elements it binds to. This can be done simply by
creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate
the meta-class information as we generate the assets model). Then, we
can ask the model type if a given element is valid (check a set of
stereotypes).

Formatters are themselves meta-model elements, and they bind to other
meta-model elements (which raises the question: which meta-model
elements are bindable? we can't allow a formatter to bind to a
formatter...). Perhaps we need another type of model, which is a
"generation model". This is where we can either declare new technical
spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by
allowing users to specify the TS/facet when declaring the
formatter. If not specified, then the user must declare a facet in the
package containing the formatter. Similarly with TSs.

Note also that the formatter binding code is "inserted" directly
during generation into the CPP file. Its not possible to change
it. Same with the includes. This ensures the user cannot bypass the
model type system by mistake. Also, by having a formatter meta-model
type, we can now declare the header file as we please, and ensure the
shape of the implementation. Now, the stitch template can be
restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course
require the move to PDMs and the removal of the helper code. This also
means that anyone can declare new meta-model elements; they will
register themselves, and correctly expand across archetype
space. However, we do not have the adaption code nor do we have
containers for these modeling elements. We need a separate story for
this use case.

Destinations are meta-model elements too. In the generation.cpp model
we will declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets
in the archetype space, which express themselves as directories in the
artefact path when we project from archetype space into artefact
space. More generally: assets in asset space are projected into the
multidimensional archetype space. Archetypes are projected into
artefact space, but the dimensions of archetype space are flattened
into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for
file extensions, but conceivably could also be used for other
purposes.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the binaries, allowing users to consume
it:

- in the Dogen case, it means users can create plugins for Dogen;
- in the PDM case, it means users can make use of the PDM in their own
  models;
- for user models, it means you can consume a product in another
  product by referencing its models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.
- the product model itself should be at the top-most directory of the
  git repository. We also need a "models" directory to store models
  which are not expressed as source code (profiles, PDMs, etc). Then,
  for each component, we should have the models at the root directory
  of the component. Whilst this is not in line with our OCD, it is
  required in order for the product model to be able to locate the
  component models. An alternative is to have a convention that we
  always look into a "models" directory (which can be renamed via a
  meta-data parameter) for models, plus any additional directories in
  the "model path". We must inject the model file names to dogen so
  that we do not delete the models.

*** Formatters can only belong to one facet                           :story:

Up to know there was an agreement that generation space was
hierarchical and formatters could only belong to one facet. This has
been true until now, but with the addition of CMake support to tests,
we now have an exception: we need to honour both the tests facet and
the cmake facet. If either of them are off, then we should not emit
the CMake file. This means that we need to somehow map one formatter
to multiple facets. For now we just hacked it and used one of the
facets. It means that if you disable CMake but enable testing you'll
still end up with the testing CMake file.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Consider bucketing elements by meta-type in model                 :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

This can be done in the generation model, as part of the generation
clean up.

*** Add a C++ version to types                                        :story:

Not all system model types are available for all versions. This
applies to the C++ standard (e.g. 98, 11, 14 etc) but also to
boost. We need to be able to mark a type against a version; the user
then declares which version it is using in the model. If the user
attempts to use types that are not available for that version we
should throw.

*** Add facet validation against language standard                    :story:

With the move of enablement to yarn, we can no longer validate facets
against the language standard. For example, we should not allow
hashing on C++ 98. The code was as follows:

#+begin_src c++
void enablement_expander::validate_enabled_facets(
    const global_enablement_configurations_type& gcs,
    const formattables::cpp_standards cs) const {
    BOOST_LOG_SEV(lg, debug) << "Validating enabled facets.";

    if (cs == formattables::cpp_standards::cpp_98) {
        using formatters::hash::traits;
        const auto arch(traits::class_header_archetype());

        const auto i(gcs.find(arch));
        if (i == gcs.end()) {
            BOOST_LOG_SEV(lg, error) << archetype_not_found << arch;
            BOOST_THROW_EXCEPTION(expansion_error(archetype_not_found + arch));
        }

        const auto& gc(i->second);
        if (gc.facet_enabled()) {
            const auto fctn(gc.facet_name());
            BOOST_LOG_SEV(lg, error) << incompatible_facet << fctn;
            BOOST_THROW_EXCEPTION(expansion_error(incompatible_facet + fctn));
        }
    }

    BOOST_LOG_SEV(lg, debug) << "Validated enabled facets.";
}
#+end_src

It was called from the main transform method in enablement transform,
prior to uptading facet enablement.

What we really need is the concept of a technical space in the
metamodel, as well as a "version" for that technical space, and then
also the concept of a facet. Then we are effectively building
(weaving?) an instance of a theoretical TS based on the configuration
(positive variability). We can then validate the configuration. This
should all now be part of archetypes. The versions can be attributes
of technical space with a string version (e.g. "c++ 98) and a numeric
version (1 say) so that we can make comparisons (e.g. c++ 17 > c++
98). Each formatter can then declare its compatibility against the
versions of the technical space.

Merged stories

*Facets incompatible with standards*

Some facets may not be supported for all settings of a language. For
example the hash facet is not compatible with C++ 98. We need to have
some kind of facet/formatter level validation for this.

*** Create the concept of a technical space version                   :story:

We need a simple way to compare versions of technical spaces and have
them mapped into "identifiers" that users can relate to. For example,
C++ versions such as C++ 98 etc are the identifier; we should also
have a simple natural number mapping for each of these. We also need
to take into account the TRs - e.g. a type may be defined on a TR but
not be available on a version.

This should be done when we add technical spaces to the meta-model.

Merged stories:

*Drop the "c++-" prefix in meta-data for standard*

At present we do:

: quilt.cpp.standard=c++-98

The "c++-" seems a bit redundant.

*** Technical space composition                                       :story:

There are some formatters which are really not specific to a technical
space:

- CMake can be used with several languages such as C, C++, etc.
- Visual studio solutions are common to many technical spaces (F#, C#,
  C++, etc).

It seems we need to create a set of generation models which can be
used in conjunction with the "dominant" technical space. These are
triggered by the presence of meta-elements. Or perhaps we can just say
that we iterate through all "non-dominant" technical spaces ("main"
and "secondary"?  "subsidiary"?) and generate anything for which there
is an enabled and matching meta-element.

*** Setting include and source directory to empty                     :story:

At present it does not seem possible to set either the include or
source directories to empty. This probably just requires annotations
to understand empty values, e.g.

: a.b.c=

*** Throw on unsupported stereotypes for specific kernels             :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

*** Add support for multi-components in a model                       :story:

In the world of cross-model transformations (see story), we need lots
of separate models just because they need to generate their own
libraries or executables. It is a bit of a shame that we need to have
a number of "modelets", each for its own component. An alternative
would be to support multiple components from a single model, but this
would be a bit tricky. Thoughts:

- the model would have a multi-component mode, set at the top. No
  model elements are allowed at the top level.
- each package has a stereotype of =dogen::component= (not the best of
  names given it conflicts with UML component diagrams). Dogen
  generates each of these namespaces as a separate component
  (e.g. shared library or executable).
- the top-level model name becomes the first model name, the package
  name the second model name. Interestingly, this should mean dogen
  will generate all components on the top-level directory without any
  additional work.
- the easiest thing to do in terms of the existing pipeline is to
  create the concept of components at the meta-model level and then
  create a transform that takes a component based model and generates
  one model per component and processes them one at a time with the
  existing pipeline. However, we need to be careful because one model
  will contain all of the business logic whereas the other models are
  simple references to it. This could be addressed by having
  references, based on the existing model references.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

** Deprecated
*** CANCELLED Disabling facet after regeneration does not delete file :story:
    CLOSED: [2020-02-17 Mon 18:08]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

Steps to reproduce:

- enable tests for all types.
- generate model.
- disable tests for one type.
- generate model.

Expected that disabling tests for type would result in file
deletion. Instead nothing happens. However, if one deletes the
generated file for the type, then the next generation will correctly
not generate code for the type.

It seems there is some weird mismatch between enablement and lint
removal: we are probably adding the file to the list of expected
files, regardless of whether the facet is enabled or not. However,
this is not always the case because we've proven that enabling and
disabling a facet correctly results in the deletion of files. It must
be something to do with how local enablement is handled.

*** CANCELLED Naming of saved yarn/Dia files is incorrect             :story:
    CLOSED: [2020-02-17 Mon 18:11]

*Rationale*: we don't really use this functionality as is, and in the
future we will create new "top-level" "types for serialisaton instead
of using hand-crafted code, so the story does not add value.

For some random reason when we use dogen to save yarn/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.
*** CANCELLED Incorrect generation when changing external modules     :story:
    CLOSED: [2020-02-17 Mon 18:16]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

When fixing the C# projects, we updated the external modules, from
=dogen::test_models= to =CSharpRefImpl=. Regenerating the model
resulted in updated project files but the rest of the code did not
change. It worked by using =-f=. It should have worked without forcing
the write.

*** CANCELLED Tests for error conditions in libxml                    :story:
    CLOSED: [2020-02-17 Mon 18:19]

*Rationale*: we will move to RapidXML.

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.
*** CANCELLED =Nameable= concept moved position on code generation    :story:
    CLOSED: [2020-02-17 Mon 18:23]

*Rationale*: hasn't happened again for a long time.

During the exogenous model work, yarn's =Nameable= concept moved
position. We need to look at how the parent changes were done to see
if they are stable or not.

*** CANCELLED Consider automatic injection of helpers                 :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

At present we are manually calling:

: a.add_helper_methods();

On each of the class implementation formatters in order to inject
helpers. This is fine for existing cases, but its a bit less obvious
when adding the first helper to an existing template: one does not
quite know why the helper is not coming through without
investigating. One possible solution is to make the helper generation
more "mandatory". Its not entirely obvious how this would work.

*** CANCELLED Clean-up helper terminology                             :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

The name "helper" was never really thought out. It makes little
sense - anything can be a helper. In addition, we have helpers that do
not behave in the same manner (inserter vs every other helper). We
need to come up with a good vocabulary around this.

- static aspects: those that are baked in to the file formatter.
- dynamic aspects: those that are inserted in to the file formatter at
  run time.
- type-dependent dynamic aspects: those that are connected to the
  types used in the file formatter.

Merged stories:

*Type-bound helpers and generic helpers*

Not all helpers are bound to a type. We have the case of inserter
helper in io which is used by main formatters directly. We need to
make this distinction in the manual.
*** CANCELLED Helper methods should have their own includes           :story:
    CLOSED: [2020-02-17 Mon 18:25]

*Rationale*: helpers will be removed.

This should be fairly straightforward:

- ensure we compute helpers before we do includes in formattables
  factory;
- add include API to helpers (=inclusion_dependencies=)
- during inclusion expansion, go through all helpers associated with a
  element and ask them for their dependencies.
- note that we still need a good solution for the "special helpers" in
  order for this to work.

*Previous Understanding*

When a formatter relies on the helper methods, we have a problem: we
need to determine the required includes from the main formatter
without knowing what the helper methods may need. We have hacked this
with things like the "special includes" but there must be a cleaner
way of doing this. For example, we could ask the helper methods
formatter to provide its includes and it would be its job to either
delegate further or to compute the includes. This would at least
remove the duplication of code between io and types.

This task will be made much easier once we have stitch support
for named regions.

As part of the work to make helpers dynamic we reached the following
conclusions:

Note: when time comes to support includes in helper methods, we can
take a similar approach as we do for formatters now. The helper method
implements some kind of include provider interface, which is then used
by the inclusion dependencies builder. The only slight snag is that we
need to first resolve the type into a type family and then go to the
helper interface.
*** CANCELLED Create different kinds of master header files           :story:
    CLOSED: [2020-03-02 Mon 15:22]

*Rationale*: this feature has been removed.

#+begin_quote
*Story*: As a dogen user, I don't want to include every object in a
model when I use includers.
#+end_quote

At present we are using the facet includers in unit tests. This is not
ideal because it means that every time we do a change in a service
header, all tests recompile. In reality we should have two types of
inclusions:

- canned tests should include only value objects, etc - e.g. no
  services.
- service tests should include the header for the service and any
  additional dependencies the service may require.

Perhaps we could have a second type of includer that only has value
objects, etc.

Another way to look at this is that there should be user-configurable
master header files:

#+begin_quote
*Story*: As a dogen user, I want to create master header files for
user defined sets of files so that I don't have to do it manually.
#+end_quote

Merged stories:

*Add an includer for all includers*

#+begin_quote
*Story*: As a dogen user, I need a quick and dirty way of including
all files in a model so that I can test them without having to
include every file manually.
#+end_quote

It would be nice to totally include a model. For that we need an
includer that includes all other includers. This should be as easy as
keeping track of the different includers for each facet in the map
inside of the includer service.

We need to find a good use case for this.

Taking into account the "[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-renaming-includers][master header]]" rename, this would be a
"master master include" file?
*** CANCELLED Stitch does not handle directories very well            :story:
    CLOSED: [2020-03-02 Mon 15:26]

*Rationale*: stand-alone stitch is no longer supported.

At present we seem to generate log files called =.= when we use stitch
against a directory. This should only happen if we use =.= on the
target parameter, e.g.:

: --target .

Not sure why it is happening when we call stitch from CMake since it
should use the full path to the =cpp= directory.
*** CANCELLED Consider changing fields where "qualified name" is not qualified :story:
    CLOSED: [2020-03-05 Thu 12:58]

*Rationale*: this story will be impacted by the current variability
refactor.

At present, the the qualified field name is not always a prefix +
simple name. For example, for general settings and for stitch, the
qualified field names do not have a prefix. We could just add a prefix
to make everything symmetric (e.g. =formatters.copyright_notice=) but
it would make the fields less readable at the usage point and this was
the reason why we didn't add it in the first place. For now, we will
leave stitch as it is. This is a bit more meaningful with the
annotation rename.

This may even be a more wide-ranging question: why do users need to
know who owns the field? e.g. =dia.comment=, do I care?
*** CANCELLED Add a modeline to stitch                                :story:
    CLOSED: [2020-03-09 Mon 16:17]

*Rationale*: this story is deprecated in light of the changes to
stitch templates and will be even more deprecated when we finish the
templating refactor.

It would be nice to be able to supply the mode and other emacs
properties to stitch templates. For that we just need a special KVP
used at the top that contains the modeline:

: <#@ modeline="-*- mode: poly-stitch; tab-width: 4; indent-tabs-mode: nil; -*-" #>

Stitch can read this KVP and ignore it.

*** CANCELLED Add support for command line meta-data parameters       :story:
    CLOSED: [2020-03-09 Mon 16:20]

*Rationale*: variability overrides solve most of the problems in this
story, and we haven't had use cases for what is not solved, so the
story is considered deprecated.

We do not want to force end users to change their existing file
format. However, it is sometimes necessary to supply parameters into
dogen which are not representable in the existing format. We could
create a very simple extension to the command line arguments that
would generate scribbles; these would then be appended to the model
during the yarn workflow. Example:

: --kvp a=b

or:

: --meta-data a=b

These are in effect model-module level tagged values. We should be
able to supply them in a file or as command-line parameters.

This could cause all sorts of weird and wonderful problems such as
unrepeatable behaviour, so we need to find a very good use case for it
first.

One use case for this is the "enable all" functionality, useful for
testing. It would put dogen in a mode where all facets would be
enabled, regardless.
*** CANCELLED Throw on profiles that refer to invalid fields          :story:
    CLOSED: [2020-03-09 Mon 16:32]

*Rationale*: compatibility mode has been implemented and stand-alone
weaving has been deprecated, so this story has bit-rotted too much to
be useful.

At present during profile instantiation, if we detect a field which
does not exist we skip the profile. This was done in the past because
we had different binaries for stitch, knit etc, which meant that we
could either split profiles by application or skip errors
silently. Now we have a single binary, we could enable this
validation. However, the stitch tests still rely on this
behaviour. The right solution for this is to have some kind of
override flag ("compatibility mode" springs to mind) which is off by
default but can be used (judiciously).

We put a fix in but it seems weave is still borked. The problem
appears to be that we do something in the generation path that is not
done for weaving (and presumably for conversion). The hack was put
back in for now.

This story is dependent on moving annotations out of stitch. Once this
is done we can enable validation.
