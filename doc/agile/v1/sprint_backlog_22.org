#+title: Sprint Backlog 22
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Start the generation refactor.
- Get a good grasp of the physical domain model.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2020-02-17 Mon 18:27]
| <75>                                |        |      |      |       |
| Headline                            | Time   |      |      |     % |
|-------------------------------------+--------+------+------+-------|
| *Total time*                        | *0:44* |      |      | 100.0 |
|-------------------------------------+--------+------+------+-------|
| Stories                             | 0:44   |      |      | 100.0 |
| Active                              |        | 0:44 |      | 100.0 |
| Sprint and product backlog grooming |        |      | 0:44 | 100.0 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** Edit release notes for previous sprint                            :story:

Add github release notes for previous sprint.

Title: Dogen v1.0.20, "Oasis do Arco"

#+BEGIN_SRC markdown
**DRAFT: release notes are still being worked on**

![Oasis do Arco](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Mini_oasis_in_the_namibe_desert%2C_Angola.JPG/800px-Mini_oasis_in_the_namibe_desert%2C_Angola.JPG)
_Arco Oasis, Namibe, Moçamedes, Angola. (C) 2011 [Paulo César Santos](https://commons.wikimedia.org/wiki/File:Mini_oasis_in_the_namibe_desert,_Angola.JPG)_

# Introduction

New year, new Dogen sprint! At around two months of elapsed time for 83 hours worth of commitment, this was yet another long, drawn-out affair, and the festive period most certainly did not help matters. Having said that, the sprint was reasonably focused on the mission at hand: making the relational model _just about_ usable. In doing so, it provided its fair share of highs and lows, and taught a great deal of lessons - more than we ever wished for. Ah, the joys, the joys. But, onwards we march!

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. There were only a few small features this sprint, and there are no breaking changes.

[![Sprint 1.0.20 Demo](https://img.youtube.com/vi/TkYQTW_jAGk/0.jpg)](https://youtu.be/TkYQTW_jAGk)
_Video 1: Sprint 20 Demo._

## Add ODB type overrides to primitives

ORM type overrides had not been used in anger until the relational model was introduced (see below), and, as a result, we did not notice any problems with its implementation. Because the relational model makes heavy use of JSONB, we quickly spotted an issue when declaring type overrides inline with the column (_i.e._, at the attribute level):

```
#DOGEN masd.orm.type_override=postgresql,JSONB
```

According to the [ODB manual](https://www.codesynthesis.com/products/odb/doc/manual.xhtml#14.8), this incantation is not sufficient to cope with conversion functions and other more complex uses. And so, with this sprint, type mapping was updated to take advantage of ODB's flexibility. You can now define type mappings at the element level:

```
#DOGEN masd.orm.type_override=postgresql,JSONB
#DOGEN masd.orm.type_mapping=postgresql,JSONB,TEXT,to_jsonb((?)::jsonb),from_jsonb((?))
#DOGEN masd.orm.type_mapping=sqlite,JSON_TEXT,TEXT,json((?))
```

You can then make use of it at attribute level, as previously. An even better scenario is to define a ```masd::primitive``` for the type, which takes care of it for you, and generates code like so:

```
#pragma db member(json::value_) column("") pgsql:type("JSONB")
```

For example uses of JSONB, please look at the discussion on the relational model in section _Significant Internal Stories_ below.

## Allow outputting the model's SHA1 hash in decoration

The decoration marker has been expanded to allow recording the SHA1 hash of the target model. This is intended as a simple way to keep track of which model was used to generate the source code. In order to switch it on, simply add ```add_origin_sha1_hash``` to the generation marker:

![Decoration marker](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/decoration_sha1_hash_example.png)
_Figure 1: Sample decoration marker, obtained from the C++ Reference Model._

The generated code will then contain the SHA1 hash:

```c++
/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This is a code-generated file.
 *
 * Model SHA1 hash: be42bdb7f246ad4040f17dbcc953222492e1a3bf
 * WARNING: do not edit this file manually.
 * Generated by MASD Dogen v1.0.21
```

Sadly the SHA1 hash does not match the [git hash](https://stackoverflow.com/questions/5290444/why-does-git-hash-object-return-a-different-hash-than-openssl-sha1); however, one can easily use ```sha1sum``` to compute the hash manually:

```
$ sha1sum cpp_ref_impl.lam_model.dia
be42bdb7f246ad4040f17dbcc953222492e1a3bf  cpp_ref_impl.lam_model.dia
```

Before we move on, there are a couple of points worthy of note with regards to this feature. First and foremost, please heed the following warning:

> :warning: : **Important**: Remember that SHA1 hashes in Dogen **are NOT a security measure**; they exist **only** for informational purposes.

Secondly, as we mentioned in the past, features such as these (_e.g._ date/time, Dogen version, SHA1 hash, _etc._) should be used with caution since they may cause unnecessary changes to generated code and thus trigger expensive rebuilds. As such, we recommend that careful consideration is given before enabling it.

## Improvements in generation timestamps

For the longest time, Dogen has allowed users to stamp each file it generates with a _generation timestamp_. This is enabled via the parameter ```add_date_time```, which is part of the  generation marker meta-element; for an example of this meta-element see [the screenshot above](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/decoration_sha1_hash_example.png), where it is disabled.

When enabled, a typical output looks like so:

```c++
/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This is a code-generated file.
 *
 * Generation timestamp: 2020-01-22T08:29:41
 * WARNING: do not edit this file manually.
 * Generated by MASD Dogen v1.0.21
 *
```

In this sprint we did some minor improvements around the sourcing of this timestamp. Previously, we obtained it individually for each and every generated file, resulting in a (possibly) moving timestamp across a model generation. With this release, the timestamp for a given activity - _e.g._ conversion, generation, _etc._ - is now obtained once upfront and reused by all those who require it. Not only is this approach more performant but it yields a better outcome because users are not particularly interested in the precise second _any given file_ was generated, but care more about knowing when _a given model_ was generated.

In addition, we decided to allow users to control this timestamp externally. The main rationale for this was unit testing, where having a moving timestamp with each test run was just asking for trouble. While we were at it, we also deemed sensible to allow users to override this timestamp, if, for whatever reason, they need to. Now, lest you start to think we are enabling "tampering", we repeat the previous warning:

> :warning: **Important**: Remember that generation timestamps in Dogen **are NOT a security measure**; they exist **only** for informational purposes.

With that disclaimer firmly in hand, lets see how one can override the generation timestamp. A new command line argument was introduced:

```
Processing:
<SNIP>
  --activity-timestamp arg       Override the NOW value used for the activity
                                 timestamp. Format: %Y-%m-%dT%H:%M:%S
```

For instance, to change the generation timestamp of the example above, one could set it to ```--activity-timestamp 2020-02-01T01:01:01```, obtaining the following output:

```c++
/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * This is a code-generated file.
 *
 * Generation timestamp: 2020-02-01T01:01:01
 * WARNING: do not edit this file manually.
 * Generated by MASD Dogen v1.0.21
```

Clearly, this is more of a troubleshooting feature than anything else, but it may prove to be useful.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_20.org).

## Milestones

The 9999th commit was made to Dogen this sprint.

![100th release](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/milestones_9999_commits.png)
_Figure 2: GitHub repo at the 9999th commit._

## Significant Internal Stories

The sprint was mostly dominated by one internal story, which this section describes in detail.

### Add relational tracing support

This sprint brought to a close work on the relational model. It was the culmination of a multi-sprint effort that required some significant changes to the core of Dogen - particularly to the tracing subsystem, as well as to ORM. The hard-core Dogen fan may be interested in a series of videos which captured the design and development of this feature:

[![MASD - Dogen Coding: Relational Model for Tracing - Part 1](https://img.youtube.com/vi/re36Sr1u0Iw/0.jpg)](https://www.youtube.com/watch?v=re36Sr1u0Iw&list=PLwfrwe216gF3EzrrvwNFivnLSZ2l8X9k6&index=2)
_Video 2: Playlist "MASD - Dogen Coding: Relational Model for Tracing"._

The (rather long) series of videos will hopefully reach its "climax" next sprint, but (spoiler alert) its "TL; DR" is that it is now possible to dump all information produced by a Dogen run into a relational database. This includes both tracing data as well as all logging, at the user-chosen log level. It is important to note that a full run in this manner is slow: dumping all of Dogen's models (18, at the present count) can take the best part of an hour. Interestingly, the majority of the cost comes from dumping the log at debug level. A dump with just tracing information takes less than 10 minutes, making it reasonably useful. Regardless of the wait, once the data is in the database, the full power of SQL and Postgres can be harnessed.

Implementation-wise, we decided to take path of least resistance and create a small number of tables, code-generated by Dogen and [ODB](https://www.codesynthesis.com/products/odb/):

```
musseque=> \dt
            List of relations
 Schema |      Name       | Type  | Owner
--------+-----------------+-------+-------
 DOGEN  | LOG_EVENT       | table | build
 DOGEN  | RUN_EVENT       | table | build
 DOGEN  | TRANSFORM_EVENT | table | build
(3 rows)
```

Models and other complex data types stored in JSONB fields, _e.g._:

```
musseque=> \dS "RUN_EVENT"
                            Table "DOGEN.RUN_EVENT"
     Column     |            Type             | Collation | Nullable | Default
----------------+-----------------------------+-----------+----------+---------
 TIMESTAMP      | timestamp without time zone |           |          |
 RUN_ID         | text                        |           | not null |
 EVENT_TYPE     | integer                     |           | not null |
 VERSION        | text                        |           | not null |
 PAYLOAD        | jsonb                       |           | not null |
 ACTIVITY       | text                        |           | not null |
 LOGGING_IMPACT | text                        |           | not null |
 TRACING_IMPACT | text                        |           | not null |
Indexes:
    "RUN_EVENT_pkey" PRIMARY KEY, btree ("RUN_ID", "EVENT_TYPE")
```

Though by no means trivial, this approach required fewer changes to Dogen itself, pushing instead the complexity to the queries over the generated dataset. This seemed like a worthwhile trade-off at the time, because normalising a Dogen model in code was a non-trivial exercise. Nonetheless, as we sooon find out, writing queries with complex JSON documents and multiple rows is not an entirely trivial exercise either. As an example, the following query returns objects in a Dia diagram:

```sql
create or replace function classes_in_diagram(in p_transform_instance_id text)
    returns table("ID" text, "NAME" text)
as $$
    select "ID", substring(attrs."ATTRIBUTES"->'values'->0->'data'->>'value', 2,
            length(attrs."ATTRIBUTES"->'values'->0->'data'->>'value') - 2
        ) "NAME"
    from (
        select
            objects."OBJECT"->>'id' "ID",
            objects."OBJECT"->>'type' "TYPE",
            jsonb_array_elements(objects."OBJECT"->'attributes') "ATTRIBUTES"
            from (
                select * from dia_objects_in_diagram(p_transform_instance_id)
            ) as objects
     ) as attrs
     where
         attrs."ATTRIBUTES"->>'name' like 'name' and "TYPE" like 'UML - Class';
$$ language 'sql';
```

This function can be used as follows:

```
=> select * from dia_objects_names_and_stereotypes('8ce7069e-6261-4f9f-b701-814bed17cafb');
 ID  |    NAME     |        STEREOTYPES
-----+-------------+----------------------------
 O1  | cpp         | masd::decoration::modeline
 O2  | cs          | masd::decoration::modeline
 O3  | cmake       | masd::decoration::modeline
 O4  | odb         | masd::decoration::modeline
 O5  | xml         | masd::decoration::modeline
 O7  | xml         | masd::decoration::modeline
 O8  | odb         | masd::decoration::modeline
 O9  | cmake       | masd::decoration::modeline
 O10 | cs          | masd::decoration::modeline
 O11 | cpp         | masd::decoration::modeline
 O13 | apache_v2_0 | masd::decoration::licence
 O14 | bsl_v1_0    | masd::decoration::licence
 O15 | gpl_v2      | masd::decoration::licence
 O16 | gpl_v3      | masd::decoration::licence
 O17 | proprietary | masd::decoration::licence
 O18 | sln         | masd::decoration::modeline
 O19 | sln         | masd::decoration::modeline
````
A library of assorted functions was assembled this way (see [functions.sql](https://github.com/MASD-Project/dogen/blob/master/projects/dogen.relational/sql/functions.sql)), and proved useful enough to track the problem at hand which was to figure out why the [new meta-element registrar](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_20.org#move-registrar-into-assets) was not being generated. In addition, the expectation is that, over time, more and more powerful queries will be written, allowing us to better exploit the available information. However, it must be said that the complexity of writing JSONB queries is much higher than anticipated, and as such, the feature is not quite as useful as we envisioned. With a bit of luck, next sprint we shall produce a blog post narrating in more detail the saga and its somewhat surprising conclusions.

## Resourcing

Now that we have moved to part-time sprints, looking only at the overall commitment makes less sense; after all, by definition, one is guaranteed to have around 80 hours of work on a sprint. Whilst pondering on this matter, another interesting measure popped up on our radars: the _utilisation rate_ - though, perhaps, not yet its final name. The utilisation rate is computed as the number of days on a full time sprint (_e.g._, 14) divided by the total number of days elapsed since the previous sprint. The utilisation rate measures how "expensive" a day of work is in terms of elapsed days. A high utilisation rate is good, and a low one is bad; on a good sprint we are aiming for close to 50%. In this particular sprint our utilisation rate was around 23%. Since the previous sprint involved a long stretch where we were not doing any work at all, we do not have any comparative figures, but we'll keep tracking this number from now on and hopefully it will became a useful indicator.In terms of our more traditional measurements, the sprint was rather well behaved, as the chart demonstrates:

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_20_pie_chart.jpg)
_Figure 3: Cost of stories for sprint 20._

Some 45% of the total committed time was taken by the relational model and related activities; and even diversions such as the SHA1 hashes (6.8%) and improvements on generation timestamps (2.3%) were actually byproducts of this work. In terms of process, this was an expensive sprint: whilst the demo was cheap (3%), the release notes were very expensive (13.7%) and so was backlog grooming (5.7%), resulting on an overall figure of 22.4% for process - one of the most costly sprints in this department. Part of this is related to the amount of "uncoordinated" work that had been carried out previously and which was difficult to describe in a manner suitable for the release notes  (remember that demo and release notes describe the work of the _previous sprint_, _e.g. sprint 19 in this case). All and all, for a part time sprint, it was a rather successful one, though we are clearly aiming for a higher utilisation rate for the next one.

## Roadmap

We still haven't quite managed to get the roadmap to work for us, but it seems to provide some kind of visual indication of just how long the road ahead is so we're keeping it for now. However, for it to became truly useful in our current process it requires some more tuning. Perhaps some time spent learning [task juggler](http://taskjuggler.org/) is in order...

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_20_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_20_resource_allocation_graph.png)

# Next Sprint

Now that the relational model is out of the way, the focus on meta-model entities and the fabric clean-up is resumed once more. We are hoping to get one or two of these entities out of the way by sprint end.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.20_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.20/dogen_1.0.20_amd64-applications.deb)
- [dogen-1.0.20-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.20/DOGEN-1.0.20-Darwin-x86_64.dmg)
- [dogen-1.0.20-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.20-Windows-AMD64.msi)

**Note:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+END_SRC markdown

- [[https://twitter.com/MarcoCraveiro/status/1220738254158344196][twitter]]
- [[https://www.linkedin.com/posts/marco-craveiro-31558919_dogen-the-masd-code-generator-generates-activity-6626505315070332929-a5pv/][https://www.linkedin.com/posts/marco-craveiro-31558919_masd-projectdogen-activity-6626505954353569792-JAue]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

https://lnkd.in/eAwwjRv

*** Create a demo and presentation for previous sprint                :story:

Time spent creating the demo and presentation. Use the demo project:

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2020-02-17 Mon 17:43]--[2020-02-17 Mon 18:27] =>  0:44
    :END:

Updates to sprint and product backlog.

*** COMPLETED Adding reference to itself results in resolution errors :story:
    CLOSED: [2020-02-17 Mon 18:17]

*Rationale*: this was fixed a few sprints ago.

Whilst trying to fix the JSON models we inadvertently added a
self-reference in =dogen.generation.json=:

:    "yarn.reference": "dogen.generation.json",

This resulted in some puzzling errors:

: 2018-10-18 19:15:00.861210 [ERROR] [yarn.transforms.enablement_transform] Duplicate element archetype: quilt.cpp.serialization.registrar_implementation <dogen><generation><registrar>

Ideally we should either warn and ignore or fail to process models
with self-references.

*** Cannot see source file in coveralls                               :story:

 At present the path of source files in coveralls is incorrect:

 : /cpp_ref_impl.boost_model/src/types/class_a.cpp

 : SOURCE NOT AVAILABLE
 : The file "cpp_ref_impl.boost_model/src/types/class_a.cpp" isn't available on github. Either it's been removed, or the repo root directory needs to be updated.

 We have the same problem in codecove, only there is worse because we
 also can't see the fake commit we did.

*** Integration of archetypes into assets                             :story:

Up to recently, there was a belief that the archetypes model was
distinct from the assets model. The idea was that the projection of
assets into archetype space could be done without knowledge of the
things we are projecting. However, that is demonstrably false: n order
to project we need a name. That name contains a location. The location
is a point on a one-dimensional asset space.

In reality, what we always had is:

- a first dimension within assets space: "modeling dimension",
  "logical dimension"? It has an associated location.
- a second dimension within assets space: "physical dimension", with
  an associated location. Actually we cannot call it physical because
  physical is understood to mean the filesystem.

So it is that concepts such as archetype, facet and technical space
are all part of assets - they just happen to be part of the
two-dimensional projection. Generation is in effect a collection of
model to text transforms that adapts the two-dimensional element
representation into the extraction meta-model. Formatters are model to
text transforms which bind to locations in the physical dimension.

In this view of the world, we have meta-model elements to declare
archetypes, with their associated physical locations. This then
results in the injection of these meta-elements. Formatters bind to
these locations.

However, note that formatters provide dependencies. This is because
these are implementation dependent. This means we still need some
transforms to occur at the generation level. However, all of the
dependencies which are modeling related should happen within
assets. Only those which are formatter specific should happen in
generation. The problem though is that at present we deem all
dependencies to be formatter specific and each formatter explicitly
names its dependencies against which facets. It does make sense for
these to be together.

Perhaps what we are trying to say is that there are 3 distinct
concepts:

- modeling locations;
- logical locations;
- physical locations.

The first two are within the domain of assets. The last one is in the
domain of generation and extraction. Assets should make the required
data structures available, but it is the job of generation to populate
this information. Thus directory themes, locator, etc are all
generation concepts.

One could, with a hint of humour, call the "logical dimension" the
meta-physical dimension. This is because it provides the meta-concepts
for the physical dimension.

A backend provides a translation into a representation considered
valid according to the rules of a technical space. A backend can be
the primary or secondary backend for a technical space. A component
can only have a primary backend, and any number of secondary
backends. Artefacts produced by a backend must have a unique physical
location. In LAM mode, the component is split into multiple
components, each with their own primary technical space.

*** Define a combined logical-physical space                          :story:

It now seems that we have been searching for a meta-model that
combines both aspects of logical modeling as well as physical
modeling. Facets, archetypes etc are all parts of the physical
dimension of this space. We need to find all stories on this topic and
organise them to see if we can come up with a consistent system of
meaning.

Notes:

- archetypes must support a notion of "kind". This is so we can have
  public include headers, private include headers and implementation
  files. This "kind" affects the topology of the physical dimension.
- the locator is a function that takes points in the logical-physical
  space and maps them to filesystem locations. It uses properties of
  those meta-model elements to configure the mapping.

*** Create a archetypes locator                                       :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present;
they should be a transform inside generation.

*** Clean-up archetype locations modeling                             :story:

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*** Move dependencies into archetypes                                 :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Move formatting styles into generation                            :story:

We need to support the formatting styles at the meta-model level.

*** Make creating new facets easier                                   :story:

For types that are stitchable such as formatters, we need to always
copy and paste the template form another formatter and then update
values. It would be great if we could have dogen generate a bare-bones
stitch template. This is pretty crazy so it requires a bit of
concentration to understand what we're doing here:

- detect that the =yarn::object= is annotated as
  =quilt.cpp.types.class_implementation.formatting_style= =stitch=.
- find the corresponding expected stitch file. If none is available,
  /dynamically/ change the =formatting_style= to =stock= and locate a
  well-known stitch formatter.
- the stitch formatter uses a stitch template that generates stitch
  templates. Since we cannot escape stitch markup, we will have to use
  the assistant. One problem we have is that the formatter does not
  state all of the required information such as what yarn types does
  it format and so forth. We probably need a meta-model concept to
  capture the idea of formatters - and this could be in yarn - and
  make sure it has all of this information. This also has the
  advantage of making traits, initialisers etc easier. We can do the
  same for helpers too.
- an additional wrinkle is that we need different templates for
  different languages. However, perhaps these are just wale templates
  in disguise rather than stitch templates? Then we can have the
  associated default wale templates, very much in the same way we have
  wale templates for the header files. They just happen to have stitch
  markup rather than say C++ code.

This is a radically different way from looking at the code. We are now
saying that yarn should have concepts for:

- facets: specialisation of modules with meta-data such as facet name
  etc. This can be done via composition to make our life easier.
- formatters and helpers: elements which belong to a facet and know of
  their archetype, wale templates, associated yarn element and so
  forth.

We then create stereotypes for these just like we did for
=enumeration=. As part of the yarn parsing we instantiate these
meta-objects with all of their required information. In addition, we
need to create what we are calling at present "profiles" to define
their enablement and to default some of its meta-data.

When time comes for code-generation, these new meta-types behave in a
more interesting way:

- if there is no stitch template, we use wale to generate it.
- once we have a stitch template, we use stitch to generate the c++
  code. From then on, we do not touch the stitch template. This
  happens because overwrite is set to false on the enablement
  "profile".

Merged stories:

*Code generate initialisers and traits*

If we could mark the modules containing facets with a stereotype
somehow - say =facet= for example, we could automatically inject two
meta-types:

- =initialzer=: for each type marked as =requires_initialisation=,
  register the formatter. Register the types as a formatter or as a
  helper.
- =traits=: for each formatter in this module (e.g. classes with the
  stereotype of =C++ Artefact Formatter= or =C# Artefact Formatter=),
  ask for their archetype. The formatters would have a meta-data
  parameter to set their archetype. In fact we probably should have a
  separate meta-data parameter (archetype source? archetype?).

We may need to solve the stereotype registration problem though, since
only C++ would know of this facet. Or we could hard-code it in yarn
for now.

Notes:

- how does the initialiser know the formatter is a =quilt.cpp=
  formatter rather than say a C# formatter? this could be done via the
  formatter's archetype - its the kernel.
- users can make use of this very same mechanism to generate their own
  formatters. We can then load up the DLL with boost plugin. Note that
  users are not constrained by the assets meta-model. That is to say,
  they can create new meta-types and inject them into assets. Whilst
  we don't support this use case at present, we should make sure the
  framework does not preclude it. Their DLL then defines the
  formatters which are able to process those meta-types. The only snag
  in all of this is the expansion machinery. We use static visitors
  all over the place, and without somehow dynamically knowing about
  the new types, they will not get expanded. We need to revisit
  expansion in this light to see if there is a way to make it more
  dynamic somehow, or at least have a "default" behaviour for all
  unknown types where we do the generic things to them such as
  computing the file path, etc. This is probably sufficient for the
  vast majority of use cases. The other wrinkle is also locator. We
  are hard-coding paths. If the users limit themselves to creating
  "regular" entities rather than say CMakeLists/msbuild like entities
  which have some special way to compute their names, then we don't
  have a problem. But there should be a generic way to obtain all path
  elements apart from the file name from locator. And also perhaps
  have facets that do not have a facet directory so that we can place
  types above the facet directories such as SLNs, CMakeLists, etc.

*** Create the notion of project destinations                         :story:

At present we have conflated the notion of a facet, which is a logical
concept, with the notion of the folders in which files are placed - a
physical concept. We started thinking about addressing this problem by
adding the "intra-backend segment properties", but as the name
indicates, we were not thinking about this the right way. In truth,
what we really need is to map facets (better: archetype locations) to
"destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly
  if this is done at the element level. That is, you can easily define
  a global mapping for all formatters, and then override it
  locally. This solves the long standing problem of creating say types
  in tests and so forth. With this approach you can create anything
  anywhere.
- we need to have some tests that ensure we don't end up with multiple
  files with the same name at the same destination. This is a
  particular problem for CMake. One alternative is to allow the
  merging of CMake files, but we don't yet have a use case for
  this. The solution would be to have a "merged file flag" and then
  disable all other facets.
- this will work very nicely with profiles: we can create a few out of
  the box profiles for users such as flat project, common facets and
  so on. Users can simply apply the stereotype to their models. These
  are akin to "destination themes". However, we will also need some
  kind of "variable replacement" so we can support cases like
  =include/masd.cpp_ref_impl.northwind/types=. In fact, we also have
  the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder
  structure that depends on package names etc or choose to flatten it
  and can do so for one or all facets. This means for example that we
  could use nested folders for =include=, not use model modules for
  =src= and then flatten it all for =tests=.
- actually it is a bit of a mistake to think of these destinations as
  purely physical. In reality, we may also need them to contribute to
  namespaces. For example, in java the folders and namespaces must
  match. We could solve this by having a "module contribution" in the
  destination. These would then be used to construct the namespace for
  a given facet. Look for java story on backlog for this.
- this also addresses the issue of having multiple serialisation
  formats and choosing one, but having sensible folder names. For
  example, we could have boost serialisation mapped to a destination
  called =serialisation=. Or we could map it to say RapidJSON
  serialisation. Or we could support two methods of serialisation for
  the same project. The user chooses where to place them.

*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe
  a given component type such as a library or an executable. Thus,
  they themselves have sub-types.
- profile models: useful to keep the configuration separate. However,
  it may make more sense to place them in the product model, since its
  shared across components?
- PDMs: these describe platforms.

At present there is no concept of model types, so any meta-model
element can be placed in any model. This is convenient, but in the
future it may make things too complicated: users may end up placing
types in PDMs when they didn't meant to do so, etc. What seems to
emerge from here is that, just as with variability, there is a concept
of a binding point at the model level too. That is, meta-model
elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that
represents each model type. We then instantiate this class within one
of the dogen models to register the different model types. Its
code-generation representation is the registration. It also binds to
all the meta-model elements it binds to. This can be done simply by
creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate
the meta-class information as we generate the assets model). Then, we
can ask the model type if a given element is valid (check a set of
stereotypes).

Formatters are themselves meta-model elements, and they bind to other
meta-model elements (which raises the question: which meta-model
elements are bindable? we can't allow a formatter to bind to a
formatter...). Perhaps we need another type of model, which is a
"generation model". This is where we can either declare new technical
spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by
allowing users to specify the TS/facet when declaring the
formatter. If not specified, then the user must declare a facet in the
package containing the formatter. Similarly with TSs.

Note also that the formatter binding code is "inserted" directly
during generation into the CPP file. Its not possible to change
it. Same with the includes. This ensures the user cannot bypass the
model type system by mistake. Also, by having a formatter meta-model
type, we can now declare the header file as we please, and ensure the
shape of the implementation. Now, the stitch template can be
restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course
require the move to PDMs and the removal of the helper code. This also
means that anyone can declare new meta-model elements; they will
register themselves, and correctly expand across archetype
space. However, we do not have the adaption code nor do we have
containers for these modeling elements. We need a separate story for
this use case.

Destinations are meta-model elements too. In the generation.cpp model
we will declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets
in the archetype space, which express themselves as directories in the
artefact path when we project from archetype space into artefact
space. More generally: assets in asset space are projected into the
multidimensional archetype space. Archetypes are projected into
artefact space, but the dimensions of archetype space are flattened
into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for
file extensions, but conceivably could also be used for other
purposes.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the binaries, allowing users to consume
it:

- in the Dogen case, it means users can create plugins for Dogen;
- in the PDM case, it means users can make use of the PDM in their own
  models;
- for user models, it means you can consume a product in another
  product by referencing its models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.
- the product model itself should be at the top-most directory of the
  git repository. We also need a "models" directory to store models
  which are not expressed as source code (profiles, PDMs, etc). Then,
  for each component, we should have the models at the root directory
  of the component. Whilst this is not in line with our OCD, it is
  required in order for the product model to be able to locate the
  component models. An alternative is to have a convention that we
  always look into a "models" directory (which can be renamed via a
  meta-data parameter) for models, plus any additional directories in
  the "model path". We must inject the model file names to dogen so
  that we do not delete the models.

*** Formatters can only belong to one facet                           :story:

Up to know there was an agreement that generation space was
hierarchical and formatters could only belong to one facet. This has
been true until now, but with the addition of CMake support to tests,
we now have an exception: we need to honour both the tests facet and
the cmake facet. If either of them are off, then we should not emit
the CMake file. This means that we need to somehow map one formatter
to multiple facets. For now we just hacked it and used one of the
facets. It means that if you disable CMake but enable testing you'll
still end up with the testing CMake file.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Consider bucketing elements by meta-type in model                 :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

This can be done in the generation model, as part of the generation
clean up.

*** Add a C++ version to types                                        :story:

Not all system model types are available for all versions. This
applies to the C++ standard (e.g. 98, 11, 14 etc) but also to
boost. We need to be able to mark a type against a version; the user
then declares which version it is using in the model. If the user
attempts to use types that are not available for that version we
should throw.

*** Add facet validation against language standard                    :story:

With the move of enablement to yarn, we can no longer validate facets
against the language standard. For example, we should not allow
hashing on C++ 98. The code was as follows:

#+begin_src c++
void enablement_expander::validate_enabled_facets(
    const global_enablement_configurations_type& gcs,
    const formattables::cpp_standards cs) const {
    BOOST_LOG_SEV(lg, debug) << "Validating enabled facets.";

    if (cs == formattables::cpp_standards::cpp_98) {
        using formatters::hash::traits;
        const auto arch(traits::class_header_archetype());

        const auto i(gcs.find(arch));
        if (i == gcs.end()) {
            BOOST_LOG_SEV(lg, error) << archetype_not_found << arch;
            BOOST_THROW_EXCEPTION(expansion_error(archetype_not_found + arch));
        }

        const auto& gc(i->second);
        if (gc.facet_enabled()) {
            const auto fctn(gc.facet_name());
            BOOST_LOG_SEV(lg, error) << incompatible_facet << fctn;
            BOOST_THROW_EXCEPTION(expansion_error(incompatible_facet + fctn));
        }
    }

    BOOST_LOG_SEV(lg, debug) << "Validated enabled facets.";
}
#+end_src

It was called from the main transform method in enablement transform,
prior to uptading facet enablement.

What we really need is the concept of a technical space in the
metamodel, as well as a "version" for that technical space, and then
also the concept of a facet. Then we are effectively building
(weaving?) an instance of a theoretical TS based on the configuration
(positive variability). We can then validate the configuration. This
should all now be part of archetypes. The versions can be attributes
of technical space with a string version (e.g. "c++ 98) and a numeric
version (1 say) so that we can make comparisons (e.g. c++ 17 > c++
98). Each formatter can then declare its compatibility against the
versions of the technical space.

Merged stories

*Facets incompatible with standards*

Some facets may not be supported for all settings of a language. For
example the hash facet is not compatible with C++ 98. We need to have
some kind of facet/formatter level validation for this.

*** Create the concept of a technical space version                   :story:

We need a simple way to compare versions of technical spaces and have
them mapped into "identifiers" that users can relate to. For example,
C++ versions such as C++ 98 etc are the identifier; we should also
have a simple natural number mapping for each of these. We also need
to take into account the TRs - e.g. a type may be defined on a TR but
not be available on a version.

This should be done when we add technical spaces to the meta-model.

Merged stories:

*Drop the "c++-" prefix in meta-data for standard*

At present we do:

: quilt.cpp.standard=c++-98

The "c++-" seems a bit redundant.

*** Technical space composition                                       :story:

There are some formatters which are really not specific to a technical
space:

- CMake can be used with several languages such as C, C++, etc.
- Visual studio solutions are common to many technical spaces (F#, C#,
  C++, etc).

It seems we need to create a set of generation models which can be
used in conjunction with the "dominant" technical space. These are
triggered by the presence of meta-elements. Or perhaps we can just say
that we iterate through all "non-dominant" technical spaces ("main"
and "secondary"?  "subsidiary"?) and generate anything for which there
is an enabled and matching meta-element.

*** Setting include and source directory to empty                     :story:

At present it does not seem possible to set either the include or
source directories to empty. This probably just requires annotations
to understand empty values, e.g.

: a.b.c=

*** Throw on unsupported stereotypes for specific kernels             :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

*** Add support for multi-components in a model                       :story:

In the world of cross-model transformations (see story), we need lots
of separate models just because they need to generate their own
libraries or executables. It is a bit of a shame that we need to have
a number of "modelets", each for its own component. An alternative
would be to support multiple components from a single model, but this
would be a bit tricky. Thoughts:

- the model would have a multi-component mode, set at the top. No
  model elements are allowed at the top level.
- each package has a stereotype of =dogen::component= (not the best of
  names given it conflicts with UML component diagrams). Dogen
  generates each of these namespaces as a separate component
  (e.g. shared library or executable).
- the top-level model name becomes the first model name, the package
  name the second model name. Interestingly, this should mean dogen
  will generate all components on the top-level directory without any
  additional work.
- the easiest thing to do in terms of the existing pipeline is to
  create the concept of components at the meta-model level and then
  create a transform that takes a component based model and generates
  one model per component and processes them one at a time with the
  existing pipeline. However, we need to be careful because one model
  will contain all of the business logic whereas the other models are
  simple references to it. This could be addressed by having
  references, based on the existing model references.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.


** Deprecated
*** CANCELLED Disabling facet after regeneration does not delete file :story:
    CLOSED: [2020-02-17 Mon 18:08]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

Steps to reproduce:

- enable tests for all types.
- generate model.
- disable tests for one type.
- generate model.

Expected that disabling tests for type would result in file
deletion. Instead nothing happens. However, if one deletes the
generated file for the type, then the next generation will correctly
not generate code for the type.

It seems there is some weird mismatch between enablement and lint
removal: we are probably adding the file to the list of expected
files, regardless of whether the facet is enabled or not. However,
this is not always the case because we've proven that enabling and
disabling a facet correctly results in the deletion of files. It must
be something to do with how local enablement is handled.

*** CANCELLED Naming of saved yarn/Dia files is incorrect             :story:
    CLOSED: [2020-02-17 Mon 18:11]

*Rationale*: we don't really use this functionality as is, and in the
future we will create new "top-level" "types for serialisaton instead
of using hand-crafted code, so the story does not add value.

For some random reason when we use dogen to save yarn/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.
*** CANCELLED Incorrect generation when changing external modules     :story:
    CLOSED: [2020-02-17 Mon 18:16]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

When fixing the C# projects, we updated the external modules, from
=dogen::test_models= to =CSharpRefImpl=. Regenerating the model
resulted in updated project files but the rest of the code did not
change. It worked by using =-f=. It should have worked without forcing
the write.

*** CANCELLED Tests for error conditions in libxml                    :story:
    CLOSED: [2020-02-17 Mon 18:19]

*Rationale*: we will move to RapidXML.

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.
*** CANCELLED =Nameable= concept moved position on code generation    :story:
    CLOSED: [2020-02-17 Mon 18:23]

*Rationale*: hasn't happened again for a long time.

During the exogenous model work, yarn's =Nameable= concept moved
position. We need to look at how the parent changes were done to see
if they are stable or not.

*** CANCELLED Consider automatic injection of helpers                 :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

At present we are manually calling:

: a.add_helper_methods();

On each of the class implementation formatters in order to inject
helpers. This is fine for existing cases, but its a bit less obvious
when adding the first helper to an existing template: one does not
quite know why the helper is not coming through without
investigating. One possible solution is to make the helper generation
more "mandatory". Its not entirely obvious how this would work.

*** CANCELLED Clean-up helper terminology                             :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

The name "helper" was never really thought out. It makes little
sense - anything can be a helper. In addition, we have helpers that do
not behave in the same manner (inserter vs every other helper). We
need to come up with a good vocabulary around this.

- static aspects: those that are baked in to the file formatter.
- dynamic aspects: those that are inserted in to the file formatter at
  run time.
- type-dependent dynamic aspects: those that are connected to the
  types used in the file formatter.

Merged stories:

*Type-bound helpers and generic helpers*

Not all helpers are bound to a type. We have the case of inserter
helper in io which is used by main formatters directly. We need to
make this distinction in the manual.
*** CANCELLED Helper methods should have their own includes           :story:
    CLOSED: [2020-02-17 Mon 18:25]

*Rationale*: helpers will be removed.

This should be fairly straightforward:

- ensure we compute helpers before we do includes in formattables
  factory;
- add include API to helpers (=inclusion_dependencies=)
- during inclusion expansion, go through all helpers associated with a
  element and ask them for their dependencies.
- note that we still need a good solution for the "special helpers" in
  order for this to work.

*Previous Understanding*

When a formatter relies on the helper methods, we have a problem: we
need to determine the required includes from the main formatter
without knowing what the helper methods may need. We have hacked this
with things like the "special includes" but there must be a cleaner
way of doing this. For example, we could ask the helper methods
formatter to provide its includes and it would be its job to either
delegate further or to compute the includes. This would at least
remove the duplication of code between io and types.

This task will be made much easier once we have stitch support
for named regions.

As part of the work to make helpers dynamic we reached the following
conclusions:

Note: when time comes to support includes in helper methods, we can
take a similar approach as we do for formatters now. The helper method
implements some kind of include provider interface, which is then used
by the inclusion dependencies builder. The only slight snag is that we
need to first resolve the type into a type family and then go to the
helper interface.
