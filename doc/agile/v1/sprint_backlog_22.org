#+title: Sprint Backlog 22
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Start the generation refactor.
- Get a good grasp of the physical domain model.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2020-02-19 Wed 19:01]
| <75>                                               |        |      |      |       |
| Headline                                           | Time   |      |      |     % |
|----------------------------------------------------+--------+------+------+-------|
| *Total time*                                       | *7:59* |      |      | 100.0 |
|----------------------------------------------------+--------+------+------+-------|
| Stories                                            | 7:59   |      |      | 100.0 |
| Active                                             |        | 7:59 |      | 100.0 |
| Edit release notes for previous sprint             |        |      | 4:11 |  52.4 |
| Create a demo and presentation for previous sprint |        |      | 0:25 |   5.2 |
| Sprint and product backlog grooming                |        |      | 1:03 |  13.2 |
| Define a combined logical-physical space           |        |      | 2:20 |  29.2 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2020-02-18 Tue 20:35]
    :LOGBOOK:
    CLOCK: [2020-02-18 Tue 20:30]--[2020-02-18 Tue 20:44] =>  0:14
    CLOCK: [2020-02-18 Tue 19:04]--[2020-02-18 Tue 19:34] =>  0:30
    CLOCK: [2020-02-18 Tue 18:02]--[2020-02-18 Tue 18:37] =>  0:35
    CLOCK: [2020-02-17 Mon 23:16]--[2020-02-18 Tue 00:23] =>  1:07
    CLOCK: [2020-02-17 Mon 22:44]--[2020-02-17 Mon 23:15] =>  0:31
    CLOCK: [2020-02-17 Mon 20:00]--[2020-02-17 Mon 20:10] =>  0:10
    CLOCK: [2020-02-17 Mon 19:51]--[2020-02-17 Mon 19:59] =>  0:08
    CLOCK: [2020-02-17 Mon 19:02]--[2020-02-17 Mon 19:58] =>  0:56
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.21, "Nossa Senhora do Rosário"

#+BEGIN_SRC markdown
![Igreja de Nossa Senhora do Rosário](
https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Church_in_Tombua%2C_Namibe%2C_Angola.JPG/1280px-Church_in_Tombua%2C_Namibe%2C_Angola.JPG)
_Igreja de Nossa Senhora do Rosário, Tombwa, Namibe, Angola. (C) 2010 Paulo César Santos._

# Introduction

Very much like an iceberg, this sprint was deceptively small on user features but big on internal changes: after several sprints of desperate chasing, we finally completed the mythical "fabric refactor". The coding work was not exactly glamorous, as we engaged on a frontal attack on all "quasi-meta-types" we had previously scattered across the codebase. One by one, each type was polished and moved into the assets meta-model, to be reborn anew as a fully-fledged modeling element. All the while, we tried to avoid breaking the world - but nevertheless did so, frequently. It was grueling work. Having said that, the end of the refactor made for a very exciting sprint, and though the war remains long, we can't help but feel an important battle was won.

So let's have a look at how it all went down.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. All features this sprint are related to the addition of new meta-model types, which resulted in a number of breaking changes. These we have highlighted with :warning:.

[![Sprint 1.0.21 Demo](https://img.youtube.com/vi/J5duq-gw-nI/0.jpg)](https://youtu.be/J5duq-gw-nI)
_Video 1: Sprint 21 Demo._

## New meta-model elements

As we explored the lay of the land of our problem domain, we inadvertently found ourselves allowing Dogen to evolve a "special" set of meta-types. These we used to model files deemed inferior in stature to _real source code_: mostly build-related material, but also some more "regular" source code which could be derived from existing elements - _e.g._ visitors, serialisation registrars and the like. Due to its second-class-citizen nature, these  "special types" were controlled via variability in haphazard ways. Over the years, a plethora of meta-data switches was introduced at the model level but, in the absence of a coherent overall plan, these were _ad-hoc_ and inconsistent. On the main, the switches were used to enable or disable the emission of these "special types", as well as to configure some of their properties. Table 1 provides a listing of these switches.

|Meta-data key|Description|
|-------------------------------------------------------------------------------------|---------------------------------------------------------|
|```masd.generation.cpp.cmake.enabled```|Enable the CMake facet.|
|```masd.generation.cpp.cmake.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.cmake.source_cmakelists.enabled```|Enable the CMakeLists file in ```src``` directory.|
|```masd.generation.cpp.cmake.source_cmakelists.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.cmake.include_cmakelists.enabled```|Enable the CMakeLists file in ```include``` directory.|
|```masd.generation.cpp.cmake.include_cmakelists.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.msbuild.enabled```|Enable the MSBuild facet.|
|```masd.generation.cpp.msbuild.targets.enabled```|Enable the MSBuild formatter for ODB targets.|
|```masd.generation.cpp.msbuild.targets.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.enabled```|Enable the Visual Studio facet.|
|```masd.generation.cpp.visual_studio.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.project_solution_guid```|GUID for the Visual studio solution.|
|```masd.generation.cpp.visual_studio.project_guid```|GUID for the Visual studio project.|
|```masd.generation.cpp.visual_studio.solution.enabled```|Enables a Visual Studio solution for C++.|
|```masd.generation.cpp.visual_studio.solution.postfix```|Postfix to use for filename.|
|```masd.generation.cpp.visual_studio.project.enabled```|Enables a Visual Studio solution for C++.|
|```masd.generation.cpp.visual_studio.project.postfix```|Postfix to use for filename.|
|```masd.generation.csharp.visual_studio.project_solution_guid```|GUID for the Visual studio solution.|
|```masd.generation.csharp.visual_studio.project_guid```|GUID for the Visual studio project.|
|```masd.generation.csharp.visual_studio.solution.enabled```|Enables a Visual Studio solution for C#.|
|```masd.generation.csharp.visual_studio.solution.postfix```|Postfix to use for filename.|
|```masd.generation.csharp.visual_studio.project.enabled```|Enables a Visual Studio project for C#.|
|```masd.generation.csharp.visual_studio.project.postfix```|Postfix to use for filename.|

_Table 1: Meta-data switches related to "special" types._

The meta-data was then latched on to model properties, like so:

```
#DOGEN masd.generation.cpp.msbuild.enabled=true
#DOGEN masd.generation.csharp.visual_studio.project_guid=9E645ACD-C04A-4734-AB23-C3FCC0F7981B
#DOGEN masd.generation.csharp.visual_studio.project_solution_guid=FAE04EC0-301F-11D3-BF4B-00C04F79EFBC
#DOGEN masd.generation.cpp.cmake.enabled=true
...
```

As we continued to mull over the problem across sprints, the entire idea of "implicit" element types - injected into the model and treated differently from regular elements - was [ultimately understood to be harmful](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_11.org#analyse-the-state-of-the-mess-of-refactors). The approach is in fact an _abuse_ of variability, due to how these elements had been (_mis-_)modeled. And it had consequences:

- **Invisibility**: it was not possible to manage variability of the injected types in the same fashion as for all other elements because they were "invisible" to the modeler.
- **Difficulty in troubleshooting**: it was hard to diagnose when something didn't work as expected, because all of the magic was internal to the code generator.
- **Inconsistency in generation**: we had a rather inconsistent way of handling different element types; some "just appeared" due to the state of the model (like ```registrar```); others were a consequence of enabling formatters (_e.g._ ```CMakeLists.txt```); still others required the presence of stereotypes (_e.g._ ```visitor```). It was very hard to explain the rationale for each of these to an unsuspecting user.
- **Inconsistency in population**: properties that were common to other elements had to be handled specially via meta-data. For example, adding comments or changing decoration for these elements required bespoke meta-data and associated transforms, even though we already had a pipeline which operated on "regular" elements.
- **Inconsistencies in facet spaces**: the types did not follow the existing facet conventions - _i.e._, to be placed on a folder named after the facet, _etc_. Even in that they were "special".

Programming is nothing if not a quest for the generalisation and removal of special cases, and these types had been a major thorn in the design. Thus the idea of refactoring fabric out of existence was born. With this release we finally removed all of the above meta-data keys, and replaced them with regular meta-model elements, instantiable via the appropriate stereotypes (Table 2). Sadly, a single use case was left, due to the specificity of its implementation: visitors. These shall be addressed on a future release.

> :warning: **Breaking change**: Users need to update any models which make use of the meta-data in Table 1 and replace them with the corresponding elements and stereotypes.

|Stereotype|Description|
|--------------|---------------|
|```masd::serialization::type_registrar```|The serialisation type registrar used mainly for boost serialisation support.|
|```masd::visual_studio::solution```|Visual Studio solution support.|
|```masd::visual_studio::project```|Visual Studio solution support.|
|```masd::entry_point```| Provides an entry point to a component, _e.g._ ```main```.|
|```masd::orm::common_odb_options```|Element modeling the common arguments for ODB.|
|```masd::visual_studio::msbuild_targets```|Element modeling ODB targets using MSBuild.|
|```masd::build::cmakelists```|Element modeling build files using CMake.|
|```masd::assistant```|C# helper type.|

_Table 2: Stereotypes for the new meta-model elements ._

Now, the observant reader won't fail to notice that _the generated code has not changed_ in any way - well, at least not intentionally. All of these new meta-model elements already existed, but in their previous incantation variability was used to trigger them (mostly). With this release they are modeled as proper meta-model elements, controlled by the user, and processed in the exact same way as all other elements. This means we can make use of all of the existing machinery in Dogen such as profiles.

![Use of new meta-elements in C++](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cpp_new_meta_elements.png)
_Figure 1: Use of new meta-elements in a C++ model._

Whilst this is a big improvement in usability, there are still a number of pitfalls:

- users now need to remember to add types where Dogen used to inject them automatically. This is the case with ```registrar```, which was generated automatically when a model made use of inheritance.
- there are no errors or warnings when a diagram is on an inconsistent state due to the choice of elements used. For example, one can add a solution without a project.

![Use of new meta-elements in C#](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/csharp_new_meta_elements.png)
_Figure 2: Use of new meta-elements in a C# model._

These are problems that will hopefully be looked into once we eventually reach the validation work, in a few sprints time.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_21.org).

## Significant Internal Stories

The sprint was mostly dominated by a number of stories dealing with moving fabric types, but since they have user visible consequences, they have been dealt with in _User visible changes_. The only other story of note is described below.

### Remove support for element extensions

In the past we created another "special" concept: element extensions. These allowed two meta-model elements to share the same position in modeling space, _i.e._ two elements sharing the same name. Whilst this may sound crazy on first sight, the initial idea behind it was more or less sound. Files such as forward declarations in C++, or ODB options, were better modeled when using "lightweight" meta-model elements which provided the specific data needed. In order for this to work, we needed to have some kind of way of containing meta-elements within meta-elements, and thus "element extensions" were born. As with fabric types, element extensions did not stood the test of time and added a lot of complexity and special cases. Now that the last fabric types that made use of element extensions were removed, we managed to remove the extensions themselves from the meta-model, greatly simplifying things.

## Resourcing

This sprint was a "model" sprint (if you pardon the pun) in terms of Dogen development. At an overall elapsed time of four weeks, our utilisation rate improved significantly from 23% to an amazing 56%. In other words, we managed to maintain a steady pace and clocked around 20 hours every week. Furthermore, a staggering _75.5%_ of the overall ask was spent on stories directly related to the sprint's mission of refactoring fabric. This is quite possibly the highest in Dogen's eight-year history, as far as I can recall. We spent 19.6% on process related activities, which whilst not the smallest amount ever, its also in line with recent sprints - particularly when we have video recording activities. The remainder of the sprint was used chasing minor spikes such as problems with the setup (1.4%), errors in tests (0.8%), issues with coveralls (0.3%) and so on. Overall, from a resource management perspective, this was a very successful sprint.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_pie_chart.jpg)
_Figure 3: Cost of stories for sprint 21._

## Roadmap

Two very minor changes were made to the road map this sprint. First and foremost, we finally removed the fabric refactor from the roadmap, which is extremely pleasing. Secondly, we bumped up resource usage by a fair (if somewhat random) amount, which projected timescales in time somewhat, in a more realistic manner. How realistic is up to debate, but at least it is hopefully slightly less wrong.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_21_resource_allocation_graph.png)

# Next Sprint

The next great big refactoring battle is with the generation model. We need to move all concepts that had been incorrectly placed in generation to the meta-model, and, with it, reduce the huge code duplication we have between backends.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.21_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.21/dogen_1.0.21_amd64-applications.deb)
- [dogen-1.0.21-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.21/DOGEN-1.0.21-Darwin-x86_64.dmg)
- [dogen-1.0.21-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.21-Windows-AMD64.msi)

**Note:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+END_SRC markdown

- [[https://twitter.com/MarcoCraveiro/status/1229849866416816129][twitter]]
- [[https://www.linkedin.com/posts/marco-craveiro-31558919_masd-projectdogen-activity-6635632094846476289-oXZM][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    CLOSED: [2020-02-18 Tue 19:03]
    :LOGBOOK:
    CLOCK: [2020-02-18 Tue 18:38]--[2020-02-18 Tue 19:03] =>  0:25
    :END:

Time spent creating the demo and presentation. Use the demo project:

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2020-02-17 Mon 19:31]--[2020-02-17 Mon 19:50] =>  0:19
    CLOCK: [2020-02-17 Mon 17:43]--[2020-02-17 Mon 18:27] =>  0:44
    :END:

Updates to sprint and product backlog.

*** COMPLETED Adding reference to itself results in resolution errors :story:
    CLOSED: [2020-02-17 Mon 18:17]

*Rationale*: this was fixed a few sprints ago.

Whilst trying to fix the JSON models we inadvertently added a
self-reference in =dogen.generation.json=:

:    "yarn.reference": "dogen.generation.json",

This resulted in some puzzling errors:

: 2018-10-18 19:15:00.861210 [ERROR] [yarn.transforms.enablement_transform] Duplicate element archetype: quilt.cpp.serialization.registrar_implementation <dogen><generation><registrar>

Ideally we should either warn and ignore or fail to process models
with self-references.

*** STARTED Define a combined logical-physical space                  :story:
    :LOGBOOK:
    CLOCK: [2020-02-19 Wed 18:01]--[2020-02-19 Wed 19:01] =>  1:00
    CLOCK: [2020-02-19 Wed 08:02]--[2020-02-19 Wed 08:54] =>  0:52
    CLOCK: [2020-02-19 Wed 07:02]--[2020-02-19 Wed 07:30] =>  0:28
    :END:

It now seems that we have been searching for a meta-model that
combines both aspects of logical modeling as well as physical
modeling. Facets, archetypes etc are all parts of the physical
dimension of this space. We need to find all stories on this topic and
organise them to see if we can come up with a consistent system of
meaning.

Notes:

- archetypes must support a notion of "kind". This is so we can have
  public include headers, private include headers and implementation
  files. This "kind" affects the topology of the physical dimension.
- the locator is a function that takes points in the logical-physical
  space and maps them to filesystem locations. It uses properties of
  those meta-model elements to configure the mapping.
- actually, the separation of technical spaces and backends is
  somewhat artificial. In reality, if we were to clean up all backends
  such that they only contain a single technical space then we
  wouldn't have this distinction. However, there are problems with
  this approach. Some features span across multiple technical spaces,
  such as ODB. It requires:

  - c++ support in generating the pragmas,
  - ODB options files.
  - msbuild for odb targets
  - cmake for odb targets

  It would be tempting to say that ODB is not a technical space, but
  just a feature. In which case we need options files to be a
  technical space not solely connected to ODB. This is possible,
  provided we can find evidence of other systems using options
  files. If we could generalise this then the problem would be
  solved. However, it is not yet clear if ODB is a special case or an
  indicator of a pattern which we are ignoring.
- in this world, we would have a top-level =techspace= model,
  equivalent to generation at present. It would be responsible for
  knowing about all available technical spaces. Component models would
  have one or more representations. A representation can have one
  primary technical space and zero or many secondary technical
  spaces.
- input and output technical spaces are modeling errors. In reality,
  models have types: they are either PIMs or PSMs. If a model is a
  PIM, he must only refer to other PIMs. However, an additional
  wrinkle is that in order to load the mappings, we need to have
  access to the references (as these contain the mappings). Perhaps we
  can allow any reference, but then when resolving, we need to ensure
  that the types are all consistent.
- perhaps we are looking at this in the wrong way. In reality, there
  are only the following permutations:

  - if a model has a single representation, then either a) the input
    technical space is the same as the output (e.g. PSM) or b) its a
    PIM in which case we need to perform the mapping.
  - if a model has more than one representation, then it must be a
    PIM.

  If a model had a way to declare itself PIM, then in resolver we
  could ensure that all types are referencing only other PIM
  types. However, it would still be possible for a C++ model to
  reference a C# model. For this validation to take place, we would
  need a way to associate a technical space to an element and then
  check that on reference resolution. Actually, if we ensure we map
  before we resolve (which we probably already do) then we can rely on
  the fact that only PSM types will exist. If we had a way of knowing
  which types in a PIM need mapping, then we could detect which ones
  did not map. Then we could issue a mapping error. This way the world
  would be cleanly divided between PIM and PSM, and we could ignore
  technical spaces for PIMs. We cannot know at mapping time
- one aspect that is not very clean is that we should only allow more
  than one representation on a model prior to mapping. After mapping,
  there can only be one representation (the technical space we have
  mapped to).

*** Integration of archetypes into assets                             :story:

Up to recently, there was a belief that the archetypes model was
distinct from the assets model. The idea was that the projection of
assets into archetype space could be done without knowledge of the
things we are projecting. However, that is demonstrably false: n order
to project we need a name. That name contains a location. The location
is a point on a one-dimensional asset space.

In reality, what we always had is:

- a first dimension within assets space: "modeling dimension",
  "logical dimension"? It has an associated location.
- a second dimension within assets space: "physical dimension", with
  an associated location. Actually we cannot call it physical because
  physical is understood to mean the filesystem.

So it is that concepts such as archetype, facet and technical space
are all part of assets - they just happen to be part of the
two-dimensional projection. Generation is in effect a collection of
model to text transforms that adapts the two-dimensional element
representation into the extraction meta-model. Formatters are model to
text transforms which bind to locations in the physical dimension.

In this view of the world, we have meta-model elements to declare
archetypes, with their associated physical locations. This then
results in the injection of these meta-elements. Formatters bind to
these locations.

However, note that formatters provide dependencies. This is because
these are implementation dependent. This means we still need some
transforms to occur at the generation level. However, all of the
dependencies which are modeling related should happen within
assets. Only those which are formatter specific should happen in
generation. The problem though is that at present we deem all
dependencies to be formatter specific and each formatter explicitly
names its dependencies against which facets. It does make sense for
these to be together.

Perhaps what we are trying to say is that there are 3 distinct
concepts:

- modeling locations;
- logical locations;
- physical locations.

The first two are within the domain of assets. The last one is in the
domain of generation and extraction. Assets should make the required
data structures available, but it is the job of generation to populate
this information. Thus directory themes, locator, etc are all
generation concepts.

One could, with a hint of humour, call the "logical dimension" the
meta-physical dimension. This is because it provides the meta-concepts
for the physical dimension.

A backend provides a translation into a representation considered
valid according to the rules of a technical space. A backend can be
the primary or secondary backend for a technical space. A component
can only have a primary backend, and any number of secondary
backends. Artefacts produced by a backend must have a unique physical
location. In LAM mode, the component is split into multiple
components, each with their own primary technical space.

*** Create a archetypes locator                                       :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present;
they should be a transform inside generation.

*** Clean-up archetype locations modeling                             :story:

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*** Move dependencies into archetypes                                 :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Move formatting styles into generation                            :story:

We need to support the formatting styles at the meta-model level.

*** Make creating new facets easier                                   :story:

For types that are stitchable such as formatters, we need to always
copy and paste the template form another formatter and then update
values. It would be great if we could have dogen generate a bare-bones
stitch template. This is pretty crazy so it requires a bit of
concentration to understand what we're doing here:

- detect that the =yarn::object= is annotated as
  =quilt.cpp.types.class_implementation.formatting_style= =stitch=.
- find the corresponding expected stitch file. If none is available,
  /dynamically/ change the =formatting_style= to =stock= and locate a
  well-known stitch formatter.
- the stitch formatter uses a stitch template that generates stitch
  templates. Since we cannot escape stitch markup, we will have to use
  the assistant. One problem we have is that the formatter does not
  state all of the required information such as what yarn types does
  it format and so forth. We probably need a meta-model concept to
  capture the idea of formatters - and this could be in yarn - and
  make sure it has all of this information. This also has the
  advantage of making traits, initialisers etc easier. We can do the
  same for helpers too.
- an additional wrinkle is that we need different templates for
  different languages. However, perhaps these are just wale templates
  in disguise rather than stitch templates? Then we can have the
  associated default wale templates, very much in the same way we have
  wale templates for the header files. They just happen to have stitch
  markup rather than say C++ code.

This is a radically different way from looking at the code. We are now
saying that yarn should have concepts for:

- facets: specialisation of modules with meta-data such as facet name
  etc. This can be done via composition to make our life easier.
- formatters and helpers: elements which belong to a facet and know of
  their archetype, wale templates, associated yarn element and so
  forth.

We then create stereotypes for these just like we did for
=enumeration=. As part of the yarn parsing we instantiate these
meta-objects with all of their required information. In addition, we
need to create what we are calling at present "profiles" to define
their enablement and to default some of its meta-data.

When time comes for code-generation, these new meta-types behave in a
more interesting way:

- if there is no stitch template, we use wale to generate it.
- once we have a stitch template, we use stitch to generate the c++
  code. From then on, we do not touch the stitch template. This
  happens because overwrite is set to false on the enablement
  "profile".

Merged stories:

*Code generate initialisers and traits*

If we could mark the modules containing facets with a stereotype
somehow - say =facet= for example, we could automatically inject two
meta-types:

- =initialzer=: for each type marked as =requires_initialisation=,
  register the formatter. Register the types as a formatter or as a
  helper.
- =traits=: for each formatter in this module (e.g. classes with the
  stereotype of =C++ Artefact Formatter= or =C# Artefact Formatter=),
  ask for their archetype. The formatters would have a meta-data
  parameter to set their archetype. In fact we probably should have a
  separate meta-data parameter (archetype source? archetype?).

We may need to solve the stereotype registration problem though, since
only C++ would know of this facet. Or we could hard-code it in yarn
for now.

Notes:

- how does the initialiser know the formatter is a =quilt.cpp=
  formatter rather than say a C# formatter? this could be done via the
  formatter's archetype - its the kernel.
- users can make use of this very same mechanism to generate their own
  formatters. We can then load up the DLL with boost plugin. Note that
  users are not constrained by the assets meta-model. That is to say,
  they can create new meta-types and inject them into assets. Whilst
  we don't support this use case at present, we should make sure the
  framework does not preclude it. Their DLL then defines the
  formatters which are able to process those meta-types. The only snag
  in all of this is the expansion machinery. We use static visitors
  all over the place, and without somehow dynamically knowing about
  the new types, they will not get expanded. We need to revisit
  expansion in this light to see if there is a way to make it more
  dynamic somehow, or at least have a "default" behaviour for all
  unknown types where we do the generic things to them such as
  computing the file path, etc. This is probably sufficient for the
  vast majority of use cases. The other wrinkle is also locator. We
  are hard-coding paths. If the users limit themselves to creating
  "regular" entities rather than say CMakeLists/msbuild like entities
  which have some special way to compute their names, then we don't
  have a problem. But there should be a generic way to obtain all path
  elements apart from the file name from locator. And also perhaps
  have facets that do not have a facet directory so that we can place
  types above the facet directories such as SLNs, CMakeLists, etc.

*** Create the notion of project destinations                         :story:

At present we have conflated the notion of a facet, which is a logical
concept, with the notion of the folders in which files are placed - a
physical concept. We started thinking about addressing this problem by
adding the "intra-backend segment properties", but as the name
indicates, we were not thinking about this the right way. In truth,
what we really need is to map facets (better: archetype locations) to
"destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly
  if this is done at the element level. That is, you can easily define
  a global mapping for all formatters, and then override it
  locally. This solves the long standing problem of creating say types
  in tests and so forth. With this approach you can create anything
  anywhere.
- we need to have some tests that ensure we don't end up with multiple
  files with the same name at the same destination. This is a
  particular problem for CMake. One alternative is to allow the
  merging of CMake files, but we don't yet have a use case for
  this. The solution would be to have a "merged file flag" and then
  disable all other facets.
- this will work very nicely with profiles: we can create a few out of
  the box profiles for users such as flat project, common facets and
  so on. Users can simply apply the stereotype to their models. These
  are akin to "destination themes". However, we will also need some
  kind of "variable replacement" so we can support cases like
  =include/masd.cpp_ref_impl.northwind/types=. In fact, we also have
  the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder
  structure that depends on package names etc or choose to flatten it
  and can do so for one or all facets. This means for example that we
  could use nested folders for =include=, not use model modules for
  =src= and then flatten it all for =tests=.
- actually it is a bit of a mistake to think of these destinations as
  purely physical. In reality, we may also need them to contribute to
  namespaces. For example, in java the folders and namespaces must
  match. We could solve this by having a "module contribution" in the
  destination. These would then be used to construct the namespace for
  a given facet. Look for java story on backlog for this.
- this also addresses the issue of having multiple serialisation
  formats and choosing one, but having sensible folder names. For
  example, we could have boost serialisation mapped to a destination
  called =serialisation=. Or we could map it to say RapidJSON
  serialisation. Or we could support two methods of serialisation for
  the same project. The user chooses where to place them.

*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe
  a given component type such as a library or an executable. Thus,
  they themselves have sub-types.
- profile models: useful to keep the configuration separate. However,
  it may make more sense to place them in the product model, since its
  shared across components?
- PDMs: these describe platforms.

At present there is no concept of model types, so any meta-model
element can be placed in any model. This is convenient, but in the
future it may make things too complicated: users may end up placing
types in PDMs when they didn't meant to do so, etc. What seems to
emerge from here is that, just as with variability, there is a concept
of a binding point at the model level too. That is, meta-model
elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that
represents each model type. We then instantiate this class within one
of the dogen models to register the different model types. Its
code-generation representation is the registration. It also binds to
all the meta-model elements it binds to. This can be done simply by
creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate
the meta-class information as we generate the assets model). Then, we
can ask the model type if a given element is valid (check a set of
stereotypes).

Formatters are themselves meta-model elements, and they bind to other
meta-model elements (which raises the question: which meta-model
elements are bindable? we can't allow a formatter to bind to a
formatter...). Perhaps we need another type of model, which is a
"generation model". This is where we can either declare new technical
spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by
allowing users to specify the TS/facet when declaring the
formatter. If not specified, then the user must declare a facet in the
package containing the formatter. Similarly with TSs.

Note also that the formatter binding code is "inserted" directly
during generation into the CPP file. Its not possible to change
it. Same with the includes. This ensures the user cannot bypass the
model type system by mistake. Also, by having a formatter meta-model
type, we can now declare the header file as we please, and ensure the
shape of the implementation. Now, the stitch template can be
restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course
require the move to PDMs and the removal of the helper code. This also
means that anyone can declare new meta-model elements; they will
register themselves, and correctly expand across archetype
space. However, we do not have the adaption code nor do we have
containers for these modeling elements. We need a separate story for
this use case.

Destinations are meta-model elements too. In the generation.cpp model
we will declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets
in the archetype space, which express themselves as directories in the
artefact path when we project from archetype space into artefact
space. More generally: assets in asset space are projected into the
multidimensional archetype space. Archetypes are projected into
artefact space, but the dimensions of archetype space are flattened
into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for
file extensions, but conceivably could also be used for other
purposes.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the binaries, allowing users to consume
it:

- in the Dogen case, it means users can create plugins for Dogen;
- in the PDM case, it means users can make use of the PDM in their own
  models;
- for user models, it means you can consume a product in another
  product by referencing its models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.
- the product model itself should be at the top-most directory of the
  git repository. We also need a "models" directory to store models
  which are not expressed as source code (profiles, PDMs, etc). Then,
  for each component, we should have the models at the root directory
  of the component. Whilst this is not in line with our OCD, it is
  required in order for the product model to be able to locate the
  component models. An alternative is to have a convention that we
  always look into a "models" directory (which can be renamed via a
  meta-data parameter) for models, plus any additional directories in
  the "model path". We must inject the model file names to dogen so
  that we do not delete the models.

*** Formatters can only belong to one facet                           :story:

Up to know there was an agreement that generation space was
hierarchical and formatters could only belong to one facet. This has
been true until now, but with the addition of CMake support to tests,
we now have an exception: we need to honour both the tests facet and
the cmake facet. If either of them are off, then we should not emit
the CMake file. This means that we need to somehow map one formatter
to multiple facets. For now we just hacked it and used one of the
facets. It means that if you disable CMake but enable testing you'll
still end up with the testing CMake file.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Consider bucketing elements by meta-type in model                 :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

This can be done in the generation model, as part of the generation
clean up.

*** Add a C++ version to types                                        :story:

Not all system model types are available for all versions. This
applies to the C++ standard (e.g. 98, 11, 14 etc) but also to
boost. We need to be able to mark a type against a version; the user
then declares which version it is using in the model. If the user
attempts to use types that are not available for that version we
should throw.

*** Add facet validation against language standard                    :story:

With the move of enablement to yarn, we can no longer validate facets
against the language standard. For example, we should not allow
hashing on C++ 98. The code was as follows:

#+begin_src c++
void enablement_expander::validate_enabled_facets(
    const global_enablement_configurations_type& gcs,
    const formattables::cpp_standards cs) const {
    BOOST_LOG_SEV(lg, debug) << "Validating enabled facets.";

    if (cs == formattables::cpp_standards::cpp_98) {
        using formatters::hash::traits;
        const auto arch(traits::class_header_archetype());

        const auto i(gcs.find(arch));
        if (i == gcs.end()) {
            BOOST_LOG_SEV(lg, error) << archetype_not_found << arch;
            BOOST_THROW_EXCEPTION(expansion_error(archetype_not_found + arch));
        }

        const auto& gc(i->second);
        if (gc.facet_enabled()) {
            const auto fctn(gc.facet_name());
            BOOST_LOG_SEV(lg, error) << incompatible_facet << fctn;
            BOOST_THROW_EXCEPTION(expansion_error(incompatible_facet + fctn));
        }
    }

    BOOST_LOG_SEV(lg, debug) << "Validated enabled facets.";
}
#+end_src

It was called from the main transform method in enablement transform,
prior to uptading facet enablement.

What we really need is the concept of a technical space in the
metamodel, as well as a "version" for that technical space, and then
also the concept of a facet. Then we are effectively building
(weaving?) an instance of a theoretical TS based on the configuration
(positive variability). We can then validate the configuration. This
should all now be part of archetypes. The versions can be attributes
of technical space with a string version (e.g. "c++ 98) and a numeric
version (1 say) so that we can make comparisons (e.g. c++ 17 > c++
98). Each formatter can then declare its compatibility against the
versions of the technical space.

Merged stories

*Facets incompatible with standards*

Some facets may not be supported for all settings of a language. For
example the hash facet is not compatible with C++ 98. We need to have
some kind of facet/formatter level validation for this.

*** Create the concept of a technical space version                   :story:

We need a simple way to compare versions of technical spaces and have
them mapped into "identifiers" that users can relate to. For example,
C++ versions such as C++ 98 etc are the identifier; we should also
have a simple natural number mapping for each of these. We also need
to take into account the TRs - e.g. a type may be defined on a TR but
not be available on a version.

This should be done when we add technical spaces to the meta-model.

Merged stories:

*Drop the "c++-" prefix in meta-data for standard*

At present we do:

: quilt.cpp.standard=c++-98

The "c++-" seems a bit redundant.

*** Technical space composition                                       :story:

There are some formatters which are really not specific to a technical
space:

- CMake can be used with several languages such as C, C++, etc.
- Visual studio solutions are common to many technical spaces (F#, C#,
  C++, etc).

It seems we need to create a set of generation models which can be
used in conjunction with the "dominant" technical space. These are
triggered by the presence of meta-elements. Or perhaps we can just say
that we iterate through all "non-dominant" technical spaces ("main"
and "secondary"?  "subsidiary"?) and generate anything for which there
is an enabled and matching meta-element.

*** Setting include and source directory to empty                     :story:

At present it does not seem possible to set either the include or
source directories to empty. This probably just requires annotations
to understand empty values, e.g.

: a.b.c=

*** Throw on unsupported stereotypes for specific kernels             :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

*** Add support for multi-components in a model                       :story:

In the world of cross-model transformations (see story), we need lots
of separate models just because they need to generate their own
libraries or executables. It is a bit of a shame that we need to have
a number of "modelets", each for its own component. An alternative
would be to support multiple components from a single model, but this
would be a bit tricky. Thoughts:

- the model would have a multi-component mode, set at the top. No
  model elements are allowed at the top level.
- each package has a stereotype of =dogen::component= (not the best of
  names given it conflicts with UML component diagrams). Dogen
  generates each of these namespaces as a separate component
  (e.g. shared library or executable).
- the top-level model name becomes the first model name, the package
  name the second model name. Interestingly, this should mean dogen
  will generate all components on the top-level directory without any
  additional work.
- the easiest thing to do in terms of the existing pipeline is to
  create the concept of components at the meta-model level and then
  create a transform that takes a component based model and generates
  one model per component and processes them one at a time with the
  existing pipeline. However, we need to be careful because one model
  will contain all of the business logic whereas the other models are
  simple references to it. This could be addressed by having
  references, based on the existing model references.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

** Deprecated
*** CANCELLED Disabling facet after regeneration does not delete file :story:
    CLOSED: [2020-02-17 Mon 18:08]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

Steps to reproduce:

- enable tests for all types.
- generate model.
- disable tests for one type.
- generate model.

Expected that disabling tests for type would result in file
deletion. Instead nothing happens. However, if one deletes the
generated file for the type, then the next generation will correctly
not generate code for the type.

It seems there is some weird mismatch between enablement and lint
removal: we are probably adding the file to the list of expected
files, regardless of whether the facet is enabled or not. However,
this is not always the case because we've proven that enabling and
disabling a facet correctly results in the deletion of files. It must
be something to do with how local enablement is handled.

*** CANCELLED Naming of saved yarn/Dia files is incorrect             :story:
    CLOSED: [2020-02-17 Mon 18:11]

*Rationale*: we don't really use this functionality as is, and in the
future we will create new "top-level" "types for serialisaton instead
of using hand-crafted code, so the story does not add value.

For some random reason when we use dogen to save yarn/Dia files the
names look like this:

: test_data/dia_sml/expected/boost_model.xmldia
: test_data/dia_sml/expected/std_model.xmldia

but our tests expect:

: test_data/dia_sml/expected/boost_model.diaxml
: test_data/dia_sml/expected/std_model.diaxml

This must be part of a refactoring that wasn't completed properly.
*** CANCELLED Incorrect generation when changing external modules     :story:
    CLOSED: [2020-02-17 Mon 18:16]

*Rationale*: this issue is likely to do with the fact that we have
ignore regexes for =test= and =tests= on most models.

When fixing the C# projects, we updated the external modules, from
=dogen::test_models= to =CSharpRefImpl=. Regenerating the model
resulted in updated project files but the rest of the code did not
change. It worked by using =-f=. It should have worked without forcing
the write.

*** CANCELLED Tests for error conditions in libxml                    :story:
    CLOSED: [2020-02-17 Mon 18:19]

*Rationale*: we will move to RapidXML.

We do not have any errors that check for error conditions directly in
libxml. This is why the coverage of these functions is red.
*** CANCELLED =Nameable= concept moved position on code generation    :story:
    CLOSED: [2020-02-17 Mon 18:23]

*Rationale*: hasn't happened again for a long time.

During the exogenous model work, yarn's =Nameable= concept moved
position. We need to look at how the parent changes were done to see
if they are stable or not.

*** CANCELLED Consider automatic injection of helpers                 :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

At present we are manually calling:

: a.add_helper_methods();

On each of the class implementation formatters in order to inject
helpers. This is fine for existing cases, but its a bit less obvious
when adding the first helper to an existing template: one does not
quite know why the helper is not coming through without
investigating. One possible solution is to make the helper generation
more "mandatory". Its not entirely obvious how this would work.

*** CANCELLED Clean-up helper terminology                             :story:
    CLOSED: [2020-02-17 Mon 18:24]

*Rationale*: helpers will be removed.

The name "helper" was never really thought out. It makes little
sense - anything can be a helper. In addition, we have helpers that do
not behave in the same manner (inserter vs every other helper). We
need to come up with a good vocabulary around this.

- static aspects: those that are baked in to the file formatter.
- dynamic aspects: those that are inserted in to the file formatter at
  run time.
- type-dependent dynamic aspects: those that are connected to the
  types used in the file formatter.

Merged stories:

*Type-bound helpers and generic helpers*

Not all helpers are bound to a type. We have the case of inserter
helper in io which is used by main formatters directly. We need to
make this distinction in the manual.
*** CANCELLED Helper methods should have their own includes           :story:
    CLOSED: [2020-02-17 Mon 18:25]

*Rationale*: helpers will be removed.

This should be fairly straightforward:

- ensure we compute helpers before we do includes in formattables
  factory;
- add include API to helpers (=inclusion_dependencies=)
- during inclusion expansion, go through all helpers associated with a
  element and ask them for their dependencies.
- note that we still need a good solution for the "special helpers" in
  order for this to work.

*Previous Understanding*

When a formatter relies on the helper methods, we have a problem: we
need to determine the required includes from the main formatter
without knowing what the helper methods may need. We have hacked this
with things like the "special includes" but there must be a cleaner
way of doing this. For example, we could ask the helper methods
formatter to provide its includes and it would be its job to either
delegate further or to compute the includes. This would at least
remove the duplication of code between io and types.

This task will be made much easier once we have stitch support
for named regions.

As part of the work to make helpers dynamic we reached the following
conclusions:

Note: when time comes to support includes in helper methods, we can
take a similar approach as we do for formatters now. The helper method
implements some kind of include provider interface, which is then used
by the inclusion dependencies builder. The only slight snag is that we
need to first resolve the type into a type family and then go to the
helper interface.
