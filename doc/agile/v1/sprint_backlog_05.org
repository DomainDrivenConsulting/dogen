#+title: Sprint Backlog 05
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Sort out concepts and profiles.
- Resume and progress the work on moving "generic" types from the
  quilt models into yarn.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2017-09-14 Thu 06:30]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *40:48* |       |       | 100.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 40:48   |       |       | 100.0 |
| Active                                                                      |         | 40:48 |       | 100.0 |
| COMPLETED Edit release notes for previous sprint                            |         |       |  0:44 |   1.8 |
| STARTED Sprint and product backlog grooming                                 |         |       |  1:33 |   3.8 |
| COMPLETED Remove object types in yarn                                       |         |       |  0:52 |   2.1 |
| COMPLETED Clean-up readme                                                   |         |       |  1:15 |   3.1 |
| COMPLETED Rename transformers to adapters                                   |         |       |  0:28 |   1.1 |
| COMPLETED Analysis on annotations and profiles                              |         |       |  2:26 |   6.0 |
| COMPLETED Use namespaced stereotypes                                        |         |       |  1:49 |   4.5 |
| COMPLETED Tailor dogen models                                               |         |       |  0:21 |   0.9 |
| COMPLETED Rename yarn concepts                                              |         |       |  7:49 |  19.2 |
| COMPLETED Remove extra copying in mapper                                    |         |       |  0:05 |   0.2 |
| COMPLETED Investigate the JSON break on Northwind model                     |         |       |  0:57 |   2.3 |
| COMPLETED Rename ODB parameters                                             |         |       |  0:11 |   0.4 |
| COMPLETED Investigate usage of origin type                                  |         |       |  0:15 |   0.6 |
| COMPLETED Improve dumping of models                                         |         |       | 12:59 |  31.8 |
| COMPLETED Create yarn options                                               |         |       |  1:06 |   2.7 |
| COMPLETED Merge knit with yarn                                              |         |       |  1:13 |   3.0 |
| STARTED Add canonical archetype support to yarn                             |         |       |  6:45 |  16.5 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2017-08-30 Wed 22:57]
    CLOCK: [2017-08-30 Wed 22:13]--[2017-08-30 Wed 22:57] =>  0:44

Add github release notes for previous sprint.

Title: Dogen v1.0.04, "Zona dos Riscos"

#+begin_src markdown
![Zona dos Riscos](http://www.almadeviajante.com/wp-content/uploads/deserto-do-namibe.jpg)
_Zona dos Riscos, Namibe, Angola. (C) Alma de Viajante, 2017._

Overview
=======
As usual, yarn internal refactoring is the bulk of the work in this sprint. The refactoring work had three major themes:

- **Use shared pointers across the board** for yarn elements, from frontend to the backend. This was done as a requirement for the exogenous models changes described below; as it happens, it has the nice side-effect of reducing the number of copies of model elements.
- **Finish exogenous models support**: frontends now have a special purpose model type, designed only for the kind of operations supported at the frontend level. This cleaned up transformations quite a bit, making it obvious which ones apply at which stage. The conceptual model is now somewhat cleaner, with the introduction of _exomodels_ (previously "exogenous models") and _endomodels_ (previously "intermediate models"), which specific purposes.
- **Name processing now done in core**: as part of the exogenous models change, we also moved the external and model module processing away from the frontends and into the core. This means less code duplication across frontends.

In addition to these, there were a couple of additional stories that had user facing impact, described in the next section.

User visible changes
================
This sprint introduced a number of user visible changes, all related to the internal clean-up work:

- Upsilon support was considered deprecated, since the customer for which we developed it no longer requires it. Since it was a custom-made frontend with no real application outside of this specific use case, all code related to upsilon has been removed.
- Continuing the meta-name work, JSON now represents these as regular yarn names. Sadly this makes the JSON more verbose, but at least it's more consistent now. This change breaks backwards compatibility, so users with JSON models need to update them. Sample change:
```
-    "meta_type": "module",
+    "meta_name": {
+      "simple": "module",
+      "external_modules": "dogen",
+      "model_modules": "yarn",
+      "internal_modules": "meta_model"
+    }
```
- A new command line flag was introduced: ```--compatibility-mode```. The objective of this flag is to disable some of the model validation code, where the errors are known to be caused by a forwards or backwards incompatible change. However: a) this is an experimental flag, very incomplete at present; and b) even when finished, the generated code may just be invalid.

For more details of the work carried out this sprint, see the [sprint log](https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_04.org).

Next Sprint
===========
Next sprint we'll resume the work on moving kernel-agnostic transformations from the kernels into yarn, and start looking at the meta-data/concepts clean-up.

Binaries
======
You can download binaries from [Bintray](https://bintray.com/domaindrivenconsulting/Dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.04_amd64-applications.deb](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen_1.0.04_amd64-applications.deb)
- [dogen-1.0.04-Darwin-x86_64.dmg](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen-1.0.04-Darwin-x86_64.dmg)
- [dogen-1.0.04-Windows-AMD64.msi](https://dl.bintray.com/domaindrivenconsulting/Dogen/dogen-1.0.04-Windows-AMD64.msi)

**Note**: They are produced by CI so they may not yet be ready.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/903140257218088960][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6308906359798657024/][LinkedIn]]
- [[https://gitter.im/DomainDrivenConsulting/dogen][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2017-09-10 Sun 00:19]--[2017-09-10 Sun 00:30] =>  0:11
    CLOCK: [2017-09-08 Fri 13:45]--[2017-09-08 Fri 13:53] =>  0:08
    CLOCK: [2017-09-02 Sat 12:45]--[2017-09-02 Sat 13:14] =>  0:29
    CLOCK: [2017-09-01 Fri 15:52]--[2017-09-01 Fri 16:07] =>  0:15
    CLOCK: [2017-08-30 Wed 22:58]--[2017-08-30 Wed 23:25] =>  0:27
    CLOCK: [2017-08-30 Wed 22:09]--[2017-08-30 Wed 22:12] =>  0:03

Updates to sprint and product backlog.

*** COMPLETED Remove object types in yarn                             :story:
    CLOSED: [2017-08-31 Thu 08:58]
    CLOCK: [2017-08-31 Thu 08:32]--[2017-08-31 Thu 09:01] =>  0:29
    CLOCK: [2017-08-31 Thu 08:08]--[2017-08-31 Thu 08:31] =>  0:23

We need to figure out if this enumeration is still in use and if not
what needs to be done to remove it.

Seems like we are only using associative container at present. We
could probably replace the enumeration with a simple flag

*** COMPLETED Clean-up readme                                         :story:
    CLOSED: [2017-09-01 Fri 11:09]
    CLOCK: [2017-09-01 Fri 11:19]--[2017-09-01 Fri 11:28] =>  0:09
    CLOCK: [2017-09-01 Fri 11:10]--[2017-09-01 Fri 11:18] =>  0:08
    CLOCK: [2017-09-01 Fri 10:11]--[2017-09-01 Fri 11:09] =>  0:58

There are a number of minor changes that need to be done to the readme
file:

- fix typos
- bintray binaries are no longer experimental as we've been using them
- make build instructions a bit less repetitive

*** COMPLETED Rename transformers to adapters                         :story:
    CLOSED: [2017-09-01 Fri 11:55]
    CLOCK: [2017-09-01 Fri 11:48]--[2017-09-01 Fri 11:55] =>  0:07
    CLOCK: [2017-09-01 Fri 11:45]--[2017-09-01 Fri 11:47] =>  0:02
    CLOCK: [2017-09-01 Fri 11:29]--[2017-09-01 Fri 11:44] =>  0:15
    CLOCK: [2017-08-31 Thu 09:02]--[2017-08-31 Thu 09:06] =>  0:04

In the past we used the term "transformer" to mean a class that
converts types from one representation to another. However, now that
we are using domain terminology, the term "transforms" is taken to
mean a model transformation. To avoid confusion we should rename the
existing transformers to converters, adapters or some other
out-of-the-way name.

Affected models:

- quilt.cpp
- quilt.csharp
- yarn.dia

*** COMPLETED Analysis on annotations and profiles                    :story:
    CLOSED: [2017-09-02 Sat 13:06]
    CLOCK: [2017-09-01 Fri 15:25]--[2017-09-01 Fri 15:51] =>  0:26
    CLOCK: [2017-09-01 Fri 11:56]--[2017-09-01 Fri 13:56] =>  2:00

We can broadly divide annotations into three parts:

- *The annotations type system*. This is at present done by loading type
  templates. The right thing to do is to allow each model to create
  annotation types; these are then code-generated into a class which
  returns a list of type templates. Yarn needs to have a registrar for
  the type templates, which is populated during
  initialisation. Context factory talks to the registrar to obtain the
  type templates and initialises the type repository with it. The key
  thing is that the type system is static, and is more or less only a
  dogen concern (albeit users can define and consume their types, via
  the registrar). Thus we can quite easily solve the problems with the
  type system.
- *The templating machinery*. Annotations profiles are, on the main, a
  way to dynamically introduce annotation templates. These are
  "dynamic" because it is conceivable that each user will want to
  create its own set of annotation templates. There are two use cases:
  a) a common set of profiles, reused by several models (e.g. enable
  all facets, etc) b) a specific set of profiles useful only for one
  model (e.g. c++ artefact formatter). The profiling machinery
  requires a bit more thinking.
- *The configuration machinery*: the final piece of the puzzle is
  reading out data from annotations and using it in C++ code. For this
  we have three components: a) the type group classes, which aggregate
  the required annotation types b) the "configuration" classes, which
  are strongly typed representations of data stored in
  annotations; and c) a factory class responsible for using the type
  group class to populate the corresponding configuration class. In
  most simple cases, we could automate the generation of this triplet
  of classes.

The templating machinery is the most complex side of annotations then.
However, as it turns out, plain UML machinery can be used to handle
annotations profiles: UML profiles and UML stereotypes. Let's first
look into how we use stereotypes. We have:

- *Hard-coded static stereotypes*, which are in effect ways to map
  yarn meta-types into UML. For these we can simply hard-code the
  values and not worry about it. This is the right thing to do because
  the meta-types will not change often and when they do it will
  require a lot of manual work in yarn and in the frontends.
- *Dynamic stereotypes*. At present, this is how annotation profiles
  bind to element instances. We can make use of the labels in the
  annotation profile and refer to it in an element. During annotations
  transform, we expand the stereotype to the profile.

We can tackle this problem as follows:

- create a new element called =stereotype= (or perhaps
  =meta_stereotype=?). It has attributes which contain all of the
  required properties to create annotation profiles.
- create a new reference type called =profiles=. Problem: at present
  references are supplied by meta-data in target; however, we must
  process the profiles before reading in the target. The command line
  option solves this problem, at the cost of creating an inconsistency
  between references and profiles.
- update the exogenous model chain with a "profile" mode. In this
  case, the exogenous model can only contain =stereotypes=. No other
  UML stereotype is allowed. Create a context with an annotations
  group factory that loads no annotations profiles.
- create a transform that takes in the stereotypes and produces
  annotation profiles.
- create a second context based on the first one, but using a
  annotation group factory populated with all the annotation
  profiles. This will be the final context, used for all models.
- For each exogenous model: if the model contains stereotypes,
  transform them into annotation profiles before performing the
  annotations transform. This allows each model to supply its local
  stereotypes, visible only to the model. Only profile models have
  global stereotypes.

This would all be made easier if somehow it was possible to provide
annotation profiles externally to the context; this way we could
supply them globally or locally (ideally both), just before we do the
annotations transform. We need to look into changing the annotations
group factory API to cope with this.

Merged stories:

*Investigate code-generation around annotations*

We have two cases where code-generation makes sense for
annotations. Let look at them in turn.

Type templates

At present we are supplying JSON files with type templates. In truth
these are not really "data files" because changing them will cause
problems to the system; its tightly coupled to them. It would make
more sense to allow models to define their type templates inside the
model itself. We could use a stereotype of
=annotations::type_template= and then use meta-data for all of the
fields, as per JSON, e.g.:

:  {
:    "name": {
:      "simple": "profile",
:      "qualified": "annotations.profile"
:    },
:    "archetype_location": {
:      "family": "annotations"
:    },
:    "value_type": "text",
:    "template_kind": "instance",
:    "scope": "any"
:  }

We then code-generate the insertion of the type template into the
annotations type templates repository via an initialisers-like
framework.

It may make more sense to have one UML class with all the type
templates; the type templates then become attributes of that
class. The problem then is what to name that class. also, we may want
to have a couple of these, to group type templates logically (for
example we want the top-level templates like =enabled= separated from
the namespace-specific templates).

But the gist of it is that its very straightforward to add some
machinery that generates the code required to inject the type
templates into the system, and that it is triggered during
initialisation, replacing JSON loading.

Use of annotations

We then have the following usage pattern:

- define a class with all the related fields (with types of the type
  templates above). We call this class =type_group=. We may need to
  instantiate it for specific fields, or by facet, etc. We need to
  look at all of the examples in the code-base. Note that the layout
  of this class will (likely) bear no resemblance to the type
  templates grouping - this is just a "bag" with all of the available
  type templates, whereas the type group aggregation does normally
  have some useful meaning (e.g. =orm_properties=, etc).
- define a "factory" class for the type group class that uses the
  traits to locate the types (instances of type templates). For this,
  the type group class attributes need to refer to the fully qualified
  field name (possibly requiring some inputs such as kernel, facet).
- define a c++ class with the properties we're interested in. We
  normally call this class =_configuration= if its just used to read
  the meta-data, or =_properties= if its used as a real type. Note
  that at present we have allowed the layout of the type group class
  and the properties/configuration classes to be possibly quite
  different; we gather _all_ of the types of interest in the type
  group class, but then have multiple properties/configuration classes
  to match our needs.
- finally, we define a "factory" class that takes in the type group
  and produces the configuration/properties class.

In a code-generated world:

- we need to somehow force the type group class to match the
  configuration class; this will probably result on a lot of
  duplication. For example, for the ORM properties, we probably have a
  couple in common across object/model/attributes.
- we need to map C++ types into annotation types such that we can back
  out the annotation type from a c++ type. For example, given an
  enumeration, we want to create a annotation type of "text" but then
  automatically generate the "from/to" converters for the enumeration.

*Code generation of dynamic instances*

We seem to have a pretty well established usage pattern for dynamic,
so it may be a candidate for code generation. All we need is:

- a stereotype to mark a class as dynamic; the attributes of the class
  are dynamic fields, and their types must be one of the valid values
  for dynamic fields. The default value is used for the field's
  default value. Qualified name, ownership hierarchy, definition type,
  scope, etc are supplied as meta-data.
- stereotype name should be something like =DynamicFieldGroup=.
- the injection of the settings class is done by looking at the
  =DynamicFieldGroup= class and mapping the dynamic types to C++
  types. Note: this mapping should be dynamic too so that we can use
  it for other languages. We just need a meta-data tag for this, like
  we do with default enum value.
- the injection of the settings factory class is a bit more
  complicated; we need to mark the object as a settings factory. At
  present we have object types, but it was supposed to be removed
  after a refactoring. Actually we just need to create a new kind of
  element (=dynamic_settings_factory=?). In addition, settings factory
  may also need to take in some parameters such as facet/formatter.
- a stitch template that generates the settings factory.
- a stitch template that registers the dynamic field definition;
  instead of JSON we can just generate c++ code to perform the
  injection.
- we could also generate the repository and in most cases the
  repository factory. The only case where this breaks down is when we
  need to look at properties too.
- we should have a number of knobs to control generation: a) generate
  field injection b) generate settings factory c) generate repository
  d) generate repository factory.

We also need to merge the traits class directly into the factory. In
the majority of cases, we have traits just to access the fields. But
there are a few cases where we use traits for other purposes such as
formatter naming.

*Add support for "one off" profiles*

At present one can define top-level profiles. These are useful, but in
practice we ended up still defining a lot of things in each model. We
need a way to associate a profile with a model by supplying it on the
command line. That way users can create profiles and store them next
to the model rather than having to create a data directory, etc etc.

Actually the problem is that profiles aren't really implemented
correctly. First we should not call them profiles at all since they
are not UML profiles and overloading the term just generates
confusion. Second its important to understand how Dogen profiles come
about:

- we extend the UML meta-model via stereotypes to support all of the
  required yarn and quilt concepts.
- when we instantiate the yarn/quilt types via a UML model, we need to
  supply the values for the attributes which have been extended. If
  done properly this would happen via UML tagged values. Dia does not
  support these. At any rate, at present we use Dogen meta-data which
  is almost like tagged values.
- Dogen profiles are then an attempt to create bundles of tagged
  values with pre-populated values so that we do not need to manually
  populate them for every type. Instead, we can associate a stereotype
  with the type and then the system will automatically populate the
  values from the bundle.
- From all of this it follows that it should be possible to define
  these "bundles" directly in a UML diagram. If we were to use UML
  properly (or at least almost properly), we would define a class with
  a stereotype of =stereotype=, a name of the stereotype we'd like to
  define (say =Serializable=) and then its tagged values are the keys
  and values of the meta-data we want to define. This is strictly
  speaking not correct UML because we are stating we are augmenting
  the UML meta-model (hence =stereotype=) but then we end up
  instantiating a meta-model class with some predefined values. Its
  not clear how to express this in UML. Note that we have exactly the
  same issue with concepts.
- and, after some thinking, we are trying to do exactly the same thing
  as we are already doing for concepts: i.e. some kind of meta-level
  operation that allows us to add structural features to an
  element. Thus we can just use concepts, which are not even defined
  in UML - augmenting its meaning will not take us away from the
  literature. We can very simply add a last step to concepts transform
  which merges the annotations of the concept objects, using exactly
  the machinery we defined for profiles. The only slight problem is
  that we cannot reuse concepts across models.

Tasks:

- add annotations merging to concepts processing. Should cause no
  changes at all on all models.
- create a model in dogen defining basic concepts.

Links:

- [[https://msdn.microsoft.com/en-us/library/dd465146.aspx][Standard stereotypes for UML models]]

*** COMPLETED Use namespaced stereotypes                              :story:
    CLOSED: [2017-09-03 Sun 18:55]
    CLOCK: [2017-09-03 Sun 17:39]--[2017-09-03 Sun 18:54] =>  1:15
    CLOCK: [2017-09-02 Sat 20:35]--[2017-09-02 Sat 21:09] =>  0:34

Originally we added a space in the ORM stereotypes:

: orm value

This is not a particularly good idea. We should just add support for
namespaced stereotypes:

: orm::value

We should also change all of the existing stereotypes to have a
namespace:

: modeling::object

And so forth. The namespace name probably needs a bit of thinking.

Actually, we should name all of the static stereotypes with a
namespace, and making it clear they are connected to yarn. Example:

: yarn::enumeration
: yarn::orm::value

and so forth.

*** COMPLETED Tailor dogen models                                     :story:
    CLOSED: [2017-09-08 Fri 09:23]
    CLOCK: [2017-09-08 Fri 09:02]--[2017-09-08 Fri 09:23] =>  0:21

We are making a lot of changes to the JSON frontend, but our tests are
not exactly comprehensive. It would be good to start tailoring the
dogen models too, just to see what changes - even if we know we cannot
knit them yet.

The only snag is that we broke indent all json for now, but we'll have
to live with it. Northwind model seems borked.

*** COMPLETED Rename yarn concepts                                    :story:
    CLOSED: [2017-09-08 Fri 13:51]
    CLOCK: [2017-09-08 Fri 13:15]--[2017-09-08 Fri 13:45] =>  0:30
    CLOCK: [2017-09-08 Fri 11:48]--[2017-09-08 Fri 12:01] =>  0:13
    CLOCK: [2017-09-08 Fri 08:56]--[2017-09-08 Fri 09:01] =>  0:05
    CLOCK: [2017-09-08 Fri 08:25]--[2017-09-08 Fri 08:55] =>  0:30
    CLOCK: [2017-09-07 Thu 20:05]--[2017-09-07 Thu 20:54] =>  0:49
    CLOCK: [2017-09-07 Thu 08:53]--[2017-09-07 Thu 08:57] =>  0:04
    CLOCK: [2017-09-07 Thu 08:39]--[2017-09-07 Thu 08:53] =>  0:14
    CLOCK: [2017-09-07 Thu 08:32]--[2017-09-07 Thu 08:38] =>  0:06
    CLOCK: [2017-09-07 Thu 08:05]--[2017-09-07 Thu 08:31] =>  0:26
    CLOCK: [2017-09-05 Tue 22:55]--[2017-09-05 Tue 23:32] =>  0:37
    CLOCK: [2017-09-05 Tue 21:51]--[2017-09-05 Tue 22:54] =>  1:03
    CLOCK: [2017-09-05 Tue 21:39]--[2017-09-05 Tue 21:50] =>  0:11
    CLOCK: [2017-09-05 Tue 20:51]--[2017-09-05 Tue 21:39] =>  0:48
    CLOCK: [2017-09-05 Tue 18:43]--[2017-09-05 Tue 18:54] =>  0:11
    CLOCK: [2017-09-05 Tue 18:06]--[2017-09-05 Tue 18:42] =>  0:36
    CLOCK: [2017-09-05 Tue 07:35]--[2017-09-05 Tue 08:20] =>  0:45
    CLOCK: [2017-09-04 Mon 21:58]--[2017-09-04 Mon 22:39] =>  0:41

When concepts were introduced, it was clear that something was not
quite right on the naming. We used the C++ terminology because it
mapped well enough to the idea, but it was understood that we were
talking about two different things. As part of the clean-up required
for profiles, its time to revisit concepts.

A yarn concept is, really, an "object template". That is, it allows us
to create a template of a subset of the structure of a yarn object,
which can then be instantiated (pasted?) into actual objects. One
would like to avoid the use of the word "template", due to its C++
connotations, but sadly it seems there isn't a more appropriate word.

Unlike objects, object templates support multiple inheritance.

We consume the object templates via stereotypes.

Thus so far we should just rename concepts to object templates.

However, the downside is that we now have a verbose stereotype:

: object_template
: yarn::object_template

Having said that, we cannot have values for all attributes in an
object, just the attribute collection.

Tasks:

- rename yarn element and model collections.
- rename object's modeled concepts
- rename concept transform, tests
- rename meta-name, update JSON for tests
- tidy-up mock factory (variables, method names)
- rename stereotype
- rename test model types with concept in the name.
- check that tailor generates correct code.
- ORM transform uses expand instead of transform

*** COMPLETED Remove extra copying in mapper                          :story:
    CLOSED: [2017-09-08 Fri 13:59]
    CLOCK: [2017-09-08 Fri 13:54]--[2017-09-08 Fri 13:59] =>  0:05

It seems in addition to cloning the model in mapper, we are also
copying the objects. This seems wrong. Try not copying and see what
happens.

*** COMPLETED Investigate the JSON break on Northwind model           :story:
    CLOSED: [2017-09-08 Fri 14:58]
    CLOCK: [2017-09-08 Fri 14:32]--[2017-09-08 Fri 14:58] =>  0:26
    CLOCK: [2017-09-08 Fri 14:00]--[2017-09-08 Fri 14:31] =>  0:31

At present we cannot indent all JSON documents because one of the
models does not validate. It must be a tailor bug. Fix it.

The problem is with ODB pragmas:

: "odb_pragma" : "column("LASTNAME")"

We need to somehow escape the quotes. However, why do we even need to
have these set? Actually these are required because we are renaming
the field (from =last_name=).

*** COMPLETED Rename ODB parameters                                   :story:
    CLOSED: [2017-09-08 Fri 15:10]
    CLOCK: [2017-09-08 Fri 14:59]--[2017-09-08 Fri 15:10] =>  0:11

At present we use the following form:

: #DOGEN odb_pragma=no_id

Finally we should no longer attempt to derive the ODB pragma
context. We should just add it verbatim.

We need to use the new naming style =quilt.cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

*** COMPLETED Investigate usage of origin type                        :story:
    CLOSED: [2017-09-08 Fri 22:50]
    CLOCK: [2017-09-08 Fri 22:35]--[2017-09-08 Fri 22:50] =>  0:15

With the current setup of the transforms, we always know who the
target model is. Thus the =origin_types= flag may not be used
correctly at present.

Actually, we need way to figure out which types to generate, and which
types are references. So we still need this flag.

*** COMPLETED Improve dumping of models                               :story:
    CLOSED: [2017-09-13 Wed 21:41]
    CLOCK: [2017-09-13 Wed 19:25]--[2017-09-13 Wed 21:41] =>  2:16
    CLOCK: [2017-09-12 Tue 20:25]--[2017-09-12 Tue 21:40] =>  1:15
    CLOCK: [2017-09-12 Tue 07:35]--[2017-09-12 Tue 08:17] =>  0:42
    CLOCK: [2017-09-11 Mon 20:42]--[2017-09-11 Mon 22:54] =>  2:12
    CLOCK: [2017-09-11 Mon 19:20]--[2017-09-11 Mon 20:41] =>  1:21
    CLOCK: [2017-09-10 Sun 18:58]--[2017-09-10 Sun 19:33] =>  0:35
    CLOCK: [2017-09-10 Sun 18:31]--[2017-09-10 Sun 18:57] =>  0:26
    CLOCK: [2017-09-10 Sun 18:25]--[2017-09-10 Sun 18:30] =>  0:05
    CLOCK: [2017-09-10 Sun 15:27]--[2017-09-10 Sun 16:37] =>  1:10
    CLOCK: [2017-09-10 Sun 15:14]--[2017-09-10 Sun 15:26] =>  0:12
    CLOCK: [2017-09-10 Sun 14:49]--[2017-09-10 Sun 15:13] =>  0:24
    CLOCK: [2017-09-10 Sun 14:35]--[2017-09-10 Sun 14:48] =>  0:13
    CLOCK: [2017-09-10 Sun 11:49]--[2017-09-10 Sun 13:04] =>  1:15
    CLOCK: [2017-09-10 Sun 10:55]--[2017-09-10 Sun 11:48] =>  0:53

At present it is very difficult to find the log information regarding
models at different stages in the pipeline. It seems we are reaching
the limits for what logging can do for us here:

- the models are so large even emacs is struggling with the long line
  sizes.
- if we dump all models, we end up with extremely large log files. But
  in practice we tend to be looking for specific dumps: a model at
  transformation x or between x and y.

It would be much better if:

- there was a command line option that triggered the saving of
  models to a user supplied directory.
- we came up with a directory/file structure that allowed one to
  quickly find the model one is after. For example, the nesting of
  transformations could be the folder structure.

To start off with we could dump all models for all transformations to
get us up and running quickly, but in an ideal world we should be able
to supply the stages/transformations for which we want dumps. We can
then take the dumps and diff them from emacs.

The folder structure could also include a "before" and "after" for
each transform. Disk space is not an issue given that we would only be
using this when things went wrong. It also means we can save the files
as formatted JSON given that new lines are no longer an issue as they
are with the log file.

Similar to the context class, we could create a dumping context that
keeps track of the nesting of transforms and their names and knows if
dumping is enabled.

For tests we should default to the directory of the logs. It should be
possible to enable this feature for just one test, given that we will
generate very large amounts of data.

In a service setup we need a way for this data to be pushed somewhere
else like a cache, but this will have to wait until there is clarity
on just how IO will be implemented.

Notes:

- create a =probe= class that is supplied to all transforms. It is
  const; any changeable state is marked as mutable, since it is
  morally const.
- If probing is off, nothing happens when we call methods of this
  class.
- a probe supports the following operations: start/finish chain,
  start/finish transform. These pairs are called scopes.
- Start operations take:
  - a name which is the id of the transform. All transforms must now
    have an ID.
  - input: one of the  three kinds of models plus code generation
    output for the model to text transforms.
- Finish operations take:
  - output.
- each scope is logged as a guid. This makes it easier to correlate
  the dumps with the log file.
- the probe is initialised with a directory where all files will be
  dumped.
- every time we enter a chain scope, we create a new directory.
- every time we enter a transform scope we bump the transform counter
  by one. When we dump inputs and outputs, we write the files as:

: [TRANSFORM_COUNTER]-[TRANSFORM_ID]-[GUID]-[input|output].json
: 001-yarn.transforms.some_transform-e9d67262-f8f6-4291-a259-ebabe89b217a-input.json
: 001-yarn.transforms.some_transform-e9d67262-f8f6-4291-a259-ebabe89b217a-output.json

- command line arguments are:

: transforms-probe-stats
: transforms-probe-stats-graph
: transforms-probe-all
: transforms-probe-directory

- if =transform-print-stats= is enabled, a report is generated with
  the transform graph and the total execution time taken by each
  transform and chain. This is written in JSON for easy diffing.
- must ensure the dump stats output states:
  - if build is debug or release
  - version
  - log level
  - if dump data is enabled

Links:

- [[http://www.randygaul.net/2015/06/15/printing-pretty-ascii-trees/][Printing Pretty Ascii Trees]]

*** COMPLETED Create yarn options                                     :story:
    CLOSED: [2017-09-13 Wed 22:53]
    CLOCK: [2017-09-13 Wed 21:55]--[2017-09-13 Wed 22:53] =>  0:58
    CLOCK: [2017-09-13 Wed 21:46]--[2017-09-13 Wed 21:54] =>  0:08

We need to replace the dependency on the options model and create a
stand alone yarn options class.

*** COMPLETED Merge knit with yarn                                    :story:
    CLOSED: [2017-09-13 Wed 23:31]
    CLOCK: [2017-09-14 Thu 06:21]--[2017-09-14 Thu 06:30] =>  0:09
    CLOCK: [2017-09-13 Wed 23:56]--[2017-09-14 Thu 00:03] =>  0:07
    CLOCK: [2017-09-13 Wed 23:32]--[2017-09-13 Wed 23:55] =>  0:23
    CLOCK: [2017-09-13 Wed 23:24]--[2017-09-13 Wed 23:31] =>  0:07
    CLOCK: [2017-09-13 Wed 23:17]--[2017-09-13 Wed 23:23] =>  0:06
    CLOCK: [2017-09-13 Wed 23:06]--[2017-09-13 Wed 23:16] =>  0:10
    CLOCK: [2017-09-13 Wed 22:54]--[2017-09-13 Wed 23:05] =>  0:11

There isn't a lot of reason to have a knit model, really, now that
yarn has taken over the model to text transforms. We should just move
housekeeper and the rest of knit into helpers and update the
code-generator to perform the complete workflow.

*** STARTED Add canonical archetype support to yarn                   :story:
    CLOCK: [2017-09-10 Sun 00:08]--[2017-09-10 Sun 00:18] =>  0:10
    CLOCK: [2017-09-09 Sat 23:40]--[2017-09-10 Sun 00:07] =>  0:27
    CLOCK: [2017-09-09 Sat 23:28]--[2017-09-09 Sat 23:39] =>  0:11
    CLOCK: [2017-09-09 Sat 22:50]--[2017-09-09 Sat 23:27] =>  0:37
    CLOCK: [2017-09-09 Sat 22:05]--[2017-09-09 Sat 22:49] =>  0:44
    CLOCK: [2017-09-09 Sat 21:02]--[2017-09-09 Sat 21:29] =>  0:27
    CLOCK: [2017-09-09 Sat 19:52]--[2017-09-09 Sat 20:15] =>  0:23
    CLOCK: [2017-09-09 Sat 19:12]--[2017-09-09 Sat 19:51] =>  0:39
    CLOCK: [2017-09-09 Sat 15:40]--[2017-09-09 Sat 16:08] =>  0:28
    CLOCK: [2017-09-09 Sat 13:49]--[2017-09-09 Sat 15:39] =>  1:50
    CLOCK: [2017-09-09 Sat 08:55]--[2017-09-09 Sat 09:27] =>  0:32
    CLOCK: [2017-09-08 Fri 22:51]--[2017-09-08 Fri 23:08] =>  0:17

We need to add a new attribute in context which captures the canonical
archetypes.

Notes:

- kernel must also return canonical archetype by element type
  index. Perhaps we should have a struct that aggregates both:
  archetype locations for meta-type? Or kernel can just return a
  =std::pair=.
- at present we have placed the canonical archetype resolution as part
  of the element properties. However, we do not need to have this at
  the element level since its a meta-type property and can be
  determined up-front. We do need to resolve a name into a meta-type
  before we can resolve a meta-type into a concrete archetype.
- we need to unpick the notion of whether a formatter is "includible"
  or not from the notion of canonical archetypes. Canonical archetypes
  is meta-model concept: given a facet and a meta-model type, which
  archetype represents the "key" definition of the element. It just so
  happens that this function has a use in identifying the files to
  include.
- before we focus too much on adding canonical archetype support to
  yarn, its important to understand just exactly how it gets used. We
  are doing far too many look-ups at present, given the information
  that is known. Canonical archetypes are a way to refer to a type for
  a given formatter without knowing who exactly that formatter
  is. This is useful for example when we need to include the
  definition of a type but we do not know if its an enum, object
  etc. So, at present, we proceed as follows:
  - for a given name, we first resolve the archetype; if its not
    canonical nothing happens. If its canonical, it gets resolved into
    a concrete archetype.
  - we then find the element associated with the name and get its
    properties. If the archetype is not enabled, there is nothing to
    do.
  - if its enabled, we then need to look for its directives group. If
    none exist, then there is nothing to do.

  The gist of this exercise is that we could get away with a single
  look-up; for this we would need to map the canonical directives
  group as well. Actually this will not work because an archetype can
  be disabled on one element but enabled on another, so we need to
  separate enablement from the directives group. But we certainly can
  map all the model elements to the meta-model elements and those to
  the canonical archetypes. So we can have an "enablement manager" of
  some kind, who consumes a model and the context, and creates a map
  of element and archetype to bool. Steps:
  - directive group repository factory needs to insert against the
    canonical artefact too.
  - dependencies builder needs to use the original (non-resolved)
    artefact name.
  - create a "enablement manager" in yarn that pre-processes the model
    and creates the da+ta structures as described above (for element
    id + archetype returns enabled flag)
  - supply the "enablement manager" to the kernel and from there to
    the dependencies builder.
  - at this point, all C++ specific enablement infrastructure can be
    deleted.

  An even simpler way of looking at this is to generate a set of pairs
  of strings during enablement transform for all elements + artefacts
  that are enabled; put that in the endomodel; merge the sets as part
  of the merger (throwing if duplicates are found). Then supply the
  set to the dependencies builder somehow (or create a helper in yarn
  that acts as the enablement manager but is just doing a look-up on
  the set).

Tasks:

- add class in annotations to generate the canonical form from a
  archetype location.
- update kernels to return archetype location groups, with canonical
  archetypes populated.
- add a set of pairs of strings to endomodel: enabled archetypes for
  element. Populate it during enablement transform. Add canonical
  archetype there too.
- in directive group repository factory, for each archetype that is
  the canonical archetype, populate its entry too. Remove use of the
  canonical resolver in dependencies builder.
- create a class to query the set: "enablement manager"? Instantiate
  it in C++ kernel and supply it to dependencies builder factory.
- remove all references of formattables and element properties in
  dependencies builder.

Notes:

- make is_enabled private in dependencies builder and see what breaks.

*** Remove dumping of models from log                                 :story:

Once probing is in place, we need to remove all the dumps we are doing
of models at present. These just add size to the log files for no
reason.

*** Drop the original extension in tailor                             :story:

Filenames in tailor look weird:

: dart.dia.json

it should just be:

: dart.json

*** Move model sorter from helpers                                    :story:

This is really just a sorting transform, not a helper.

*** Move enablement into yarn                                         :story:

It seems that the concepts around enablement are actually not kernel
specific but instead can be generalised at the meta-model level. We
need to create adequate representations in yarn to handle facets,
etc. We then need to move across the code that computes enablement
into yarn so that all kernels can make use of it.

Problems:

- we are checking to see if the hash facet is enabled with c++ 98; if
  so, we throw as this facet is incompatible. We cannot do this from
  yarn since we do not know what c++ standards are.
- because we do not have a mapping between a archetype location and
  the meta-type, we will be enabling/disabling all archetype locations
  across all meta-types.
- because we do not have element segmentation, the element extensions
  will be disabled. Actually this will probably work just the same,
  given that all elements exist.
- enablement must be done after external transformations so it picks
  up fabric types.
- we need to support formatting styles in order to be able to use the
  artefact properties from the meta-model.
- in quilt.cpp, someone did an upfront generation of all archetype
  properties against the archetype locations. We not doing that in
  yarn, so nothing is coming out. This was done during transformation
  in formattables.
- with a move into yarn, we seem to have broken the overwrite flag
  logic; changes no longer result in new code being generated.
- we also have borked the includes: dependency builder is looking into
  the formattables instead of element. However, we then run into
  segmentation issues because we cannot find forward declarations on
  the main element.

To do:

- kernel registrar type index map - done.
- c# formatter registrar type index map - done.
- bug in template instantiating: artefact expansions do not seem to
  take kernel into account - done.
- use new enabled fields.
- delete all enablement classes in c++ and enabled/overwrite properties.

*Previous Understanding*

We need to make use of the exact same logic as implemented in
=quilt.cpp= for enablement. Perhaps all of the enablement related
functionality can be lifted and grafted onto quilt without any major
changes.

*** Move dependencies into yarn                                       :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Use element ids for associations                                  :story:

There doesn't seem a need for having entire names for associations;
these are used to find information by ID anyway. We should try to
convert them to element id's instead and see what breaks.

- transparent, opaque associations
- base, derived visitor
- contained by

We can't do this for:

- visitor: we use the name in the formatter.

*** Code-generate annotations type templates                          :story:

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it. Name: =yarn::annotation_type_template=?
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** Cannot make qualified references to concepts                      :story:

At present it is not possible to consume concepts defined in a
referenced model, nor is it possible to refer to a concept in a
different module from the module in which the element is in, e.g.: say
concept C0 is declared in module M0; all types of M0 can have C0 as
stereotype and that will resolve. However any types on any other
module cannot see the concept.

One suggestion is to allow scoped names in stereotypes:
=module::Concept=.

The heuristic for concept resolution is then:

- external modules are never part of the scoped name;
- on a scoped concept with M names, we first start by assuming that
  the first name is the model module and M-2 is/are the internal
  module(s). We try this for all names in M-2, e.g. first two names
  are model modules and M-3 names are internal modules and so forth.

*** Add support for object templates that work cross-model            :story:

We've implemented support for cross-model inheritance in sprint 87 but
we did not cover object templates. Most of the approach is the same,
but unfortunately we can't just reuse it.

Tasks:

- we need a refines field which is a text collection.
- we need refinement settings, factory etc.
- update parsing expander.

*** Move formatting styles into yarn                                  :story:

We need to support the formatting styles at the meta-model level.

*** Throw on unsupported stereotypes                                  :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

*** Change order of includes according to Lakos major design rule     :story:

Lakos says:

#+begin_quote
The .c file of every component should include its own .h file as the
first substantive line of code.
#+end_quote

We decided to include it as the last line. However, Lakos approach has
the side-effect of automatically detecting headers that are missing
includes. We used to do this manually by generating =.cpp= files that
just included the header but then had to remove it because it was
slowing down compilation. With Lakos approach we get the best of both
worlds.

We need to also update the generated code to follow this
approach. This will require some thinking.

*** Move element segmentation into yarn                               :story:

We've added the notion that an element can be composed of other
elements in quilt, in order to handle forward declarations. However,
with a little bit of effort we can generalise it into yarn. It would
be useful for other things such as inner classes. We don't need to
actually implement inner classes right now but we should make sure the
moving of this feature into yarn is compatible with it.

Notes:

- seems like we have two use cases: a) we need all elements, master
  and extensions and we don't really care about which is which. b) we
  only want masters. However, we must be able to access the same
  element properties from either the master or the extension. Having
  said all that, it seems we don't really need all of the element
  properties for both - forward declarations probably only need:
  decoration and artefact properties.
- we don't seem to use the map in formattables model anywhere, other
  than to find master/extension elements.
- Yarn model could have two simple list containers (masters and
  all). Or maybe we don't even need this to start off with, we can
  just iterate and skip extensions where required.
- so in conclusion, we to move decoration, enablement and dependencies
  into yarn (basically decoration and artefact properties) first and
  then see where segmentation ends.

Tasks:

- add a concept for element extensions: =Extensible=. Contains a list
  of element pointers.
- populate it with the extensions.
- change enablement to merge all element properties of extensible
  elements.

*** Create a yarn locator                                             :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator. It will live in fabric.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

*** Generate file paths as a transform                                :story:

Add a fabric transform for file path generation.

*** Add a modeline to stitch                                          :story:

It would be nice to be able to supply the mode and other emacs
properties to stitch templates. For that we just need a special KVP
used at the top that contains the modeline:

: <#@ modeline="-*- mode: poly-stitch; tab-width: 4; indent-tabs-mode: nil; -*-" #>

Stitch can read this KVP and ignore it.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

*** Move helpers into yarn                                            :story:

Looking at helpers, it is clear that they are common to all
languages. We just need to rename the terminology slightly -
particularly wrt to streaming properties - and then move this code
across into yarn.

*** Move facet properties into yarn                                   :story:

We should be able to handle these generically in yarn.

*** Move ORM camel-case and databases into yarn                       :story:

We should handle this property at the ORM level, rather than at the
ODB level.

Similarly, we should move the ODB databases into yarn and make that a
ORM-level concept.

*** Distinguish between meta-types that require canonical archetypes  :story:

At present it is not possible to know which meta-types require
canonical archetypes and which don't. In the validation we said:

:         * We must have one canonical formatter per type per facet.
:         * FIXME: this check is broken at the moment because this is
:         * only applicable to yarn types, not fabric types. It is also
:         * not applicable to forward declarations. We need some
:         * additional information from yarn to be able to figure out
:         * which types must have a canonical archetype.

We should have some kind of flag in yarn to distinguish. This still
requires a bit of thinking.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

** Deprecated
*** CANCELLED Make the Zeta model compilable                          :story:
    CLOSED: [2017-08-30 Wed 23:01]

*Rationale*: not required since Upsilon has been deprecated.

We need to work through the list of issues with the Zeta model and get
it to a compilable state.

*** CANCELLED Registrar in quilt is not being generated               :story:
    CLOSED: [2017-08-30 Wed 23:14]

*Rationale*: quilt model has been deleted.

We don't seem to change the contents of this file when regenerating.

*** CANCELLED Stitcher log file names look weird                      :story:
    CLOSED: [2017-08-30 Wed 23:19]

*Rationale*: they look ok with the current release.

At present we are writing files with names like:

: dogen.stitcher...log

*** CANCELLED ODB options file is generated to incorrect location     :story:
    CLOSED: [2017-08-30 Wed 23:21]

*Rationale*: ODB options generation changed dramatically recently (one
per type, etc).

Models with composite names seem to have their ODB options file
generated under the =projects= directory, e.g.:

: projects/vtk/geometry/src/options.odb
