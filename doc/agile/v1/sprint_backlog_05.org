#+title: Sprint Backlog 05
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Sort out concepts and profiles.
- Resume and progress the work on moving "generic" types from the
  quilt models into yarn.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2017-09-01 Fri 16:07]
| <75>                                                                        |        |      |      |       |
| Headline                                                                    | Time   |      |      |     % |
|-----------------------------------------------------------------------------+--------+------+------+-------|
| *Total time*                                                                | *6:30* |      |      | 100.0 |
|-----------------------------------------------------------------------------+--------+------+------+-------|
| Stories                                                                     | 6:30   |      |      | 100.0 |
| Active                                                                      |        | 6:30 |      | 100.0 |
| COMPLETED Edit release notes for previous sprint                            |        |      | 0:44 |  11.3 |
| STARTED Sprint and product backlog grooming                                 |        |      | 0:45 |  11.5 |
| COMPLETED Remove object types in yarn                                       |        |      | 0:52 |  13.3 |
| COMPLETED Clean-up readme                                                   |        |      | 1:15 |  19.2 |
| COMPLETED Rename transformers to adapters                                   |        |      | 0:28 |   7.2 |
| STARTED Analysis on profiles and concepts                                   |        |      | 2:26 |  37.4 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2017-08-30 Wed 22:57]
    CLOCK: [2017-08-30 Wed 22:13]--[2017-08-30 Wed 22:57] =>  0:44

Add github release notes for previous sprint.

Title: Dogen v1.0.04, "Zona dos Riscos"

#+begin_src markdown
![Zona dos Riscos](http://www.almadeviajante.com/wp-content/uploads/deserto-do-namibe.jpg)
_Zona dos Riscos, Namibe, Angola. (C) Alma de Viajante, 2017._

Overview
=======
As usual, yarn internal refactoring is the bulk of the work in this sprint. The refactoring work had three major themes:

- **Use shared pointers across the board** for yarn elements, from frontend to the backend. This was done as a requirement for the exogenous models changes described below; as it happens, it has the nice side-effect of reducing the number of copies of model elements.
- **Finish exogenous models support**: frontends now have a special purpose model type, designed only for the kind of operations supported at the frontend level. This cleaned up transformations quite a bit, making it obvious which ones apply at which stage. The conceptual model is now somewhat cleaner, with the introduction of _exomodels_ (previously "exogenous models") and _endomodels_ (previously "intermediate models"), which specific purposes.
- **Name processing now done in core**: as part of the exogenous models change, we also moved the external and model module processing away from the frontends and into the core. This means less code duplication across frontends.

In addition to these, there were a couple of additional stories that had user facing impact, described in the next section.

User visible changes
================
This sprint introduced a number of user visible changes, all related to the internal clean-up work:

- Upsilon support was considered deprecated, since the customer for which we developed it no longer requires it. Since it was a custom-made frontend with no real application outside of this specific use case, all code related to upsilon has been removed.
- Continuing the meta-name work, JSON now represents these as regular yarn names. Sadly this makes the JSON more verbose, but at least it's more consistent now. This change breaks backwards compatibility, so users with JSON models need to update them. Sample change:
```
-    "meta_type": "module",
+    "meta_name": {
+      "simple": "module",
+      "external_modules": "dogen",
+      "model_modules": "yarn",
+      "internal_modules": "meta_model"
+    }
```
- A new command line flag was introduced: ```--compatibility-mode```. The objective of this flag is to disable some of the model validation code, where the errors are known to be caused by a forwards or backwards incompatible change. However: a) this is an experimental flag, very incomplete at present; and b) even when finished, the generated code may just be invalid.

For more details of the work carried out this sprint, see the [sprint log](https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_04.org).

Next Sprint
===========
Next sprint we'll resume the work on moving kernel-agnostic transformations from the kernels into yarn, and start looking at the meta-data/concepts clean-up.

Binaries
======
You can download binaries from [Bintray](https://bintray.com/domaindrivenconsulting/Dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.04_amd64-applications.deb](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen_1.0.04_amd64-applications.deb)
- [dogen-1.0.04-Darwin-x86_64.dmg](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen-1.0.04-Darwin-x86_64.dmg)
- [dogen-1.0.04-Windows-AMD64.msi](https://dl.bintray.com/domaindrivenconsulting/Dogen/dogen-1.0.04-Windows-AMD64.msi)

**Note**: They are produced by CI so they may not yet be ready.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/887172610487922688][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6292938732865617920/][LinkedIn]]
- [[https://gitter.im/DomainDrivenConsulting/dogen][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2017-09-01 Fri 15:52]--[2017-09-01 Fri 16:07] =>  0:15
    CLOCK: [2017-08-30 Wed 22:58]--[2017-08-30 Wed 23:25] =>  0:27
    CLOCK: [2017-08-30 Wed 22:09]--[2017-08-30 Wed 22:12] =>  0:03

Updates to sprint and product backlog.

*** COMPLETED Remove object types in yarn                             :story:
    CLOSED: [2017-08-31 Thu 08:58]
    CLOCK: [2017-08-31 Thu 08:32]--[2017-08-31 Thu 09:01] =>  0:29
    CLOCK: [2017-08-31 Thu 08:08]--[2017-08-31 Thu 08:31] =>  0:23

We need to figure out if this enumeration is still in use and if not
what needs to be done to remove it.

Seems like we are only using associative container at present. We
could probably replace the enumeration with a simple flag

*** COMPLETED Clean-up readme                                         :story:
    CLOSED: [2017-09-01 Fri 11:09]
    CLOCK: [2017-09-01 Fri 11:19]--[2017-09-01 Fri 11:28] =>  0:09
    CLOCK: [2017-09-01 Fri 11:10]--[2017-09-01 Fri 11:18] =>  0:08
    CLOCK: [2017-09-01 Fri 10:11]--[2017-09-01 Fri 11:09] =>  0:58

There are a number of minor changes that need to be done to the readme
file:

- fix typos
- bintray binaries are no longer experimental as we've been using them
- make build instructions a bit less repetitive

*** COMPLETED Rename transformers to adapters                         :story:
    CLOSED: [2017-09-01 Fri 11:55]
    CLOCK: [2017-09-01 Fri 11:48]--[2017-09-01 Fri 11:55] =>  0:07
    CLOCK: [2017-09-01 Fri 11:45]--[2017-09-01 Fri 11:47] =>  0:02
    CLOCK: [2017-09-01 Fri 11:29]--[2017-09-01 Fri 11:44] =>  0:15
    CLOCK: [2017-08-31 Thu 09:02]--[2017-08-31 Thu 09:06] =>  0:04

In the past we used the term "transformer" to mean a class that
converts types from one representation to another. However, now that
we are using domain terminology, the term "transforms" is taken to
mean a model transformation. To avoid confusion we should rename the
existing transformers to converters, adapters or some other
out-of-the-way name.

Affected models:

- quilt.cpp
- quilt.csharp
- yarn.dia

*** STARTED Analysis on profiles and concepts                         :story:
    CLOCK: [2017-09-01 Fri 15:25]--[2017-09-01 Fri 15:51] =>  0:26
    CLOCK: [2017-09-01 Fri 11:56]--[2017-09-01 Fri 13:56] =>  2:00

When concepts were introduced, it was clear that something was not
quite right on the naming. We used the C++ terminology because it
mapped well enough to the idea, but it was understood that we were
talking about two different things. As part of the clean-up required
for profiles, its time to revisit concepts.

A yarn concept is, really, an "object template". That is, it allows us
to create a template of a subset of the structure of a yarn object,
which can then be instantiated (pasted?) into actual objects. One
would like to avoid the use of the word "template", due to its C++
connotations, but sadly it seems there isn't a more appropriate word.

Unlike objects, object templates support multiple inheritance.

We consume the object templates via stereotypes.

Thus so far we should just rename concepts to object templates.

Annotations are a different beast altogether. We can broadly divide it
into three parts:

- *The annotations type system*. This is at present done by loading type
  templates. The right thing to do is to allow each model to create
  annotation types; these are then code-generated into a class which
  returns a list of type templates. Yarn needs to have a registrar for
  the type templates, which is populated during
  initialisation. Context factory talks to the registrar to obtain the
  type templates and initialises the type repository with it. The key
  thing is that the type system is static, and is more or less only a
  dogen concern (albeit users can define and consume their types, via
  the registrar). Thus we can quite easily solve the problems with the
  type system.
- *The templating machinery*. Annotations profiles are, on the main, a
  way to dynamically introduce annotation templates. These are
  "dynamic" because it is conceivable that each user will want to
  create its own set of annotation templates. There are two use cases:
  a) a common set of profiles, reused by several models (e.g. enable
  all facets, etc) b) a specific set of profiles useful only for one
  model (e.g. c++ artefact formatter). The profiling machinery
  requires a bit more thinking.
- *The configuration machinery*: the final piece of the puzzle is
  reading out data from annotations and using it in C++ code. For this
  we have three components: a) the type group classes, which aggregate
  the required annotation types b) the "configuration" classes, which
  are strongly typed representations of data stored in
  annotations; and c) a factory class responsible for using the type
  group class to populate the corresponding configuration class. In
  most simple cases, we could automate the generation of this triplet
  of classes.

The templating machinery is the most complex side of annotations then.
However, as it turns out, plain UML machinery can be used to handle
annotations profiles: UML profiles and UML stereotypes. Let's first
look into how we use stereotypes. We have:

- *Hard-coded static stereotypes*, which are in effect ways to map
  yarn meta-types into UML. For these we can simply hard-code the
  values and not worry about it. This is the right thing to do because
  the meta-types will not change often and when they do it will
  require a lot of manual work in yarn and in the frontends.
- *Dynamic stereotypes*. At present, this is how annotation profiles
  bind to element instances. We can make use of the labels in the
  annotation profile and refer to it in an element. During annotations
  transform, we expand the stereotype to the profile.

We can tackle this problem as follows:

- create a new element called =stereotype= (or perhaps
  =meta_stereotype=?). It has attributes which contain all of the
  required properties to create annotation profiles.
- create a new reference type called =profiles=. Problem: at present
  references are supplied by meta-data in target; however, we must
  process the profiles before reading in the target. The command line
  option solves this problem, at the cost of creating an inconsistency
  between references and profiles.
- update the exogenous model chain with a "profile" mode. In this
  case, the exogenous model can only contain =stereotypes=. No other
  UML stereotype is allowed. Create a context with an annotations
  group factory that loads no annotations profiles.
- create a transform that takes in the stereotypes and produces
  annotation profiles.
- create a second context based on the first one, but using a
  annotation group factory populated with all the annotation
  profiles. This will be the final context, used for all models.
- For each exogenous model: if the model contains stereotypes,
  transform them into annotation profiles before performing the
  annotations transform. This allows each model to supply its local
  stereotypes, visible only to the model. Only profile models have
  global stereotypes.

This would all be made easier if somehow it was possible to provide
annotation profiles externally to the context; this way we could
supply them globally or locally (ideally both), just before we do the
annotations transform. We need to look into changing the annotations
group factory API to cope with this.

*** Code-generate annotations type templates                          :story:

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it.
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** Investigate code-generation around annotations                    :story:

We have two cases where code-generation makes sense for
annotations. Let look at them in turn.

*Type templates*

At present we are supplying JSON files with type templates. In truth
these are not really "data files" because changing them will cause
problems to the system; its tightly coupled to them. It would make
more sense to allow models to define their type templates inside the
model itself. We could use a stereotype of
=annotations::type_template= and then use meta-data for all of the
fields, as per JSON, e.g.:

:  {
:    "name": {
:      "simple": "profile",
:      "qualified": "annotations.profile"
:    },
:    "archetype_location": {
:      "family": "annotations"
:    },
:    "value_type": "text",
:    "template_kind": "instance",
:    "scope": "any"
:  }

We then code-generate the insertion of the type template into the
annotations type templates repository via an initialisers-like
framework.

It may make more sense to have one UML class with all the type
templates; the type templates then become attributes of that
class. The problem then is what to name that class. also, we may want
to have a couple of these, to group type templates logically (for
example we want the top-level templates like =enabled= separated from
the namespace-specific templates).

But the gist of it is that its very straightforward to add some
machinery that generates the code required to inject the type
templates into the system, and that it is triggered during
initialisation, replacing JSON loading.

*Use of annotations*

We then have the following usage pattern:

- define a class with all the related fields (with types of the type
  templates above). We call this class =type_group=. We may need to
  instantiate it for specific fields, or by facet, etc. We need to
  look at all of the examples in the code-base. Note that the layout
  of this class will (likely) bear no resemblance to the type
  templates grouping - this is just a "bag" with all of the available
  type templates, whereas the type group aggregation does normally
  have some useful meaning (e.g. =orm_properties=, etc).
- define a "factory" class for the type group class that uses the
  traits to locate the types (instances of type templates). For this,
  the type group class attributes need to refer to the fully qualified
  field name (possibly requiring some inputs such as kernel, facet).
- define a c++ class with the properties we're interested in. We
  normally call this class =_configuration= if its just used to read
  the meta-data, or =_properties= if its used as a real type. Note
  that at present we have allowed the layout of the type group class
  and the properties/configuration classes to be possibly quite
  different; we gather _all_ of the types of interest in the type
  group class, but then have multiple properties/configuration classes
  to match our needs.
- finally, we define a "factory" class that takes in the type group
  and produces the configuration/properties class.

In a code-generated world:

- we need to somehow force the type group class to match the
  configuration class; this will probably result on a lot of
  duplication. For example, for the ORM properties, we probably have a
  couple in common across object/model/attributes.
- we need to map C++ types into annotation types such that we can back
  out the annotation type from a c++ type. For example, given an
  enumeration, we want to create a annotation type of "text" but then
  automatically generate the "from/to" converters for the enumeration.

Merged stories:

*Code generation of dynamic instances*

We seem to have a pretty well established usage pattern for dynamic,
so it may be a candidate for code generation. All we need is:

- a stereotype to mark a class as dynamic; the attributes of the class
  are dynamic fields, and their types must be one of the valid values
  for dynamic fields. The default value is used for the field's
  default value. Qualified name, ownership hierarchy, definition type,
  scope, etc are supplied as meta-data.
- stereotype name should be something like =DynamicFieldGroup=.
- the injection of the settings class is done by looking at the
  =DynamicFieldGroup= class and mapping the dynamic types to C++
  types. Note: this mapping should be dynamic too so that we can use
  it for other languages. We just need a meta-data tag for this, like
  we do with default enum value.
- the injection of the settings factory class is a bit more
  complicated; we need to mark the object as a settings factory. At
  present we have object types, but it was supposed to be removed
  after a refactoring. Actually we just need to create a new kind of
  element (=dynamic_settings_factory=?). In addition, settings factory
  may also need to take in some parameters such as facet/formatter.
- a stitch template that generates the settings factory.
- a stitch template that registers the dynamic field definition;
  instead of JSON we can just generate c++ code to perform the
  injection.
- we could also generate the repository and in most cases the
  repository factory. The only case where this breaks down is when we
  need to look at properties too.
- we should have a number of knobs to control generation: a) generate
  field injection b) generate settings factory c) generate repository
  d) generate repository factory.

We also need to merge the traits class directly into the factory. In
the majority of cases, we have traits just to access the fields. But
there are a few cases where we use traits for other purposes such as
formatter naming.

*** Use namespaced stereotypes                                        :story:

Originally we added a space in the ORM stereotypes:

: orm value

This is not a particularly good idea. We should just add support for
namespaced stereotypes:

: orm::value

We should also change all of the existing stereotypes to have a
namespace:

: modeling::object

And so forth. The namespace name probably needs a bit of thinking.

Actually, we should name all of the static stereotypes with a
namespace, and making it clear they are connected to yarn. Example:

: yarn::enumeration
: yarn::orm::value

and so forth.

*** Throw on unsupported stereotypes                                  :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

*** Add a modeline to stitch                                          :story:

It would be nice to be able to supply the mode and other emacs
properties to stitch templates. For that we just need a special KVP
used at the top that contains the modeline:

: <#@ modeline="-*- mode: poly-stitch; tab-width: 4; indent-tabs-mode: nil; -*-" #>

Stitch can read this KVP and ignore it.

*** Change order of includes according to Lakos major design rule     :story:

Lakos says:

#+begin_quote
The .c file of every component should include its own .h file as the
first substantive line of code.
#+end_quote

We decided to include it as the last line. However, Lakos approach has
the side-effect of automatically detecting headers that are missing
includes. We used to do this manually by generating =.cpp= files that
just included the header but then had to remove it because it was
slowing down compilation. With Lakos approach we get the best of both
worlds.

We need to also update the generated code to follow this
approach. This will require some thinking.

*** Investigate usage of origin type                                  :story:

With the current setup of the transforms, we always know who the
target model is. Thus the =origin_types= flag may not be used
correctly at present.

*** Rename ODB parameters                                             :story:

At present we use the following form:

: #DOGEN ODB_PRAGMA=no_id

We need to use the new naming style =cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

Finally we should no longer attempt to derive the ODB pragma
context. We should just add it verbatim.

*** Cannot make qualified references to concepts                      :story:

At present it is not possible to consume concepts defined in a
referenced model, nor is it possible to refer to a concept in a
different module from the module in which the element is in, e.g.: say
concept C0 is declared in module M0; all types of M0 can have C0 as
stereotype and that will resolve. However any types on any other
module cannot see the concept.

One suggestion is to allow scoped names in stereotypes:
=module::Concept=.

The heuristic for concept resolution is then:

- external modules are never part of the scoped name;
- on a scoped concept with M names, we first start by assuming that
  the first name is the model module and M-2 is/are the internal
  module(s). We try this for all names in M-2, e.g. first two names
  are model modules and M-3 names are internal modules and so forth.

*** Add support for cross-model concept refinement                    :story:

We've implemented support for cross-model inheritance in sprint 87 but
we did not cover concepts. Most of the approach is the same, but
unfortunately we can't just reuse it.

Tasks:

- we need a refines field which is a text collection.
- we need refinement settings, factory etc.
- update parsing expander.

*** Add support for "one off" profiles                                :story:

At present one can define top-level profiles. These are useful, but in
practice we ended up still defining a lot of things in each model. We
need a way to associate a profile with a model by supplying it on the
command line. That way users can create profiles and store them next
to the model rather than having to create a data directory, etc etc.

Actually the problem is that profiles aren't really implemented
correctly. First we should not call them profiles at all since they
are not UML profiles and overloading the term just generates
confusion. Second its important to understand how Dogen profiles come
about:

- we extend the UML meta-model via stereotypes to support all of the
  required yarn and quilt concepts.
- when we instantiate the yarn/quilt types via a UML model, we need to
  supply the values for the attributes which have been extended. If
  done properly this would happen via UML tagged values. Dia does not
  support these. At any rate, at present we use Dogen meta-data which
  is almost like tagged values.
- Dogen profiles are then an attempt to create bundles of tagged
  values with pre-populated values so that we do not need to manually
  populate them for every type. Instead, we can associate a stereotype
  with the type and then the system will automatically populate the
  values from the bundle.
- From all of this it follows that it should be possible to define
  these "bundles" directly in a UML diagram. If we were to use UML
  properly (or at least almost properly), we would define a class with
  a stereotype of =stereotype=, a name of the stereotype we'd like to
  define (say =Serializable=) and then its tagged values are the keys
  and values of the meta-data we want to define. This is strictly
  speaking not correct UML because we are stating we are augmenting
  the UML meta-model (hence =stereotype=) but then we end up
  instantiating a meta-model class with some predefined values. Its
  not clear how to express this in UML. Note that we have exactly the
  same issue with concepts.
- and, after some thinking, we are trying to do exactly the same thing
  as we are already doing for concepts: i.e. some kind of meta-level
  operation that allows us to add structural features to an
  element. Thus we can just use concepts, which are not even defined
  in UML - augmenting its meaning will not take us away from the
  literature. We can very simply add a last step to concepts transform
  which merges the annotations of the concept objects, using exactly
  the machinery we defined for profiles. The only slight problem is
  that we cannot reuse concepts across models.

Tasks:

- add annotations merging to concepts processing. Should cause no
  changes at all on all models.
- create a model in dogen defining basic concepts.

Links:

- [[https://msdn.microsoft.com/en-us/library/dd465146.aspx][Standard stereotypes for UML models]]

*** Add canonical archetype support to yarn                           :story:

We need to add a new attribute in context which captures the canonical
archetypes.

Notes:

- kernel must also return canonical archetype by element type
  index. Perhaps we should have a struct that aggregates both:
  archetype locations for meta-type? Or kernel can just return a
  =std::pair=.
- at present we have placed the canonical archetype resolution as part
  of the element properties. However, we do not need to have this at
  the element level since its a meta-type property and can be
  determined up-front. However, we do need to resolve a name into a
  meta-type before we can resolve a meta-type into a concrete
  archetype.
- we need to unpick the notion of whether a formatter is "includible"
  or not from the notion of canonical archetypes. Canonical archetypes
  is meta-model concept: given a facet and a meta-model type, which
  archetype represents the "key" definition of the element. It just so
  happens that this function has a use in identifying the files to
  include.

Tasks:

- add a map from name id to meta-name id in intermediate model.
- add a map from meta name id to map of canonical archetype to
  archetype location.

*** Move enablement into yarn                                         :story:
It seems that the concepts around enablement are actually not kernel
specific but instead can be generalised at the meta-model level. We
need to create adequate representations in yarn to handle facets,
etc. We then need to move across the code that computes enablement
into yarn so that all kernels can make use of it.

Problems:

- we are checking to see if the hash facet is enabled with c++ 98; if
  so, we throw as this facet is incompatible. We cannot do this from
  yarn since we do not know what c++ standards are.
- because we do not have a mapping between a archetype location and
  the meta-type, we will be enabling/disabling all archetype locations
  across all meta-types.
- because we do not have element segmentation, the element extensions
  will be disabled. Actually this will probably work just the same,
  given that all elements exist.
- enablement must be done after external transformations so it picks
  up fabric types.
- we need to support formatting styles in order to be able to use the
  artefact properties from the meta-model.
- in quilt.cpp, someone did an upfront generation of all archetype
  properties against the archetype locations. We not doing that in
  yarn, so nothing is coming out. This was done during transformation
  in formattables.
- with a move into yarn, we seem to have broken the overwrite flag
  logic; changes no longer result in new code being generated.
- we also have borked the includes: dependency builder is looking into
  the formattables instead of element. However, we then run into
  segmentation issues because we cannot find forward declarations on
  the main element.

To do:

- kernel registrar type index map - done.
- c# formatter registrar type index map - done.
- bug in template instantiating: artefact expansions do not seem to
  take kernel into account - done.

*Previous Understanding*

We need to make use of the exact same logic as implemented in
=quilt.cpp= for enablement. Perhaps all of the enablement related
functionality can be lifted and grafted onto quilt without any major
changes.

*** Move formatting styles into yarn                                  :story:

We need to support the formatting styles at the meta-model level.

*** Move element segmentation into yarn                               :story:

We've added the notion that an element can be composed of other
elements in quilt, in order to handle forward declarations. However,
with a little bit of effort we can generalise it into yarn. It would
be useful for other things such as inner classes. We don't need to
actually implement inner classes right now but we should make sure the
moving of this feature into yarn is compatible with it.

Notes:

- seems like we have two use cases: a) we need all elements, master
  and extensions and we don't really care about which is which. b) we
  only want masters. However, we must be able to access the same
  element properties from either the master or the extension. Having
  said all that, it seems we don't really need all of the element
  properties for both - forward declarations probably only need:
  decoration and artefact properties.
- we don't seem to use the map in formattables model anywhere, other
  than to find master/extension elements.
- Yarn model could have two simple list containers (masters and
  all). Or maybe we don't even need this to start off with, we can
  just iterate and skip extensions where required.
- so in conclusion, we to move decoration, enablement and dependencies
  into yarn (basically decoration and artefact properties) first and
  then see where segmentation ends.

Tasks:

- add a concept for element extensions: =Extensible=. Contains a list
  of element pointers.
- populate it with the extensions.
- change enablement to merge all element properties of extensible
  elements.

*** Create a yarn locator                                             :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator. It will live in fabric.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

*** Move dependencies into yarn                                       :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Generate file paths as a transform                                :story:

Add a fabric transform for file path generation.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

*** Move helpers into yarn                                            :story:

Looking at helpers, it is clear that they are common to all
languages. We just need to rename the terminology slightly -
particularly wrt to streaming properties - and then move this code
across into yarn.

*** Move facet properties into yarn                                   :story:

We should be able to handle these generically in yarn.

*** Move ORM camel-case and databases into yarn                       :story:

We should handle this property at the ORM level, rather than at the
ODB level.

Similarly, we should move the ODB databases into yarn and make that a
ORM-level concept.

*** Distinguish between meta-types that require canonical archetypes  :story:

At present it is not possible to know which meta-types require
canonical archetypes and which don't. In the validation we said:

:         * We must have one canonical formatter per type per facet.
:         * FIXME: this check is broken at the moment because this is
:         * only applicable to yarn types, not fabric types. It is also
:         * not applicable to forward declarations. We need some
:         * additional information from yarn to be able to figure out
:         * which types must have a canonical archetype.

We should have some kind of flag in yarn to distinguish. This still
requires a bit of thinking.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

** Deprecated
*** CANCELLED Make the Zeta model compilable                          :story:
    CLOSED: [2017-08-30 Wed 23:01]

*Rationale*: not required since Upsilon has been deprecated.

We need to work through the list of issues with the Zeta model and get
it to a compilable state.

*** CANCELLED Registrar in quilt is not being generated               :story:
    CLOSED: [2017-08-30 Wed 23:14]

*Rationale*: quilt model has been deleted.

We don't seem to change the contents of this file when regenerating.

*** CANCELLED Stitcher log file names look weird                      :story:
    CLOSED: [2017-08-30 Wed 23:19]

*Rationale*: they look ok with the current release.

At present we are writing files with names like:

: dogen.stitcher...log

*** CANCELLED ODB options file is generated to incorrect location     :story:
    CLOSED: [2017-08-30 Wed 23:21]

*Rationale*: ODB options generation changed dramatically recently (one
per type, etc).

Models with composite names seem to have their ODB options file
generated under the =projects= directory, e.g.:

: projects/vtk/geometry/src/options.odb
