#+title: Sprint Backlog 19
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Incorporate the relational model into Dogen.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-11-21 Thu 00:16]
| <75>                                   |        |      |      |       |
| Headline                               | Time   |      |      |     % |
|----------------------------------------+--------+------+------+-------|
| *Total time*                           | *4:57* |      |      | 100.0 |
|----------------------------------------+--------+------+------+-------|
| Stories                                | 4:57   |      |      | 100.0 |
| Active                                 |        | 4:57 |      | 100.0 |
| Edit release notes for previous sprint |        |      | 4:26 |  89.6 |
| Sprint and product backlog grooming    |        |      | 0:31 |  10.4 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** STARTED Edit release notes for previous sprint                    :story:
    :LOGBOOK:
    CLOCK: [2019-11-21 Thu 00:16]--[2019-11-21 Thu 00:30] =>  0:14
    CLOCK: [2019-11-20 Wed 23:00]--[2019-11-21 Thu 00:15] =>  1:15
    CLOCK: [2019-11-20 Wed 18:49]--[2019-11-20 Wed 19:47] =>  0:58
    CLOCK: [2019-11-19 Tue 17:41]--[2019-11-19 Tue 18:28] =>  0:47
    CLOCK: [2019-11-19 Tue 07:45]--[2019-11-19 Tue 08:28] =>  0:43
    CLOCK: [2019-11-18 Mon 17:30]--[2019-11-18 Mon 18:13] =>  0:43
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.19, "Impala Cine"

#+begin_src markdown
![Imapala Cine](https://cdn.record.pt/images/2019-02/img_920x519$2019_02_11_02_32_57_1503852.jpg)

_The Impala Cine, Moçâmedes, Namibe, Angola. (C) 2019 [Jornal O Record](https://www.record.pt/modalidades/aventura/detalhe/mocamedes---a-terra-do-faria)_

# Introduction

This sprint was marked by the return to "part-time" development on  Dogen. After a cadence of eight successful 2-week sprints, it was rather difficult to adjust back to the long, drawn-out process of cobbling together a release from whatever spare time one can find. To recap, the target for a "part-time" sprint is to clock around 80 hours worth of work over a rather unpredictable period of time. Most of Dogen has been developed in this fashion but, clearly, it is not ideal fodder for programming.

Part-time sprints naturally lend themselves to more fragmentary work, given the typically short duration of the time slots available, and make it much harder to focus on complex tasks that require loading a lot of state into the brain - a process we've described at length in [Nerd Food: Dogen: Lessons in Incremental Coding](https://mcraveiro.blogspot.com/2014/09/nerd-food-dogen-lessons-in-incremental.html). Having said that, not all was gloom and doom with Sprint 19, and much was achieved - both invisible and visible to end-users. Furthermore, efficiency went markedly up after adding a new tool to our arsenal of "motivational tools": the recording coding sessions.

So, brace yourselves for a review of yet another exciting Dogen sprint.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing the features in more detail.

[![Sprint 1.0.19 Demo](https://img.youtube.com/vi/TkYQTW_jAGk/0.jpg)](https://youtu.be/TkYQTW_jAGk)

## Add support for variability overrides in Dogen

The sprint's key feature is _variability overrides_. It was specifically designed to allow for the overriding of model profiles. In order to understand how the feature came about, we need to revisit a bit of Dogen history. As you may recall, since its early days, Dogen has enabled users to supply meta-data to determine  what source code gets generated for each modeling element - _i.e._, model element _A_ can have say hashing support whereas model element _B_ can have say lexical casting support. These are simple toggles that switch facets on and off.

Soon we figured out that the toggle switches could be organised into "configuration" bundles -  an idea which eventually evolved into the present notion of "profiles". Profiles are named configurations which provide a defaulting mechanism for individual configurations so that they could be reused across modeling elements and, eventually, across models. That is to say, profiles stem from the very simple observation that the meta-data used for configuration is, in many cases, common to several models and therefore should be shared. In the [MDE](https://en.wikipedia.org/wiki/Model-driven_engineering) domain, these ideas have been generalised into "variability modeling", because, taken as a whole, they give you a dimension in which you can "vary" how any given modeling element is expressed - hence they are known in Dogen as "variability modeling".

![Dogen's profile model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/profiles_model.png)
_Figure 1: Snippet from "dogen.profiles" model._

Of course, like all variability information carried in Dogen models, profiles are _themselves_ associated to models via nothing but plain old meta-data - that is, its just configuration too . A typical Dogen model contains an entry like so:

```
#DOGEN masd.variability.profile=dogen.profiles.base.default_profile
```

The ```masd.variability.profile``` tells Dogen to reuse the configuration defined by the profile called ```default_profile``` - an entitty in the referenced model ```dogen.profiles``` (_c.f._ Figure 1).

This approach has served us well thus far, but it carried an implicit assumption: that models are associated with  _only one profile_. As always, reality turned out to be messier than this simplistic vision. After some thinking, we realised that we have two distinct and conflicting requirements when generating Dogen's own models:

- **parsimony**: from a production perspective, we want to generate the smallest amount of code required so that we avoid bloating our binaries with unnecessary kruft. Thus we want our profiles to be lean and mean and our builds to be fast.
- **coverage**: from a development and Q&A perspective, we want to test all possible facets with realistic use cases so that we can validate empirically the quality of the generated code. Dogen's own models are a great sample point for this validation, and should therefore make use of as many facets as possible. In this scenario, we don't mind slow builds and big binaries if it means a higher probability of generating correct code.

This problem was not entirely obvious at the start because we could afford to generate _all_ facets for _all_ models and just ignore the bloat. However, as the number of facets increased and as the number of elements in each model grew, we eventually started to ran out of build time to compile all of the generated code. We had experienced this problem in the past, leading us to separate the reference models for [C#](https://github.com/MASD-Project/csharp_ref_impl) and [C++](https://github.com/MASD-Project/cpp_ref_impl) from the core Dogen product. But now the problem was happening to Dogen _itself_, and there is nothing left to offload. Interestingly, I do not blame the "short" build times offered by the CI systems; instead, I see it as a feature, and not a bug, because the limited build time has forced us to consider very carefully the impact of growth in our code base.

At any rate, as in the past with the reference models, we limped along yet again for a number of sprints, and resorted to "clever" hacks to allow these two conflicting requirements to coexist, such as enabling only a few facets in certain models. As a result, the CI was becoming less and less useful because you started to increasingly ignore build statuses. Not being able to trust your CI is a showstopper, of course, so this sprint we finally sat down to solve this problem in a somewhat general manner. We decided to have two separate builds, one for each use case: Nightlies for the coverage, since it runs over night and no one is waiting for them, and CI for the regular production case. And as you probably guessed by now, we needed a way to have a comprehensive profile for Nightlies that generates [everything but the kitchen sink](https://knowyourphrase.com/everything-but-the-kitchen-sink) whereas for regular CI we wanted to create the aforementioned lean and mean profiles.

Variability overrides was the chosen solution, and it was implemented as follows. A new command line option was added to the Processing section, named ```--variability-override ```. The option expects a tuple of up to five elements:

```[MODEL_ID,][ELEMENT_ID,][ATTRIBUTE_ID,]KEY,VALUE```

The first three optional elements are used to bind to the target of the override. The binding logic is somewhat contrived:

1. if no model is supplied, the override applies to _any_ model, else it applies to the requested model;
2. if no element is supplied, the override is applicable only to the model itself;
3. if an element is supplied, the binding applies to that specific element;
4. an attribute can only be supplied if an element is supplied. The binding will only activate if it finds a matching element and a matching attribute.

To be fair, given our use case, we only really needed the first type of binding; but since we didn't want to hard-code the functionality, we came up with the simplest possible generalisation we can think of and implemented it. There are no use cases for overrides outside of profiles, so this implementation is as good as any; as soon as we have use cases, the rules can be refined. Dogen uses this new command line option like so:

```
    if (WITH_FULL_GENERATION)
        set(DOGEN_PROCESSING_OPTIONS ${DOGEN_DIFFING_OPTIONS}
            --variability-override masd.variability.profile,dogen.profiles.base.test_all_facets)
    endif()
```
By supplying ```WITH_FULL_GENERATION``` for the nightlies, we generate all facets and tests for all facets, most of which are passing at present.

![CDash](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_builds.png)
_Figure 2: Continuous and nightly builds in CDash after the change._


Generate model dependency graph

Tracer numbering of dumped models is incorrect


Creating reference cycles produces strange errors
Error on duplicate references


Remove master headers
Meta-data keys are in the inverse order


# Development Matters

## Significant Internal Stories

## Resourcing

## Planning

# Next Sprint

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.19_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.19/dogen_1.0.19_amd64-applications.deb)
- [dogen-1.0.19-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.19/DOGEN-1.0.19-Darwin-x86_64.dmg)
- [dogen-1.0.19-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.19-Windows-AMD64.msi)

**Note:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+end_src markdown

- [[https://twitter.com/MarcoCraveiro/status/1135567734010523648][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6541333935140458497][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** Create a demo and presentation for previous sprint                :story:

Time spent creating the demo and presentation.

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2019-11-18 Mon 08:51]--[2019-11-18 Mon 08:58] =>  0:07
    CLOCK: [2019-11-18 Mon 08:41]--[2019-11-18 Mon 08:50] =>  0:09
    CLOCK: [2019-11-18 Mon 08:38]--[2019-11-18 Mon 08:40] =>  0:02
    CLOCK: [2019-11-18 Mon 08:24]--[2019-11-18 Mon 08:37] =>  0:13
    :END:

Updates to sprint and product backlog.

*** Add relational tracing support                                    :story:

Whenever we bump into a problem we seem to spend a lot of time going
through the log files and trace files trying to figure out where the
problem is happening. Have a quick go in trying to implement a
relational model for tracing to see if we can transfer the bulk of the
data into a relational format which we can query via SQL.

We've created a basic relational model for tracing. The relational
part of it seems straightforward (ish); the problem is the integration
of the tracer with the relational model. At present we rely on the
fact that all traceable objects have IO enabled; this works because
the code generator creates the IO facet, which is then used by the
write method in utility to convert any model type into a
string. However, we now need to change the approach: we need multiple
tracing backends:

- file tracer
- database tracer.

The file tracer is more or less the current tracer. The database
tracer needs to decompose the objects in existing models into a
relational representation. In an ideal world, the user would configure
the tracer to use one of the two backends and the remaining usage
would be transparent. However, we cannot have an interface for the
tracer backend that uses template methods because then we'd need
virtual template functions, it seems.

Another alternative is to make the tracer aware of the model objects
it is tracing. This is also not ideal because we would create cycles
in the design.

In effect we need to somehow implement a similar approach to the existing
tracer: rely on global template functions a-la =operator<<= to
decompose objects into their relational representations and then
supply those to the backend. It is not very clear how this would
work. For now we've postponed this approach as it seems its not going
to be a quick win.

We should approach this incrementally. Next time we have a bit of
spare time, we need to generate the model and then create the adapters
from existing models. Finally we can look at how it will be integrated
with tracing.

Notes:

- compilation generates an ODB error:

: FATALODB include directories not defined.

- the key difference between northwind and tracing is that we have a
  namespace. The application of the schema pragma is probably not
  working due to this. We need to look into the transform to see how
  that pragma propagates.
- the problem arises because we are only populating the primitive's
  properties if there is a top-level pragma. As the schema is not
  populated for the namespace, there isn't one. It is not clear why
  one would want to skip properties such as DB member if there isn't a
  schema, but perhaps this is due to some ODB error. We should
  probably issue an error or warning if we cannot generate code
  without a schema name.
- with regards to the relational model, the problem is that we can't
  really create a schema for each namespace in a model because schemas
  are not really like namespaces. The entities in a schema should
  really be self-contained and not refer to other schemas or else the
  database will be confusing to use. For example in postgres we will
  need to set the schema path, etc in order to see the different
  tables. One possible solution is to set the schema name to the same
  value for all namespaces (e.g. =dogen=). This would then allow us to
  have namespaces in C++ but not in the database.
- it seems foreign keys are not supported at present. We probably need
  support for this in order to query quickly or else we will have to
  manually setup indices for each of these joining fields.
- we need a command line option to choose the tracing backend
  (e.g. file or database). We also need the database configuration
  parameters: hostname, port, database, user.
- we need to refactor tracer as follows:
  - update the tracer interface to take actual types rather than
    templates.
  - create a top-level interface for the notion of a backend.
  - create two implementations of the backend: file and relational.
  - move all the file related code to the file backend.
  - implement adapters for each model to convert them into relational
    model types.
  - implement the relational backend.
- Add relational model to the dogen model tests.

Merged stories:

*Scripts for loading traces into postgres*

- rationale: this story is superseded by having a relational model.

It would be really nice if as part of the tracing generation we also
generated a set of SQL scripts that:

- created a number of tables
- copied all of the generated data into the database
- added a number of utility functions such as get elements in model, etc.

Over time we could build up functionality but to start off with we
just want something really simple that copies all of the
files. Interestingly this "looks" like a job for dogen. It would be
nice to have a meta-model element for this etc.

In the future it would be nice to have a think about the schema so
that we could do joins etc. For example:

- show me all transforms with element of type X (the state of the
  element at each transform).

We should also take into account multiple runs. Perhaps a more
adequate solution is to create a dogen library that has the ORM
support for this. Once we have proper JSON serialisation we can store
the objects as JSON serialisable, allowing us to re-run transforms,
etc.

Notes:

- ensure we upload the file name or at least the coordinates to the
  transform graph with the data so that we know what it refers to.
- rename relational database enum to just database
- rename hostname to just host

*Improved understanding*

Better than uploading a whole load of JSON blobs and then having to do
a number of really complex queries, is to have a ORM schema that is
designed to capture the data in the format we're interested in. Then
we could do very simple queries. What we really care about is
capturing all attributes of the model as it changes across the
transformations. We also care about the relationships between
transformations. We also need a way to uniquely identify elements
across their entire lifecycle. A simple way would be to create a hash
of the file name of the model, column and line number. We can then
associate other IDs to this one such as dia ID, etc.

We need to create a set of adaptors that convert an existing model
(injection, coding, etc) into the ORM model and then write the ORM
model into the database. The ORM model does not need as much detail
and structure as a regular model; for example, names can be flattened
or linked into IDs (e.g. name table), etc. Whatever makes sense from a
relational perspective.

It would also be nice to dump the log into the database so that we
could do simple correlations such as "what was logged between the
start and end of this transform?"

Interestingly, this would also allow us to compare things between
runs. The schema should be designed with this in mind.

*** Move registrar into assets                                        :story:

Move the registrar type into assets, in the quickest way possible.

Notes:

- In order to avoid blocking due to lots of analysis, we need
  to split this story into three:
  - first, we need to just move the registrar as is into assets.
  - a second story is to clean up the existing registrar code to have
    less templates and possibly address the existing registration
    bugs. We could also look into calling the registrars for
    referenced models automatically as part of this work (at present
    we are doing this manually).
  - finally, we need some meta-level refactoring to figure out if the
    pattern can be generalised to include initialisers, etc.
  In general that should be our approach: try to split out the
  capturing of patterns into as many steps as possible, to make sure
  we don't get overwhelmed as we implement things.
- we need to keep track of all type registrars on referenced models,
  not on the referenced models themselves. We need to know which
  models we referenced directly, and then find the registrars for
  those models.
- leaves need to know of the registrar. This is so that we can call it
  in their generated tests. We could use the registrar transform to go
  and find all leaves and populate their registrar name.
- current state is that we cannot generate the registrar for some
  reason.
- test model with registrar is C++ model. Type is called
  registrar. Its probably not a good idea to also call it registrar -
  wouldn't that clash with the existing type?
- we should have a warning/error: if using boost serialisation with a
  model that has inheritance, the registrar should be present. Added
  to warnings story.

*** Setup laptop to work on dogen                                     :story:

We haven't used the laptop for dogen for quite a bit so its behind the
main machine. Get it in a shape to do development again.

Items missing:

- dir locals for projects
- polymode
- build2
- odb

*** Generate ORM tests                                                :story:

We do not seem to be testing the generated ODB code. We don't need to
test ODB per se, but we should at least have some sanity checks that
test CRUD functionality.

Notes:

- for this we need a "masd database".
- tests should only trigger if postgres or some other relational
  database is detected.
- if foreign keys are used we need to detect them and ensure we
  populate the data accordingly.

*** Schema name in ORM should be transitive                           :story:

At present when we define the schema name on a top-level namespace, we
don't "inherit" it from child namespaces. The problem is compounded by
the fact that we need the schema name in order to output ODB pragmas
(separate bug). It seems more logical to propagate the schema name to
child namespaces.

*** ODB pragmas not populated when schema name is not set             :story:

At present we have a bug whereby not setting the schema name results
in not having most ODB pragmas set. We should always populate them
even if the schema name is not set. To be precise, the problem is not
directly related to the schema name - we just require some ORM
property to be set. AS it happens, it normally tends to be the schema
name, because it makes sense to set it when defining a relational
model. This is why we never bumped into this problem before.

*** Make =scoped_tracer= header only                                  :story:

At present we are generating the cpp for this file for no reason, use
the correct profile for header only.

** Deprecated
