#+title: Sprint Backlog 19
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Incorporate the relational model into Dogen.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-11-21 Thu 21:03]
| <75>                                   |        |      |      |       |
| Headline                               | Time   |      |      |     % |
|----------------------------------------+--------+------+------+-------|
| *Total time*                           | *7:37* |      |      | 100.0 |
|----------------------------------------+--------+------+------+-------|
| Stories                                | 7:37   |      |      | 100.0 |
| Active                                 |        | 7:37 |      | 100.0 |
| Edit release notes for previous sprint |        |      | 7:06 |  93.2 |
| Sprint and product backlog grooming    |        |      | 0:31 |   6.8 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** STARTED Edit release notes for previous sprint                    :story:
    :LOGBOOK:
    CLOCK: [2019-11-21 Thu 19:52]--[2019-11-21 Thu 20:57] =>  1:05
    CLOCK: [2019-11-21 Thu 18:30]--[2019-11-21 Thu 19:51] =>  1:21
    CLOCK: [2019-11-21 Thu 00:16]--[2019-11-21 Thu 00:30] =>  0:14
    CLOCK: [2019-11-20 Wed 23:00]--[2019-11-21 Thu 00:15] =>  1:15
    CLOCK: [2019-11-20 Wed 18:49]--[2019-11-20 Wed 19:47] =>  0:58
    CLOCK: [2019-11-19 Tue 17:41]--[2019-11-19 Tue 18:28] =>  0:47
    CLOCK: [2019-11-19 Tue 07:45]--[2019-11-19 Tue 08:28] =>  0:43
    CLOCK: [2019-11-18 Mon 17:30]--[2019-11-18 Mon 18:13] =>  0:43
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.19, "Impala Cine"

#+BEGIN_SRC markdown
![Imapala Cine](https://cdn.record.pt/images/2019-02/img_920x519$2019_02_11_02_32_57_1503852.jpg)

_The open air cinema Impala Cine, in the city of Moçâmedes, Namibe, Angola. (C) 2019 [Jornal O Record](https://www.record.pt/modalidades/aventura/detalhe/mocamedes---a-terra-do-faria)_

# Introduction

Whilst a long time in coming due to our move back into gainful employment, Sprint 19 still managed to pack a punch both in terms of commitment as well as exciting new features. To be fair, we didn't really plan to add _any_ of these features beforehand - instead, we found ourselves having to so in order to progress the real work we _should_ have been focusing on. Alas, nothing ever changes in the life and times of a software developer.

But lets not dilly-dally. Without further ado, brace yourselves for a review of yet another roller-coaster of a Dogen sprint!

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing the features in more detail.

[![Sprint 1.0.19 Demo](https://img.youtube.com/vi/TkYQTW_jAGk/0.jpg)](https://youtu.be/TkYQTW_jAGk)

## Add support for variability overrides in Dogen

The sprint's key feature is _variability overrides_. It was specifically designed to allow for the overriding of model profiles. In order to understand how the feature came about, we need to revisit a fair bit of Dogen history. As you may recall, since early on, Dogen has enabled users to supply meta-data to determine  what source code gets generated for each modeling element. By toggling different meta-data switches, we can express quite differently two otherwise identical model elements: say, one can generate hashing support whereas the other can generate serialisation.

Observing its usage, we soon realised that the toggle switches provided more value when organised into "configuration sets" that modeling elements could _bind_ against, and this idea eventually morphed into the present concept of _profiles_. Profiles are named configurations which provide a defaulting mechanism for individual configurations so that they could be reused across modeling elements and, eventually, across models. That is to say, profiles stem from the very simple observation that the meta-data used for configuration is, in many cases, common to several models and therefore should be shared. In the [MDE](https://en.wikipedia.org/wiki/Model-driven_engineering) domain, these ideas have been generalised into the field of _Variability Modeling_, because, taken as a whole, they give you a dimension in which you can "vary" how any given modeling element is expressed; hence why they are also known in Dogen as "variability modeling", as we intend to be as close as possible to domain terminology.

![Dogen's profile model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/profiles_model.png)
_Figure 1: Snippet from ```dogen.profiles.dia``` model._

Of course, like all variability information carried in Dogen models, profiles are _themselves_ associated to models via nothing but plain old meta-data - that is, its just configuration too . A typical Dogen model contains an entry like so:

```
#DOGEN masd.variability.profile=dogen.profiles.base.default_profile
```

The ```masd.variability.profile``` tells Dogen to reuse the configuration defined by the profile called ```default_profile``` - an entitty in the referenced model ```dogen.profiles``` (_c.f._ Figure 1).

This approach has served us well thus far, but it carried an implicit assumption: that models are associated with  _only one profile_. As always, reality turned out to be far messier than our simplistic views. After some thinking, we realised that we have not one but _two_ distinct and conflicting requirements for the generation of Dogen's own models:

- **parsimony**: from a production perspective, we want to generate the smallest amount of code required so that we avoid bloating our binaries with unnecessary kruft. Thus we want our profiles to be lean and mean and our builds to be fast.
- **coverage**: from a development and Q&A perspective, we want to test all possible facets with realistic use cases so that we can validate empirically the quality of the generated code. Dogen's own models are a great sample point for this validation, and should therefore make use of as many facets as possible. In this scenario, we don't mind slow builds and big binaries if it means a higher probability of detecting incorrect code.

This problem was not entirely obvious at the start because we could afford to generate _all_ facets for _all_ models and just ignore the bloat. However, as the number of facets increased and as the number of elements in each model grew, we eventually started to ran out of build time to compile all of the generated code. We had experienced this problem in the past, leading us to separate the reference models for [C#](https://github.com/MASD-Project/csharp_ref_impl) and [C++](https://github.com/MASD-Project/cpp_ref_impl) from the core Dogen product. But now the problem was happening to Dogen _itself_, and there is nothing left to offload. Interestingly, I do not blame the "short" build times offered by the free CI systems; instead, I see it as a feature, not a bug, because the limited build time has forced us to consider very carefully the impact of growth in our code base.

At any rate, as in the past with the reference models, we limped along yet again for a number of sprints, and resorted to "clever" hacks to allow these two conflicting requirements to coexist, such as enabling only a few facets in certain models. As a result, the CI was becoming less and less useful because you started to increasingly ignore build statuses. Not being able to trust your CI is a showstopper, of course, so this sprint we finally sat down to solve this problem in a somewhat general manner. We decided to have two separate builds, one for each use case: Nightlies for the coverage, since it runs over night and no one is waiting for them, and CI for the regular production case. And as you probably guessed by now, we needed a way to have a comprehensive profile for Nightlies that generates [everything but the kitchen sink](https://knowyourphrase.com/everything-but-the-kitchen-sink) whereas for regular CI we wanted to create the aforementioned lean and mean profiles.

Variability overrides was the chosen solution, and it was implemented as follows. A new command line option was added to the Processing section, named ```--variability-override```:

```
Processing:
<snip>
  --variability-override arg     CSV string with a variability override. Must
                                 have the form of [MODEL_NAME,][ELEMENT_NAME,][ATT
                                 RIBUTE_NAME,]KEY,VALUE
```

The first three optional elements are used to bind to the target of the override (_e.g._, ```[MODEL_ID,][ELEMENT_ID,][ATTRIBUTE_ID,]```). The binding logic is somewhat contrived:

1. if no model is supplied, the override applies to _any_ model, else it applies to the requested model;
2. if no element is supplied, the override is applicable only to the model itself;
3. if an element is supplied, the binding applies to that specific element;
4. an attribute can only be supplied if an element is supplied. The binding will only activate if it finds a matching element and a matching attribute.

To be honest, given our use case, we only really needed the first type of binding; but since we didn't want to hard-code the functionality, we came up with the simplest possible generalisation we can think of and implemented it. There are no use cases for overrides outside of profiles, so this implementation is as good as any; as soon as we have use cases, the rules can be refined.

Dogen uses this new command line option like so:

```
    if (WITH_FULL_GENERATION)
        set(DOGEN_PROCESSING_OPTIONS ${DOGEN_DIFFING_OPTIONS}
            --variability-override masd.variability.profile,dogen.profiles.base.test_all_facets)
    endif()
```
By supplying ```WITH_FULL_GENERATION``` to the nightlies CMake, we then generate all facets and tests for all facets. We then build and run all of the generated code, including generated tests. Surprisingly, we did not have many issues with most generated code - with a few exceptions, which we had to ignore for now. There are also two failures which require investigation and shall be looked into next sprint.

![CDash](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/cdash_builds.png)
_Figure 2: Continuous and nightly builds in CDash after the change._

## Tracing of model dependencies

The second feature implemented this sprint is the addition of model references tracing. This work was done in the same vein as the transforms tracing (See [Sprint 12](https://github.com/MASD-Project/dogen/releases/tag/v1.0.12) for details) and reused much of the same infrastructure; you'll get the new tracing reports for free when you enable tracing via the existing flags. As an example, Dogen uses the following configuration when we require tracing:

 ```
--tracing-enabled --tracing-level detail --tracing-format org-mode --tracing-guids-enabled
```
Like with transforms, we can generate three different types of tracing reports depending on the choice of ```--tracing-format```:  ```plain```, ```org-mode``` and ```graphviz```. ```plain``` is just a text mode representation of the references graph:

![Dogen's profile model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/plain_references_graph.png)
_Figure 3: References graph in ```plain``` format._

The ```org-mode``` version offers the usual interactivity available to org-mode diagrams in Emacs such as folding, unfolding, querying and so on:

![Dogen's profile model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/org_mode_references_graph.png)
_Figure 4: References graph in ```org-mode``` format._

Finally, [as before](https://github.com/MASD-Project/dogen/releases/tag/v1.0.12), the ```graphviz``` output requires further processing with the dot tool before it can be visualised:

```
dot -Tpdf references_graph.dot -O
```
The resulting PDF can be opened with any PDF viewer. We find it very useful because it gives a clear indication of the "complexity" of a given model. Of course, at some point in the future, we will want to convert these visual "complexity" indicators into metrics that can be used to determine the "health" of a model, but, as always, there are just not enough hours in the day to implement all these cool features.

![Dogen's profile model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/graphviz_references_graph.png)
_Figure 4: References graph in ```graphviz``` format, after processing with ```dot``` tool._

## Small bug fixes

Several small but important bug fixes went in with this release:

- **Meta-data keys are processed in the inverse order**: A very old but rather annoying bug we had in Dogen is that meta-data keys were being processed in _reverse_ order of entry. For example, if a model _A_ referenced models _B_ and _C_, for some unfathomable reason, would process it as _C_ and _B_. This resulted in a great deal of confusion when troubleshooting because we assumed all references in log files _etc._ would first start with _B_, not _C_. This release fixes the bug, but as a result, a lot of the generated code will move places. It should be semantically equivalent, just with a different order.
- **Tracer numbering of dumped models is incorrect**: Our traces for some reason were skipping numbers (e.g. ```000``` then ```002```, and so forth. This was very distracting when trying to analyse a problem. In addition, the previous logic of numbering the traces after a transform was abandoned; instead of having ```000``` for both the input and output of a transform, we now have ```000``` and ```001```. It was a nice thought but required a lot of complexity to implement.
- **Creating reference cycles produces strange errors**: In the past, adding a reference cycle in a model resulted in very puzzling errors, entirely unconnected to the problem at hand. With this release we now correctly detect cycles and refuse to generate code. We do not yet have use cases for models with cycles, so for now we just took the brute force approach. Note that we also check for references to the model itself - a typo that in the past resulted in long investigations. It is now correctly detected and reported to the user.
- **Error on duplicate references**: Similarly to cycles, adding the same reference more than once is now considered a bug and it is detected and reported to the user. The main reason why is because it normally happens as a result of copy and paste bugs, and so its best to inform users immediately.

## Deprecations

"Master headers" were a feature of Dogen which we haven't really used all that much. It enabled you to have a single include file for all files in a facet (_e.g._ a serialisation include, or a hashing include) and a top-level include file that included every single file in a model. These were used in the past when we had manual tests for the generated code, just to save us the effort of manually including a whole load of files. With the arrival of generated tests the feature was no longer used within Dogen. In addition, these days most C++ developers consider these "master includes" as anti-patterns, and a violation of "pay for what you use" because you invariably end up including more files than you need. Due to all of this we removed the feature from Dogen.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_19.org).

## Milestones

This is the 100th release of Dogen made from GitHub. Overall, its the 120th release, but had a private repo for those first 20 releases and the tags were lost in translation somewhere.

![100th release](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/milestones_100_releases.png)
_Figure 5: 100th release of Dogen from GitHub._

## Significant Internal Stories

Given that most stories had a user-facing impact, this sprint is short on user facing stories. There are a couple that are worth a mention though.

### Updating to Boost 1.70

We've started yet another of those mammoth efforts of trying to update all of our dependencies to use the latest version of Boost. It would be fairer to call this story "updating of toolchains across the estate" since it more or less involves that kind of effort. Now that we are on vcpkg, this should be a straightforward task, but in practice it never is. The main problems are OSX and Windows, two operative systems that somehow seem to always cause weird and wonderful problems. Predictably, we completed the work for Linux, did some of it for Windows and pretty much none of it for OSX. At present, our local setup on OSX is, well and truly borked and we just do not have enough cycles to work on fixing it so the story will remain parked for the foreseeable future.

### Implementing the relational model

We had great ambitions this sprint of implementing a relational model for tracing that would enable us to write complex queries to diagnose problems across the Dogen pipeline. We did do quite a lot of work on this, but it was entirely overshadowed by the other problems we had to solve. We won't spend too much time talking about this feature this sprint, waiting instead for its completion.

### Split generated tests from manual tests

Originally, we thought it would be a good idea to mix-and-match generated tests with manual tests, as we do with all other facets. However, given the requirements discussed above in the profiles story, it was rather inconvenient to have this mixture because it meant we could not rely on the presence of the required build files. This sprint we took the decision to split generated tests from manual tests, and it must be said, it has improved the project design a fair bit. After all, the purpose of generated tests is just to make sure Dogen generated code is working as expected, and that is largely an internal concern of Dogen developers. More work is required in this area to polish up the support for manual tests though.

### Recording of coding sessions

Since we've started Dogen all those years ago, we've been searching for "motivational tools" that enable us to continue working on such a long term endeavour without losing the initial hunger. A few successful tools have been incorporated in this way:

- agile management of sprints using org-mode, giving us a fine grained view of the activity on a sprint - _e.g._ [Sprint Backlog](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_19.org).
- blog posts narrating particular aspects of Dogen development - _e.g._ [Nerd Food: The Refactoring Quagmire](https://mcraveiro.blogspot.com/2018/01/nerd-food-refactoring-quagmire.html].
- creation of release notes at the end of every sprint as a way to reflect on what was achieved - the document you are reading.
- creation of demos to visualise the features implemented.

This sprint we found yet another "motivational tool": the recording of coding sessions as YouTube videos. We created a playlist with 13 episodes narrating much of the coding that happened this sprint: [MASD - Dogen Coding: Relational Model for Tracing](https://www.youtube.com/playlist?list=PLwfrwe216gF3EzrrvwNFivnLSZ2l8X9k6). At over 10h of video, the playlist is for the true die-hard fan of Dogen, to be sure. But the most important aspect from our perspective was that the recording of videos had a positive impact:

- it impeled us to work on days were perhaps we wouldn't have. This may be the novelty factory of seeing oneself on YouTube, of course, but it certainly worked for this sprint.
- it forces  you to think about what you're doing, just as when you are pair programming.

The one downside is that it is very difficult to focus on complex tasks whilst talking and recording. It is thus not a silver bullet, but certainly a useful weapon in the arsenal. We shall continue recording videos next sprint.

[![MASD - Dogen Coding: Relational Model for Tracing - Part 1](https://img.youtube.com/vi/re36Sr1u0Iw/0.jpg)](https://www.youtube.com/watch?v=re36Sr1u0Iw&list=PLwfrwe216gF3EzrrvwNFivnLSZ2l8X9k6&index=2)

## Resourcing

This sprint was marked by the return to "part-time" development on  Dogen. After a cadence of eight successful 2-week sprints, it was rather difficult to adjust back to the long, drawn-out process of cobbling together a release from whatever spare time one can find. As you may recall, the target for a "part-time" sprint is to clock around 80 hours worth of work over a rather unpredictable period of time. To be fair, most of Dogen has been developed in this fashion, but it is just not ideal fodder for programming. This is because part-time sprints naturally lend themselves to more fragmented work, given both the typically short-duration time slots available, and the fact that most of these are of rather dubious quality. The 22:00 slot comes particularly to mind - also fondly known known as the graveyard shift. Whilst there are advantages to _some_ resource starvation - described at length in [Nerd Food: Dogen: Lessons in Incremental Coding](https://mcraveiro.blogspot.com/2014/09/nerd-food-dogen-lessons-in-incremental.html) - it is also undoubtedly true that it is much harder to focus on complex tasks that require loading a lot of state into the brain. Nonetheless, “you go to war with the army you have, not the army you might want or wish to have at a later time”, and excuses do not write code, so one must make the most of the prevailing conditions.

To be fair, not all was gloom and doom with Sprint 19, and much was achieved. Let's review how the resourcing was distributed across stories. At 11.5% of the ask, upgrading to Boost 1.70 was the biggest story this sprint, closely followed by the work on the relational model (11%).   Several stories hovered around the 6-7% mark, in particular the splitting of generated tests from manual tests (6.7%), the far-out thought experiments on org-mode as a carrier format for modeling (6.5% - we clearly got carried away here), and the improvements around check for reference cycles (6.4%). Very much hidden in the list of stories is what we'd consider the "target" story - moving registrar into assets (6.3%) - but it was blocked because we are having some hard-to-debug issues with it, and require the support of the relational model to proceed. At 6% we have the meta-data overrides support, followed by a long tail of smaller stories - all the way from 5.7% creating the modeling reports in tracing to a minuscule 0.1% for upgrading to Clang 9 and GCC 9. The sprint is clearly demonstrating the impact of moving to part-time work, as expected. Finally, an important mention goes to the almost 16% spent in process related activities (backlog grooming, release notes, video editing for demo and coding sessions), down from 19% from the previous sprint. This is rather unexpected given that we've spent a lot of time recording the coding sessions this sprint, and implies they are very low overhead.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_19_pie_chart.jpg)

## Roadmap

We've renamed the "Planning" section to roadmap because it more adequately reflects its role: we are not actually forecasting, merely keeping track of outstanding activities and making some very weak correlations between them and a potential end date. The roadmap was clearly affected by the move to part-time, and looks more or less as was last sprint - just projected forwards in time.

# Next Sprint

The main focus next sprint is going to be to wrap things up with the relational model and to use it to diagnose problems when moving elements from generation to assets.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.19_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.19/dogen_1.0.19_amd64-applications.deb)
- [dogen-1.0.19-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.19/DOGEN-1.0.19-Darwin-x86_64.dmg)
- [dogen-1.0.19-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.19-Windows-AMD64.msi)

**Note:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+END_SRC markdown

- [[https://twitter.com/MarcoCraveiro/status/1135567734010523648][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6541333935140458497][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** Create a demo and presentation for previous sprint                :story:

Time spent creating the demo and presentation.

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2019-11-18 Mon 08:51]--[2019-11-18 Mon 08:58] =>  0:07
    CLOCK: [2019-11-18 Mon 08:41]--[2019-11-18 Mon 08:50] =>  0:09
    CLOCK: [2019-11-18 Mon 08:38]--[2019-11-18 Mon 08:40] =>  0:02
    CLOCK: [2019-11-18 Mon 08:24]--[2019-11-18 Mon 08:37] =>  0:13
    :END:

Updates to sprint and product backlog.

*** Create a profile called =untested=                                :story:

We should make it really easy to spot which models have modeling
elements that we are not testing. We should also add some comments as
well.

*** Add relational tracing support                                    :story:

Whenever we bump into a problem we seem to spend a lot of time going
through the log files and trace files trying to figure out where the
problem is happening. Have a quick go in trying to implement a
relational model for tracing to see if we can transfer the bulk of the
data into a relational format which we can query via SQL.

We've created a basic relational model for tracing. The relational
part of it seems straightforward (ish); the problem is the integration
of the tracer with the relational model. At present we rely on the
fact that all traceable objects have IO enabled; this works because
the code generator creates the IO facet, which is then used by the
write method in utility to convert any model type into a
string. However, we now need to change the approach: we need multiple
tracing backends:

- file tracer
- database tracer.

The file tracer is more or less the current tracer. The database
tracer needs to decompose the objects in existing models into a
relational representation. In an ideal world, the user would configure
the tracer to use one of the two backends and the remaining usage
would be transparent. However, we cannot have an interface for the
tracer backend that uses template methods because then we'd need
virtual template functions, it seems.

Another alternative is to make the tracer aware of the model objects
it is tracing. This is also not ideal because we would create cycles
in the design.

In effect we need to somehow implement a similar approach to the existing
tracer: rely on global template functions a-la =operator<<= to
decompose objects into their relational representations and then
supply those to the backend. It is not very clear how this would
work. For now we've postponed this approach as it seems its not going
to be a quick win.

We should approach this incrementally. Next time we have a bit of
spare time, we need to generate the model and then create the adapters
from existing models. Finally we can look at how it will be integrated
with tracing.

Notes:

- compilation generates an ODB error:

: FATALODB include directories not defined.

- the key difference between northwind and tracing is that we have a
  namespace. The application of the schema pragma is probably not
  working due to this. We need to look into the transform to see how
  that pragma propagates.
- the problem arises because we are only populating the primitive's
  properties if there is a top-level pragma. As the schema is not
  populated for the namespace, there isn't one. It is not clear why
  one would want to skip properties such as DB member if there isn't a
  schema, but perhaps this is due to some ODB error. We should
  probably issue an error or warning if we cannot generate code
  without a schema name.
- with regards to the relational model, the problem is that we can't
  really create a schema for each namespace in a model because schemas
  are not really like namespaces. The entities in a schema should
  really be self-contained and not refer to other schemas or else the
  database will be confusing to use. For example in postgres we will
  need to set the schema path, etc in order to see the different
  tables. One possible solution is to set the schema name to the same
  value for all namespaces (e.g. =dogen=). This would then allow us to
  have namespaces in C++ but not in the database.
- it seems foreign keys are not supported at present. We probably need
  support for this in order to query quickly or else we will have to
  manually setup indices for each of these joining fields.
- we need a command line option to choose the tracing backend
  (e.g. file or database). We also need the database configuration
  parameters: hostname, port, database, user.
- we need to refactor tracer as follows:
  - update the tracer interface to take actual types rather than
    templates.
  - create a top-level interface for the notion of a backend.
  - create two implementations of the backend: file and relational.
  - move all the file related code to the file backend.
  - implement adapters for each model to convert them into relational
    model types.
  - implement the relational backend.
- Add relational model to the dogen model tests.

Merged stories:

*Scripts for loading traces into postgres*

- rationale: this story is superseded by having a relational model.

It would be really nice if as part of the tracing generation we also
generated a set of SQL scripts that:

- created a number of tables
- copied all of the generated data into the database
- added a number of utility functions such as get elements in model, etc.

Over time we could build up functionality but to start off with we
just want something really simple that copies all of the
files. Interestingly this "looks" like a job for dogen. It would be
nice to have a meta-model element for this etc.

In the future it would be nice to have a think about the schema so
that we could do joins etc. For example:

- show me all transforms with element of type X (the state of the
  element at each transform).

We should also take into account multiple runs. Perhaps a more
adequate solution is to create a dogen library that has the ORM
support for this. Once we have proper JSON serialisation we can store
the objects as JSON serialisable, allowing us to re-run transforms,
etc.

Notes:

- ensure we upload the file name or at least the coordinates to the
  transform graph with the data so that we know what it refers to.
- rename relational database enum to just database
- rename hostname to just host

*Improved understanding*

Better than uploading a whole load of JSON blobs and then having to do
a number of really complex queries, is to have a ORM schema that is
designed to capture the data in the format we're interested in. Then
we could do very simple queries. What we really care about is
capturing all attributes of the model as it changes across the
transformations. We also care about the relationships between
transformations. We also need a way to uniquely identify elements
across their entire lifecycle. A simple way would be to create a hash
of the file name of the model, column and line number. We can then
associate other IDs to this one such as dia ID, etc.

We need to create a set of adaptors that convert an existing model
(injection, coding, etc) into the ORM model and then write the ORM
model into the database. The ORM model does not need as much detail
and structure as a regular model; for example, names can be flattened
or linked into IDs (e.g. name table), etc. Whatever makes sense from a
relational perspective.

It would also be nice to dump the log into the database so that we
could do simple correlations such as "what was logged between the
start and end of this transform?"

Interestingly, this would also allow us to compare things between
runs. The schema should be designed with this in mind.

*** Move registrar into assets                                        :story:

Move the registrar type into assets, in the quickest way possible.

Notes:

- In order to avoid blocking due to lots of analysis, we need
  to split this story into three:
  - first, we need to just move the registrar as is into assets.
  - a second story is to clean up the existing registrar code to have
    less templates and possibly address the existing registration
    bugs. We could also look into calling the registrars for
    referenced models automatically as part of this work (at present
    we are doing this manually).
  - finally, we need some meta-level refactoring to figure out if the
    pattern can be generalised to include initialisers, etc.
  In general that should be our approach: try to split out the
  capturing of patterns into as many steps as possible, to make sure
  we don't get overwhelmed as we implement things.
- we need to keep track of all type registrars on referenced models,
  not on the referenced models themselves. We need to know which
  models we referenced directly, and then find the registrars for
  those models.
- leaves need to know of the registrar. This is so that we can call it
  in their generated tests. We could use the registrar transform to go
  and find all leaves and populate their registrar name.
- current state is that we cannot generate the registrar for some
  reason.
- test model with registrar is C++ model. Type is called
  registrar. Its probably not a good idea to also call it registrar -
  wouldn't that clash with the existing type?
- we should have a warning/error: if using boost serialisation with a
  model that has inheritance, the registrar should be present. Added
  to warnings story.

*** Setup laptop to work on dogen                                     :story:

We haven't used the laptop for dogen for quite a bit so its behind the
main machine. Get it in a shape to do development again.

Items missing:

- dir locals for projects
- polymode
- build2
- odb

*** Generate ORM tests                                                :story:

We do not seem to be testing the generated ODB code. We don't need to
test ODB per se, but we should at least have some sanity checks that
test CRUD functionality.

Notes:

- for this we need a "masd database".
- tests should only trigger if postgres or some other relational
  database is detected.
- if foreign keys are used we need to detect them and ensure we
  populate the data accordingly.

*** Schema name in ORM should be transitive                           :story:

At present when we define the schema name on a top-level namespace, we
don't "inherit" it from child namespaces. The problem is compounded by
the fact that we need the schema name in order to output ODB pragmas
(separate bug). It seems more logical to propagate the schema name to
child namespaces.

*** ODB pragmas not populated when schema name is not set             :story:

At present we have a bug whereby not setting the schema name results
in not having most ODB pragmas set. We should always populate them
even if the schema name is not set. To be precise, the problem is not
directly related to the schema name - we just require some ORM
property to be set. AS it happens, it normally tends to be the schema
name, because it makes sense to set it when defining a relational
model. This is why we never bumped into this problem before.

*** Make =scoped_tracer= header only                                  :story:

At present we are generating the cpp for this file for no reason, use
the correct profile for header only.

** Deprecated
