#+title: Sprint Backlog 07
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Finish moving file locator and dependencies into yarn.
- Start sorting out object templates and profiles.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2017-12-09 Sat 12:32]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *56:45* |       |       | 100.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 56:45   |       |       | 100.0 |
| Active                                                                      |         | 56:45 |       | 100.0 |
| COMPLETED Edit release notes for previous sprint                            |         |       |  1:03 |   1.9 |
| STARTED Sprint and product backlog grooming                                 |         |       |  2:23 |   4.2 |
| COMPLETED Remove remnants of upsilon                                        |         |       |  0:35 |   1.0 |
| COMPLETED Create a well-defined set of stereotypes                          |         |       |  8:41 |  15.3 |
| COMPLETED Fix rtags setup in laptop                                         |         |       | 13:43 |  24.2 |
| COMPLETED Analysis on moving naming away from sewing terms                  |         |       |  2:21 |   4.1 |
| COMPLETED Implement exomodel in terms of exoelements                        |         |       | 16:03 |  28.3 |
| COMPLETED Fix JSON tests                                                    |         |       |  0:25 |   0.7 |
| COMPLETED Remove all deprecated code after exomodels clean up               |         |       |  2:00 |   3.5 |
| COMPLETED Add some kind of identity to exomodels                            |         |       |  0:12 |   0.4 |
| COMPLETED Clean up tagged values                                            |         |       |  0:23 |   0.7 |
| COMPLETED Simplify input model generation in cmake                          |         |       |  0:38 |   1.1 |
| COMPLETED Create a =probing= model                                          |         |       |  1:22 |   2.4 |
| STARTED Finish setting up coveralls                                         |         |       |  2:57 |   5.2 |
| STARTED Create a single binary for all of dogen                             |         |       |  1:03 |   1.9 |
| STARTED Investigate current implementation of the origin transform          |         |       |  0:18 |   0.5 |
| STARTED Create the =external= model                                         |         |       |  2:38 |   4.6 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2017-11-06 Mon 21:40]
    CLOCK: [2017-11-06 Mon 21:41]--[2017-11-06 Mon 21:59] =>  0:18
    CLOCK: [2017-11-06 Mon 21:03]--[2017-11-06 Mon 21:40] =>  0:37
    CLOCK: [2017-11-06 Mon 20:55]--[2017-11-06 Mon 21:03] =>  0:08

Add github release notes for previous sprint.

Title: Dogen v1.0.06, "Praia da Mariquita"

#+begin_src markdown
![Praia da Mariquita,](http://files.praiadamariquita.webnode.pt/200000109-50eaf52e2d/2015-10-17%2020.02.17.jpg)
_Praia da Mariquita, Namibe, Angola. (C) Praia da Mariquita Lodge, 2014._

Overview
=======
Our long road towards the clean up of the backends continued with another long and arduous sprint. The bulk of the work in this sprint was focused on two activities:

- **File locators work**: clean up the backend-specific file locators and move them into yarn. In order to do this we needed to generalise a large number of data structures that were originally designed to be language-specific. This has proven to be quite a challenge, and we probably still have another full sprint ahead of us on this work.
- **Additional exomodel work**: in the previous sprint we introduced the concept of _exomodels_; these originally used the regular meta-model elements such as ```yarn::object``` and so forth. This sprint it became obvious that a further round of simplification is still required, moving away from the core meta-model elements within the frontends. This work has only started but we can already see two obvious benefits: a) creating a frontend will be much easier, with very little code required b) the final JSON format will be quite trivial, making it easy for users to generate it or to map it from other tooling.

In addition to this, a number of "fun" activities where also undertaken to break away from the monotony of refactoring. These also provided tangible benefits in terms of Dogen development:

- **Consolidation of responsibilities in Yarn**: A number of classes were tidied up and moved into Yarn, making the meta-model more cohesive (file housekeeping, artefact writing, etc). Other classes already in Yarn were improved (better naming, remove classes that did not add any value, etc).
- **Integration of CCache in Travis**: most of our builds are now much quicker (in the order of tens of minutes or less) due to caching of translation units. Unfortunately, this work does not extend to GCC's Debug build (for some not yet understood reason) nor to OSX (given the peculiarities of its many packaging systems, we still haven't quite figure out how to install CCache) nor to Windows (its not clear that AppVeyor and/or MSVC support CCache or a CCache like tool).
- **Use of colour in Dia's UML diagrams**: as described below, we started colour-coding UML classes in Dia.
- **Revamp of project logo**: Dogen now sports a slightly more professional project logo [in Github](https://github.com/DomainDrivenConsulting/dogen).

User visible changes
================
The only user visible change this sprint is the introduction of a simple colour scheme for Dia UML Diagrams. This idea was largely copied from this paper: [Instinct: A Biologically Inspired Reactive Planner for Embedded Environments](http://www.robwortham.com/wp-content/uploads/2016/05/ICAPS-2016-PlanRob-Instinct-Planner.pdf). Note that the colours have no meaning to Dogen itself, but they do make interpreting diagrams a lot easier.

![Coloured UML Diagrams](https://github.com/DomainDrivenConsulting/dogen/raw/master/doc/blog/images/colour_coded_uml_diagrams.png)

Colouring is performed via a simple python script [available here](https://github.com/DomainDrivenConsulting/dogen/blob/master/projects/dia/python/colour.py), which can be executed in Dia's interactive python console.

As always, for gory details on the work carried out this sprint, see the [sprint log](https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_06.org).

Next Sprint
===========
Next sprint we'll continue working on the new exomodel classes and resume the work on the backend-agnostic file locator.

Binaries
======
You can download binaries from [Bintray](https://bintray.com/domaindrivenconsulting/Dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.06_amd64-applications.deb](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.06/dogen_1.0.06_amd64-applications.deb)
- [dogen-1.0.06-Darwin-x86_64.dmg](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.06/dogen-1.0.06-Darwin-x86_64.dmg)
- [dogen-1.0.06-Windows-AMD64.msi](https://dl.bintray.com/domaindrivenconsulting/Dogen/dogen-1.0.06-Windows-AMD64.msi)

**Note**: They are produced by CI so they may not yet be ready.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/927655421531361280][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6333421782644719616][LinkedIn]]
- [[https://gitter.im/DomainDrivenConsulting/dogen][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    CLOCK: [2017-12-09 Sat 12:22]--[2017-12-09 Sat 12:32] =>  0:10
    CLOCK: [2017-12-05 Tue 08:49]--[2017-12-05 Tue 08:54] =>  0:05
    CLOCK: [2017-12-01 Fri 12:51]--[2017-12-01 Fri 13:14] =>  0:23
    CLOCK: [2017-12-01 Fri 11:29]--[2017-12-01 Fri 11:46] =>  0:17
    CLOCK: [2017-11-30 Fri 22:32]--[2017-11-30 Fri 23:02] =>  0:30
    CLOCK: [2017-11-30 Thu 22:22]--[2017-11-30 Thu 22:55] =>  0:33
    CLOCK: [2017-11-28 Tue 07:33]--[2017-11-28 Tue 07:46] =>  0:13
    CLOCK: [2017-11-28 Tue 07:20]--[2017-11-28 Tue 07:32] =>  0:12

Updates to sprint and product backlog.

*** COMPLETED Remove remnants of upsilon                              :story:
    CLOSED: [2017-11-07 Tue 09:00]
    CLOCK: [2017-11-07 Tue 08:39]--[2017-11-07 Tue 09:00] =>  0:21
    CLOCK: [2017-11-07 Tue 08:25]--[2017-11-07 Tue 08:39] =>  0:14

Originally we had removed upsilon as a frontend but it seems there are
still some remnants around yarn. Delete them.

*** COMPLETED Create a well-defined set of stereotypes                :story:
    CLOSED: [2017-11-17 Fri 15:56]
    CLOCK: [2017-11-17 Fri 17:15]--[2017-11-17 Fri 17:38] =>  0:23
    CLOCK: [2017-11-17 Fri 15:23]--[2017-11-17 Fri 15:56] =>  0:33
    CLOCK: [2017-11-17 Fri 15:12]--[2017-11-17 Fri 15:22] =>  0:10
    CLOCK: [2017-11-17 Fri 14:51]--[2017-11-17 Fri 15:11] =>  0:20
    CLOCK: [2017-11-17 Fri 14:02]--[2017-11-17 Fri 14:50] =>  0:48
    CLOCK: [2017-11-17 Fri 10:47]--[2017-11-17 Fri 13:21] =>  2:34
    CLOCK: [2017-11-17 Fri 10:10]--[2017-11-17 Fri 10:46] =>  0:36
    CLOCK: [2017-11-17 Fri 09:15]--[2017-11-17 Fri 10:09] =>  0:54
    CLOCK: [2017-11-09 Thu 20:12]--[2017-11-09 Thu 20:15] =>  0:03
    CLOCK: [2017-11-09 Thu 20:04]--[2017-11-09 Thu 20:11] =>  0:07
    CLOCK: [2017-11-09 Thu 19:35]--[2017-11-09 Thu 20:03] =>  0:28
    CLOCK: [2017-11-09 Thu 07:55]--[2017-11-09 Thu 08:40] =>  0:45
    CLOCK: [2017-11-07 Tue 18:09]--[2017-11-07 Tue 19:00] =>  0:51
    CLOCK: [2017-11-07 Tue 09:02]--[2017-11-07 Tue 09:11] =>  0:09

We should process the stereotypes that are hard-coded into yarn
separately from those that are user supplied.

*Previous understanding*

Instead of mapping to yarn element types, we should be using the
stereotypes directly.

Actually the right way of doing this is to split out the well-known
stereotypes from the other stereotypes.

Tasks:

- rename unknown stereotypes to something else like "non-well-known
  stereotypes". Basically these may or may not be known. Or perhaps
  even better: user-defined? Or we can just have static and dynamic
  stereotypes.

*** COMPLETED Fix rtags setup in laptop                               :story:
    CLOSED: [2017-11-28 Tue 07:27]
    CLOCK: [2017-11-10 Fri 09:10]--[2017-11-10 Fri 12:01] =>  2:51
    CLOCK: [2017-11-10 Fri 14:05]--[2017-11-10 Fri 18:01] =>  3:56
    CLOCK: [2017-11-11 Sat 09:05]--[2017-11-11 Sat 16:01] =>  6:56

For some reason rtags is not working in the laptop.

This was due to using symlinks to the SSD. We must make sure we setup
the project in rtags using exactly the same path as we use in emacs.

*** COMPLETED Analysis on moving naming away from sewing terms        :story:
    CLOSED: [2017-12-01 Fri 13:18]
    CLOCK: [2017-12-01 Fri 13:15]--[2017-12-01 Fri 13:52] =>  0:37
    CLOCK: [2017-11-30 Thu 07:22]--[2017-11-30 Thu 08:11] =>  0:49
    CLOCK: [2017-11-28 Tue 17:52]--[2017-11-28 Tue 18:47] =>  0:55

Originally we came up with the sewing naming convention because there
was this idea that we'd end up with a large number of little tools,
each with their own model and binary. However, with the hindsight of
several years of development and better understanding of the domain,
it now seems that the big building blocks are as follows:

- =frontend=: what we are calling =exomodel= at present and associated
  types; its interface, registrar, etc; the dia and json frontends.
- "middle-end": which we could call =modeling= core meta-model types,
  transformations and helpers.
- =backend=: effectively the =cpp= and =csharp= backends. Conceptually
  these could make up the =quilt= kernel but this can just be a string
  rather than a namespace. We are not even sure if we will ever
  require more than one kernel. We could also call this component
  =codegen= given its only concern is to generate code. Alternatively:
  =generation=.
- =templating=: wale and stitch.
- =annotations=: this can stay as it is, though it would be nice to
  have a class called tagged value, in keeping with the literature.
- =formatters=: this can be renamed to =formatting=?
- =utility=: stays as is.

The big question to ask here though is what is the purpose of the code
structure. In theory, one could be renaming and refactoring for ever,
so there must be some kind of halting function that tells us when we
reached some kind of stable state that is good enough. This could be
achieved via criteria. We can come up with a laundry list of what the
project structure should promote.

Notes on project structure:

- it should make it easy to add new frontends. A developer should not
  need to know anything about the internals of yarn/modeling in order
  to add a new frontend. The current structure fails on this regard
  because we have merged the frontends with the middle-end.
- it should make it easy to add new backends. This is already the
  case, more or less, given the decoupling we've done of quilt and
  yarn.
- the names should provide a good indication of what the model does,
  at least to someone familiar to the domain. We fail on this regard
  due to the use of sewing terms which are not used commonly in the
  model driven literature.
- the dependencies between projects should not have cycles. This is
  the case at present, but having said that we still have yarn
  connected to both the frontends and the backends (even if this is
  achieved via interfaces). An ideal world would be where the three
  components would be linked in linear fashion only. However, one has
  to be wary of foolish consistency here. In terms of the domain
  literature, making everything a transform is the correct approach
  and this is what we've achieved at present (e.g. frontends and
  backends are merely transformations). Also the dependency is
  cyclical only if one considers its run-time aspects rather than
  compile time. The middle-end compiles fine in isolation, but one
  cannot run its tests because they rely on the presence of frontends
  and backends.
- another way of looking at the problem is to say that we need a
  mirror structure for frontends/middlened/backends: they all have a
  meta-model and transforms. Each can expose transform chains. This in
  effect moves us a bit backwards the old world where we had knit as a
  top-level model but we don't have a good name for what "knit" would
  be. Its responsibility would be to hook together the top-level
  transforms. We moved away from it because knit was mainly an empty
  model with only two or three classes, so the overhead did not
  justify its existence.
- if we were to move what we currently call =model= into a =backend=
  project, and move all the associated transforms as well, we would
  have a slightly meatier model (e.g. as opposed to =quilt= which had
  only a couple of classes). This would also help in terms of
  symmetry: three tiers, each with its meta-model and transforms. You
  only need to know about the transforms on a given tier when you are
  doing changes there. One slight wrinkle to this symmetric nirvana is
  that we still have a =model= and a =text_model=, both of which would
  live (presumably) in codegen. Or if not, then middleend would have a
  similar issue (endomodel and model). The latter makes more sense. We
  could probably get away with endomodel - in fact it becomes even
  more meaningful, the model used for internal purposes only. All
  other models can be rename to just "model".
- all of this leaves us with the perennial question of who guards the
  guardians. We need a top-level model that glues together the other
  three. This is knit by another name. The engineering decision that
  has to be made is whether having a trivial model like knit (for
  which we do not have a good name) and making the project structure
  clean outweighs having little "modelets" with very little
  responsibility.
- the model that sits at the top could be called =orchestrator= or
  =orchestration= because it orchestrates all components.

In conclusion, we'd have the following libraries:

- =annotations=: unchanged.
- =formatting=: simple rename. No longer =formatters= as this is not
  the place where all formatters are defined, but instead provides the
  primitives for formatting.
- =dia=: unchanged.
- =exogenous=, including =exogenous.dia= and =exogenous.json=:
  frontends and associated transforms. With this name, we don't have
  to worry about finding a good name for middle-end. Also frontend and
  backend imply there is only one way to hook together the components,
  which is not right.
- =modeling=: endomodel and all associated transforms and
  helpers.
- =codegen=, including =codegen.cpp= and =codegen.csharp=. model and
  all associated transforms and helpers move to =codegen=. Model now
  becomes more like formattables model; we probably need to introduce
  a class like augmented element that aggregates element and element
  properties.
- =templates=: merges stitch and wale; these become namespaces.
- =orchestration=: top-level transforms (e.g. knit, tailor). Depends
  on all other libraries.

And the following binaries:

- =cli=: (producing =dogen.cli=): command-line interface for all
  functionality.
- =web=: wt based site.
- =http=: beast based api.
- =server=: raw sockets api.

*** COMPLETED Implement exomodel in terms of exoelements              :story:
    CLOSED: [2017-12-05 Tue 08:37]
    CLOCK: [2017-12-04 Mon 22:47]--[2017-12-04 Mon 23:46] =>  0:59
    CLOCK: [2017-12-04 Mon 22:42]--[2017-12-04 Mon 22:46] =>  0:04
    CLOCK: [2017-12-04 Mon 22:35]--[2017-12-04 Mon 22:41] =>  0:06
    CLOCK: [2017-12-04 Mon 20:41]--[2017-12-04 Mon 22:34] =>  1:53
    CLOCK: [2017-12-03 Sun 08:07]--[2017-12-03 Sun 08:44] =>  0:37
    CLOCK: [2017-12-03 Sun 00:22]--[2017-12-03 Sun 00:45] =>  0:23
    CLOCK: [2017-12-02 Sat 23:49]--[2017-12-03 Sun 00:21] =>  0:32
    CLOCK: [2017-12-02 Sat 23:18]--[2017-12-02 Sat 23:48] =>  0:30
    CLOCK: [2017-12-02 Sat 23:09]--[2017-12-02 Sat 23:17] =>  0:08
    CLOCK: [2017-12-02 Sat 22:01]--[2017-12-02 Sat 23:08] =>  1:07
    CLOCK: [2017-12-02 Sat 21:48]--[2017-12-02 Sat 22:00] =>  0:12
    CLOCK: [2017-12-02 Sat 21:16]--[2017-12-02 Sat 21:47] =>  0:31
    CLOCK: [2017-12-02 Sat 20:55]--[2017-12-02 Sat 21:15] =>  0:20
    CLOCK: [2017-12-02 Sat 20:44]--[2017-12-02 Sat 20:47] =>  0:03
    CLOCK: [2017-12-02 Sat 20:37]--[2017-12-02 Sat 20:43] =>  0:06
    CLOCK: [2017-12-02 Sat 20:29]--[2017-12-02 Sat 20:36] =>  0:07
    CLOCK: [2017-12-02 Sat 19:32]--[2017-12-02 Sat 20:28] =>  0:56
    CLOCK: [2017-12-02 Sat 17:06]--[2017-12-02 Sat 17:45] =>  0:39
    CLOCK: [2017-12-02 Sat 16:55]--[2017-12-02 Sat 17:05] =>  0:10
    CLOCK: [2017-12-02 Sat 15:40]--[2017-12-02 Sat 16:54] =>  1:14
    CLOCK: [2017-12-02 Sat 12:24]--[2017-12-02 Sat 12:28] =>  0:04
    CLOCK: [2017-12-02 Sat 12:05]--[2017-12-02 Sat 12:23] =>  0:18
    CLOCK: [2017-12-02 Sat 11:31]--[2017-12-02 Sat 12:04] =>  0:33
    CLOCK: [2017-12-01 Fri 23:29]--[2017-12-01 Fri 23:31] =>  0:02
    CLOCK: [2017-12-01 Fri 23:21]--[2017-12-01 Fri 23:28] =>  0:07
    CLOCK: [2017-12-01 Fri 23:06]--[2017-12-01 Fri 23:20] =>  0:14
    CLOCK: [2017-12-01 Fri 22:46]--[2017-12-01 Fri 23:05] =>  0:19
    CLOCK: [2017-12-01 Fri 22:35]--[2017-12-01 Fri 22:45] =>  0:10
    CLOCK: [2017-12-01 Fri 21:14]--[2017-12-01 Fri 22:34] =>  1:20
    CLOCK: [2017-12-01 Fri 21:03]--[2017-12-01 Fri 21:13] =>  0:10
    CLOCK: [2017-12-01 Fri 18:42]--[2017-12-01 Fri 18:51] =>  0:09
    CLOCK: [2017-12-01 Fri 15:58]--[2017-12-01 Fri 16:06] =>  0:08
    CLOCK: [2017-12-01 Fri 14:25]--[2017-12-01 Fri 15:46] =>  1:21
    CLOCK: [2017-12-01 Fri 13:53]--[2017-12-01 Fri 14:24] =>  0:31

For details on the analysis, see the comments in the previous sprint.

Notes:

- now that there is no longer a mismatch between dia's model and
  yarn's model we can probably do away with the processed object and
  processed comment, and simply map dia directly into yarn.

Tasks:

- change yarn.dia to remember the "contained by" name rather than the
  module name. Construct the object names from the contained by
  name. Actually this won't work; the reason why we remember the
  entire module is because we need to do a lookup in order to find the
  module so we can update the documentation. We will still have this
  problem when it comes to exoelements. Best to just create another
  map this time to exoelement and follow the pattern. Actually, we can
  clean this up slightly: create a map of exoelements
- add exoelement, exoattribute.
- create a parallel infrastructure in dia that populates the
  exoelements.
- create a new transform that converts exoelements into
  endomodels. Somehow isolate the dia part of the pipeline so we can
  switch between new world and old world. Actually we could very
  simply check the exoelements container; if not empty use that,
  otherwise use legacy.
- once we get the dia side of the pipeline working, delete all classes
  related to old world in yarn.dia.
- create an hydrator that reads the new json and creates
  exoelements. Add some basic feature switch so we can alternate
  between new world and old world.

Problems:

- modules do not have a stereotype
- add yarn element types enum to yarn and a method that given a
  container of strings, returns the types. Use these in yarn.dia
- add string constants for element stereotypes and use these to mark
  the exoelements. Use this method in the stereotypes transforms in
  yarn.
- name does not have the module (e.g. contained by is not working).

Tasks:

- add a new boolean flag to switch between new world and old
  world. Set it only on yarn.dia for now.
- move naming transform to endomodels.
- add code in exomodel to endomodel transform to convert exolements
  into elements. Look at yarn.dia for this.
- handle root module in terms of exoelements.
- handle annotations. We need to create some kind of factory that uses
  the annotation groups factory logic but just for a single
  annotation.
- create new JSON format for exomodels. Update JSON parser to
  read/write it. Set flag to true in JSON.
- JSON needs to explicitly contain fallback stereotype or else tailor
  roundtrip will fail. We should check that fallback is not default,
  if so do not bother outputting it.

*** COMPLETED Fix JSON tests                                          :story:
    CLOSED: [2017-12-05 Tue 08:48]
    CLOCK: [2017-12-05 Tue 08:38]--[2017-12-05 Tue 08:48] =>  0:10
    CLOCK: [2017-12-05 Tue 08:22]--[2017-12-05 Tue 08:37] =>  0:15

After implementing the exomoel in terms of exoelements, we broke the
JSON tests.

*** COMPLETED Remove all deprecated code after exomodels clean up     :story:
    CLOSED: [2017-12-05 Tue 21:12]
    CLOCK: [2017-12-05 Tue 20:55]--[2017-12-05 Tue 21:16] =>  0:21
    CLOCK: [2017-12-05 Tue 20:31]--[2017-12-05 Tue 20:54] =>  0:23
    CLOCK: [2017-12-05 Tue 19:25]--[2017-12-05 Tue 19:41] =>  0:16
    CLOCK: [2017-12-05 Tue 18:22]--[2017-12-05 Tue 18:45] =>  0:23
    CLOCK: [2017-12-05 Tue 18:16]--[2017-12-05 Tue 18:21] =>  0:05
    CLOCK: [2017-12-05 Tue 18:11]--[2017-12-05 Tue 18:15] =>  0:04
    CLOCK: [2017-12-05 Tue 17:53]--[2017-12-05 Tue 18:10] =>  0:17
    CLOCK: [2017-12-05 Tue 09:01]--[2017-12-05 Tue 09:07] =>  0:06
    CLOCK: [2017-12-05 Tue 08:55]--[2017-12-05 Tue 09:00] =>  0:05

Remove all deprecated code:

- yarn.json: hydrator related classes
- yarn: drop exoelement properties, drop new code logic, groups in
  context, annotations transform, nameable/metanameable from exomodel.
- annotations: scribble groups and related classes.
- stitch: drop usage of scribbles

*** COMPLETED Add some kind of identity to exomodels                  :story:
    CLOSED: [2017-12-05 Tue 21:30]
    CLOCK: [2017-12-05 Tue 21:17]--[2017-12-05 Tue 21:29] =>  0:12

We need some way of identifying exomodels. We cannot use the name
"name" given that this is computed based on meta-data. However, we
could use either "id" or filename.

*** COMPLETED Clean up tagged values                                  :story:
    CLOSED: [2017-12-05 Tue 21:55]
    CLOCK: [2017-12-05 Tue 21:37]--[2017-12-05 Tue 21:55] =>  0:18
    CLOCK: [2017-12-05 Tue 21:31]--[2017-12-05 Tue 21:36] =>  0:05

Tasks:

- rename entries in annotations to tagged values.
- rename key value pairs to tagged values in yarn.dia processed
  object.

*** COMPLETED Simplify input model generation in cmake                :story:
    CLOSED: [2017-12-07 Thu 08:59]
    CLOCK: [2017-12-07 Thu 08:23]--[2017-12-07 Thu 09:01] =>  0:38

At present we have copy and pasted the input model targets in cmake,
for both JSON and dia. This is not ideal:

- its a pain to add new targets
- JSON and dia options can start to diverge over time.

Factor out all the common code and create targets using a loop.

*** COMPLETED Create a =probing= model                                :story:
    CLOSED: [2017-12-09 Sat 12:28]
    CLOCK: [2017-12-09 Sat 12:14]--[2017-12-09 Sat 12:21] =>  0:07
    CLOCK: [2017-12-09 Sat 12:02]--[2017-12-09 Sat 12:13] =>  0:11
    CLOCK: [2017-12-09 Sat 11:45]--[2017-12-09 Sat 12:01] =>  0:16
    CLOCK: [2017-12-09 Sat 11:29]--[2017-12-09 Sat 11:44] =>  0:15
    CLOCK: [2017-12-09 Sat 11:01]--[2017-12-09 Sat 11:28] =>  0:27
    CLOCK: [2017-12-07 Thu 09:01]--[2017-12-07 Thu 09:07] =>  0:06

We need to move the probing logic into a transforms model, so we can
use it outside of yarn. We should also move:

- context, context factory
- options

Actually it makes more sense to just have the probing infrastructure.

We need a way to generalise the "initial input" dumping. In effect,
what we are really saying is that within a transform we may need to
dump more state than just the initial inputs. We need a way to express
this in the probing API.

*** STARTED Finish setting up coveralls                               :story:
    CLOCK: [2017-11-29 Wed 23:48]--[2017-11-30 Thu 00:21] =>  0:33
    CLOCK: [2017-11-29 Wed 22:45]--[2017-11-29 Wed 23:47] =>  1:02
    CLOCK: [2017-11-29 Wed 21:50]--[2017-11-29 Wed 22:20] =>  0:30
    CLOCK: [2017-11-29 Wed 19:02]--[2017-11-29 Wed 19:54] =>  0:52

Remaining issues:

- we are generating far too much output. We need to keep it quieter or
  we will break travis.
- we are not filtering out non-project files from initial
  processing. There must be a gcov option to ignore files.

: Process: /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc-5/Debug/projects/quilt/spec/CMakeFiles/quilt.spec.dir/main.cpp.gcda
: ------------------------------------------------------------------------------
: File '../../../../projects/quilt/spec/main.cpp'
: Lines executed:62.50% of 8
: Creating '^#^#^#^#projects#quilt#spec#main.cpp.gcov'
:
: File '/usr/local/personal/include/boost/smart_ptr/detail/sp_counted_impl.hpp'
: Lines executed:60.00% of 20
: Creating '#usr#local#personal#include#boost#smart_ptr#detail#sp_counted_impl.hpp.gcov'

See also:

- [[https://github.com/JoakimSoderberg/coveralls-cmake-example/blob/master/CMakeLists.txt][example use of coveralls-cmake]]
- [[https://github.com/SpinWaveGenie/SpinWaveGenie/blob/master/libSpinWaveGenie/CMakeLists.txt][SpinWaveGenie's support for Coveralls]]
- maybe we should just use a different coverage provider. [[https://codecov.io/gh/DomainDrivenConsulting/dogen][CodeCov]]
  seems to be used by the kool kids. Example: [[https://github.com/ChaiScript/ChaiScript/blob/develop/CMakeLists.txt][ChaiScript]]. Example repo
  [[https://github.com/codecov/example-cpp11][here]] and for CMake specifically, [[https://github.com/codecov/example-cpp11-cmake][here]].
- we should generate coverage from the clang debug build only since
  that is the fastest build we have. We should use the clang coverage
  tool. See [[https://clang.llvm.org/docs/SourceBasedCodeCoverage.html][this document]].

Previous story [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_84.org#add-initial-support-for-coveralls][here]].

Notes:
- problems with python dependencies: [[https://github.com/micropython/micropython/issues/3246][cpp-coveralls 0.4.0 came and
  broke Travis build]]

*** STARTED Create a single binary for all of dogen                   :story:
    CLOCK: [2017-12-01 Fri 11:47]--[2017-12-01 Fri 12:50] =>  1:03

As per analysis, we need to create a single dogen binary, like so:

: dogen.cli COMMAND COMMAND_SPECIFIC_OPTIONS

Where =COMMAND= is:

- =transform=: functionality that is currently in tailor.
- =generate=: functionality that is currently in knitter.
- =expand=: functionality that is currently in stitcher plus expansion
  of wale templates.
- =make=: functionality in darter: create project, structure etc.

In order to support sub-commands we need to do a lot of hackery with
program options:

- [[https://gist.github.com/randomphrase/10801888][cmdoptions.cpp]]: Demonstration of how to do subcommand option
  processing with boost program_options
- [[https://stackoverflow.com/questions/15541498/how-to-implement-subcommands-using-boost-program-options][How to implement subcommands using Boost.Program_options?]]

*** STARTED Investigate current implementation of the origin transform :story:
    CLOCK: [2017-12-06 Wed 20:25]--[2017-12-06 Wed 20:43] =>  0:18

Do we need to have the origin expansion? can we not just supply the
origin type to the exomodel adapter directly?

Actually this cannot be done. The problem is we still need to
distinguish between dogen models and non-dogen models; we need to
register all dogen models. This is done via meta-data. We cannot use
the meta-data until we have converted into an endomodel. We could
consider having a flag at the exomodel level for this - it is a
concept at this level - but we still need to map it to origin
types. However, it is perhaps cleaner to express this concept at the
exomodel level rather than the endomodel level given we are saying
there are two different kinds of exomodels: proxyness is a fundamental
property of an exomodel. If we do this we can then do the mapping in
flight as we transform from exomodel to endomodel.

*** STARTED Create the =external= model                               :story:
    CLOCK: [2017-12-06 Wed 20:43]--[2017-12-06 Wed 23:21] =>  2:38

Create a new model called =external= and move all exogenous model
related class to it.

Consider naming it =codec=.

We should also split the model into the usual transforms, helpers,
meta-model etc split. The top level transforms should be:

- to dia diagram
- to processed object
- to model

Notes:

- we should have a dia to codec model chain.
- rename adapter to factory to match processed object.

*** Add support for "ad-hoc" probing                                  :story:

We have a set of inputs supplied to the prober called "initial
input". This is not ideal. We need a way to generalise the "initial
input" dumping. In effect, what we are really saying is that within a
transform we may need to dump more state than just the initial
inputs. We need a way to express this in the probing API.

*** Rename the =transform= method to =apply=                          :story:

Its a bit silly to name classes =x_transform= and then to have their
main method also called =transform=. We should rename these to
something like =apply=.

*** Clean up exoelement                                               :story:

Need to add: can_be_primitive_underlier, in_global_module,
can_be_enumeration_underlier, is_default_enumeration_type,
is_associative_container

*** Rename =yarn= to =modeling=                                       :story:

As per analysis story, we are moving away from the sewing terms.

*** Rename =quilt.cpp= to =generation.cpp=                            :story:

As per analysis story, we are moving away from the sewing terms.

*** Rename =quilt.csharp= to =generation.csharp=                      :story:

As per analysis story, we are moving away from the sewing terms.

*** Rename =yarn.dia= to =external.dia=                               :story:

As per analysis story, we are moving away from the sewing terms.

*** Rename =yarn.json= to =external.json=                             :story:

As per analysis story, we are moving away from the sewing terms.

*** Rename =formatters= to =formatting=                               :story:

As per analysis story, we are moving away from the sewing terms.

*** Create the =templates= model                                      :story:

As per analysis story, we are moving away from the sewing terms.

Merge stitch and wale into a new model called =templates=.

*** Create the =generation= model                                     :story:

Create a new model called =generation= and move all code-generation
related class to it.

We need to create classes for element properties and make model have a
collection that is a pair of element and element properties. We need a
good name for this pair:

- extended element
- augmented element
- decorated element: though not using the decorator pattern; also, we
  already have decoration properties so this is confusing.

Alternatively we could just call it =element= and make it contain a
modeling element.

*** Create the =orchestration= model                                  :story:

Create a model with the top-level transforms.

*** Create a new exoelement chain                                     :story:

We need to create a new exoelement chain that uses the new exoelements
to bootstrap a endomodel.

*** Consider having a single executable for dogen                     :story:

We started off by creating lots of little executables: knitter,
darter, tailor, stitcher. Each of these has its own project,
command-line options etc. However, now that we are concentrating all
of the domain knowledge in yarn, it seems less useful to have so many
executables that are simply calling yarn transforms. Instead, it may
make more sense to use an approach similar to git and have a
"sub-command":

: dogen knit
: dogen tailor

And so forth. Of course, we could also take this opportunity and clean
up these names to making them more meaningful to end users. Perhaps:

: dogen codegen
: dogen transform

Each of these sub-commands or modes would have their own set of
associated options. We need to figure out how this is done using boost
program options. We also need to spend a bit of time working out the
sub-commands to make sure they make sense across the board.

In terms of names, we can't really call the project "dogen". We should
call it something allusive to the command line, such as cli. However,
the final binary should be called dogen or perhaps, =dogen.cli=. This
fits in with other binaries such as =dogen.web=, =dogen.http=,
=dogen.gui= etc.

*** Add stereotypes support at the attribute level                    :story:

At present dia does not have stereotypes in attributes. This means
things like ORM primary keys etc are being supplied as tagged values;
in reality, its more natural (from a UML perspective) to supply them
as stereotypes. We could add some meta-data that creates a tagged
value for stereotypes.

*** Exclude profiles from stereotypes processing                      :story:

At present we are manually excluding profiles from the stereotypes
transform. This was just a quick hack to get us going. We need to
replace this with a call to annotations to get a list of profile names
and exclude those.

We should also rename =is_stereotype_handled_externally= to something
more like "is profile" or "matches profile name".

Actually the right thing may even be to just remove all of the profile
stereotypes during annotations processing. However, we should wait
until we complete the exomodel work since that will remove scribble
groups, etc. Its all in the annotations transform.

*** Tailor does not output static stereotypes                         :story:

At present we only output static stereotypes. However, there is no
point on fixing this until we move to the new JSON format.

*** Generate file paths as a transform                                :story:

See the comments in the previous sprint.

*** Split registrar into two classes                                  :story:

At present we do not distinguish between the setting up of the
registrar and the usage of the registrar. Up to know this is not a
major issue, although its a bit of a smell that we have to call
validate at some arbitrary point.

However, with the new parts/builder setup, this becomes even more of a
problem because we only want to build the parts once we have
registered all of the formatters. The right thing would have been to
have:

- a registrar builder, used during registration;
- a build step which returns the (validated) registrar. Once build is
  called, we should throw if anyone attempts to add more formatters.

This makes it hard to misuse the API.

Notes:

- how does this affect plugins? will it still be possible to register
  formatters from a shared library?

Tasks:

- create a registrar builder with most of the existing registrar
  interface. On build it computes the parts, generates the repository,
  etc and then supplies that to the registrar. The registrar itself is
  no longer static, just a member of the workflow.

*** Add a file format parameter to probing                            :story:

At present we are dumping all models in probing as JSON. It would be
nice to be able to dump them as boost serialisation so we can plug
them into tests or to reproduce some problem. It would be even nicer
if we could plug that data back in to dogen but its not obvious how
that would work; we need to have some kind of concept of "stages", and
then supply the inputs and the stage so that dogen could continue from
there.

*** Update backend shape to match yarn                                :story:

In an ideal world, the backends should be made up of two components:

- *meta-model*: a set of types that augment yarn with backend
  specific elements. This is what we call fabric at present.
- *transforms*: of these we have two kinds:
  - the model-to-model transforms that involve either yarn meta-model
    elements or backened specific meta-model elements. These live in
    fabric at present.
   - the model-to-text transforms that convert a meta-model element
     (yarn or backend specific) into an artefact. These we call
     formatters at present.

The ultimate destination for the backend is then to have a shape that
reflects this:

- rename formatters to transforms
- move artefact formatter into yarn; with this it means we can also
  move all of the top-level workflow formatting logic into
  yarn. However, before we can do this we must make all of the backend
  specific code in the formatter interface go away.
- note that at this point we no longer need to know what formatters
  belong to what backend other than perhaps to figure out if the
  backend is enabled. This means yarn can now have the registrars for
  formatters and organise them by backend. Which means the
  model-to-text chain will own all of these. However, we still have
  the managed directories to worry about; somehow, someone has to be
  able to compute the managed directories per kernel. This could be
  done at yarn level if the locator is clever enough.

Of course, before we can contemplate this change, we must first get
rid of formattables altogether.

We must also somehow model canonical formatters in yarn. Take this
into account when we do:

:        /*
:         * We must have one canonical formatter per type per facet.
:         * FIXME: this check is broken at the moment because this is
:         * only applicable to yarn types, not fabric types. It is also
:         * not applicable to forward declarations. We need some
:         * additional information from yarn to be able to figure out
:         * which types must have a canonical archetype.
:         */

*** Improvements to dia model                                         :story:

Assorted notes on cleaning-up the dia model:

- create a base class such as =value= and make all values inherit from
  it instead of using boost variant.
- according to DTD, a composite can be made up of either composites or
  attributes. We incorrectly modeled it as having just one inner
  composite.
- perhaps this is better thought of slightly differently: an attribute
  has child nodes. The child nodes can either be leaf nodes, in which
  case they are values, or non-leaf nodes in which case they are
  composite nodes. Composite nodes themselves can have child nodes. If
  they are leaf nodes they are values; if they are non-leaf nodes they
  are either attributes or composites.
- note that we do not need to use shared pointers in composite: we
  could simply have an attribute by value. However, we still need to
  handle the case where the children are either composite or
  attributes. So if we somehow could get composite and attribute to
  have a common base class, we could have a container of that base
  class in composite. For this we would need a shared pointer.
- consider adding the postfix =node= to class names and make it a real
  tree, as per dia's implementation.
- covert all vectors to lists since we do not know their sizes on
  construction.
- one thing to bear in mind is that if we fix the tree structure, we
  will break the XML parsing code in hydrator, which took quite a
  while to get right (and has hacks such as "inner composite").
- its not obvious why we need to treat =dia::string= in a different
  way from all other attribute values (except for =dia::font=).

*** Consider bucketing elements by meta-type in model                 :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

*** Properties vs configuration                                       :story:

Originally we had defined properties to mean things which are computed
and configuration to mean things which are read directly from the
meta-data and not touched afterwards. This made life easier in
determining how each class was used. However, this was not strictly
enforced and now there are many cases where properties are used when
configuration should have been (and probably vice-versa). In addition,
we have cases where we should have used configuration but used nothing
(type parameters springs to mind). We need to do a clean up of the
meta-model.

*** Create a text model post-processing chain                         :story:

The following transforms can be done after generation of the text model:

- clang format
- protected regions: read the file on disk, replace contents of the
  protected region with the data read from disk.

These can be contained in a post-processing chain for the text model.

Note that we need artefacts to have an associated language so that we
can use the correct clang format configuration. If a language is not
supported by clang format (e.g. c#) we should just skip the files. The
text model could group files by language.

*** Postfix and directory fields in annotations look weird            :story:

Why are we manually instantiating postfix and directory for each
formatter/facet instead of using templates?

*** Rename options to transformation request                          :story:

These are not really "options"; it is a request made into yarn to
code-generate a model. We haven't yet got a proper name but it has to
somehow involve the word "request". The best way is to visualise this
as part of some API where may such requests can be made (and handled
concurrently).

This also means we need to split out the request from the context. We
should have an initialisation phase where we construct the context and
then we should be able to reuse the pipeline for many requests. This
also means that the right place to put the transform metrics is in the
request - not the context - given that these are request specific.

The best way to go about it may be to have two contexts:

- transformation context: const; loaded at start-up.
- request context: request specific context, including probing and the
  request itself.

Then:

- clients are responsible for setting up the transformation
  context. This ensures we do it only once.
- clients are also responsible for setting up the request context, but
  they then do it for each request.

Note also that a request should support multiple target models.

*** Detect unqualified stereotypes                                    :story:

If a user enters say =enumeration= instead of =yarn::enumeration= we
are providing an unhelpful error message:

: Error: Attribute type is empty: structured

This is because we validate the class as if it was an object and then
figure out that there are no types against the attributes. One easy
way to make things more useful is to detect unqualified stereotypes
and error straight away with a more useful message such as "did you
mean yarn::xyz?".

We could also do the same if the stereotype is blank ("did you mean
enumeration?").

*** Tidy-up fabric                                                    :story:

Now we have dynamic transforms, we don't really need all the classlets
we've created in fabric. We can get away with probably just the
dynamic transform, calling all the factories.

*** Clean-up archetype locations modeling                             :story:

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*** Use element ids for associations                                  :story:

There doesn't seem a need for having entire names for associations;
these are used to find information by ID anyway. We should try to
convert them to element id's instead and see what breaks.

- transparent, opaque associations
- base, derived visitor
- contained by

We can't do this for:

- visitor: we use the name in the formatter.

Actually there is a reason for this: we use the names to build the
file paths and the includes. We need to add some comments.

*** Add facet validation against language standard                    :story:

With the move of enablement to yarn, we can no longer validate facets
against the language standard. For example, we should not allow
hashing on C++ 98. The code was as follows:

#+begin_src c++
void enablement_expander::validate_enabled_facets(
    const global_enablement_configurations_type& gcs,
    const formattables::cpp_standards cs) const {
    BOOST_LOG_SEV(lg, debug) << "Validating enabled facets.";

    if (cs == formattables::cpp_standards::cpp_98) {
        using formatters::hash::traits;
        const auto arch(traits::class_header_archetype());

        const auto i(gcs.find(arch));
        if (i == gcs.end()) {
            BOOST_LOG_SEV(lg, error) << archetype_not_found << arch;
            BOOST_THROW_EXCEPTION(expansion_error(archetype_not_found + arch));
        }

        const auto& gc(i->second);
        if (gc.facet_enabled()) {
            const auto fctn(gc.facet_name());
            BOOST_LOG_SEV(lg, error) << incompatible_facet << fctn;
            BOOST_THROW_EXCEPTION(expansion_error(incompatible_facet + fctn));
        }
    }

    BOOST_LOG_SEV(lg, debug) << "Validated enabled facets.";
}
#+end_src

It was called from the main transform method in enablement transform,
prior to uptading facet enablement.

*** Tidy-up assistant API                                             :story:

Now we have element in assistant we can start removing the need for
element in the calls, making the templates simpler.

*** Facets incompatible with standards                                :story:

Some facets may not be supported for all settings of a language. For
example the hash facet is not compatible with C++ 98. We need to have
some kind of facet/formatter level validation for this.

*** Handcrafted templates                                             :story:

At present we generate constructors, swap, etc. for handcrafted
classes. Ideally users should be able to create a profile that enables
the things they want to see on a template and then associate it with a
stereotype. For this we will need aspect support.

*** Drop the original extension in tailor                             :story:

Filenames in tailor look weird:

: dart.dia.json

it should just be:

: dart.json

*** Move dependencies into yarn                                       :story:

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*** Consider folding quilt into yarn                                  :story:

In the far distant future, when we finally finish merging all the
quilt specific stuff into yarn (e.g. formattables), it actually makes
sense to deprecate quilt as a concept. Yarn then becomes the central
point, and frontends and backends are just implementations that hook
into it. Thus we then have simply =yarn.cpp= and =yarn.csharp=.

However, there is still a concept that needs to be captured: the
kernel. That is, a set of backends that work together to provide some
kind of "service". In quilt's case the basic type definitions. We
could potentially want to implement other backends that are totally
distinct from quilt. However, we still do not have a concrete use case
for this. Thus it may make more sense to just fold now and worry about
these more flexible use cases when they arrive. We can always rename.

*** Code-generate annotations type templates                          :story:

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it. Name: =yarn::annotation_type_template=?
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** Cannot make qualified references to concepts                      :story:

At present it is not possible to consume concepts defined in a
referenced model, nor is it possible to refer to a concept in a
different module from the module in which the element is in, e.g.: say
concept C0 is declared in module M0; all types of M0 can have C0 as
stereotype and that will resolve. However any types on any other
module cannot see the concept.

One suggestion is to allow scoped names in stereotypes:
=module::Concept=.

The heuristic for concept resolution is then:

- external modules are never part of the scoped name;
- on a scoped concept with M names, we first start by assuming that
  the first name is the model module and M-2 is/are the internal
  module(s). We try this for all names in M-2, e.g. first two names
  are model modules and M-3 names are internal modules and so forth.

*** Add support for object templates that work cross-model            :story:

We've implemented support for cross-model inheritance in sprint 87 but
we did not cover object templates. Most of the approach is the same,
but unfortunately we can't just reuse it.

Tasks:

- we need a refines field which is a text collection.
- we need refinement settings, factory etc.
- update parsing expander.

*** Move formatting styles into yarn                                  :story:

We need to support the formatting styles at the meta-model level.

*** Throw on unsupported stereotypes                                  :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

*** Change order of includes according to Lakos major design rule     :story:

Lakos says:

#+begin_quote
The .c file of every component should include its own .h file as the
first substantive line of code.
#+end_quote

We decided to include it as the last line. However, Lakos approach has
the side-effect of automatically detecting headers that are missing
includes. We used to do this manually by generating =.cpp= files that
just included the header but then had to remove it because it was
slowing down compilation. With Lakos approach we get the best of both
worlds.

We need to also update the generated code to follow this
approach. This will require some thinking.

*** Move element segmentation into yarn                               :story:

We've added the notion that an element can be composed of other
elements in quilt, in order to handle forward declarations. However,
with a little bit of effort we can generalise it into yarn. It would
be useful for other things such as inner classes. We don't need to
actually implement inner classes right now but we should make sure the
moving of this feature into yarn is compatible with it.

Notes:

- seems like we have two use cases: a) we need all elements, master
  and extensions and we don't really care about which is which. b) we
  only want masters. However, we must be able to access the same
  element properties from either the master or the extension. Having
  said all that, it seems we don't really need all of the element
  properties for both - forward declarations probably only need:
  decoration and artefact properties.
- we don't seem to use the map in formattables model anywhere, other
  than to find master/extension elements.
- Yarn model could have two simple list containers (masters and
  all). Or maybe we don't even need this to start off with, we can
  just iterate and skip extensions where required.
- so in conclusion, we to move decoration, enablement and dependencies
  into yarn (basically decoration and artefact properties) first and
  then see where segmentation ends.

Tasks:

- add a concept for element extensions: =Extensible=. Contains a list
  of element pointers.
- populate it with the extensions.
- change enablement to merge all element properties of extensible
  elements.

*** Create a yarn locator                                             :story:

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator. It will live in fabric.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

*** Add a modeline to stitch                                          :story:

It would be nice to be able to supply the mode and other emacs
properties to stitch templates. For that we just need a special KVP
used at the top that contains the modeline:

: <#@ modeline="-*- mode: poly-stitch; tab-width: 4; indent-tabs-mode: nil; -*-" #>

Stitch can read this KVP and ignore it.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

*** Move helpers into yarn                                            :story:

Looking at helpers, it is clear that they are common to all
languages. We just need to rename the terminology slightly -
particularly wrt to streaming properties - and then move this code
across into yarn.

*** Move facet properties into yarn                                   :story:

We should be able to handle these generically in yarn.

*** Move ORM camel-case and databases into yarn                       :story:

We should handle this property at the ORM level, rather than at the
ODB level.

Similarly, we should move the ODB databases into yarn and make that a
ORM-level concept.

*** Distinguish between meta-types that require canonical archetypes  :story:

At present it is not possible to know which meta-types require
canonical archetypes and which don't. In the validation we said:

:         * We must have one canonical formatter per type per facet.
:         * FIXME: this check is broken at the moment because this is
:         * only applicable to yarn types, not fabric types. It is also
:         * not applicable to forward declarations. We need some
:         * additional information from yarn to be able to figure out
:         * which types must have a canonical archetype.

We should have some kind of flag in yarn to distinguish. This still
requires a bit of thinking.

*** Tidy-up of inclusion terminology                                  :story:

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Add support for qualified class names in dia                      :story:

#+begin_quote
*Story*: As a dogen user, I don't want to have to define packages in
certain cases.
#+end_quote

It has become apparent that creating large packages in dia and placing
all classes in a large package is cumbersome:

- there are issues with the large package implementation in dia,
  making copying and pasting a dark art; its not very obvious how one
  copies into a package (e.g. populating the child node id correctly).
- models do not always have a neat division between packages; in
  dogen, where packages would be useful, there are all sorts of
  connections (e.g. inheritance, association) between the package and
  the model "package" or other packages. Thus is very difficult to
  produce a representative diagram.

A solution to this problem would be to support qualified names in
class names; these would be interpreted as being part of the current
model. One would still have to define a large package, but it could be
empty, or contain only the types which only have connections inside
the package, plus comments for the package, etc.

** Deprecated
*** CANCELLED Move some of the more verbose logging to trace          :story:
    CLOSED: [2017-11-30 Thu 22:41]

We have a category for finer debug logging (=TRACE=) but we are not
making use of it. There is some rather verbose logging that could be
moved to it. Go through all the logging and move some to =TRACE=.

One strategy would be to put in the final object of each workflow as
=DEBUG= (say the expanded model, etc) but the intermediate steps as
=TRACE=. This mirrors the way we investigate the problem: we
could check if each sub-system has done it's job correctly, and spot
the one that didn't; we can then just enable that one sub-system's
=TRACE= (when that is supported).

We probably should only do this at the end, as we want to make sure
that the code generator is usable with full logging on. Or perhaps set
the default to =TRACE=. We should also add a command line option,
perhaps really verbose or extra verbose.

*** CANCELLED Create a "utility" model like formatters for frontends  :story:
    CLOSED: [2017-11-30 Thu 22:42]

We have a number of utilities that are common to several backends,
similar to what happened to formatters. We should probably extract
those into a common model. At present we have:

- =identifier_parser=: in dia to sml but should also be used from JSON
when we support full models.
- "method identifier": this will be used by the merger to identify
methods and to link them back to language specific methods. Not
quite frontend, but not far.
*** CANCELLED Remove new lines from all text to be logged             :story:
    CLOSED: [2017-11-30 Thu 22:43]

We should strive to write to the log one line per "record". This makes
grepping etc much easier. We should create a method to convert new
lines to a marker (say =<new_line>= or whatever we are already doing
for JSON output). This should be applied to all cases where there is a
potential to have new lines (comments, etc).

*** CANCELLED Remove references to namespace when within namespace    :story:
    CLOSED: [2017-11-30 Thu 22:44]

Due to moving classes around, we seem to have lots of cases where code
in a namespace (say =sml=) refers to types in that namespace with
qualification (say =sml::qname=). We need to do a grep in each project
to look for instances of a namespace and ensure they are valid.

*** CANCELLED Use diagram files to setup test models in cmakefile     :story:
    CLOSED: [2017-11-30 Thu 22:48]

In the CMakeLists for the test models we are already looping through
all the diagrams:

: foreach(dia_model ${all_dia_test_models})

We should take advantage of this to define =include_directories= and
=add_subdirectory=. At present we are doing these manually.

*** CANCELLED Setup containing module correctly in mock factory       :story:
    CLOSED: [2017-11-30 Thu 22:49]

We did not update the yarn mock model factory to populate the
containing type. We also did not setup the members of the module.
*** CANCELLED Make features optional at compile time                  :story:
    CLOSED: [2017-11-30 Thu 22:50]

#+begin_quote
*Story*: As a dogen user, I want to ignore all facets in a model that
I don't need so that I don't have to install unnecessary third-party
dependencies.
#+end_quote

One scenario we haven't accounted for is for compile time
optionality. For example, say we have several serialisation facets,
all of them useful to a general model; however, individual users of
that model may only be interested in one of the several
alternatives. In these cases, users should be able to opt out from
compiling some of the facets and only include those that they are
interested in. This is different from the current optionality we
support in that we allow the user to determine what to code
generate. In this case, the mainline project wants to code generate
all facets, but the users of the model may choose to compile only a
subset of the facets.

To implement this we need a trait - say =optional= - that when set
results in a set of macros that get defined to protect the facet. The
user can then pass in that macro to cmake to disable the facet. This
is not the same as the "feature" macros we use for ODB and EOS. These
are actually not Dogen macros, just hand-crafted macros we put in to
allow users to compile Dogen without support for EOS and ODB.

The macros should follow the standard notation of =MODEL.FACET= or
perhaps =MODEL.FACET.FEATURE=, e.g. =cpp.boost_serialization= to make
the whole of serialisation optional or
=cpp.boost_serialization.main_header= to make the header optional. Not
sure if the latter has any use.

*** CANCELLED Move test model diagrams into main diagrams directory   :story:
    CLOSED: [2017-11-30 Thu 22:52]

For some reason - lost in the mists of time - we decided to split the
test model diagrams from the main models; the first is in the =diagrams=
directory, the latter is in the rather non-obvious location of
=test_data/dia_sml/input/=. All source code goes into =projects=
though, so this seems like a spurious split. Also, the test data
directory should really only have data that we generate as part of
testing (e.g. where there is a pairing of expected and actual) and
the test model diagrams are not of this kind - we never output dia
diagrams, at least at present.

The right thing to do is to move them into the =diagrams=
directory. This is not an easy undertaking because:

- there is hard-coding in the test model sets pointing to these
- the CMake scripts rely on the location of the diagrams to copy them
  across

We should create =production= and =test= sub-directories for
diagrams. Or we could just create a sub-directory of test models like
we did in projects.

*** CANCELLED Forward declaration is not always correct for services  :story:
    CLOSED: [2017-11-30 Thu 22:53]

In cases where we used a service as a way of declaring a stand alone
function (such as the traversals in yarn), the forward declarations do
not match the header file at all. In this cases we should use
=nongeneratable= rather than =service= stereotypes, and perhaps when
that happens we should switch off forward declarations?

In addition, in some cases we may want to use a =struct= rather than a
=class=. At present we are always forward declaring as =class= but
sometimes declaring as =struct=.

*** CANCELLED Refactor node according to composite pattern in dia to sml :story:
    CLOSED: [2017-11-30 Thu 22:54]

This is not required if we decide to [[*Add%20composite%20stereotype][implement]] the composite
pattern. We should just follow the composite pattern.

*** CANCELLED Use dogen models to test dogen                          :story:
    CLOSED: [2017-11-30 Thu 22:54]

We should really use the dogen models in the dogen unit tests. The
rationale is as follows:

- if somebody changes a diagram but forgets to code generate, we want
  the build to break;
- if somebody changes the code generator but forgets to regenerate all
  the dogen models and verify that the code generator still works, we
  want the build to break.

This will cause some inconvenience during development because it will
mean that some tests will fail until a feature is finished (or that
the developer will have to continuously rebase the dogen models), but
the advantages are important.
*** CANCELLED Adding new knit tests is hard                           :story:
    CLOSED: [2017-12-01 Fri 11:41]

In order to test models at the knit level one needs to first generate
the dia input. This can be done as follows:

: ./dogen_knitter --save-dia-model xml --stop-after-merging
: -t ../../../../dogen/test_data/dia_sml/input/boost_model.dia

From the bin directory. We need to make these steps a bit more
obvious. Why do we even need this?

*** CANCELLED Check if we've replaced =assert_object= with =assert_file= :story:
    CLOSED: [2017-12-01 Fri 11:42]

Assert file is now able to do intelligent comparisons based on the
extension of the file. From a cursory look, all the usages we have of
assert object can be replaced by assert file. If that's the case we
can also remove this function.

*** CANCELLED Replace old style for iterations in IO                  :story:
    CLOSED: [2017-12-01 Fri 11:43]

At present we are still doing C++-03 iterations in the STL IO files
such as =vector_io=, =list_io=, etc. We should be using the new =for=
syntax for C++-11.
