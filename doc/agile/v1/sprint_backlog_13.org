#+title: Sprint Backlog 13
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- complete the testing story to ensure we have good coverage for model
  changes.
- start work on moving decorations and profiles to metamodel.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-03-14 Thu 10:15]
| <75>                                                     |         |       |      |       |
| Headline                                                 | Time    |       |      |     % |
|----------------------------------------------------------+---------+-------+------+-------|
| *Total time*                                             | *28:03* |       |      | 100.0 |
|----------------------------------------------------------+---------+-------+------+-------|
| Stories                                                  | 28:03   |       |      | 100.0 |
| Active                                                   |         | 28:03 |      | 100.0 |
| Edit release notes for previous sprint                   |         |       | 2:22 |   8.4 |
| Sprint and product backlog grooming                      |         |       | 2:16 |   8.1 |
| Create a video demo for the sprint's features            |         |       | 3:47 |  13.5 |
| Make projects directory env optional                     |         |       | 0:10 |   0.6 |
| Clang-cl builds are failing due to memory leak reporting |         |       | 0:45 |   2.7 |
| Add tests for tracing, reporting and diffing             |         |       | 5:46 |  20.6 |
| Delete extra files appears broken at present             |         |       | 6:26 |  22.9 |
| Code generation of tests for dogen models                |         |       | 3:33 |  12.7 |
| Empty directories should be deleted                      |         |       | 2:58 |  10.6 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2019-03-11 Mon 10:44]
    :LOGBOOK:
    CLOCK: [2019-03-12 Tue 08:11]--[2019-03-12 Tue 08:42] =>  0:31
    CLOCK: [2019-03-11 Mon 16:18]--[2019-03-11 Mon 16:22] =>  0:04
    CLOCK: [2019-03-11 Mon 16:07]--[2019-03-11 Mon 16:17] =>  0:10
    CLOCK: [2019-03-11 Mon 09:07]--[2019-03-11 Mon 10:44] =>  1:37
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.12, "Estádio"

#+begin_src markdown
![Estádio](https://imgs.sapo.pt/jornaldeangola/img/thumb1/20181222123311moraris.jpg)
_Estádio Joaquim Morais, Moçamedes, Namibe. [(C) 2018 Jornal de Angola](http://jornaldeangola.sapo.ao/desporto/joaquim_morais__beneficia_de_obras__de_restauracao)_.

# Overview

With Sprint 12 we are hoping to finally return to a regular release cadence. This was a much more predictable two-week sprint, which largely delivered on the sprint's mission statement of cleaning up the mess of refactors and reactivating system testing. As such, it was not a particularly exciting sprint in terms of end user features, but still got us very excited because we are finally paying off years of technical debt in a manner that respects established MDE theory.

# Internal Changes

The key internal changes are described in the next sections.

## Complete the orchestration refactor

We have now finally got a proper pipeline of tasks, with well-defined roles and terminology:

- **injection**: responsible for importing external models into MASD. The name "injection" comes from the MDE concept of injecting external technical spaces into a technical space.
- **coding**: meta-model responsible for modeling software engineering entities.
- **generation**: meta-model responsible for the expansion into facets, providing a multidimensional extension to the coding model. The role of generation is to get the meta-model as close as possible to the requirements of code-generation.
- **extraction**: responsible for extracting a model out of MASD into an external technical space. Again, the name "extraction" comes from the MDE notion of extracting content from one technical space into another.

The biggest advantage of this architecture is that we now have a simple pipeline of transformations, taking us from the original external model into the final generated code:

![Processing pipeline](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/blog/images/orchestration_pipeline.png)

This orchestration pipeline is conceptually similar to the architecture of a compiler, and each of these high-level transforms can be thought of as a "lowering phase" where we move to lower and lower levels of abstraction. However, for a proper technical explanation of the approach you'll have to wait for the PhD thesis to be published.

This work has enabled us to do a number of important clean ups, such as:

- core models now have a uniform structure, with distinct meta-models, transform-sets and transform contexts. We don't have special cases any more.
- all of the mix-and-match processing that occurred inside of the coding model is now gone (e.g. injection work, extraction work, etc).
- the creation of the extraction transform pipeline made things significantly easier to implement features such as diffing and the dry run mode (see user visible changes).

## Reactivate all system tests

One of the biggest problems we faced of late has been a lack of adequate testing. Whilst we were experimenting with the architecture, we had to disable all system tests as they became completely out of sync with the ([admittedly crazy](http://mcraveiro.blogspot.com/2018/01/nerd-food-refactoring-quagmire.html)) experiments we were carrying out. However, before we can enter the last few refactors, we desperately needed to have system tests again.

This sprint saw a lot of infrastructural work to enable a more sensible approach to system testing; one that takes into account both reference models (C++ and C#) as well as using dogen's own models. In order to make this practical, we ended up having to improve the conversion of Dia models into JSON as well. On the plus side, our code coverage has experienced a marked uptick:

![Coverage over time](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/blog/images/code_coverage_after_system_tests.png)

However, analysis reveals that we are still not testing a fair bit of the generated code, so next sprint the objective is to close the gap further in code coverage and testing.

# User visible changes

The key user visible changes are described in the next sections. In addition, we've finally got round creating a video to demo the user visible features added in this sprint:

[![Sprint 1.0.12 Demo](https://img.youtube.com/vi/Tt34P3JXzeE/0.jpg)](https://www.youtube.com/watch?v=Tt34P3JXzeE)

Hopefully this will become a habit from now on.

## Improvements on handling of references

There were two key changes on how references are processed. First, we no longer automatically include system models. From now on, these are treated just like any other model and must be included manually. As an example, a C++ model using the STL, C++ built-in types and boost would now need to have the following references:

```
#DOGEN masd.injection.reference=cpp.builtins
#DOGEN masd.injection.reference=cpp.std
#DOGEN masd.injection.reference=cpp.boost
```

Whilst it is a bit of an inconvenience to have to add these to every other model (specially ```builtins``` and ```std```), this does mean that there are now no special cases and no need for "speculative processing" of models. In the past we loaded all system models and there was a lot of extra logic to determine which ones where needed by whom (e.g. do not load C# system models for a C++ model, but maybe load it for a LAM model, etc). We have now placed the onus of determining what should be loaded onto the user, who knows what models to load.

A second related change is that references are now transitive. This means that if model A depends on model B which depends on model C, you no longer need to add a reference to model C in model A as you had to in the past; the reference from model B to model C will be honoured. Sounds like a trivial change, but in reality this was only possible because of the move towards a simplified pipeline (as outlined in the previous section).

## Dry-run mode

One of the biggest annoyances we've had is the need to code generate in order to see what _would_ change. The problem with C++ is that, if the generated code is not what you'd expect - a fairly common occurrence when you are developing the code generator, as it turns out - you end up with a large number of rebuilt translation units for no good reason. Thus we copied the idea from vcpkg and others of a "dry-run mode": in effect, do all the transforms and produce all the generated code, but don't actually write it to the filesystem. Of course, the logical conclusion is that some kind of diffing mechanism is required in order to see what would change. For this we relied on the nifty [Diff Template Library](https://github.com/cubicdaiya/dtl), which provides a very simple way of producing unified diffs from C++. Sadly it was not on vcpkg, but the most excellent vcpkg developers responded [quickly to our PR](https://github.com/Microsoft/vcpkg/pull/5541), so you if you'd like to use it, you can now simply ```vcpkg install dtl```.

As a result, with a fairly simple incantation, you can now see what dogen would like to do to your current state. For example, say we've updated the comment for ```property``` attribute of the ```hello_world.dia``` test model; to check our changes, we could do:

```
$ ./masd.dogen.cli generate --target hello_world.dia --dry-run-mode-enabled --diffing-enabled --diffing-destination console
diff -u include/dogen.hello_world/types/one_property.hpp include/dogen.hello_world/types/one_property.hpp
Reason: Changed generated file.
---  include/dogen.hello_world/types/one_property.hpp
+++  include/dogen.hello_world/types/one_property.hpp
@@ -33,7 +33,7 @@

 public:
     /**
-     * @brief This is a sample property.
+     * @brief This is a sample property. Test diff
      */
     /**@{*/
     const std::string& property() const;
```

Whilst the arguments required may appear a bit excessive at this point, we decided to roll out the feature as is to gain a better understanding of how we use it. We will then clean up the arguments as required (for example, should dry run mode default to ```--diffing-enabled --diffing-destination console```?).

As an added bonus, if you choose to output to file instead of console, we generate a patch file which can be patched on the command line via ```patch```. We don't have a particular use case for this as of yet, but it just seems nice.

## Reporting

A feature that is related to dry-run mode is reporting. We originally merged the two together but then realised that reporting might be useful even when you don't require a diff or a dry run, so we ended up implementing it stand alone. Reporting provides an overview of the operations dogen performed (or would have performed, if you are in dry run mode) to your file system. And, as with tracing, you can visualise it on org mode, making it really easy to navigate if you are a vi or emacs user:

![Reporting in org-mode](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/blog/images/dogen_reporting_mode_org_mode.png)

However, if you'd like to grep for specific types of operations, you can use the plain report instead:

![Reporting in plain text](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/blog/images/dogen_reporting_mode_plain.png)

To enable reporting, simply do:

```
./masd.dogen.cli generate --target hello_world.dia --dry-run-mode-enabled --reporting-enabled --reporting-style org-mode
```

Replacing ```org-mode``` with ```plain``` as required.

## Byproducts directory

Even before the advent of diffing and reporting, we were already generating a large number of non-code related files, all of which were fairly randomly placed in the filesystem. With this release, we just couldn't continue with this approach so, instead, all of the non-generated files are now created under a "byproducts" directory. This includes:

- log files
- traces
- diff reports, when outputting to file
- reports

And any future functionality we may add. This means that you can now safely delete the byproducts directory and know that you have got rid of all files. We write to ```masd.dogen.byproducts``` by default, but if you'd like to place it elsewhere, use ```--byproduct-directory```. The directory is organised by "run identifier", allowing you to generate multiple models into the same directory:

```
$ tree
.
├── cli.generate.hello_world.dia
│   ├── cli.generate.hello_world.dia.log
│   ├── hello_world_report.org
│   └── hello_world_report.txt
├── tests.code_generation.masd.dogen.annotations.dia
│   ├── annotations.patch
│   └── annotations_report.org
├── tests.code_generation.masd.dogen.annotations.json
│   ├── annotations.patch
│   └── annotations_report.org
```

## Graph of Transforms

A minor feature that was added this sprint was the ability to print a [GraphViz](https://www.graphviz.org/) graph of transforms. This is done by exporting tracing information with the dot format, e.g.:

```
./masd.dogen.cli   generate --target hello_world.dia --dry-run-mode-enabled --reporting-enabled --reporting-style plain  --log-enabled  --tracing-enabled --tracing-format graphviz
```

The output can then be post processed with dot to generate a PDF:

```
$ cd masd.dogen.byproducts/cli.generate.hello_world.dia/tracing/
$ dot -Tpdf transform_stats.dot -O
```

The PDF is quite large because the transform graph is getting extremely complex. This small sample is representative of the output:

![Graph of transforms](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/graph_of_transforms.png)

## Other

As usual, for more details of the work carried out this sprint, see [the sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_12.org).

# Next Sprint

Now that we have the testing in place, our key objective for next sprint is to move all of the decoration related code into the meta-model. This means that much of what currently exists as assorted files that dogen loads on startup would become regular model entities, paving the way for a much more configurable model.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.12_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.12/dogen_1.0.12_amd64-applications.deb)
- [dogen-1.0.12-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.12/dogen-1.0.12-Darwin-x86_64.dmg)
- [dogen-1.0.12-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/dogen-1.0.11-Windows-AMD64.msi)

**Note**: There was a bug in windows builds; the binaries are incorrectly labelled as the previous release.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/1105141000589193216][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6506470333200023552][LinkedIn]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2019-03-12 Tue 08:43]--[2019-03-12 Tue 08:56] =>  0:13
    CLOCK: [2019-03-11 Mon 18:46]--[2019-03-11 Mon 19:07] =>  0:21
    CLOCK: [2019-03-11 Mon 08:02]--[2019-03-11 Mon 08:53] =>  0:51
    CLOCK: [2019-03-11 Mon 07:15]--[2019-03-11 Mon 07:24] =>  0:09
    CLOCK: [2019-03-11 Mon 06:44]--[2019-03-11 Mon 07:14] =>  0:30
    CLOCK: [2019-03-11 Mon 06:31]--[2019-03-11 Mon 06:43] =>  0:12
    :END:

 Updates to sprint and product backlog.

*** COMPLETED Disable global hashing on coding                        :story:
    CLOSED: [2019-03-11 Mon 06:47]

*Rationale*: already implemented.

We are generating hash for all types at present in coding but we only
need it for two types: name and location. Try to switch it off
globally and on just for those two types.

*** COMPLETED JSON models in dogen are out of sync                    :story:
    CLOSED: [2019-03-11 Mon 06:47]

*Rationale*: already done and won't happen again after changes to
system tests.

Problems:

- tailor generation results in files with the wrong name (=dia.json=)
- input models were copied into test data.

*** COMPLETED Contents change check is done twice                     :story:
    CLOSED: [2019-03-11 Mon 06:56]

*Rationale*: moving away from writer.

We seem to check twice if a file has changed:

: 2015-04-26 12:37:28.451464 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.
: 2015-04-26 12:37:28.451486 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.

This is in stitch but it should be the same for knit.

*** COMPLETED Add reporting support to dogen model testing            :story:
    CLOSED: [2019-03-11 Mon 07:01]

*Rationale*: whilst we didn't implement exactly this vision, the work
on the byproduct directory is almost like this.

Dogen should have a mode which generates a report for a run rather
than code generate. The report could look like so:

:              /project_a
:                  /summary for this commit
:                  /diffs
:                  /errors
:                  /benchmark data
:                  /probing data
:                  /log

If the report was largely in HTML we could link it to the dogen docs
and save it into git. This would make troubleshooting much easier. If
the report contains the probing data it would be easier to figure out
what went wrong. We should also keep track of the model that was
generated (e.g. its location and git commit) so we can download it and
reproduce it locally.

*** COMPLETED Load system models based on language prefix             :story:
    CLOSED: [2019-03-11 Mon 07:19]

*Rationale*: this is no longer a requirement now that all models must
be loaded explicitly from the reference list.

We used a convention for system models that have the language as a
prefix:

: cpp.boost.json
: cpp.builtins.json
: cpp.std.json
: csharp.builtins.json
: csharp.system.collections.generic.json
: csharp.system.collections.json

Coincidentally, this could make life easier when it comes to filtering
models by language: we could pattern match the file name depending on
the language and only load those who match. The convention would then
become a rule for system models. With this we would not have to load
the models, process annotations, etc just to get access to the
language.

*** COMPLETED Feature models should always be tested by knit           :epic:
    CLOSED: [2019-03-11 Mon 08:10]

*Rationale*: the new system tests approach should take care of this.

#+begin_quote
*Story*: As a dogen user, I want to be sure that every feature is
comprehensively tested so that I don't have to worry about dogen bugs
when using it.
#+end_quote

We recently implemented features into dogen; these work off of CMake
detection, where by if a library is not detected, all tests associated
with it are not built and executed. However, we should still try to
codegen these models to make sure that a change we did elsewhere did
not introduce bugs in features we're not interested in. We need to
check that knit has tests for both EOS and ODB that get executed
regardless of these features being on or off.

*** COMPLETED Check packaging code for non-distro dependencies        :story:
    CLOSED: [2019-03-11 Mon 08:11]

*Rationale*: boost is statically built now so this should not be a
problem.

We are manually copying a lot of shared objects from locally built
third party libraries when creating packages, this should be replaced
with appropriate dependencies (at least for Debian packages).

*** COMPLETED Use xtime-like stopwatch in selected places to log timings :story:
    CLOSED: [2019-03-11 Mon 08:12]

*Rationale*: this was implemented as part of the tracing framework.

We should log the time it takes for certain operations in dogen so
that users can figure out if we are becoming slower (or faster) at
doing them and report regressions.

Boost used to provide a nifty little utility class called xtime. It
appears to have been deprecated by [[http://www.boost.org/doc/libs/1_55_0/doc/html/chrono/users_guide.html#chrono.users_guide.examples.duration.xtime_conversions][chrono]].

We should also provide a command line option that prints a timing
report. This would be useful so that users can compare timings between
releases.

We should also be able to grep the log for all timings and save them
down to get trends. We should add a log severity for this, perhaps
PROFILE. Not sure what priority it would be at.

We should also be able to get a command-line report, e.g. =--profile=
would show all the timings for all the components.

It should also be possible to support some kind of uploading of
metrics to a metrics server with a database etc.

*** COMPLETED Re-enable schema updates in database model              :story:
    CLOSED: [2019-03-11 Mon 08:49]

*Rationale*: fixed in northwind tests.

We are deleting the entire DB schema and re-applying it for every
invocation of the tests. This does not work on a concurrent world. We
commented it out for now, but we need a proper solution for this.

*** COMPLETED Test model sanity checks fail for enable facet serialisation :story:
    CLOSED: [2019-03-11 Mon 08:51]

*Rationale*: this was addressed some time ago as the test model is up
and running.

For some reason we are unable to compile the serialisation test for
the test model which focuses only on the serialisation facet. Test is
ignored for the moment.

*** COMPLETED Create a video demo for the sprint's features           :story:
    CLOSED: [2019-03-11 Mon 16:06]
    :LOGBOOK:
    CLOCK: [2019-03-11 Mon 15:55]--[2019-03-11 Mon 16:06] =>  0:11
    CLOCK: [2019-03-11 Mon 14:32]--[2019-03-11 Mon 14:50] =>  0:18
    CLOCK: [2019-03-11 Mon 12:34]--[2019-03-11 Mon 14:31] =>  1:57
    CLOCK: [2019-03-11 Mon 11:08]--[2019-03-11 Mon 12:07] =>  0:59
    CLOCK: [2019-03-11 Mon 10:45]--[2019-03-11 Mon 11:07] =>  0:22
    :END:

Our video is extremely old and misleading. We need to get back into
the habit of doing a video demo at the end of every sprint talking
about the work of the sprint.

*** COMPLETED Implement the new dogen product API                     :story:
    CLOSED: [2019-03-11 Mon 19:03]

*Rationale*: this was done as part of the CLI work.

Now the API has been designed and generated, we need to implement it.

*** COMPLETED Make projects directory env optional                    :story:
    CLOSED: [2019-03-12 Tue 09:36]
    :LOGBOOK:
    CLOCK: [2019-03-12 Tue 09:26]--[2019-03-12 Tue 09:36] =>  0:10
    :END:

We are now stopping the build if the projects directory is not
defined:

: * Starting C++ build.
: -- CMake Version: 3.13.4
: CMake Error at CMakeLists.txt:35 (message):
:  MASD_DOGEN_PROJECT_DIRECTORY env variable not defined

This means that a user that just wants to compile dogen out of git
will now be stuck trying to figure out what this is. In reality the
projects directory for Dogen is always known to CMake. We should just
set it from CMake.

*** COMPLETED Clang-cl builds are failing due to memory leak reporting :story:
    CLOSED: [2019-03-12 Tue 12:45]
    :LOGBOOK:
    CLOCK: [2019-03-11 Mon 16:23]--[2019-03-11 Mon 17:08] =>  0:45
    :END:

It seems our clang-cl debug builds are taking longer and longer due to
some memory leaks. The leaks are showing on MSVC as well. We are not
always exceeding maximum build time, so sometimes it goes unnoticed.

We've managed to ignore the leaks for now. Once we have cleared up all
of the valgrind warnings we need to get a windows development
environment to investigate these properly.

Links:

- [[https://docs.microsoft.com/en-us/visualstudio/debugger/finding-memory-leaks-using-the-crt-library?view=vs-2017][Find memory leaks with the CRT library]]
- [[https://social.msdn.microsoft.com/Forums/vstudio/en-US/0e6746b9-b042-4402-84ba-d3e38a65a6f4/how-to-disable-memory-leaks-dumping-in-ms-vs?forum=vsdebug][How to disable Memory leaks dumping in MS VS?]]
- [[https://github.com/SaschaWillems/Vulkan/issues/111][Replace this code at WinMain() to enable memory checks on windows
  builds]]

*** COMPLETED Add tests for tracing, reporting and diffing            :story:
    CLOSED: [2019-03-13 Wed 10:03]
    :LOGBOOK:
    CLOCK: [2019-03-13 Wed 13:15]--[2019-03-13 Wed 13:31] =>  0:16
    CLOCK: [2019-03-13 Wed 10:04]--[2019-03-13 Wed 10:28] =>  0:24
    CLOCK: [2019-03-13 Wed 09:32]--[2019-03-13 Wed 10:03] =>  0:31
    CLOCK: [2019-03-13 Wed 08:10]--[2019-03-13 Wed 09:25] =>  1:15
    CLOCK: [2019-03-13 Wed 06:24]--[2019-03-13 Wed 07:18] =>  0:54
    CLOCK: [2019-03-12 Tue 15:37]--[2019-03-12 Tue 17:40] =>  2:03
    CLOCK: [2019-03-12 Tue 18:15]--[2019-03-12 Tue 18:38] =>  0:23
    :END:

At present its easy to break tracing and reporting without noticing
it. Add a simple set of tests that verify the existence of the files
and perform some basic sanity checks on the content.

*** COMPLETED Delete extra files appears broken at present            :story:
    CLOSED: [2019-03-13 Wed 17:57]
    :LOGBOOK:
    CLOCK: [2019-03-13 Wed 16:04]--[2019-03-13 Wed 17:57] =>  1:53
    CLOCK: [2019-03-13 Wed 15:27]--[2019-03-13 Wed 16:03] =>  0:36
    CLOCK: [2019-03-13 Wed 13:32]--[2019-03-13 Wed 15:26] =>  1:54
    CLOCK: [2019-03-13 Wed 10:35]--[2019-03-13 Wed 11:59] =>  1:24
    CLOCK: [2019-03-13 Wed 10:29]--[2019-03-13 Wed 10:34] =>  0:05
    CLOCK: [2019-03-12 Tue 15:20]--[2019-03-12 Tue 15:36] =>  0:16
    CLOCK: [2019-03-12 Tue 12:41]--[2019-03-12 Tue 12:59] =>  0:18
    :END:

Can't find any evidence of code in extraction to handle the case where
the flag is set to false.

Notes:

- implement it in terms of the existing operations, e.g. set it to
  ignore, reason user requested not to delete extra files .
- add test that validates the flag on and off. No need to check the
  deletion itself, we can trust remove files transform.

*** STARTED Code generation of tests for dogen models                 :story:
    :LOGBOOK:
    CLOCK: [2019-03-12 Tue 11:56]--[2019-03-12 Tue 12:04] =>  0:08
    CLOCK: [2019-03-12 Tue 11:27]--[2019-03-12 Tue 11:55] =>  0:28
    CLOCK: [2019-03-12 Tue 10:58]--[2019-03-12 Tue 11:26] =>  0:28
    CLOCK: [2019-03-12 Tue 09:27]--[2019-03-12 Tue 10:57] =>  1:30
    CLOCK: [2019-03-12 Tue 08:57]--[2019-03-12 Tue 09:26] =>  0:29
    CLOCK: [2019-03-11 Mon 17:17]--[2019-03-11 Mon 17:47] =>  0:30
    :END:

At present we are manually generating tests for each model
(serialisation, etc). The structure of the tests is very
predictable. In a world where tests are a facet, we could have some
options to control the generation of tests. This would also allow end
users to generate tests for their models and report the results. We
would need to generate the utility model for this - or perhaps we
could code generate tests in a way that no longer requires templates -
its all "hard-coded". This would make the tests easier to follow, but
we would generate a lot of code.

We could separate dogen specific tests from user tests by naming them
differently, e.g. =abc_dogen_test.cpp=. We can then create two
different test binaries, one for dogen tests and another for user
tests, so that users don't have to run dogen tests unless something
has gone wrong.

Interestingly we could even set rules to ignore tests that are known
to fail:

- if object has no members do not do equality tests
- if object has some kind of recursion do not do tests
- etc.

These can be marked as known limitations. At present the tests require
Boost.Test but it should be possible to target other frameworks
(meta-data option).

Notes:

- we've bumped into a problem: at present we created a number of
  profiles that are used by test models to enable and disable facets,
  as required by the tests. This means that in order to setup the new
  facet, we will have to update all of these profiles manually until
  the tests are ready to be tested. As a quick hack, we've disabled
  the facet from the dogen profile.

Merged stories:

*Consider creating a "test" facet*

Whilst we can't really generate tests, we can at least create the
stubs for them. For this we could have a =test= facet that uses a
stereotype, e.g. =test_suite=. Users mark classes with
these. Attributes are the test cases. At the model level users can
choose the test framework. For example for Boost.Test, it generates
the main file with fixture initialisation, etc. We could then have one
of two approaches:

- protected regions, where the test contents are protected and perhaps
  an area at the top for globals etc.
- stubs only, were we generate the original content but then users
  subsequently manage the files.

*Canned tests rely on copy constructors rather than cloning*

If an object has pointers, the canned tests will not perform a deep
copy of the object. We need to [[*Add%20support%20for%20object%20cloning][implement cloning]] and then use it in
canned tests.

*** STARTED Empty directories should be deleted                       :story:
    :LOGBOOK:
    CLOCK: [2019-03-14 Thu 09:59]--[2019-03-14 Thu 10:15] =>  0:16
    CLOCK: [2019-03-14 Thu 09:11]--[2019-03-14 Thu 09:58] =>  0:47
    CLOCK: [2019-03-14 Thu 08:55]--[2019-03-14 Thu 09:10] =>  0:15
    CLOCK: [2019-03-14 Thu 08:33]--[2019-03-14 Thu 08:54] =>  0:21
    CLOCK: [2019-03-14 Thu 08:02]--[2019-03-14 Thu 08:32] =>  0:30
    CLOCK: [2019-03-14 Thu 07:04]--[2019-03-14 Thu 07:20] =>  0:16
    CLOCK: [2019-03-14 Thu 06:59]--[2019-03-14 Thu 07:03] =>  0:04
    CLOCK: [2019-03-14 Thu 06:40]--[2019-03-14 Thu 06:58] =>  0:18
    CLOCK: [2019-03-14 Thu 06:28]--[2019-03-14 Thu 06:39] =>  0:11
    :END:

#+begin_quote
*Story*: As a dogen user, I want empty directories to be removed so
that I don't have to do it manually.
#+end_quote

When housekeeper finishes deleting all extra files, it should check
all of the processed directories to see if they are empty. If they
are, it should delete the directory.

We should probably have a command line option to control this
behaviour.

This can be implemented as a transform in extracton that executes
against the managed directories.

Links:

- [[https://www.codeproject.com/Questions/454944/how-to-remove-empty-folders-in-a-directory-using-b][How to remove empty folders in a directory using boost]]

*** Consider adding an indent JSON transform                          :story:

Once we start making use of a proper JSON library, we should output
indented JSON models as part of conversion. We always have to indent
manually anyway. For extra bonus points, it would be nice if the
indent could cope with our invalid JSON (not deleting duplicate keys).

We could even expose it as an activity/command so that we could indent
external files without going through conversion; this would be useful
for library models.

*** Make extraction model name a qualified name                       :story:

At present we are setting up the extraction model name from the simple
name of the model. It should really be the qualified name. Hopefully
this will only affect tracing and diffing.

*** Formatters have been incorrectly placed under extraction          :story:

When we did the big meta-data rename, we placed facets and formatters
in the following in extraction:

: masd.extraction.cpp.cmake.enabled

However, this is not entirely correct: facet space is a property of
generation; the formatters are model to text transformations in
generation space that produce the extraction model. When you are
enabling and disabling formatters, you are in the generation space. We
need to update these keys.

Notes:

- update extraction_properties in coding model. In fact, move them to
  the generation model.
- update most of the extraction keys in JSON and all models.

*** Handcrafted templates                                             :story:

At present we generate constructors, swap, etc. for handcrafted
classes. Ideally users should be able to create a profile that enables
the things they want to see on a template and then associate it with a
stereotype. For this we will need aspect support.

A more interesting approach would be to combine wale (or its proper
replacement, a mustache based solution) with the meta-model: if one
could create *any* text file that can behave like this kind of
template, we could arbitrarily extend dogen for trivial use cases:

- main, entry point.
- interface.
- other uses users may find. Because they can bind templates against
  elements, this would make extensibility easier.

However, this is not a replacement for stitch: it is only helpful for
trivial cases and its not even clear it would work for all - e.g. how
would one loop trough all attributes in an object?

Actually, we probably already have enough for this to work, at least
for a few simple cases:

- interfaces: wale template with correct constructors, destructors,
  etc. For extra bonus points check operations.
- trivial main.

We just need to use the wale template to create the first "draft" and
then set overwrite to false.

*** Add support for multiple profile binds per modeling element       :story:

At present we can only bind an element to one profile. The reason why
is because we've already expanded the profile graphs into a flat
annotation and if we were to apply two of these expanded annotations
with common parents, the second application would overwrite the
first. Of course, we bumped into the exact same problem when doing
profile inheritance; there it was solved by ensuring each parent
profile is applied only once for each graph.

One possible solution for this problem is to consider each model
element as a "dynamic profile" (for want of a better name; on the fly
profile?). We would create a profile which is named after each of the
profiles it includes, e.g. say we include =dogen::hashable= and
=dogen::pretty_printable= for model element e0. Then the "on the fly
profile" would be:

: dogen::hashable_dogen::pretty_printable

It would be generated by the profiler, with parents =dogen::hashable=
and =dogen::pretty_printable=, and cached so that if anyone shows up
with that same profile we can reuse it. Because of the additive nature
of profile graphs this would have the desired result. Actually we
could probably have a two pass-process; first identify all of the
required dynamic profiles and generate them; then process them. This
way we can rely on a const data structure.

This will all be made easier when we have a two-pass pipeline because
we can do the profile processing on the first pass, and we can even
generate the "dynamic profiles" as real meta-model elements, created
on the fly.

*** Updates to debian package                                         :story:

There are several problems with the debian package:

- shared folder is =dogen= not =masd.dogen=
- no hello world sample; need json and dia versions
- package name is =dogen-applications=, should be masd...

*** Single reporting format option                                    :story:

- use org-mode for tracing and reporting etc
- byproducts dir does not have =cli=

*** Update metrics in OpenHub                                         :story:

For some reason our metrics are stuck at 5 months ago or so. It is
actually mildly useful to know the number of lines of code etc.

We probably need to delete and re-add the project.

*** Rename =fallback_element_type=                                    :story:

Our JSON uses a very strangely named attribute to carry the meta-type:

:       "fallback_element_type": "masd::object",

Its not at all obvious what this is meant to do. It should just be the
=element_type=.

We introduced this because users can set the stereotype,
e.g. =masd::object= - but don't always have to (e.g. when converting a
model from Dia). In this case, the fallback element type is
used. Perhaps we can keep the "fallback" logic internally, but just
call it element type?

One possible solution is to simply populate the stereotypes with the
inferred metamodel type. For this we need to check against a list of
metamodel types ("has the user already defined a stereotype?") and if
not, use the default one. This means our conversion will not roundtrip
without differences, but at least it produces more sensible models.

*** Multiple entries of the same key is invalid in JSON               :story:

We directly mapped KVPs in UML to JSON, e.g.:

: #DOGEN masd.injection.model_modules=Masd.CSharpRefImpl.CSharpModel
: #DOGEN masd.injection.input_language=csharp
: #DOGEN masd.injection.reference=csharp.builtins
: #DOGEN masd.injection.reference=csharp.system.collections.generic
: #DOGEN masd.injection.reference=csharp.system.collections
: #DOGEN masd.injection.reference=csharp.system
: ...

maps to:

: {
:  "tagged_values": {
:    "masd.injection.dia.comment": "true",
:    "masd.injection.model_modules": "Masd.CSharpRefImpl.CSharpModel",
:    "masd.injection.input_language": "csharp",
:    "masd.injection.reference": "csharp.builtins",
:    "masd.injection.reference": "csharp.system.collections.generic",
:    "masd.injection.reference": "csharp.system.collections",
:    "masd.injection.reference": "csharp.system",
: ...

However, we cannot have duplicate keys in JSON, resulting in problems
when we indent models: the indenter removes all duplicate keys but
one. This means we have to massage models post indentation every
time. Solutions:

- use a JSON container for container keys. The problem with this is
  that our internal representation does not have a container but a
  list of KVPs. We need to somehow convert to and from this container
  representation. We also need to be able to dynamically determine if
  the value is a container or just a plain value when deserialising
  from JSON. If it's a container, we need to flatten it.

Merged stories:

*Support containers correctly in annotations*

At present we are allowing users to enter the same key multiple times
to represent a container:

: #DOGEN yarn.output_language=cpp
: #DOGEN yarn.output_language=csharp


This was an acceptable pattern from a Dia perspective, because we had
control of the KVP semantics. However, when we copied the pattern
across to the JSON representation things did not work out so
well. This is because the following JSON:

:     "yarn.output_language": "csharp",
:     "yarn.output_language": "cpp",

Is interpreted by a lot of JSON parsers as a duplicate, and results on
only a single KVP making it. We could try to solve a lot of problems
in one go and standardise all of the meta-data on JSON:

- use start and end markers to enclose the JSON when in dia. Story:
  [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-adding-a-start-and-end-dogen-variable-block-in-dia][Consider adding a start and end dogen variable block in dia]]
- this would also solve the problem with pairs (or at least part of
  it). Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#add-a-new-annotation-type-of-pair][Add a new annotation type of “pair”]]
- we could allow users to keep the JSON externally. Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#add-support-for-one-off-profiles][Add support
  for “one off” profiles]]
- the JSON would also work nicely with the concept of a dogen
  project. Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#introduce-dogen-projects][Introduce dogen projects]]

However, before we embark on this story we need to perform a lot of
analysis on this.

Notes:

- [[http://json-schema.org/][JSON Schema]]
- [[https://github.com/aspnet/Home/wiki/Project.json-file][Project.Json]]
- yarn.dia.comment is no longer necessary, just look for the
  markers.
- we should only allow arrays of simple types.
- the fragment used inside Dia should be identical to the file
  supplied as argument for the one-off profile and it should also
  identical to a fragment inside a project. Do we need to support both
  projects and one-off profiles?

Sample:

#+begin_src
  "annotation": {
    "yarn.dia.comment": true,
    "yarn.dia.external_modules": "dogen::test_models",
    "annotations.profile": "dogen",
    "yarn.input_language": "language_agnostic",
    "yarn.output_language": [ "csharp", "cpp" ]
#+end_src

This error has been picked up by codacy too:

- [[https://www.codacy.com/app/marco-craveiro/dogen/commit?cid%3D79696432&bid%3D3493157&utm_campaign%3Dnew_commit&utm_medium%3DEmail&utm_source%3DInternal][Commit 91886c6]]&

*** Conversion does not output static stereotypes                     :story:

At present we only output static stereotypes. However, there is no
point on fixing this until we move to the new JSON format.

*** Exclude profiles from stereotypes processing                      :story:

At present we are manually excluding profiles from the stereotypes
transform. This was just a quick hack to get us going. We need to
replace this with a call to annotations to get a list of profile names
and exclude those.

We should also rename =is_stereotype_handled_externally= to something
more like "is profile" or "matches profile name".

Actually the right thing may even be to just remove all of the profile
stereotypes during annotations processing. However, we should wait
until we complete the exomodel work since that will remove scribble
groups, etc. Its all in the annotations transform.

*** Using =std::set<std::string>= causes compilation errors           :story:

 In theory sets of strings (and any other type that has =operator<=
 should work out of the box, even though we do not support sets of
 dogen types. However, when we tried to use a set of strings we got a
 whole load of compilation errors in serialisation, etc.

*** Handling of unsupported dia objects                               :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of Dia shapes that are
not supported by dogen so that my diagrams can be as expressive as
required.
#+end_quote

At present when we try to use a dia object that dogen knows nothing
about we get an error; for example using a standard line results in:

: 2014-09-10 08:09:43.480906 [ERROR] [dia_to_sml.processor] Invalid value for object type: Standard - Line
: 2014-09-10 08:09:43.487060 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/processor.cpp(124): Throw in function dogen::dia_to_sml::object_types dogen::dia_to_sml::processor::parse_object_type(const std::string &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Invalid value for object type: Standard - Line

However, it may make more sense to just ignore these. To do so we
could relax the code in processor (object_types):

:    BOOST_LOG_SEV(lg, error) << invalid_object_type << ot;
:    BOOST_THROW_EXCEPTION(processing_error(invalid_object_type + ot));

We should also consider having a =strict= command line option to
enable/disable this behaviour.

*** Generate model dependency graph                                   :story:

It would be nice to generate a tracing of the model dependencies. This
may not necessarily be part of tracing.

*** Stitch is still using artefact writer                             :story:

Create a templating transform that is similar to the approach used by
extraction - in fact, stitch should probably be using a transform in
extraction.

Delete artefact writer.

*** Fix cmake emacs variable for tab width                            :story:

We need to replace uses of =tab-width= in cmake files with
=cmake-tab-width=, as explained here:

[[http://stackoverflow.com/questions/25751408/controlling-the-indent-offset-for-cmake-in-emacs][Controlling the indent/offset for CMake in emacs]]

We need to do this for both code generated and manually generated
files.

*** Default model modules from filename                               :story:

It would be nice to be able to not have to supply model modules when
its obvious from the filename.

Update hello world to demonstrate this. We basically want to make the
entry use case as simple as possible, requiring little to no
meta-data.

*** Code-generate annotations type templates                          :story:

Type templates are in effect features from a feature model. We need to
add UML support for features (e.g. add meta-model elements for them),
with code generation, and link them back to annotations.

In fact, we made a mistake by binding annotations so closely to
dogen. There are two distinct concerns here:

- the annotations library. This provides "typed support" on top of KVP
  infrastructure. The idea here is that users can define "fields" with
  "types" and retrieve information from those KVPs in a structured
  way. Instead of having to create their own validation
  infrastructure, they can rely on annotations to do all the hard work
  for them. As part of the field creation, ideas such as "scopes" and
  "archetype locations" emerge. However, these do not really belong to
  the domain of annotations; these are concepts that end users create
  and give them semantics. What annotations needs to be able to do is
  to allow the creation of arbitrary notions of "scopes" and
  "hierarchy". Basically, annotations could be a completely
  self-contained project with no dependencies and usable outside of
  dogen.
- the linkage between the annotations library and dogen. Here we can
  create metamodel elements to convey the input parameters needed to
  code generate the elements for the annotations library. In this
  sense, annotations is nothing more than a platform that the
  transforms leverage; it has nothing particularly special to do with
  dogen. It just so happens that dogen itself then makes use of
  annotations to supply metadata internally, but this is a mere
  coincidence.
- the linkage between stitch and annotations. In this view, stitch is
  yet another client of annotations, via dogen. Again, there is no
  reason why stitch needs to have any dependency on dogen, other than
  annotations. In this sense, features such as licences and other
  boilerplate must be supplied as KVP parameters into stitch, without
  it directly depending in formattables. In addition, the fact that
  stitch generates c++ is also a coincidence. We could have a
  parameter that configures stitch and generate say C#.

Interestingly, in this sense we could then say that both stitch and
annotations are stand alone libraries generated using dogen, and then
in turn consumed by dogen. This could be done as packages by means of
vcpkg. And of course, stitch could then use a proper templating engine
instead of wale (another vcpkg dependency).

Finally, the logical conclusion is that dogen can use *any* of a
number of templating engines. The parameters to the engine are
supplied using KVPs (by means of annotation). There is a generic
metamodel element representing the binding to templating, and one of
its parameters is the templating engine. These are bound to the dogen
binary at compile time. End users can also make use of this mechanism,
for any of the available facets. This means that where we supply
=formatting_style=, we should really reflect the templating
engine. And then, all parameters with a known prefix, say:

: masd.templating.ENGINE.X=Y

Are supplied as parameters to the engine. These may need to take into
account facets as well, so that we can bind each facet to a different
template and supply different parameters.

*Previous Understanding*

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it. Name: =yarn::annotation_type_template=?
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** Mappings as meta-model elements                                   :story:

Now that we started to see PDMs as a solution for proxy models, the
logical consequence is that mappings too are meta-model elements. In
effect, it is a meta-model element that maps two model elements. So
users can create their own mappings if required and PIMs then become a
user level option. We can of course provide LAM, both as an example
and proof of concept but users are free to create their own
mappings. A few things are needed:

- all mappings must be processed first. This is because when we load
  models we do the mapping.
- a model should state if its a PSM or a PIM. If a PSM it must
  reference one or more mapping models. It must not reference any
  PSMs.
- mapping models should have references to PSMs. These are loaded on
  demand if, after mapping, we find types being referenced (e.g. get a
  list of all referenced models after mapping, check for their
  presence in references list and load them).

Merged Stories:

*Allow users to choose mapping sets*

At present we load the "default" mappings, which are also the only
mappings available. It is entirely possible that users will not agree
with those mappings. If we add a name to the mappings, and provide a
meta-data tag to choose mappings we can then allow users to provide
their own and set the meta-data accordingly. Mapper then reads the
meta-data in the model and uses the requested element map. For this we
need to name the element maps and we also need to create a "mapping
set". These can be indexed by name in the mapping repository. Mapper
chooses the mapping set to use.

In keeping with the idea that profiles are model-level concepts,
mappings should be too. We should be able to import mappings in a UML
diagram and override them or define new ones too.

*** Modeline groups as meta-model elements                            :story:

As with mappings, profiles and templates, we should make modeline
groups meta-model elements too. It may require a little bit of
thinking because they are not simple KVPs - but we also have support
for arrays in annotations.

The final destination is for users to create modeline configurations
or reuse the dogen ones.

In theory we should be able to load modelines incrementally, as they
are only needed for code generation. However, order of references will
matter because we need to validate references to modelines.

*** Licences as meta-model elements                                   :story:

Continuing the trend, licences are also moeta-model elements. We can
use the comments of a class to convey the licence text. The name
becomes the license name. Users use named configurations to assign
licences to elements. All artefacts produced across all facets for an
element will share the same licence. Users can easily add their own
licence (at whichever level they choose, product line, product,
component) and then refer to it. The only change is that they must now
prefix it with the model name (e.g. =masd::licenses::gpl_v2=).

In theory we should be able to load licences incrementally, as they
are only needed for code generation. However, order of references will
matter because we need to validate references to licences.

We should also allow for both:

- full licence: used later at the product level.
- licence summary: used for preambles in files.

*** Profiles as meta-model elements                                   :story:

Initially we separated the notion of annotations and profiles from the
metamodel. This is a mistake. Profiles are metamodel
elements. Annotations are just a way to convey profiles in UML.

In the same fashion, there is a distinction between a facet (like say
types) and a facet configuration (enable types, enable default
constructors, etc). These should also be metamodel elements. User
models should create facet configurations (this is part of the profile
machinery) and then associate them with elements.  This means we could
provide out of the box configurations such as =Serialisable= which
come from dogen profiles. We could also have =JsonSerialisable=. Users
can use these or override them in their own profiles. However,
crucial, modeling elements should not reference facets directly
because this makes the metamodel very messy.

In this view of the world, the global profile could then have
associations between these facet configurations and metamodel element
types, e.g.

: object -> serialisable, hashable

These can then be overridden locally.

In effect we are extending the notion of traits from Umple. However,
we also want traits to cover facets, not just concepts.

Terminology clarification:

- traits: configuration of facets.
- profile: mapping of traits to metamodel elements, with
  defaults. E.g. =object -> serialisable, hashable=

Actually there is a problem: traits as used in MOP are close to our
templates. We should rename templates to traits to make it
consistent. However, we still need the notion of named collections of
facet configurations with inheritance support.

*Thoughts on Features*

There is a facet in dogen called "features". The facet can have
multiple backends:

- dogen/UML: special case when adding new features to dogen
  itself. Any features added to this backend will be read out by dogen
  and made available to facets.
- file based configuration: property tree or other simple system to
  read configuration from file.
- database based configuration: a database schema (defined by the
  facet) is code-generated.
- etcd: code to read and write configuration from etcd is generated.

The feature facet can be used within a component model or on its own
model. Features are specifically only product features, not properties
of users etc. They can be dynamically updated if the backend supports
it. Generated code must handle event notification.

*Thoughts on Terminology*

- traits should be used in the MOP sense.
- profiles/collections of settings/configurations should be called
  =capabilities=. This is because they normally have names like
  =serialisable= etc. When not used in the context of modeling
  elements it should be called just configuration (in keeping with
  feature modeling). A capability is a named configuration for
  reuse. The only slight snag is that there are named configurations
  that should not be called capabilities (say licensing details,
  etc). These are required for product/product line support. Perhaps
  we should just call them "named configurations". Crucially, named
  configurations should inherit the namespace of the model and there
  should not be any clashes (e.g. dogen should error). Users are
  instructed to define their product line configuration in a model
  with the name of the product line (e.g. =dogen::serialisable=
  becomes the stereotype). To make the concept symmetric, we need the
  notion of a "model level stereotype". This can easily be achieved by
  conceiving the model as a package. For the purposes of dia we can
  simply add a =dia.stereotype= which conveys the model
  stereotypes. With these we can now set named configurations at the
  model level. This then means the following:
  - define a model for dogen (the product) with all named
    configurations. These are equivalent to what we call "profiles" at
    present and may even have the same names. the only difference is
    that because they are model elements, we now call them
    =dogen::PROFILE=, e.g. =dogen::disable_odb_cmake=. We should also
    add all of the missing features to the named configurations
    (disable VS, disable C#, etc).
  - add stereotypes to each model referencing the named configuration.
- with this approach, product lines become really easy - you just need
  to create a shared model for the product line (its own git repo and
  then git submodules). Because named configurations can use
  inheritance you can easily override at the product level as well as
  at the component level.
- when a named configuration is applied to a model element, the
  features it contains must match the scope. We should stop calling
  these global/local features and instead call them after the types of
  modeling elements: model, package, element, etc.
- traits are now only used for the purposes intended by MOP.
- features are integrated with UML by adding features to the
  metamodel.
- =profiles= should be used in the UML sense only.

*Thoughts on code generation*

- create a stereotype for =dogen::feature_group=. The name of the
  feature (e.g. the path for the kvp) will be given by the model name
  and location plus package plus feature group name plus feature
  name. example =dogen.language.input= instead of
  =yarn.input_languages=.
- the UML class's attributes become the features. The types must match
  the types we use in annotation, except these are also real dogen
  types and thus must be defined in a model and must be fully
  qualified. We must reference this model. Default value of the
  attribute is the UML value.
- any properties of the feature that cannot be supplied directly are
  supplied via features:

:    "template_kind": "instance",
:    "scope": "root_module"

- note that these are features too, so there will be a feature group
  for feature properties. Interestingly, we can now solve the
  enumeration problem because we can define a
  =dogen::features::enumeration= that can only be used for features
  and can be used to check that the values are correct. One of the
  values of the type is any element who's meta-type is
  =feature_enumeration=. Actually we don't even need this, it can be a
  regular enumeration (provided it knows how to read itself from a
  string). Basically a valid type for a feature is any dogen
  enumeration.
- annotations become a very simple model. There are no types in
  annotation itself, just functions to cast strings. These will be
  used by generated code. The profile merging code remains the same,
  but now it has no notion of artefact location; it simply merges KVPs
  based on a graph of inheritance (this time given by model
  relationships, but with exactly the same result as the JSON
  approach).
- annotation merging still takes place, both at the named
  configuration levels, and then subsequently at the element
  level. Named configurations are just meta-model entities so we can
  locate them by name, and literally copy across any key that we do
  not have (as we do now).
- code generation creates a factory for the feature group containing:
  - a registration method. We still need some kind of registration of
    key to scope so that we can validate that a key was not used in
    the wrong scope.
  - a class with all the members of the feature group in c++ types;
  - a factory method that takes in a KVP or an annotation and returns
    the class.
- there are no templates any longer; we need to manually create each
  feature in the appropriate feature group. Also, at present we are
  reading features individually in each transform. Going forward this
  is inefficient because we'd end up creating the configuration many
  times. We need some kind of way of caching features against
  types. At present we do this via properties. We could create
  something like a "configuration" class and then just initialise all
  features in one go. The transforms can then use these. Model
  elements are associated with configurations. The easiest way is to
  have a base class for configurations and then cast them as required
  (or even have a visitor, since we know of the types). Alternatively,
  we need to change the transforms so that we process a feature group
  all in one go. This would be the cleanest way of doing it but
  perhaps quite difficult given the current structure of the code.
- we could also always set the KVP value to be string and use a
  separator for containers and make it invalid to use it in strings
  (something like |). Then we could split the string on the fly when
  time comes for creating a vector/list.

Notes:

- loading profiles as meta-model elements is going to be a challenge,
  especially in a world where any model can make use of them. The
  problem is we must have access to all profile data before we perform
  an annotation expansion; at present this is done during the creation
  of the context in a very non-obvious way (the annotation_factory
  loads up profiles on construction). We either force users to have
  configuration models (CMs, configuration models?) in which case we
  can simply load all of these up first or we need a two-pass approach
  in which we load up the models but only process the mappings,
  initialise the annotation factory and then do the regular
  processing. The other problem is that we are only performing
  resolution later on, whereas we are now saying we need to expand the
  stereotype into a full blown annotation by resolving the stereotype
  into a name quite early in the pipeline. In the past this worked
  because we were only performing a very shallow resolution (string
  matching and always in the same model?) whereas now we are asking
  for full location resolution, across models. This will also be a
  problem for mappings as meta-model elements.
- a possible solution is to split processing into the following
  phases:
  1. load up target model.
  2. read references from target, load references. Need also to
     process model name via annotations. This means its not possible
     to use external modules as a named configuration (or else its
     recursive, we cannot find a configuration because its missing
     EMs, and its missing EMs because we did not process the named
     configuration). In a world where external modules are merged with
     model modules, this becomes cleaner since the model module must
     be unique for each model.
  3. collect all elements that need pre-processing and pre-process
     them: mappings, licences, named configurations/profiles. Not
     traits/object templates. All initialised structures are placed in
     the context. Note that we are actually processing only these
     elements into the endomodel, everything else is untouched. Also
     we need to remove these elements from the model as well so that
     they are not re-processed on the second phase. In addition, we
     need resolution for the meta-elements on the first phase, so we
     need to prime the resolver with these entities somehow,
     independently of the model merging. Or better, we need to create
     a first phase model-merge that only contains entities for the
     first phase and process that. So: load target, collect all
     first-phase meta-elements and remove from target, add target to
     cache. Then repeat process with references. Then merge this model
     and process it.
  4. Second phase is as at present, except we no longer load the
     models, we reuse them from an in-memory cache, after the
     filtering has taken place.
- note that the new meta-model elements are marked as non-generatable
  so a model that only contains these is non-generatable. Same with
  object templates/traits.
- the only slight problem with this approach is that we wanted the
  context to be const. This way we need to do all of these transforms
  before we can initialise the context. One possible solution is to
  split out first pass from second pass (different namespaces) so that
  "context" means different things. We can then say that the second
  phase context depends on first phase transform chain (in fact the
  input for the second phase is the output of the first phase,
  including cached models etc).

Links:

- https://cruise.eecs.uottawa.ca/umple/Traits.html

*** Improve handling of stereotypes                                   :story:

At present we can add any string as a stereotype. If anyone binds to
that string, we will do "something" if no one binds, we will do
"nothing". This is not ideal:

- its not easy to tell what stereotypes are available and what they
  do.
- if a user is expecting some functionality to come out based on a
  stereotype, they won't know why it didn't.
- more than one consumer may exist for a single stereotype - e.g. a
  stereotype may have more than one meaning by mistake.

Ideally we should have:

- a central registry of stereotypes with associated descriptions;
- a validation check that all stereotypes match registered stereotypes
  and a fatal error if not (perhaps overridable?)
- a command-line parameter to dump available stereotypes and their
  descriptions so that users know whats available.
- a check that a stereotype has not yet been registered so only one
  consumer can bind to it.

*** Move wale templates from the data directory                       :story:

At present we have wale templates under the data directory. This is
not the right location. These are part of a model just like stitch
templates. There is one slight wrinkle though: if a user attempts to
create a dogen formatter (say if plugins were supported), then we need
access to the template from the debian package. So whilst they should
live in the appropriate model (e.g. =generation.cpp=,
=generation.csharp=), they also need to be packaged and shipped.

Interestingly, so will all dogen models which are defining annotations
and profiles. We need to rethink the data directory, separating system
models from dogen models somehow. In effect, the data directory will
be, in the future, the system models directory.

So, in conclusion, two use cases for wale templates:

- regular model defines a wale template and makes use of it. Template
  should be with the model, just like stitch templates. However,
  unlike stitch, there should be a directory for them.
- user model wants to define a new formatter. It will make use of
  dogen profiles and wale templates. These must be in the future data
  directory somehow.

*** Consider making fully generated files read-only                   :story:

We could add emacs/vi tags to make fully generated files read-only -
as opposed to partially generated files such as services, which are
expected to be modified by the user. Example:

: /* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 buffer-read-only: t -*-

There must be a vi equivalent. There is =view= but its not clear how
to set it into a modeline. The alternative is to write the files as
read only.

: /* vim: tw=60: ts=2: view=t: set ro: */

Requires changes  to =.vimrc=:

: set modeline

It would be even better if we could make parts of a file read only, so
that only the protected regions could be written on.

Links:

- [[https://stackoverflow.com/questions/20023363/emacs-remove-region-read-only][emacs remove region read-only]]
- [[https://www.emacswiki.org/emacs/FoldingMode][Folding mode]]

*** Replace boost property tree with real JSON support                :story:

Once we support JSON fully we should go through all of the uses of
JSON we have at present and replace them with the JSON serialised
version of the types.

*** Add support for decoration configuration overrides                :story:

At present we have hard-coded the decoration configuration to be read
from the root object only. In an ideal world, we should be able to
override some of these such as the copyrights. It may not make sense
to be able to override them all though.

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders.

*** Add annotation types description                                  :story:

It would be useful to have a description of the purpose of the field
so that we could print it to the command line. We could simply add a
JSON attribute to the field called description to start off with. But
ideally we need a command line argument to dump all fields and their
descriptions so that users know what's available.

This should be sorted by qualified name.

** Deprecated
*** CANCELLED Add tests for yarn main workflow                        :story:
    CLOSED: [2019-03-11 Mon 08:16]

*Rationale*: code has changed considerably since this story was
written.

A few come to mind:

- model with no generatable types returns false
- model with generatable types returns true
- multiple models get merged
- system models get injected

*** CANCELLED Sort model dependencies                                 :story:
    CLOSED: [2019-03-11 Mon 08:19]

*Rationale*: code has changed considerably since this story was
written.

It seems the order of registration of models has moved with recent
builds of dogen (1418). Investigate if we sort the dependencies and if
not, sort them.

*** CANCELLED Consider adding a start and end dogen variable block in dia :story:
    CLOSED: [2019-03-11 Mon 08:34]

*Rationale*: this is going to complicate the parsing for no real
advantage. Users will forget to add the end bit, etc.

At present we defined a special market to find dogen kvp's in dia's
comments: =#DOGEN=. The problem with this is that, as we start adding
more and more knobs to dynamic, we have to repeat it more and more:

: #DOGEN dia.comment=true
: #DOGEN licence_name=gpl_v3
: #DOGEN copyright_notice=Copyright (C) 2012 Kitanda <info@kitanda.co.uk>
: #DOGEN modeline_group_name=emacs

It would be nice to be able to create a block instead, maybe (first stab):

: #DOGEN_START
: dia.comment=true
: licence_name=gpl_v3
: copyright_notice=Copyright (C) 2012 Kitanda <info@kitanda.co.uk>
: modeline_group_name=emacs
: #DOGEN_END

*** CANCELLED Add test to check if we are writing when file contents haven't changed :story:
    CLOSED: [2019-03-11 Mon 08:41]

*Rationale*: this is less of a problem now we have dry-run-mode.

We broke the code that detected changes and did not notice because we
don't have any changes around it. A simple test would be to generate
code for a test model, read the timestamp of a file (or even all
files), then regenerate the model and compare the timestamps. If there
are changes, the test would fail.
