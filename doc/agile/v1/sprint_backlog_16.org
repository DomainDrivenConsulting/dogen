#+title: Sprint Backlog 16
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- implement the variability model.
- move towards using the new variability model types.

* Stories

** Active
#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-04-26 Fri 18:47]
| <75>                                                   |         |       |       |       |
| Headline                                               | Time    |       |       |     % |
|--------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                           | *32:47* |       |       | 100.0 |
|--------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                | 32:47   |       |       | 100.0 |
| Active                                                 |         | 32:47 |       | 100.0 |
| Edit release notes for previous sprint                 |         |       |  2:40 |   8.1 |
| Sprint and product backlog grooming                    |         |       |  0:42 |   2.1 |
| Create a video demo for the previous sprint's features |         |       |  1:55 |   5.8 |
| Read variability papers                                |         |       |  1:04 |   3.3 |
| Implement variability model                            |         |       | 26:26 |  80.6 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2019-04-22 Mon 11:24]
    :LOGBOOK:
    CLOCK: [2019-04-22 Mon 16:01]--[2019-04-22 Mon 16:37] =>  0:36
    CLOCK: [2019-04-22 Mon 12:21]--[2019-04-22 Mon 12:29] =>  0:08
    CLOCK: [2019-04-22 Mon 11:27]--[2019-04-22 Mon 11:47] =>  0:20
    CLOCK: [2019-04-22 Mon 09:50]--[2019-04-22 Mon 11:26] =>  1:36
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.15, "Quinzinho"

#+begin_src markdown
![Joaquim Alberto da Silva](https://pbs.twimg.com/media/D4vvToKWkAEN1Ri.png:large)

_Joaquim Alberto da Silva ("Quinzinho") playing for the Angolan national team, the Palancas Negras. (C) 2001 Getty Images._

# Prelude

This release is named in memory of "Quinzinho", who [scored Angola's first goal in the Africa Cup of Nations](https://www.bbc.co.uk/sport/football/47987342). _Xala Kiambote, Guerreiro._

# Introduction

The key objective this sprint was to make inroads with regards to variability management in Dogen models [1]. Readers won't fail to notice that we've started to get more and more technical as we try to align Dogen with the PhD thesis. This trend is only set to increase, because we are approaching the business end of the research project. Also, as expected, the technical work was much harder than expected (if you pardon the pun), so we didn't get as far as exposing variability management to the end user. We are now hoping to reach this significant milestone next sprint.

# User visible changes

There were only a few minor user visible changes:

- a rather dodgy bug in C# code generation was found and fixed, whereby we somehow were not generating code for C# models. How this was missed is a veritable comedy of errors, from the way we had designed the system tests to the way diffs were being made. Suffices to say that many lessons were learned and a tightening of the process was put into place to avoid this particular problem from happening again.
- CMake files now use the correct tab variable for emacs, i.e. ```cmake-tab-width``` instead of ```tab-width```.
- CMake files are no longer hard-coded to generate static libraries. You can generate a shared library by using the CMake variable ```-DBUILD_SHARED_LIBS=ON```. This change was also made to the Dogen codebase itself, but due to a problem with the Boost.Log build supplied by vcpkg, we can't yet build Dogen using shared libraries [2].

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_15.org).

## Significant Internal Stories

The bulk of the work was taken by redesigning the annotations model. We have spent some time re-reading the [MDE](https://en.wikipedia.org/wiki/Model-driven_engineering) theory on this subject to make sure we have aligned all terminology with the terms used by domain experts. The final result was the creation of the variability model, composed of a number of transforms. This model has not yet been fully implemented and integrated with the core.

A second significant story this sprint was the reactivation of the boilerplate tests, which was a mop-up effort left from the previous sprint.

## Resourcing

Over 54% of the sprint was taken by stories related to its mission statement. We spent around 16% of the total time on process, with just shy of 10% for backlog grooming, and the remainder related to release notes and demo. We've also had a number of interesting spikes, which were rather expensive:

- 10% of the time was spent changing our Emacs configuration. On the plus side, we are now using [clangd](https://clang.llvm.org/extra/clangd/index.html) instead of [cquery](https://github.com/cquery-project/cquery), whose development has slowed considerably. Given that Google and many other large enterprises contribute to clangd's development, it seems like the right decision. As a bonus, we've also updated clang to v8 - though, sadly, not via Debian's package management, as it is still only in unstable. Let's hope it hits testing soon.
- the bug with C# code generation cost us 5.3% of the total ask.
- we've had a number of issues with our nightly builds, costing us 2.5% of the total ask.

The complete story breakdown is as follows:

![Story Pie Chart](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/agile/v1/sprint_14_pie_chart.jpg)

## Planning

Due to the variability work being harder than expected, the project plan was bumped back by a sprint. At the end of sprint 15, the plan looks like this:

![Project Plan](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/agile/v1/sprint_14_project_plan.png)

![Resource Allocation Graph](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/agile/v1/sprint_14_resource_allocation_graph.png)

# Next Sprint

The focus on Sprint 15 is to finish the variability model, and replace the legacy classes with the new, transform-based approach. If all goes according to plan, this will finally mean we can expose our variability profiles to end users.

# Binaries

Please note that we are now shipping clang binaries on Linux rather than the GCC-generated ones. Due to the current refactorings, our GCC builds are taking too long to complete. This does mean that we are now using clang for all our builds.

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.15_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.15/dogen_1.0.15_amd64-applications.deb)
- [dogen-1.0.15-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.15/dogen-1.0.15-Darwin-x86_64.dmg)
- [dogen-1.0.15-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.15-Windows-AMD64.msi)

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!

# Footnotes

[1]  If this is not a topic you are familiar with and you'd like to understand it better, JM Jézéquel's review paper on the subject is probably of interest: ["Model-Driven Engineering for Software Product Lines"](http://downloads.hindawi.com/journals/isrn.software.engineering/2012/670803.pdf).
[2] [vcpkg #6148: Errors building shared library due to Boost Log and PIC](https://github.com/Microsoft/vcpkg/issues/6148)
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/1115302519067090947][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6526115847252041728][LinkedIn]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2019-04-26 Fri 08:25]--[2019-04-26 Fri 08:45] =>  0:20
    CLOCK: [2019-04-22 Mon 09:38]--[2019-04-22 Mon 09:48] =>  0:10
    CLOCK: [2019-04-22 Mon 09:25]--[2019-04-22 Mon 09:37] =>  0:12
    :END:

Updates to sprint and product backlog.

*** COMPLETED Create a video demo for the previous sprint's features  :story:
    CLOSED: [2019-04-22 Mon 14:36]
    :LOGBOOK:
    CLOCK: [2019-04-22 Mon 12:41]--[2019-04-22 Mon 14:36] =>  1:55
    :END:

Demo the delete empty directories feature.

*** COMPLETED Clean up annotation scope types                         :story:
    CLOSED: [2019-04-25 Thu 09:29]

*Rationale*: this work was carried out as part of the variability model
redesign.

As part of the attribute rename (which used to be called property) we
should have renamed the annotation scope as well to attribute.

In addition, we have a scope type of "entity" but the yarn meta-model
type is really "element".

We should also check if "not applicable" scope is in use, and if not
delete it.

*** STARTED Read variability papers                                   :story:
    :LOGBOOK:
    CLOCK: [2019-04-22 Mon 17:39]--[2019-04-22 Mon 17:54] =>  0:15
    CLOCK: [2019-04-22 Mon 16:50]--[2019-04-22 Mon 17:39] =>  0:49
    :END:

We still have a couple of variability papers we need to read to make
sure our approach is aligned with the literature.

- we are simultaneously a user of SPLE and a enabler of SPLE for end
  users of MASD.
- we also make use of weaving between the user model and the modeling
  of variability.
- MASD models and manages technical variability, leaving essential
  variability to the end user to handle.
- we only care about internal variability. External variability is
  left to the end user.
- the SRAP process also has a variability process. This needs to be
  documented.
- the multidimensional approach makes variation points hidden from the
  modeling. However, their application is limited to the topology of
  archetype space; it is not available to application engineering,
  only to the MASD use of domain engineering.
- we make use of binding times to bind configurations to modeling
  elements.
- we need feature groups to represent:
  - windows support
  - visual studio support
- alternatively, we need to figure out if we can't just use profiles
  to simulate configuration groups.
- feature selection is done at two levels: profiles and
  configuration. This is why we cannot name =configuration= class
  =selection=.
- we have two asset models: product and component. They both project
  into archetype space, and both bind into the same variability model.

*** STARTED Implement variability model                               :story:
    :LOGBOOK:
    CLOCK: [2019-04-26 Fri 17:56]--[2019-04-26 Fri 18:48] =>  0:52
    CLOCK: [2019-04-26 Fri 17:00]--[2019-04-26 Fri 17:29] =>  0:29
    CLOCK: [2019-04-26 Fri 16:41]--[2019-04-26 Fri 16:59] =>  0:18
    CLOCK: [2019-04-26 Fri 16:31]--[2019-04-26 Fri 16:40] =>  0:09
    CLOCK: [2019-04-26 Fri 16:23]--[2019-04-26 Fri 16:30] =>  0:07
    CLOCK: [2019-04-26 Fri 16:15]--[2019-04-26 Fri 16:22] =>  0:07
    CLOCK: [2019-04-26 Fri 16:03]--[2019-04-26 Fri 16:14] =>  0:11
    CLOCK: [2019-04-26 Fri 13:46]--[2019-04-26 Fri 14:47] =>  1:01
    CLOCK: [2019-04-26 Fri 13:36]--[2019-04-26 Fri 13:45] =>  0:09
    CLOCK: [2019-04-26 Fri 13:14]--[2019-04-26 Fri 13:35] =>  0:21
    CLOCK: [2019-04-26 Fri 11:50]--[2019-04-26 Fri 12:10] =>  0:20
    CLOCK: [2019-04-26 Fri 11:36]--[2019-04-26 Fri 11:49] =>  0:13
    CLOCK: [2019-04-26 Fri 10:55]--[2019-04-26 Fri 11:35] =>  0:40
    CLOCK: [2019-04-26 Fri 10:42]--[2019-04-26 Fri 10:54] =>  0:12
    CLOCK: [2019-04-26 Fri 10:05]--[2019-04-26 Fri 10:41] =>  0:36
    CLOCK: [2019-04-26 Fri 09:33]--[2019-04-26 Fri 10:04] =>  0:31
    CLOCK: [2019-04-26 Fri 09:17]--[2019-04-26 Fri 09:32] =>  0:15
    CLOCK: [2019-04-26 Fri 08:51]--[2019-04-26 Fri 09:16] =>  0:25
    CLOCK: [2019-04-26 Fri 07:22]--[2019-04-26 Fri 07:25] =>  1:17
    CLOCK: [2019-04-26 Fri 06:31]--[2019-04-26 Fri 07:21] =>  0:50
    CLOCK: [2019-04-25 Thu 22:09]--[2019-04-25 Thu 22:12] =>  0:03
    CLOCK: [2019-04-25 Thu 21:05]--[2019-04-25 Thu 22:08] =>  1:03
    CLOCK: [2019-04-25 Thu 20:19]--[2019-04-25 Thu 21:04] =>  0:45
    CLOCK: [2019-04-25 Thu 19:02]--[2019-04-25 Thu 19:04] =>  0:02
    CLOCK: [2019-04-25 Thu 16:54]--[2019-04-25 Thu 16:59] =>  0:05
    CLOCK: [2019-04-25 Thu 15:27]--[2019-04-25 Thu 16:53] =>  1:26
    CLOCK: [2019-04-25 Thu 14:54]--[2019-04-25 Thu 15:26] =>  0:32
    CLOCK: [2019-04-25 Thu 14:31]--[2019-04-25 Thu 14:53] =>  0:22
    CLOCK: [2019-04-25 Thu 11:32]--[2019-04-25 Thu 11:55] =>  0:23
    CLOCK: [2019-04-25 Thu 11:20]--[2019-04-25 Thu 11:31] =>  0:11
    CLOCK: [2019-04-25 Thu 11:13]--[2019-04-25 Thu 11:19] =>  0:06
    CLOCK: [2019-04-25 Thu 11:04]--[2019-04-25 Thu 11:12] =>  0:08
    CLOCK: [2019-04-25 Thu 10:55]--[2019-04-25 Thu 11:03] =>  0:08
    CLOCK: [2019-04-25 Thu 10:13]--[2019-04-25 Thu 10:54] =>  0:41
    CLOCK: [2019-04-25 Thu 10:05]--[2019-04-25 Thu 10:12] =>  0:07
    CLOCK: [2019-04-25 Thu 09:54]--[2019-04-25 Thu 10:04] =>  0:10
    CLOCK: [2019-04-25 Thu 09:37]--[2019-04-25 Thu 09:53] =>  0:16
    CLOCK: [2019-04-25 Thu 08:57]--[2019-04-25 Thu 09:36] =>  0:39
    CLOCK: [2019-04-25 Thu 07:06]--[2019-04-25 Thu 07:20] =>  0:14
    CLOCK: [2019-04-25 Thu 06:38]--[2019-04-25 Thu 07:05] =>  0:27
    CLOCK: [2019-04-24 Wed 18:45]--[2019-04-24 Wed 18:49] =>  0:04
    CLOCK: [2019-04-24 Wed 17:47]--[2019-04-24 Wed 18:44] =>  0:57
    CLOCK: [2019-04-24 Wed 17:30]--[2019-04-24 Wed 17:46] =>  0:16
    CLOCK: [2019-04-24 Wed 17:10]--[2019-04-24 Wed 17:29] =>  0:19
    CLOCK: [2019-04-24 Wed 16:29]--[2019-04-24 Wed 17:09] =>  0:40
    CLOCK: [2019-04-24 Wed 15:56]--[2019-04-24 Wed 16:12] =>  0:16
    CLOCK: [2019-04-24 Wed 15:45]--[2019-04-24 Wed 15:55] =>  0:10
    CLOCK: [2019-04-24 Wed 15:14]--[2019-04-24 Wed 15:44] =>  0:30
    CLOCK: [2019-04-24 Wed 14:16]--[2019-04-24 Wed 14:55] =>  0:39
    CLOCK: [2019-04-24 Wed 13:44]--[2019-04-24 Wed 14:15] =>  0:31
    CLOCK: [2019-04-24 Wed 13:14]--[2019-04-24 Wed 13:43] =>  0:29
    CLOCK: [2019-04-24 Wed 11:55]--[2019-04-24 Wed 12:07] =>  0:12
    CLOCK: [2019-04-24 Wed 11:11]--[2019-04-24 Wed 11:54] =>  0:43
    CLOCK: [2019-04-24 Wed 10:58]--[2019-04-24 Wed 11:10] =>  0:12
    CLOCK: [2019-04-24 Wed 10:53]--[2019-04-24 Wed 10:57] =>  0:04
    CLOCK: [2019-04-24 Wed 09:01]--[2019-04-24 Wed 10:52] =>  1:51
    CLOCK: [2019-04-23 Tue 13:42]--[2019-04-23 Tue 14:06] =>  0:24
    CLOCK: [2019-04-23 Tue 11:00]--[2019-04-23 Tue 11:52] =>  0:52
    CLOCK: [2019-04-23 Tue 10:36]--[2019-04-23 Tue 10:59] =>  0:23
    CLOCK: [2019-04-23 Tue 09:51]--[2019-04-23 Tue 10:35] =>  0:44
    CLOCK: [2019-04-23 Tue 07:21]--[2019-04-23 Tue 07:54] =>  0:33
    :END:

On the back of the redesign of the annotations model, which was
completed last sprint, we now have to implement all classes and then
hook them up to the engine.

Order of tasks:

- for the initial test of the changes, we need to obtain the feature
  model as part of the context generation in orchestration's context
  factory. We then add the feature model to injection. We then create
  a =Configurable= element in injection, side by side with
  annotation. We then use the configuration factory to create the
  configuration. Finally, we read fields using the configuration
  selector. This will prove that basic features and
  configurations work. Note that we need to duplicate all code
  creating "type groups" etc. We should probably add a flag in the
  context that determines whether to use new world or legacy and then
  populate it within orchestration.
- the second change is to add the feature model to the coding
  model. We then add a Configurable element, side-by-side
  Annotable. We then create the configuration model from a coding
  model, and execute the profile binding chain transform on it. We
  then read all features from the configuration. This will prove that
  profile binding works.
- actually we need to do all of the processing for profiles at the
  orchestration level. This is because we need access to the
  variability context, but also because it makes sense as we are
  trying to orchestrate between variability transforms and coding
  transforms (this keeps the coding model more or less clean from
  calling transforms in other models).
- finally we add feature model to generation context, and read
  remaining fields from the configuration.
- when all is working, we remove all references to annotation in
  injection, coding and generation.
- we then remove all legacy types from variability.

Notes:

- qualified name of attributes is not being added. This is probably a
  bug in adaptor.
- fabric types are not part of the profile expansion. By sheer luck,
  this is ok. At present we are also performing annotation expansion
  at the pre-assembly stage, well before fabric is injected. This
  makes sense: since we cannot configure fabric elements (they are
  injected), there is no need to process their configuration. This
  will be addressed in the future as we make them explicit meta-model
  elements.
- as a test to make sure we've caught all uses of annotation, we
  should set the pointer to null in the adapter and see if anything
  breaks.
- make configuration model =Nameable=.

*** Remove dynamic stereotypes from coding                            :story:

Now that we are intercepting the dynamic stereotypes coming in from
injection and directly populating the configuration, there is no need
to store them in the modeling element.

*** Profiles as meta-model elements                                   :story:

Initially we separated the notion of annotations and profiles from the
metamodel. This is a mistake. Profiles are metamodel
elements. Annotations are just a way to convey profiles in UML.

In the same fashion, there is a distinction between a facet (like say
types) and a facet configuration (enable types, enable default
constructors, etc). These should also be metamodel elements. User
models should create facet configurations (this is part of the profile
machinery) and then associate them with elements.  This means we could
provide out of the box configurations such as =Serialisable= which
come from dogen profiles. We could also have =JsonSerialisable=. Users
can use these or override them in their own profiles. However,
crucially, modeling elements should not reference facets directly
because this makes the metamodel very messy.

In this view of the world, the global profile could then have
associations between these facet configurations and metamodel element
types, e.g.

: object -> serialisable, hashable

These can then be overridden locally.

In effect we are extending the notion of traits from Umple. However,
we also want traits to cover facets, not just concepts.

Terminology clarification:

- traits: configuration of facets. [Actually these are now understood
  to be configurations. Traits will be the object templates, though we
  need to re-read the umple paper.]
- profile: mapping of traits to metamodel elements, with
  defaults. E.g. =object -> serialisable, hashable=. []Actually these
  are just the stereotypes.]

Actually there is a problem: traits as used in MOP are close to our
templates. We should rename templates to traits to make it
consistent. However, we still need the notion of named collections of
facet configurations with inheritance support.

*Thoughts on Features*

There is a facet in dogen called "features". The facet can have
multiple backends:

- dogen/UML: special case when adding new features to dogen
  itself. Any features added to this backend will be read out by dogen
  and made available to facets.
- file based configuration: property tree or other simple system to
  read configuration from file.
- database based configuration: a database schema (defined by the
  facet) is code-generated.
- etcd: code to read and write configuration from etcd is generated.

The feature facet can be used within a component model or on its own
model. Features are specifically only product features, not properties
of users etc. They can be dynamically updated if the backend supports
it. Generated code must handle event notification.

*Thoughts on Terminology*

- traits should be used in the MOP sense.
- profiles/collections of settings/configurations should be called
  =capabilities=. This is because they normally have names like
  =serialisable= etc. When not used in the context of modeling
  elements it should be called just configuration (in keeping with
  feature modeling). A capability is a named configuration for
  reuse. The only slight snag is that there are named configurations
  that should not be called capabilities (say licensing details,
  etc). These are required for product/product line support. Perhaps
  we should just call them "named configurations". Crucially, named
  configurations should inherit the namespace of the model and there
  should not be any clashes (e.g. dogen should error). Users are
  instructed to define their product line configuration in a model
  with the name of the product line (e.g. =dogen::serialisable=
  becomes the stereotype). To make the concept symmetric, we need the
  notion of a "model level stereotype". This can easily be achieved by
  conceiving the model as a package. For the purposes of dia we can
  simply add a =dia.stereotype= which conveys the model
  stereotypes. With these we can now set named configurations at the
  model level. This then means the following:
  - define a model for dogen (the product) with all named
    configurations. These are equivalent to what we call "profiles" at
    present and may even have the same names. the only difference is
    that because they are model elements, we now call them
    =dogen::PROFILE=, e.g. =dogen::disable_odb_cmake=. We should also
    add all of the missing features to the named configurations
    (disable VS, disable C#, etc).
  - add stereotypes to each model referencing the named configuration.
- with this approach, product lines become really easy - you just need
  to create a shared model for the product line (its own git repo and
  then git submodules). Because named configurations can use
  inheritance you can easily override at the product level as well as
  at the component level.
- when a named configuration is applied to a model element, the
  features it contains must match the scope. We should stop calling
  these global/local features and instead call them after the types of
  modeling elements: model, package, element, etc.
- traits are now only used for the purposes intended by MOP.
- features are integrated with UML by adding features to the
  metamodel.
- =profiles= should be used in the UML sense only.

*Thoughts on code generation*

- create a stereotype for =dogen::feature_group=. The name of the
  feature (e.g. the path for the kvp) will be given by the model name
  and location plus package plus feature group name plus feature
  name. example =dogen.language.input= instead of
  =yarn.input_languages=.
- the UML class's attributes become the features. The types must match
  the types we use in annotation, except these are also real dogen
  types and thus must be defined in a model and must be fully
  qualified. We must reference this model. Default value of the
  attribute is the UML value.
- any properties of the feature that cannot be supplied directly are
  supplied via features:

:    "template_kind": "instance",
:    "scope": "root_module"

- note that these are features too, so there will be a feature group
  for feature properties. Interestingly, we can now solve the
  enumeration problem because we can define a
  =dogen::features::enumeration= that can only be used for features
  and can be used to check that the values are correct. One of the
  values of the type is any element who's meta-type is
  =feature_enumeration=. Actually we don't even need this, it can be a
  regular enumeration (provided it knows how to read itself from a
  string). Basically a valid type for a feature is any dogen
  enumeration.
- annotations become a very simple model. There are no types in
  annotation itself, just functions to cast strings. These will be
  used by generated code. The profile merging code remains the same,
  but now it has no notion of artefact location; it simply merges KVPs
  based on a graph of inheritance (this time given by model
  relationships, but with exactly the same result as the JSON
  approach).
- annotation merging still takes place, both at the named
  configuration levels, and then subsequently at the element
  level. Named configurations are just meta-model entities so we can
  locate them by name, and literally copy across any key that we do
  not have (as we do now).
- code generation creates a factory for the feature group containing:
  - a registration method. We still need some kind of registration of
    key to scope so that we can validate that a key was not used in
    the wrong scope.
  - a class with all the members of the feature group in c++ types;
  - a factory method that takes in a KVP or an annotation and returns
    the class.
- there are no templates any longer; we need to manually create each
  feature in the appropriate feature group. Also, at present we are
  reading features individually in each transform. Going forward this
  is inefficient because we'd end up creating the configuration many
  times. We need some kind of way of caching features against
  types. At present we do this via properties. We could create
  something like a "configuration" class and then just initialise all
  features in one go. The transforms can then use these. Model
  elements are associated with configurations. The easiest way is to
  have a base class for configurations and then cast them as required
  (or even have a visitor, since we know of the types). Alternatively,
  we need to change the transforms so that we process a feature group
  all in one go. This would be the cleanest way of doing it but
  perhaps quite difficult given the current structure of the code.
- we could also always set the KVP value to be string and use a
  separator for containers and make it invalid to use it in strings
  (something like |). Then we could split the string on the fly when
  time comes for creating a vector/list.

Notes:

- loading profiles as meta-model elements is going to be a challenge,
  especially in a world where any model can make use of them. The
  problem is we must have access to all profile data before we perform
  an annotation expansion; at present this is done during the creation
  of the context in a very non-obvious way (the annotation_factory
  loads up profiles on construction). We either force users to have
  configuration models (CMs, configuration models?) in which case we
  can simply load all of these up first or we need a two-pass approach
  in which we load up the models but only process the mappings,
  initialise the annotation factory and then do the regular
  processing. The other problem is that we are only performing
  resolution later on, whereas we are now saying we need to expand the
  stereotype into a full blown annotation by resolving the stereotype
  into a name quite early in the pipeline. In the past this worked
  because we were only performing a very shallow resolution (string
  matching and always in the same model?) whereas now we are asking
  for full location resolution, across models. This will also be a
  problem for mappings as meta-model elements.
- a possible solution is to split processing into the following
  phases:
  1. load up target model.
  2. read references from target, load references. Need also to
     process model name via annotations. This means its not possible
     to use external modules as a named configuration (or else its
     recursive, we cannot find a configuration because its missing
     EMs, and its missing EMs because we did not process the named
     configuration). In a world where external modules are merged with
     model modules, this becomes cleaner since the model module must
     be unique for each model.
  3. collect all elements that need pre-processing and pre-process
     them: mappings, licences, named configurations/profiles. Not
     traits/object templates. All initialised structures are placed in
     the context. Note that we are actually processing only these
     elements into the endomodel, everything else is untouched. Also
     we need to remove these elements from the model as well so that
     they are not re-processed on the second phase. In addition, we
     need resolution for the meta-elements on the first phase, so we
     need to prime the resolver with these entities somehow,
     independently of the model merging. Or better, we need to create
     a first phase model-merge that only contains entities for the
     first phase and process that. So: load target, collect all
     first-phase meta-elements and remove from target, add target to
     cache. Then repeat process with references. Then merge this model
     and process it.
  4. Second phase is as at present, except we no longer load the
     models, we reuse them from an in-memory cache, after the
     filtering has taken place.
- note that the new meta-model elements are marked as non-generatable
  so a model that only contains these is non-generatable. Same with
  object templates/traits.
- the only slight problem with this approach is that we wanted the
  context to be const. This way we need to do all of these transforms
  before we can initialise the context. One possible solution is to
  split out first pass from second pass (different namespaces) so that
  "context" means different things. We can then say that the second
  phase context depends on first phase transform chain (in fact the
  input for the second phase is the output of the first phase,
  including cached models etc).

Links:

- https://cruise.eecs.uottawa.ca/umple/Traits.html

Notes:

- on a first pass, add the dot names (dogen.enable_all_facets). Remove
  this as soon as we get things to work. We should only rely on model
  names (e.g. masd::enable_all_facets). We should also remove labels.
- move generation of profile repository outside of annotation
  expander.
- remove uses of annotations expander from stitch, if any are still
  left.
- move annotation expansion from adaptor into its own transform. It is
  done against the model set.
- profile repository appears deprecated, remove it?
- we probably should rename =coding::configuration= to "unbound
  configuration" or some other name to make it distinct from
  =variability::configuration=.

*** Enablement problem is in the variability domain                   :story:

Up to now we have considered the enablement problem as a generation
model problem, but this is incorrect. The enablement problem is
basically the idea that if I set a type to be hashable (for example),
the system should implicitly determine all other types that need to be
hashable too. This means that if I have descendants, they should also
be hashable, and if I have properties, the type of those properties
must also be hashable. In reality this is just a variability
problem. We need to tell the variability model about:

- features that require "propagation across model elements". We need a
  good name for this, without referencing model elements.
- the relationship between bound configurations. This can be copied
  from the model element (the bound configuration has the exact same
  name as the model element).

Then, we can simply build a DAG for the feature model using only bound
configurations (e.g. at present, binding type of "not applicable") and
then DFS the DAG setting properties across this relationship. Call the
relationship R between a and b, where a and b are configurations; all
properties that have the "propagate" flag on will be copied across
from a to b as is (due to R). If done after building the merged model
and after stereotype expansion this will work really well:

- we don't really care how a got into the state it is at present, we
  just copy the relevant properties across.
- there is no solving, BDD, etc. However, R must not have cycles. We
  probably need to first see how many cycles we find with inheritance
  and associations.
- we may need a way to switch this off. Say we really want to
  introduce a cycle; in that case, the bound configurations should be
  ignored.

Note that we will probably need to store pointers to the configuration
in order for this to work, or else we'll end up doing a lot of lookups
and copying around (to get the configurations from the model elements
into variability, the DAG etc and then back into the model at the
end).

Interestingly, this also means that we should not move the
global/local enablement computations into archetypes as we had planned
earlier. Instead, we need to explore if it is possible to generalise
the notion of "local" and "global" configurations, with overrides and
default values. This would work as part of the configuration binding
via implicit relationships - its just that the global configuration is
not really a relationship inferred from the underlying model. We then
need to look at the cleverness that we are using for overwrite as
well. Whilst we only need this logic for enablement, it may be useful
for other fields as well in the future. We also need some kind of way
of declaring certain fields as "cloneable" (for want of a better
term). In this case, we start off with a list of these fields, and if
there is no configuration point for them locally, we take the global
configuration point; if none exists, we take the default value.

Actually its more like "hierarchical copy" because we need to take
into account the hierarchy. In addition, we don't particularly care
about say backend, facet, etc at the element level, we just want the
archetype. So we need to encode these rules as a type of bind. It can
even be hacked as a bind "special" just for this purpose, its still a
better approach.

Another interesting issue is that of "reverse references". That is,
the fact that a model m is referenced by a set of models S; each of
these models may enable facets on elements that are associated with
elements from model m. On a first pass, we need to be able to consider
the configuration requirements as "non-satisfiable". The user
requested a configuration on the target model which cannot be
satisfied unless we alter the configuration of a referenced model. On
a second pass, when we have product level support, we could consider
adding "referenced" models to each model. This means that when we are
building m we have visibility of how m is used in the product and we
can take those uses into account when building the DAG.

*** Add annotation types description                                  :story:

It would be useful to have a description of the purpose of the field
so that we could print it to the command line. We could simply add a
JSON attribute to the field called description to start off with. But
ideally we need a command line argument to dump all fields and their
descriptions so that users know what's available.

This should be sorted by qualified name.

*** Reactivate injection.dia tests                                    :story:

We seem to have a number of tests commented out in
injection.dia. Investigate why and if possible, reactivate them.

*** Location of =--byproduct-directory= not respected                 :story:

It seems that at present we are not honouring the directory supplied
by the user. This seems to only happen on convert mode.

*** Add primitives to the archetypes model                            :story:

Instead of using strings we should use primitives for:

- facets
- formatters
- backends
- simple and qualified names.
- etc.

*** Consider a test suite level logging flag                          :story:

At present we can either enable logging for all test suites in dogen
or disable it. This means that all tests run a lot slower. Maybe we
should allow enabling logging at the test suite level. However, we
only use this to troubleshoot in which case the cost of a few seconds
is not a big problem.

*** Add support for decoration configuration overrides                :story:

At present we have hard-coded the decoration configuration to be read
from the root object only. In an ideal world, we should be able to
override some of these such as the copyrights. It may not make sense
to be able to override them all though.

This functionality has been implemented but requires tests in the test
model.

*** Update copyright notices                                          :story:

We need to update all notices to reflect personal ownership until DDC
was formed, and then ownership by DDC.

- first update to personal ownership has been done, but we need to
  test if multiple copyright entries is properly supported.

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders.

This functionality has been implemented but requires tests in the test
model.

*** Duplicate elements in model                                       :story:

Whilst running queries on postgres against a model dumped in tracing,
we found evidence of duplicate elements. Query:

: select jsonb_pretty(
:           jsonb_array_elements(
:           jsonb_array_elements(data)->'elements')->'data'->'__parent_0__'->'name'->'qualified'->'dot'
:       )
: from traces;

Snippet of results after =sort | uniq -c=

:      1  "masd.dogen.generation.csharp"
:      1  "masd.dogen.generation.csharp.all"
:      1  "masd.dogen.generation.csharp.CMakeLists"
:      1  "masd.dogen.generation.csharp.entry_point"
:      1  "masd.dogen.generation.csharp.fabric"
:      2  "masd.dogen.generation.csharp.fabric.assembly_info"
:      2  "masd.dogen.generation.csharp.fabric.assembly_info_factory"
:      2  "masd.dogen.generation.csharp.fabric.assistant"
:      2  "masd.dogen.generation.csharp.fabric.assistant_factory"
:      2  "masd.dogen.generation.csharp.fabric.decoration_expander"
:      2  "masd.dogen.generation.csharp.fabric.dynamic_transform"
:      2  "masd.dogen.generation.csharp.fabric.element_visitor"
:      2  "masd.dogen.generation.csharp.fabric.initializer"
:      2  "masd.dogen.generation.csharp.fabric.injector"
:      2  "masd.dogen.generation.csharp.fabric.meta_name_factory"
:      2  "masd.dogen.generation.csharp.fabric.traits"
:      2  "masd.dogen.generation.csharp.fabric.visual_studio_configuration"
:      2  "masd.dogen.generation.csharp.fabric.visual_studio_factory"

We need to investigate the generation pipeline to understand where
this is coming from.

*** Consider renaming orchestration to "engine"                       :story:

Orchestration is a bit of a vague name. It is really the code
generation engine of dogen. Its still very vague but slightly less so.

Actually the real name of this model is something like
"component". This will make sense once we add the product model. In
addition we need to somehow share the "generation" model across coding
and product models.

*** Make extraction model name a qualified name                       :story:

At present we are setting up the extraction model name from the simple
name of the model. It should really be the qualified name. Hopefully
this will only affect tracing and diffing.

*** Move wale templates from the data directory                       :story:

At present we have wale templates under the data directory. This is
not the right location. These are part of a model just like stitch
templates. There is one slight wrinkle though: if a user attempts to
create a dogen formatter (say if plugins were supported), then we need
access to the template from the debian package. So whilst they should
live in the appropriate model (e.g. =generation.cpp=,
=generation.csharp=), they also need to be packaged and shipped.

Interestingly, so will all dogen models which are defining annotations
and profiles. We need to rethink the data directory, separating system
models from dogen models somehow. In effect, the data directory will
be, in the future, the system models directory.

So, in conclusion, two use cases for wale templates:

- regular model defines a wale template and makes use of it. Template
  should be with the model, just like stitch templates. However,
  unlike stitch, there should be a directory for them.
- user model wants to define a new formatter. It will make use of
  dogen profiles and wale templates. These must be in the future data
  directory somehow.

*** Exclude profiles from stereotypes processing                      :story:

At present we are manually excluding profiles from the stereotypes
transform. This was just a quick hack to get us going. We need to
replace this with a call to annotations to get a list of profile names
and exclude those.

We should also rename =is_stereotype_handled_externally= to something
more like "is profile" or "matches profile name".

Actually the right thing may even be to just remove all of the profile
stereotypes during annotations processing. However, we should wait
until we complete the exomodel work since that will remove scribble
groups, etc. Its all in the annotations transform.

Once we have the profiles in the model set it should be easy to supply
them to the annotations transform.

*** Getter by reference of pointee                                    :story:

A useful use case is, whenever we have a property which is of
pointer-like type (shared pointer, etc), is to return the type pointed
to by const reference. We should be able to configure the generator
for this:

- we can already detect if the type is a pointer type;
- we would need some meta-data at the property level (generate
  de-refenced const/non-const setter). If this is used but the
  property type is not a pointer then we should throw.
- the generator would look for the meta-data, if enabled it would add
  additional setters.
- we may even want to suppress the pointer getters as well.

*** Shared pointers have getters and setters with references          :story:

We should really pass shared pointers by value instead of reference.

*** Consider changing variability value into a variant                :story:

Really all we are doing is adding a lot of infrastructure to be able
to store different types of values. This is what the variant is
designed to do. In addition, we then have all of the complexities
around selection that are already handled by variant.

** Deprecated
