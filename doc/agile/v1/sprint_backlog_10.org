#+title: Sprint Backlog 10
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Finish addressing build issues; key thing is to get the tests
  working again across the board.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2018-10-24 Wed 17:03]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *70:15* |       |       |   0.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 70:15   |       |       |   0.0 |
| Active                                                                      |         | 70:15 |       |   0.0 |
| Edit release notes for previous sprint                                      |         |       |  1:10 |   0.0 |
| Sprint and product backlog grooming                                         |         |       |  1:23 |   0.0 |
| Create a document with release steps                                        |         |       |  0:11 |   0.0 |
| Add support for kcov                                                        |         |       |  3:09 |   0.0 |
| Build on tags for Windows                                                   |         |       |  0:09 |   0.0 |
| Add vcpkg support to osx builds                                             |         |       |  5:17 |   0.0 |
| Remove all references to conan                                              |         |       |  0:13 |   0.0 |
| Travis badge is not red when build is failing                               |         |       |  0:14 |   0.0 |
| Ignore all failing tests                                                    |         |       |  1:11 |   0.0 |
| CTest warnings seem to be clipped                                           |         |       |  0:15 |   0.0 |
| AppVeyor is not building on GitHub commits                                  |         |       |  0:04 |   0.0 |
| OSX build is producing a large number of warnings                           |         |       |  0:37 |   0.0 |
| Upgrade to c++ 17                                                           |         |       |  0:25 |   0.0 |
| Check that C++ 17 is enabled on MSVC                                        |         |       |  0:22 |   0.0 |
| Windows MSI is very large                                                   |         |       |  0:26 |   0.0 |
| Update c++ reference implementation build                                   |         |       |  1:43 |   0.0 |
| Linking errors on windows for debug builds                                  |         |       |  0:17 |   0.0 |
| Create bitbbucket backups for reference implementation                      |         |       |  0:18 |   0.0 |
| Update vcpkg with boost.di and visibility settings                          |         |       |  1:26 |   0.0 |
| Fix warnings on OSX                                                         |         |       |  0:34 |   0.0 |
| Rename input models directory to just models                                |         |       |  0:16 |   0.0 |
| Get JSON models to generate again                                           |         |       |  0:57 |   0.0 |
| Update emacs programming setup                                              |         |       |  0:41 |   0.0 |
| Fix warnings on windows                                                     |         |       |  4:15 |   0.0 |
| Setup a secondary machine for development                                   |         |       |  1:04 |   0.0 |
| Remove facets that are not being used                                       |         |       |  8:31 |   0.0 |
| Design a top-level Dogen API                                                |         |       | 14:46 |   0.0 |
| Add support for nested namespaces                                           |         |       |  6:24 |   0.0 |
| Read up on framework and API design                                         |         |       |  4:34 |   0.0 |
| Update dogen namespaces to match the new specification                      |         |       |  7:25 |   0.0 |
| Create a single binary for all of dogen                                     |         |       |  1:58 |   0.0 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2018-10-15 Mon 11:22]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 10:26]--[2018-10-16 Tue 10:31] =>  0:05
    CLOCK: [2018-10-15 Mon 11:35]--[2018-10-15 Mon 11:54] =>  0:19
    CLOCK: [2018-10-15 Mon 10:36]--[2018-10-15 Mon 11:22] =>  0:46
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.09, "Kubata"

#+begin_src markdown
![Kubata](http://www.africavernaculararchitecture.com/wp-content/uploads/2015/03/Angola-Flickr-Rob-and-Sophie55061521f2fff.jpg) _Traditional Angolan village house. [(C) Rob and Sophie](http://www.africavernaculararchitecture.com/angola/)_.

# Overview

As described on [the previous sprint](https://github.com/MASD-Project/dogen/releases/tag/v1.0.08), the key objective at present is to get all the infrastructure up-to-date after a hiatus of a year or so of development. This is a requirement so that we can move to C++ 17 and start to make use of all the nice new libraries available. As such, this sprint was entirely taken with infrastructure clean up. Whilst these changes are not user visible, they still provide important benefits to project development so we'll briefly summarise them here.

## MASD Project Transition

We have started to sync up the work on the PhD with the work on Dogen. This sprint, the main focus was on creating an organisation solely for _Model Assisted Software Development_ (more details on that in the future), and moving all of the infrastructure to match - [Bintray](https://bintray.com/masd-project/main/dogen), [Travis](https://travis-ci.org/MASD-Project/dogen/builds), [Gitter](https://gitter.im/MASD-Project/Lobby) and the like.

## Move to vcpkg

Historically, we've always had a problem in keeping dogen's dependencies up-to-date across the three supported platforms. The problem stems from a lack of a cross-platform package manager in C++. Whilst we tried [Conan](https://conan.io/) in the past, we never managed to get it working properly for our setup. With this sprint we started the move towards using [vcpkg](https://vcpkg.readthedocs.io/en/latest/).

Whilst it still has some deficiencies, it addresses our use case particularly well and will allow us to pick up new dependencies fairly easily going forward. This is crucial as we expand the number of facets available, which hopefully will happen over the next couple of months. In this sprint we have completed the transition to vcpkg for Linux and Windows; the next sprint will be OSX's turn. With the introduction of vcpkg we took the opportunity to upgrade to [boost 1.68](https://www.boost.org/users/history/version_1_68_0.html) on Linux and Windows.

## Add CDash support

Since we moved away from our own infrastructure we lost the ability to know which tests are passing and how long test execution is taking. With this sprint we resurrected CDash/CTest support, with a new dashboard, available [here](https://my.cdash.org/index.php?project=MASD+Project+-+Dogen). There are still a few tweaks required - a lot of tests are still failing due to setup issues - but its clearly a win as we can now see a clearer picture across the testing landscape.

## Move reference models out of Dogen's repository

For a long time we've been struggling to build Dogen within the hour given to us by Travis. An easy win was to move the reference models ([C++](https://github.com/MASD-Project/cpp_ref_impl) and [C#](https://github.com/MASD-Project/csharp_ref_impl)) away from the main repository. This is also a very logical thing to do as we want these to be examples of stand-alone Dogen products, so that we can point them out to users as an example of how to use the product. Work still remains to be done on the reference implementations (CTest/CDash integration, clean up tests) but the bulk has been done this sprint.

For more details of the work carried out this sprint, see [the sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_09.org).

# User visible changes

Two tiny featurelets were added this sprint:

- **Development Binaries**: We now generate binaries for development releases. These are overwritten with every commit on BinTray.
- **Improvements on ```--version```**: The command now outputs build information to link it back to the build agent and build number. Note that these details are used only for information purposes. We will add GPG signatures in the future to validate the binaries.

```
$ dogen.knitter  --version
Dogen Knitter v1.0.09
Copyright (C) 2015-2017 Domain Driven Consulting Plc.
Copyright (C) 2012-2015 Marco Craveiro.
License: GPLv3 - GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.
Build: Provider = 'travis' Number = '2082' Commit = '53a1a169bd6f15c4388add9da933be2a353c4cbf' Timestamp = '2018/10/14 21:54:46'
IMPORTANT: build details are NOT for security purposes.
```

# Next Sprint

Infrastructural work will hopefully conclude on the next sprint, but the next big task is getting all the tests to run and pass.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.09_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.09/:dogen_1.0.09_amd64-applications.deb)
- [dogen-1.0.09-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.09/:dogen-1.0.09-Darwin-x86_64.dmg)
- [dogen-1.0.09-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/:dogen-1.0.09-Windows-AMD64.msi)

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/1051785972206247936][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6457553749215899648/][LinkedIn]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2018-10-24 Wed 08:03]--[2018-10-24 Wed 08:24] =>  0:21
    CLOCK: [2018-10-19 Fri 11:46]--[2018-10-19 Fri 11:49] =>  0:03
    CLOCK: [2018-10-18 Thu 20:23]--[2018-10-18 Thu 20:36] =>  0:13
    CLOCK: [2018-10-17 Wed 09:31]--[2018-10-17 Wed 09:44] =>  0:13
    CLOCK: [2018-10-17 Wed 06:47]--[2018-10-17 Wed 06:54] =>  0:07
    CLOCK: [2018-10-16 Tue 19:50]--[2018-10-16 Tue 20:06] =>  0:16
    CLOCK: [2018-10-15 Mon 10:25]--[2018-10-15 Mon 10:35] =>  0:10
    :END:

Updates to sprint and product backlog.

*** COMPLETED Create a document with release steps                    :story:
    CLOSED: [2018-10-15 Mon 11:34]
    :LOGBOOK:
    CLOCK: [2018-10-15 Mon 11:23]--[2018-10-15 Mon 11:34] =>  0:11
    :END:

We seem to now have a number of steps when releasing. Create a project
document for this.

*** COMPLETED Add support for kcov                                    :story:
    CLOSED: [2018-10-16 Tue 09:58]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 09:39]--[2018-10-16 Tue 09:41] =>  0:02
    CLOCK: [2018-10-15 Mon 22:50]--[2018-10-15 Mon 23:20] =>  0:30
    CLOCK: [2018-10-15 Mon 19:01]--[2018-10-15 Mon 19:39] =>  0:38
    CLOCK: [2018-10-15 Mon 17:58]--[2018-10-15 Mon 18:34] =>  0:36
    CLOCK: [2018-10-15 Mon 17:51]--[2018-10-15 Mon 17:57] =>  0:06
    CLOCK: [2018-10-15 Mon 17:30]--[2018-10-15 Mon 17:50] =>  0:20
    CLOCK: [2018-10-15 Mon 16:05]--[2018-10-15 Mon 16:29] =>  0:24
    CLOCK: [2018-10-15 Mon 15:31]--[2018-10-15 Mon 16:04] =>  0:33
    :END:

Try to see how hard it is to integrate kcov with the current build.

Notes:

- annoyingly, kcov has some binary dependencies rather than being a
  stand alone binary:

: apt-get install libcurl4-openssl-dev zlib1g-dev libdw-dev libiberty-dev

  as we could not figure out how to install just the SO's in travis,
  we ended up installing the dev packages. These are a lot more than
  what is actually required, but it'll do for now.
- we seem to upload to coveralls, but nothing shows up on the
  site. Try to manually generate coverage first.

Links:

- https://github.com/SimonKagstrom/kcov/blob/master/doc/codecov.md

*** COMPLETED Add support for code coverage                           :story:
    CLOSED: [2018-10-17 Wed 09:43]

*Rationale*: implemented using kcov.

We started the work on code coverage but never finished it. At present
it seems we don't even have a story detailing the current state of
coverage in the backlog. From memory the problem was that the upload
was too large and the coverage was including lots of files that should
be ignored. We never got the upload to work. In the mean time, it
seems that kcov is a better approach instead of using lcov, gcov, etc.

Links:

- https://github.com/SimonKagstrom/kcov

*Previous understanding*

Finish setting up coveralls

Remaining issues:

- we are generating far too much output. We need to keep it quieter or
  we will break travis.
- we are not filtering out non-project files from initial
  processing. There must be a gcov option to ignore files.

: Process: /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc-5/Debug/projects/quilt/spec/CMakeFiles/quilt.spec.dir/main.cpp.gcda
: ------------------------------------------------------------------------------
: File '../../../../projects/quilt/spec/main.cpp'
: Lines executed:62.50% of 8
: Creating '^#^#^#^#projects#quilt#spec#main.cpp.gcov'
:
: File '/usr/local/personal/include/boost/smart_ptr/detail/sp_counted_impl.hpp'
: Lines executed:60.00% of 20
: Creating '#usr#local#personal#include#boost#smart_ptr#detail#sp_counted_impl.hpp.gcov'

See also:

- [[https://github.com/JoakimSoderberg/coveralls-cmake-example/blob/master/CMakeLists.txt][example use of coveralls-cmake]]
- [[https://github.com/SpinWaveGenie/SpinWaveGenie/blob/master/libSpinWaveGenie/CMakeLists.txt][SpinWaveGenie's support for Coveralls]]
- maybe we should just use a different coverage provider. [[https://codecov.io/gh/DomainDrivenConsulting/dogen][CodeCov]]
  seems to be used by the kool kids. Example: [[https://github.com/ChaiScript/ChaiScript/blob/develop/CMakeLists.txt][ChaiScript]]. Example repo
  [[https://github.com/codecov/example-cpp11][here]] and for CMake specifically, [[https://github.com/codecov/example-cpp11-cmake][here]].
- we should generate coverage from the clang debug build only since
  that is the fastest build we have. We should use the clang coverage
  tool. See [[https://clang.llvm.org/docs/SourceBasedCodeCoverage.html][this document]].

Previous story [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_84.org#add-initial-support-for-coveralls][here]].

Notes:
- problems with python dependencies: [[https://github.com/micropython/micropython/issues/3246][cpp-coveralls 0.4.0 came and
  broke Travis build]]

*** COMPLETED Build on tags for Windows                               :story:
    CLOSED: [2018-10-16 Tue 10:30]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 11:01]--[2018-10-16 Tue 11:10] =>  0:09
    :END:

At present we are not building and deploying for tags on Windows. This
is a major pain because it means we must remember to always push the
tag separately. We need to setup appveyor correctly.

It appears filtering on branches (master only) disables building on
tags. We've now removed this.

Links:

- [[http://help.appveyor.com/discussions/problems/6209-build-is-not-triggered-for-tag][Build is not triggered for tag]]
- [[https://help.appveyor.com/discussions/questions/2626-pushing-tag-does-not-trigger-build][Pushing tag does not trigger build]]

*** COMPLETED Add vcpkg support to osx builds                         :story:
    CLOSED: [2018-10-16 Tue 11:00]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 11:31]--[2018-10-16 Tue 11:41] =>  0:10
    CLOCK: [2018-10-16 Tue 10:55]--[2018-10-16 Tue 11:00] =>  0:05
    CLOCK: [2018-10-16 Tue 09:42]--[2018-10-16 Tue 09:58] =>  0:16
    CLOCK: [2018-10-16 Tue 09:28]--[2018-10-16 Tue 09:38] =>  0:10
    CLOCK: [2018-10-15 Mon 22:50]--[2018-10-15 Mon 23:20] =>  0:30
    CLOCK: [2018-10-15 Mon 17:58]--[2018-10-15 Mon 18:05] =>  0:07
    CLOCK: [2018-10-15 Mon 16:30]--[2018-10-15 Mon 17:29] =>  0:59
    CLOCK: [2018-10-15 Mon 12:47]--[2018-10-15 Mon 15:31] =>  2:44
    CLOCK: [2018-10-15 Mon 11:54]--[2018-10-15 Mon 12:10] =>  0:16
    :END:

Following on from our investigation, we need to add vcpkg to the
travis osx builds (clang). While we're there, update all the tools to
latest in preparation to switching to C++ 17.

Notes:

- it seems its not possible to move to XCode 10 without upgrading the
  OS. This includes the Command Line tools only package as well.
- an alternative is to install the LLVM + clang package supplied by
  the LLVM project. The disadvantage is that we probably also need to
  use this in travis because using two different versions of clang is
  probably not the best idea. We could try and see what happens first,
  but ultimately we'll end up having to install it on travis. The
  binary is 300 MB, which is not ideal but should be ok.
- vcpkg misbehaves a bit when used from clang7. [[https://github.com/Microsoft/vcpkg/issues/4476][Reported]] to
  mothership.

Links:

- [[https://github.com/Microsoft/vcpkg/issues/4437][Error linking Boost on x86-osx]]
- [[https://github.com/Microsoft/vcpkg/issues/4476][Building vcpkg on OSX using LLVM's clang]]

*** COMPLETED Remove all references to conan                          :story:
    CLOSED: [2018-10-16 Tue 11:54]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 11:42]--[2018-10-16 Tue 11:55] =>  0:13
    :END:

We still have a number of places where we are doing things for conan
(checking for its presence in CMake, stories etc). Delete all of
those.

*** COMPLETED Travis badge is not red when build is failing           :story:
    CLOSED: [2018-10-16 Tue 12:08]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 11:56]--[2018-10-16 Tue 12:10] =>  0:14
    :END:

For some reason the badge is gray with the words error. This makes it
less obvious that the build is borked. Fix it so its red.

Seems this is because we are using shields.io rather than the travis
badge.

Actually maybe this is due to the fact that there is currently an
ongoing build. Lets ignore it for now.

*** COMPLETED Ignore all failing tests                                :story:
    CLOSED: [2018-10-16 Tue 12:29]
    :LOGBOOK:
    CLOCK: [2018-10-17 Wed 06:15]--[2018-10-17 Wed 06:36] =>  0:21
    CLOCK: [2018-10-16 Tue 12:11]--[2018-10-16 Tue 12:28] =>  0:17
    CLOCK: [2018-10-16 Tue 11:16]--[2018-10-16 Tue 11:30] =>  0:24
    CLOCK: [2018-10-16 Tue 10:36]--[2018-10-16 Tue 10:55] =>  0:19
    :END:

At present we have a number of tests that are commented out but appear
as failing under cdash. This is very confusing. We need to mark them
with the ignore macro. We should not waste time fixing the tests as
they need to be re-written using the diff framework.

*** COMPLETED CTest warnings seem to be clipped                       :story:
    CLOSED: [2018-10-16 Tue 14:23]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 13:26]--[2018-10-16 Tue 13:41] =>  0:15
    :END:

We seem to have 50 warnings on both OSX and linux builds, which is a
bit of a coincidence. CTest is probably clipping the warnings.

It is the default. Configured by
=CTEST_CUSTOM_MAXIMUM_NUMBER_OF_WARNINGS=.

Zero is not a good number - it removed all the warnings. Set it to a
large but yet sensible number.

Links:

- [[https://cmake.org/cmake/help/v3.4/variable/CTEST_CUSTOM_MAXIMUM_NUMBER_OF_WARNINGS.html][CTest variable CTEST_CUSTOM_MAXIMUM_NUMBER_OF_WARNINGS]]
- [[https://blog.kitware.com/ctest-performance-tip-use-ctestcustom-cmake-not-ctest/][CTest performance tip: Use CTestCustom.cmake, not .ctest]]

*** COMPLETED AppVeyor is not building on GitHub commits              :story:
    CLOSED: [2018-10-16 Tue 17:34]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 11:11]--[2018-10-16 Tue 11:15] =>  0:04
    :END:

It seems we've lost windows builds some how. GitHub is sending the
notification but AppVeyor is refusing to build.

Links:

- [[https://help.appveyor.com/discussions/problems/17480-builds-not-triggering-from-github-commit][Builds not triggering from GitHub commit]]
*** COMPLETED OSX build is producing a large number of warnings       :story:
    CLOSED: [2018-10-22 Mon 16:34]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 14:56]--[2018-10-16 Tue 15:33] =>  0:37
    :END:

More on debug than release. Some of the warnings are related to
visibility.

Notes:

- setting visibility to hidden does not seem to make any difference.
- we don't seem to have any good answers for this, so we'll leave it
  as is for now.
- this was finally resolved by rebuilding boost with the same
  visibility settings.

Links:

- [[https://stackoverflow.com/questions/8685045/xcode-with-boost-linkerid-warning-about-visibility-settings][xcode with boost : linker(Id) Warning about visibility settings]]
- [[https://stackoverflow.com/questions/3276474/symbol-hiding-in-static-libraries-built-with-xcode/18949281#18949281][Symbol hiding in static libraries built with Xcode]]
- [[https://stackoverflow.com/questions/36567072/why-do-i-get-ld-warning-direct-access-in-main-to-global-weak-symbol-in-this][Why do I get “ld: warning: direct access in _main to global weak
  symbol” in this simple code?]]
- [[https://gcc.gnu.org/wiki/Visibility][GCC Visibility]]
- [[https://github.com/Microsoft/vcpkg/issues/4497][Boost linker warnings on OSX]]

*** COMPLETED Upgrade to c++ 17                                       :story:
    CLOSED: [2018-10-16 Tue 20:06]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 15:34]--[2018-10-16 Tue 15:48] =>  0:14
    CLOCK: [2018-10-16 Tue 13:44]--[2018-10-16 Tue 13:54] =>  0:10
    CLOCK: [2018-10-16 Tue 13:42]--[2018-10-16 Tue 13:43] =>  0:01
    :END:

There are quite a few dependencies for this to happen:

- on windows we need to somehow include =/std:c++latest=
- we need to move to latest boost as it seems Boost 1.62 breaks on
  c++-17. We should wait until Beast is included in Boost before we do
  this.
- we need to install latest CMake, which is not available on nuget; so
  we need to fetch the zip/msi from https://cmake.org/files/v3.10/ and
  unpack it. Only latest supports VS 2017. Then set the CMake
  generator:

:    $generator="Visual Studio 15 2017 Win64";

- set the appveyor image:

: image:
:  - Visual Studio 2017

- set the CMake version:

:     set(CMAKE_CXX_STANDARD 14)

We have now fulfilled all of these requirements, so try to move to
C++17.

*** COMPLETED Check that C++ 17 is enabled on MSVC                    :story:
    CLOSED: [2018-10-18 Thu 16:20]
    :LOGBOOK:
    CLOCK: [2018-10-18 Thu 16:21]--[2018-10-18 Thu 16:29] =>  0:08
    CLOCK: [2018-10-18 Thu 16:11]--[2018-10-18 Thu 16:20] =>  0:09
    CLOCK: [2018-10-18 Thu 14:46]--[2018-10-18 Thu 14:51] =>  0:05
    :END:

When looking at the log files, there are no mentions of C++ 17. Ensure
we are setting this for MSVC.

We are now compiling with C++ 17, but builds are now longer than 1h
and so are getting killed. We will leave it like that and hopefully
find ways of reducing the build time - we are also close to the limit
on Linux as well.

*** COMPLETED Windows MSI is very large                               :story:
    CLOSED: [2018-10-16 Tue 20:06]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 09:59]--[2018-10-16 Tue 10:25] =>  0:26
    :END:

Package went from 5 MB to 80 MB over the last 3 days. The cause for
this appears to be that we started including tests on the standard
package.

It seems we can no longer build off of a github commit on
appveyor. [[https://help.appveyor.com/discussions/problems/17480-builds-not-triggering-from-github-commit][Ticket raised]] with support.

The problem is we removed =WITH_MINIMAL_PACKAGING= with the move to
use CTest. We need to add it to the CTest script.

When AppVeyor is back up, check to make sure the packages have
returned to normal size.

*** COMPLETED Update c++ reference implementation build               :story:
    CLOSED: [2018-10-16 Tue 22:14]
    :LOGBOOK:
    CLOCK: [2018-10-16 Tue 20:07]--[2018-10-16 Tue 21:50] =>  1:43
    :END:

Once we got the dogen build to work, we need to update the reference
model to match it:

- C++ 17
- kcov code coverage
- CDash project and uploads
- Latest vcpkg packages
- README emblems, comments on vcpkg
- removal of ctest update
- removal of conan references if any
- removal of third party

*** COMPLETED Linking errors on windows for debug builds              :story:
    CLOSED: [2018-10-17 Wed 06:40]
    :LOGBOOK:
    CLOCK: [2018-10-17 Wed 06:37]--[2018-10-17 Wed 06:46] =>  0:09
    CLOCK: [2018-10-16 Tue 22:18]--[2018-10-16 Tue 22:26] =>  0:08
    :END:

It seems we are consistently having problems linking the debug builds
on windows:

: [00:43:42]   LINK : the 32-bit linker (C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\bin\HostX86\x64\link.exe) failed to do memory mapped file I/O on `..\..\..\stage\bin\dogen.modeling.lib' and is going to restart linking with a 64-bit linker for better throughput

It seems the error is with 32-bit linker.

Links:

- [[https://developercommunity.visualstudio.com/content/problem/160714/memory-error-for-linker-in-vs-155-x64.html][Memory error for linker in VS 15.5 x64]]

*** COMPLETED Create bitbbucket backups for reference implementation  :story:
    CLOSED: [2018-10-17 Wed 10:03]
    :LOGBOOK:
    CLOCK: [2018-10-17 Wed 09:45]--[2018-10-17 Wed 10:03] =>  0:18
    :END:

We should have all repos backed up.

Created a mirror for all projects: https://bitbucket.org/MASD-Project.

*** COMPLETED Update vcpkg with boost.di and visibility settings      :story:
    CLOSED: [2018-10-18 Thu 16:10]
    :LOGBOOK:
    CLOCK: [2018-10-18 Thu 14:46]--[2018-10-18 Thu 16:10] =>  1:24
    CLOCK: [2018-10-18 Thu 14:43]--[2018-10-18 Thu 14:45] =>  0:02
    :END:

Work for the next vcpkg update:

- pull in [[https://github.com/boost-experimental/di][boost.di]].
- add flags as per visibility warnings story.
- update docs with all the workarounds.

*** COMPLETED Fix warnings on OSX                                     :story:
    CLOSED: [2018-10-18 Thu 17:06]
    :LOGBOOK:
    CLOCK: [2018-10-18 Thu 16:51]--[2018-10-18 Thu 17:06] =>  0:15
    CLOCK: [2018-10-18 Thu 09:16]--[2018-10-18 Thu 09:35] =>  0:19
    :END:

We are seeing a lot of warnings on OSX which makes it difficult to
spot real problems. We need to get rid of the spurious ones.

Notes:

[[https://github.com/Microsoft/vcpkg/issues/4497][- Ticket raised]] with vcpkg.
- updated vcpkg's boost with visibility hidden.

*** COMPLETED Rename input models directory to just models            :story:
    CLOSED: [2018-10-18 Thu 17:17]
    :LOGBOOK:
    CLOCK: [2018-10-18 Thu 17:18]--[2018-10-18 Thu 17:28] =>  0:10
    CLOCK: [2018-10-18 Thu 17:11]--[2018-10-18 Thu 17:17] =>  0:06
    :END:

Now we no longer have test models we can follow standard dogen
conventions.

*** COMPLETED Get JSON models to generate again                       :story:
    CLOSED: [2018-10-18 Thu 20:22]
    :LOGBOOK:
    CLOCK: [2018-10-18 Thu 20:10]--[2018-10-18 Thu 20:20] =>  0:10
    CLOCK: [2018-10-18 Thu 18:35]--[2018-10-18 Thu 19:22] =>  0:47
    :END:

Problems:

- we are adding the extension to the dia filename because of how CMake
  works. We should probably remove the output parameter or at least
  allow defaulting it to a replacement of the extension.
- we are removing the dependencies due to duplicates in JSON keys.
- we are looking for .dia diagrams instead of .json for references.

*** COMPLETED Update emacs programming setup                          :story:
    CLOSED: [2018-10-19 Fri 14:12]
    :LOGBOOK:
    CLOCK: [2018-10-19 Fri 13:31]--[2018-10-19 Fri 14:12] =>  0:41
    :END:

- it seems all of the cool kids have moved from [[https://github.com/Andersbakken/rtags][rtags]] to [[https://github.com/cquery-project/cquery][cquery]] and
  LSP mode. Give it a go and see if it works better than rtags.
- it also seems logview is no longer working properly; logs do not get
  recognised properly.

*** COMPLETED Fix warnings on windows                                 :story:
    CLOSED: [2018-10-20 Sat 15:44]
    :LOGBOOK:
    CLOCK: [2018-10-20 Sat 14:01]--[2018-10-20 Sat 14:05] =>  0:04
    CLOCK: [2018-10-20 Sat 13:10]--[2018-10-20 Sat 13:28] =>  0:18
    CLOCK: [2018-10-20 Sat 07:35]--[2018-10-20 Sat 07:44] =>  0:09
    CLOCK: [2018-10-19 Fri 22:01]--[2018-10-19 Fri 22:05] =>  0:04
    CLOCK: [2018-10-19 Fri 20:19]--[2018-10-19 Fri 20:30] =>  0:11
    CLOCK: [2018-10-19 Fri 17:00]--[2018-10-19 Fri 17:04] =>  0:04
    CLOCK: [2018-10-19 Fri 14:13]--[2018-10-19 Fri 14:36] =>  0:23
    CLOCK: [2018-10-19 Fri 12:27]--[2018-10-19 Fri 12:50] =>  0:23
    CLOCK: [2018-10-19 Fri 12:19]--[2018-10-19 Fri 12:26] =>  0:07
    CLOCK: [2018-10-19 Fri 11:50]--[2018-10-19 Fri 12:18] =>  0:28
    CLOCK: [2018-10-19 Fri 11:29]--[2018-10-19 Fri 11:45] =>  0:16
    CLOCK: [2018-10-19 Fri 08:10]--[2018-10-19 Fri 09:27] =>  1:17
    CLOCK: [2018-10-18 Thu 18:12]--[2018-10-18 Thu 18:26] =>  0:14
    CLOCK: [2018-10-17 Wed 13:47]--[2018-10-17 Wed 14:04] =>  0:17
    :END:

We have a large number of warnings on windows, try to see if we can
fix them.

: dogen.formatting\types\indent_filter.hpp(164): warning C4267: 'argument': conversion from 'size_t' to 'int', possible loss of data
: dogen.probing\src\types\metrics_builder.cpp(86): warning C4244: 'argument': conversion from '_Rep' to 'const unsigned long', possible loss of data
: dogen.generation.cpp\src\types\formatters\msbuild_targets_formatter.cpp(123): warning C4267: 'argument': conversion from 'size_t' to 'const unsigned int', possible loss of data
: dogen.generation.cpp\src\types\formatters\test_data\class_implementation_formatter.cpp(212): warning C4267: 'initializing': conversion from 'size_t' to 'unsigned int'
: dogen.generation.csharp\src\types\formatters\types\class_formatter.cpp(188): warning C4267: 'argument': conversion from 'size_t' to 'const unsigned int', possible loss of data

 warning LNK4098: defaultlib 'libcmtd.lib'

Notes:

- we need to add the same level of warnings on GCC so that we can see
  them locally without having to do a windows build, and so that we do
  not introduce regressions.
- it seems we now have the right incantation but the problem is we are
  not picking up the =CMAKE_BUILD_TYPE=. This works on all platforms
  but not on windows for some reason. CTest also thinks its a debug
  build, but its not clear CMake does.

Links:

- [[https://stackoverflow.com/questions/36834799/whats-the-best-strategy-to-get-rid-of-warning-c4267-possible-loss-of-data][What's the best strategy to get rid of “warning C4267 possible loss
  of data”?]]
- [[https://stackoverflow.com/questions/2771538/why-doesnt-gcc-produce-a-warning-when-assigning-a-signed-literal-to-an-unsigned][Why doesn't GCC produce a warning when assigning a signed literal to
  an unsigned type?]]
- [[https://cboard.cprogramming.com/cplusplus-programming/97754-warning-lnk4098-defaultlib-libcmtd-conflicts-use-other-libs.html][warning LNK4098: defaultlib "LIBCMTD" conflicts with use of other
  libs]]
- [[https://msdn.microsoft.com/en-gb/library/abx4dbyh.aspx][CRT Library Features]]
- [[https://cmake.org/pipermail/cmake/2009-October/032526.html][Problem ignoring libcmt.lib on Windows]]
- [[https://stackoverflow.com/questions/18786690/cmake-for-def-and-nodefaultlib][CMAKE for /DEF and /NODEFAULTLIB]]
- [[https://stackoverflow.com/questions/11512795/ignoring-unknown-option-nodefaultliblibcmtd][“Ignoring unknown option /NODEFAULTLIB:LIBCMTD”]]
- [[http://cmake.3232098.n2.nabble.com/CMAKE-EXE-LINKER-FLAGS-for-shared-libraries-td7087564.html][CMAKE_EXE_LINKER_FLAGS for shared libraries?]]
- [[https://cmake.org/pipermail/cmake/2012-March/049621.html][how to determine debug or release mode?]]
- [[https://stackoverflow.com/questions/24460486/cmake-build-type-not-being-used-in-cmakelists-txt][CMAKE_BUILD_TYPE not being used in CMakeLists.txt]]

*** COMPLETED Setup a secondary machine for development               :story:
    CLOSED: [2018-10-22 Mon 11:34]
    :LOGBOOK:
    CLOCK: [2018-10-22 Mon 10:01]--[2018-10-22 Mon 11:05] =>  1:04
    :END:

The main machine is no longer coping with the load of C++ development
plus cquery and google chrome. We need to setup a machine just to run
the desktop processes and another to run the development environment.

*** COMPLETED Remove facets that are not being used                   :story:
    CLOSED: [2018-10-22 Mon 16:04]
    :LOGBOOK:
    CLOCK: [2018-10-22 Mon 15:39]--[2018-10-22 Mon 16:04] =>  0:25
    CLOCK: [2018-10-22 Mon 15:04]--[2018-10-22 Mon 15:38] =>  0:34
    CLOCK: [2018-10-22 Mon 14:35]--[2018-10-22 Mon 15:03] =>  0:28
    CLOCK: [2018-10-22 Mon 12:44]--[2018-10-22 Mon 13:01] =>  0:17
    CLOCK: [2018-10-22 Mon 12:38]--[2018-10-22 Mon 12:43] =>  0:05
    CLOCK: [2018-10-22 Mon 12:23]--[2018-10-22 Mon 12:37] =>  0:14
    CLOCK: [2018-10-22 Mon 11:35]--[2018-10-22 Mon 11:56] =>  0:21
    CLOCK: [2018-10-22 Mon 11:03]--[2018-10-22 Mon 11:34] =>  0:31
    CLOCK: [2018-10-22 Mon 09:41]--[2018-10-22 Mon 10:01] =>  0:20
    CLOCK: [2018-10-22 Mon 09:21]--[2018-10-22 Mon 09:40] =>  0:19
    CLOCK: [2018-10-22 Mon 08:12]--[2018-10-22 Mon 08:31] =>  0:19
    CLOCK: [2018-10-21 Sun 12:39]--[2018-10-21 Sun 12:45] =>  0:06
    CLOCK: [2018-10-20 Sat 20:56]--[2018-10-20 Sat 21:10] =>  0:14
    CLOCK: [2018-10-19 Fri 22:23]--[2018-10-19 Fri 22:40] =>  0:17
    CLOCK: [2018-10-19 Fri 22:06]--[2018-10-19 Fri 22:22] =>  0:16
    CLOCK: [2018-10-19 Fri 20:31]--[2018-10-19 Fri 22:01] =>  1:30
    CLOCK: [2018-10-19 Fri 18:20]--[2018-10-19 Fri 18:46] =>  0:26
    CLOCK: [2018-10-19 Fri 17:05]--[2018-10-19 Fri 18:15] =>  1:10
    CLOCK: [2018-10-19 Fri 16:40]--[2018-10-19 Fri 16:59] =>  0:19
    CLOCK: [2018-10-18 Thu 16:30]--[2018-10-18 Thu 16:50] =>  0:36
    :END:

Even after offloading all of the test models, we are still breaching
Travis and AppVeyor limits. We need to figure out if we have any
unused types and remove them.

The quickest thing to do is to disable: test data and hashing for all
types in the main models except for =name=.

Since even after removing serialisation from the generation models
(cpp and csharp) we are still quite close to the edge, we need drastic
measures: we need to stop generating test data and serialisation for
all models. Test data was required for testing so we need to comment
out all tests of the generated code. Serialisation was also used for
some tests, though they have probably bit-rotted. We probably should
use serialisation only for round-trip tests and not as a way of
testing components as we did in the past. We should remove all tests
that rely on serialisation, even including dia - it should be replaced
by the XML tool anyway, so no point in paying the cost now. Or perhaps
we can leave dia for last and do all other models.

Problems:

- external.json:

: ../../../../projects/dogen.external.json/src/serialization/registrar_ser.cpp:29:10: fatal error: 'dogen.options/serialization/registrar_ser.hpp' file not found
: #include "dogen.options/serialization/registrar_ser.hpp"

Links:

- [[https://blog.dachary.org/2014/02/09/figuring-out-why-ccache-misses/][figuring out why ccache misses]]

*** COMPLETED Design a top-level Dogen API                            :story:
    CLOSED: [2018-10-22 Mon 17:12]
    :LOGBOOK:
    CLOCK: [2018-10-23 Tue 09:51]--[2018-10-23 Tue 10:44] =>  0:53
    CLOCK: [2018-10-22 Mon 16:05]--[2018-10-22 Mon 17:12] =>  1:07
    CLOCK: [2018-10-22 Mon 09:03]--[2018-10-22 Mon 09:21] =>  0:18
    CLOCK: [2018-10-20 Sat 07:48]--[2018-10-20 Sat 07:55] =>  0:07
    CLOCK: [2018-10-20 Sat 07:02]--[2018-10-20 Sat 07:43] =>  0:46
    CLOCK: [2018-10-19 Fri 10:12]--[2018-10-19 Fri 11:00] =>  0:48
    CLOCK: [2018-10-18 Thu 14:32]--[2018-10-18 Thu 14:42] =>  0:10
    CLOCK: [2018-10-18 Thu 09:57]--[2018-10-18 Thu 13:14] =>  3:17
    CLOCK: [2018-10-18 Thu 09:10]--[2018-10-18 Thu 09:15] =>  0:05
    CLOCK: [2018-10-17 Wed 16:26]--[2018-10-17 Wed 18:01] =>  1:35
    CLOCK: [2018-10-17 Wed 14:39]--[2018-10-17 Wed 15:10] =>  0:31
    CLOCK: [2018-10-16 Tue 15:49]--[2018-10-16 Tue 17:43] =>  1:54
    CLOCK: [2018-10-16 Tue 13:55]--[2018-10-16 Tue 14:55] =>  1:27
    CLOCK: [2018-10-11 Thu 16:06]--[2018-10-11 Thu 18:26] =>  2:20
    :END:

Jot down ideas on the separation between the API and the
implementation in dogen products.

Notes:

- we now have the notion of "distribution channels": UI/UX (wt, qt, gtk
  mobile, etc), DX (swagger, boost asio, library itself).
- the product API should not have any dependencies in terms of storage
  mechanisms; it should have some kind of "model source" interface
  that can then be implemented in terms of the filesystem, GH repo,
  postgres database etc.
- even though it does not make a lot of sense to have a model source
  as part of the remoting API, for consistency reasons we should still
  support it. That is, a code generation end point will merely call
  some internal functions to source the models rather than call
  another endpoint, and users probably don't really need something
  that just reads a model and returns the injector version.
- the distribution channels are a function of the product API.
- according to the [[https://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/][Framework Design Guidelines]], we should design the
  API from scenarios. We should create a specification document that
  can be added to the manual for this.

Architecture vision:

- there are two distinct notions of "generation": 1) code generation as
  users understand it, that is product and component generation, end
  to end; and 2) generation as a part of the pipeline. Since most
  developers are familiar with "codegen", we can use that to signify
  the top-level generation. This means the API should use this term,
  and the high-level orchestrator should have the same name.
- the development model is used by three stages in the pipeline:
  expansion, reduction and generation. We should make the model stand
  alone, containing all of the core modeling elements, and any helpers
  that work only on those types. Then we create models for each
  element in the pipeline; each of these models has an entry point
  which is named after the model (perhaps =expander=, etc). These are
  interfaces by means of DI.
- however one slight snag with this approach is tha the model itself -
  e.g. =expansion::meta_model::model= - is needed as we move from one
  stage to the next; reduction needs to first convert the expansion
  model, generation needs to first convert the reduction model and so
  forth.
- for generation we can use the "template approach", whereby the
  generation element becomes a template.
- following on from FDG, we will from now on name all models:
  ORGANISATION.PRODUCT.COMPONENT, where ORGANISATION is the top-level
  umbrella organisation (e.g. MASD), PRODUCT is the individual product
  (e.g. dogen) and the component is the library or executable.

*** COMPLETED Add support for nested namespaces                       :story:
    CLOSED: [2018-10-24 Wed 09:13]
    :LOGBOOK:
    CLOCK: [2018-10-23 Tue 17:02]--[2018-10-23 Tue 17:38] =>  0:36
    CLOCK: [2018-10-23 Tue 16:53]--[2018-10-23 Tue 17:01] =>  0:08
    CLOCK: [2018-10-23 Tue 16:48]--[2018-10-23 Tue 16:52] =>  0:04
    CLOCK: [2018-10-23 Tue 16:43]--[2018-10-23 Tue 16:47] =>  0:04
    CLOCK: [2018-10-23 Tue 16:29]--[2018-10-23 Tue 16:42] =>  0:13
    CLOCK: [2018-10-23 Tue 15:50]--[2018-10-23 Tue 16:05] =>  0:15
    CLOCK: [2018-10-23 Tue 15:29]--[2018-10-23 Tue 15:49] =>  0:20
    CLOCK: [2018-10-23 Tue 14:50]--[2018-10-23 Tue 15:28] =>  0:38
    CLOCK: [2018-10-23 Tue 14:13]--[2018-10-23 Tue 14:49] =>  0:36
    CLOCK: [2018-10-23 Tue 14:01]--[2018-10-23 Tue 14:12] =>  0:11
    CLOCK: [2018-10-23 Tue 13:24]--[2018-10-23 Tue 13:31] =>  0:07
    CLOCK: [2018-10-23 Tue 13:01]--[2018-10-23 Tue 13:05] =>  0:04
    CLOCK: [2018-10-23 Tue 12:46]--[2018-10-23 Tue 13:00] =>  0:14
    CLOCK: [2018-10-23 Tue 11:45]--[2018-10-23 Tue 12:10] =>  0:25
    CLOCK: [2018-10-23 Tue 11:34]--[2018-10-23 Tue 11:44] =>  0:10
    CLOCK: [2018-10-23 Tue 10:45]--[2018-10-23 Tue 11:33] =>  0:48
    CLOCK: [2018-10-22 Mon 19:43]--[2018-10-22 Mon 19:57] =>  0:14
    CLOCK: [2018-10-22 Mon 19:39]--[2018-10-22 Mon 19:42] =>  0:03
    CLOCK: [2018-10-22 Mon 18:27]--[2018-10-22 Mon 18:45] =>  0:18
    CLOCK: [2018-10-22 Mon 18:23]--[2018-10-22 Mon 18:26] =>  0:03
    CLOCK: [2018-10-22 Mon 17:35]--[2018-10-22 Mon 18:22] =>  0:47
    CLOCK: [2018-10-22 Mon 17:28]--[2018-10-22 Mon 17:34] =>  0:06
    :END:

Enable c++17. - windows requires cpp latest. Then fix inner namespaces
(e.g. =a::b::c=).

We still need to support the old syntax for pre c++-17.

We need to add a new standard to =generator.cpp= and when its set to
c++-17 we should automatically use nested namespaces.

Problems:

- Need to fix boost serialisation:

: namespace boost {
: namespace serialization {

Links:

- [[https://stackoverflow.com/questions/11358425/is-there-a-better-way-to-express-nested-namespaces-in-c-within-the-header][Is there a better way to express nested namespaces in C++ within the header]]
- [[https://en.cppreference.com/w/cpp/language/namespace][C++ Namespaces]]
- [[http://www.nuonsoft.com/blog/2017/08/01/c17-nested-namespaces/][C++17: Nested Namespaces]]

*** STARTED Read up on framework and API design                       :story:
     :LOGBOOK:
     CLOCK: [2018-10-24 Wed 08:24]--[2018-10-24 Wed 09:03] =>  0:39
     CLOCK: [2018-10-23 Tue 13:07]--[2018-10-23 Tue 13:23] =>  0:16
     CLOCK: [2018-10-23 Tue 08:48]--[2018-10-23 Tue 09:50] =>  1:02
     CLOCK: [2018-10-22 Mon 08:30]--[2018-10-22 Mon 09:02] =>  0:32
     CLOCK: [2018-10-19 Fri 09:54]--[2018-10-19 Fri 10:11] =>  0:17
     CLOCK: [2018-10-19 Fri 09:28]--[2018-10-19 Fri 09:53] =>  0:25
     CLOCK: [2018-10-18 Thu 09:36]--[2018-10-18 Thu 09:56] =>  0:20
     CLOCK: [2018-10-17 Wed 14:05]--[2018-10-17 Wed 14:19] =>  0:14
     CLOCK: [2018-10-17 Wed 10:03]--[2018-10-17 Wed 10:52] =>  0:49
     :END:

Now that we are creating a top-level API for Dogen we should really
read up on books about good API design.

Namespacing guideline:

- company | project
- product | technology
- feature
- subnamespace

So in our case, =masd::dogen= and =masd::cpp_ref_impl=. We are
violating the guideline on no abbreviations with ref_impl but
=cpp_reference_implementation= seems a tad long.

It seems we have several types of classes:

- interfaces
- abstract base classes
- values
- objects where data dominates and behaviours are small or trivial
- objects where behaviour dominates and data is small or trivial
- static classes

These should be identifiable at the meta-model level, with appropriate
names.

*** STARTED Update dogen namespaces to match the new specification    :story:
    :LOGBOOK:
    CLOCK: [2018-10-24 Wed 15:42]--[2018-10-24 Wed 17:03] =>  1:21
    CLOCK: [2018-10-24 Wed 13:16]--[2018-10-24 Wed 15:41] =>  2:25
    CLOCK: [2018-10-24 Wed 12:48]--[2018-10-24 Wed 13:15] =>  0:27
    CLOCK: [2018-10-24 Wed 09:32]--[2018-10-24 Wed 12:02] =>  2:30
    CLOCK: [2018-10-24 Wed 09:04]--[2018-10-24 Wed 09:31] =>  0:27
    CLOCK: [2018-10-22 Mon 17:13]--[2018-10-22 Mon 17:28] =>  0:15
    :END:

Following on [[https://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/names-of-namespaces][from FDG]], we will from now on name all models:

: ORGANISATION.PRODUCT.COMPONENT

Where ORGANISATION is the top-level umbrella organisation (e.g. MASD),
PRODUCT is the individual product (e.g. dogen) and the component is
the library or executable.

There have been many discussions on this topic, and there are many
pluses and minuses, but one important point is that we need a
consistent approach, and this is as supported by FDG.

We will update the model modules rather than external modules so that
the directory names follow the same naming convention.

We need to update one model at a time.

*** STARTED Create a single binary for all of dogen                   :story:
    :LOGBOOK:
    CLOCK: [2018-10-19 Fri 14:41]--[2018-10-19 Fri 16:39] =>  1:58
    :END:

As per analysis, we need to create a single dogen binary, like so:

: dogen.cli COMMAND COMMAND_SPECIFIC_OPTIONS

Where =COMMAND= is:

- =transform=: functionality that is currently in tailor.
- =generate=: functionality that is currently in knitter.
- =expand=: functionality that is currently in stitcher plus expansion
  of wale templates.
- =make=: functionality in darter: create project, structure etc.

In order to support sub-commands we need to do a lot of hackery with
program options:

- [[https://gist.github.com/randomphrase/10801888][cmdoptions.cpp]]: Demonstration of how to do subcommand option
  processing with boost program_options
- [[https://stackoverflow.com/questions/15541498/how-to-implement-subcommands-using-boost-program-options][How to implement subcommands using Boost.Program_options?]]

*Merged Stories*

We started off by creating lots of little executables: knitter,
darter, tailor, stitcher. Each of these has its own project,
command-line options etc. However, now that we are concentrating all
of the domain knowledge in yarn, it seems less useful to have so many
executables that are simply calling yarn transforms. Instead, it may
make more sense to use an approach similar to git and have a
"sub-command":

: dogen knit
: dogen tailor

And so forth. Of course, we could also take this opportunity and clean
up these names to making them more meaningful to end users. Perhaps:

: dogen codegen
: dogen transform

Each of these sub-commands or modes would have their own set of
associated options. We need to figure out how this is done using boost
program options. We also need to spend a bit of time working out the
sub-commands to make sure they make sense across the board.

In terms of names, we can't really call the project "dogen". We should
call it something allusive to the command line, such as cli. However,
the final binary should be called dogen or perhaps, =dogen.cli=. This
fits in with other binaries such as =dogen.web=, =dogen.http=,
=dogen.gui= etc.

*** Rewrite name resolution in terms of lists                         :story:

Even since we did the external modules / model modules change we broke
code generation; this is because we do not go up the model modules
during name resolution. We did a quick hack to fix this but it needs
to be done properly.

Let's walk through a simple example:. Name cames in as:

- model module: =probing=
- simple: =prober=

We are in model:

- model module: =dogen.external=

Expected behaviour is to try all combinations of model modules:

- =dogen.external.probing=
- =dogen.probing
- =probing=

This highlights a fundamental problem with resolution: we view the
{external, model, internal} modules as if they are separate entities
but in reality, for the purposes of resolution, there is only one
thing that is relevant: the module path. If it matches because of
{external, model, internal} modules, well that is not relevant to
resolution. Other users of =name= do need to know this information
(for example to generate directories or file names) but not the
resolver.

Interestingly, because we are only looking for an id, it doesn't
really matter how we get to it (in terms of the internal composition
of the name), as long as it matches bitwise. This means we can look at
the process slightly differently:

- start off with the name as the user provided it. Extract all strings
  from it to create a list, in order: external, model, internal,
  simple. Try to resolve that. Call it user list.
- then create a second list from model / context: external, model,
  internal. Call it model list.
- try concantenating model list and user list, pretty printing and
  resolving it. If it fails, pop model list and concatenate again. Try
  until model list is empty.

Tasks:

- first add a quick hack just to get the code generator working
  again. For example, take the first model module of the model and try
  resolving with that. Then worry about fixing this properly.
- split the conversion of name into list from pretty printer. Printer
  should merely take a string or list of strings and do its thing. We
  need to find a good location for this method, since (for now) we
  cannot place it in the right location which is the name class
  itself.
- change resolver to obtain the lists as per above. The to list
  machinery can be used for this, though we need to handle model names
  somehow. We can copy the =model_name_mode= logic from printer.
- drop all of the logic in resolver at present and use the list logic
  as per above. Do not check references, etc.

*** Implement the new dogen product API                               :story:

Now we've designed a clean top-level API for the product, we need to
code-generate it and implement it in terms of the existing code.

*** Remove hello world model                                          :story:

 It is confusing to have it mixed up with product models. Use a regular
 dogen model to test the package. We could have it on the reference
 model as a stand alone example, or we could create a "hello dogen"
 product for a trivial example of dogen usage.

*** Move from doxygen to standardese                                  :story:

We should try to use standardese to generate the documentation for
dogen. Seems easier to use and CMake friendly. Also, it seems more c++
compliant because it uses libclang.

Once the move is done, we should update dogen to generate comments in
either markup via a meta-data parameter (documentation markup?).

Links:

- https://github.com/foonathan/standardese

*** Update ref impl namespaces to match the new specification         :story:

Perform the namespace update to the reference implementation.

*** New approach to model testsing                                    :story:

In the beginning we generated all models with all facets, even the
dogen core models. The idea was to test the generator even though
these facets were not useful for the product. This was really useful
because the dogen models are much more realistic than the test models
and due to this we picked up a number of bugs. However, we have now
hit the maximum build times on travis and we need to start removing
all ballast. This will mean we lose these valuable tests. The
alternative is to create these tests on the fly:

- create a new override flag that forces all facets to be emitted.
- create a new test facet with templates that are dependent on the
  enabled facets; each test tests the dependent facet.
- create a ctest nightly build that generates code using these new
  facets, compiles it and runs all tests.
- we need some meta-data to "ignore" some modeling elements for
  certain facets such as composition which are known to be broken. Or
  maybe we should just leave the tests as red so we know.
- the tests should be designed not to use templates etc to make the
  debug dumps really obvious (unlike the existing tests). It may even
  make more sense to test each type individually so that when the test
  fails its really obvious:

: MY_TYPE_serialisation_roundtrips_correctly

  this way when we look at CDash we know exactly which types failed to
  serialise.

During the transition phase, we will remove all of the existing tests.

*** Add support for multiple profile binds per modeling element       :story:

At present we can only bind an element to one profile. The reason why
is because we've already expanded the profile graphs into a flat
annotation and if we were to apply two of these expanded annotations
with common parents, the second application would overwrite the
first. Of course, we bumped into the exact same problem when doing
profile inheritance; there it was solved by ensuring each parent
profile is applied only once for each graph.

One possible solution for this problem is to consider each model
element as a "dynamic profile" (for want of a better name; on the fly
profile?). We would create a profile which is named after each of the
profiles it includes, e.g. say we include =dogen::hashable= and
=dogen::pretty_printable= for model element e0. Then the "on the fly
profile" would be:

: dogen::hashable_dogen::pretty_printable

It would be generated by the profiler, with parents =dogen::hashable=
and =dogen::pretty_printable=, and cached so that if anyone shows up
with that same profile we can reuse it. Because of the additive nature
of profile graphs this would have the desired result. Actually we
could probably have a two pass-process; first identify all of the
required dynamic profiles and generate them; then process them. This
way we can rely on a const data structure.

This will all be made easier when we have a two-pass pipeline because
we can do the profile processing on the first pass, and we can even
generate the "dynamic profiles" as real meta-model elements, created
on the fly.

*** Facet enablement and model references is buggy                    :story:

At present we are processing enablement as part of the
post-processing. This means that we are using the target model's
annotation profile in order to determine the facet enablement. This
can cause problems as follows: say we enable hashing on a model via
the model profile of M0. We then consume that model as a reference and
disable hashing on M1. When processing types from M0 for M1 we will
disable hashing for them as well. Thus, no includes for hashing will
be generated even if a hash map is used.

Actually this is not quite right. We are expanding annotations at the
external model transform level; this means the enablement on the
reference must be correct. However, somehow we seem to be looking at
the element on the target model when deciding to include the hash
file from reference model.

*** Consider creating a test build for all facets                     :story:

In the past we had enabled a lot of facets on the dogen models to
serve as part of the testing infrastructure. However, its no longer
feasible to do this because the build is taking too long. However, the
reference models just can't capture all of the complexity of a
codebase like dogen's so we lost some testability with this move. What
would be really nice is if we could create "test builds":

- given a set of test models, copy them somewhere, generate a product
  configuration with some kind of override that enables all facets
  everywhere. some will just not come through like ORM.
- build the product. all handcrafted code is now blank but all facets
  are coming though.
- this could be part of the ctest script, as a "mode" - product
  generation test. Every time there is a commit to a product the build
  kicks in.

Notes:

- one way to achieve this would be to force the profile of the
  model. However, we are moving away from profiles, and in the future
  there will be a list of stereotypes associated with the model. Then
  it will be much harder to figure out what stereotypes do what and to
  overwrite them.
- an alternative would be to have some kind of "test mode"; when
  handling enablement, we'd check the "mode". If we're in test mode,
  we simply enable all and ignore any other settings. We could have a
  "force enable" flag or some such like we do for
  overwriting. However, we may then hit another problem: enabling all
  facets may result in non-buildable models:
  - facets may be incompatible. This is not a problem at present.
  - handcrafted classes may result in code that does not
    compile. Shouldn't though because we are still checking the status
    of the attributes.
- the key thing though is the overall build time must be below the
  threshold. Maybe we can have this on a nightly, running on our own
  hardware.

Conclusions:

- create a new flag: =force-enablement=. When set to true, we ignore
  all enablement settings and generate all facets. We do not generate
  all kernels though (e.g. the kernel must be on in the model).
- create a script that copies the models to a new product and
  generates them with fore-enablement. This will only work when we can
  generate products.
- as facets are enabled, tests are automatically generated for them.
- build the result and run all tests.

*** Create some basic naming guidelines                               :story:

As per Framework Design Guidelines, we need some basic guidelines for
naming in Dogen. We don't need to go overboard, we just need something
to get us started and evolve it as we go along.

Links:

- [[https://isocpp.org/wiki/faq/coding-standards][C++ Coding Standards]]
- [[http://wiki.c2.com/?CapitalizationRules][Capitalization Rules]]
- [[https://en.wikipedia.org/wiki/Snake_case][Snake Case]]
- [[http://cs.smu.ca/~porter/csc/ref/stl/naming_conventions.html][Naming Conventions for these STL Reference Pages]]
- [[https://style-guides.readthedocs.io/en/latest/cpp.html][C++ coding style guide]]
- [[https://stxxl.org/tags/1.4.1/coding_style.html][Coding Style Guidelines]]
- [[https://www.fluentcpp.com/2018/04/24/following-conventions-stl/][Make Your Containers Follow the Conventions of the STL]]

*** Consider generating program options code                          :story:

If there was a syntax to describe boost program options, we should be
able to generate most of the code for it:

- the code that initialises the options;
- the domain objects that will store the options;
- the copying of values from program options objects into domain
  objects.

This would mean that creating a command line tool would be a matter of
just supplying an options file. We could then have a stereotype for
this (name to be yet identified). Marking a type with this stereotype
and supplying the appropriate meta-data so one could locate the
options file would cause dogen to emit the program options binding
code.

A similar concept seems to exist for python: [[http://docopt.org/][docopt]]. We should keep
the same syntax. We just need to have a well defined domain object for
these. The aim would be to replace config.

For models such as these, the dia representation is just overhead. It
would be great if we could do it using just JSON.

Actually even better would be if we could have a text file in docopt
format and parse it and then use it to generate the code described
above.

Actually maybe we are just making this too complicated. We probably
just need some very trivial meta-data extensions that express the
required concept:

- create a yarn element to model this new meta-class. We basically
  need to model the structure of program options with option groups
  and options.
- define a stereotype for the new yarn elements, say
  =CommandLineOptionGroup=.
- for types facet we simply generate the regular c++ code. But in
  addition, we also generate a new facet that: a) injects the
  propertties into boost program options b) instantiates the c++
  objects from boost program options.
- this means that instead of creating a new meta-type, we need to
  augment =yarn::object= with command line options stuff.

Notes:

- create stereotypes for options group, options; allow users to define
  members of type options in options group. Or should the options just
  be member variables? In which case we could have
  =command_line::options= as the stereotype.
- generate the options classes.
- inject a hand-crafted validator or consider generating the validator
  given the meta-data supplied by the user (mandatory, at most X
  times, etc).
- generate an options builder that takes on the building
  responsibilities from the parser.
- generate a parser that hooks the builder and copies data from the
  options map into the options.
- allow users to supply the help text and the version text as
  parameters; these should probably be done in a similar way to what
  we do with the modeline etc.
- allow users to set default values in the options attributes and set
  them in generated code. This is probably just adding default value
  support to dogen, for which we have a separate story.
- one very useful way in which to use program options is via
  projections. That is a given model M0 defines the configuration and
  a second model M1 defines the options parsing. In this case the
  options defined in M0 already has the required shape:
  - there is a top-level class housing all options, traditionally
    called "configuration";
  - the top-level class contains meta-data with the product blurb;
  - attributes of that class can be annotated as "modes", "groups" or
    nothing. A mode will result in a modal CLI interface. Groups
    result in top-level groupings of options. Nothing means the
    attribute must be of a simple type and will be a global option
    (e.g. =help=, =version=, etc).
  - attributes have a description, etc associated as meta-data. They
    also have other useful annotations such as optional, mandatory
    etc. These are used in validation. Interestingly this may mean we
    can also automatically generate a validator.
  - dogen generates in M1 a set of chained program option parsers
    (assuming a modal interface; otherwise just one) which generate
    the M0 options.
  - in M1, users define a class with attribute
    =masd::command_line_options=, associated with an options class.
  - users can choose the "backend": boost program options, etc. Each
    is implemented as a separate template.
  - dogen generates a parser with an associated exception
    (parser_validation_error). The exception is simply injected as a
    type.

Links:

- [[https://github.com/abolz/CmdLine2][CmdLine2]]: alternative library to program options.

*** Exclude profiles from stereotypes processing                      :story:

At present we are manually excluding profiles from the stereotypes
transform. This was just a quick hack to get us going. We need to
replace this with a call to annotations to get a list of profile names
and exclude those.

We should also rename =is_stereotype_handled_externally= to something
more like "is profile" or "matches profile name".

Actually the right thing may even be to just remove all of the profile
stereotypes during annotations processing. However, we should wait
until we complete the exomodel work since that will remove scribble
groups, etc. Its all in the annotations transform.

*** Problems in tailor generation of dogen models                     :story:

Regenerated all models, got the following errors:

- we are adding the extension to the dia filename because of how CMake
  works. We should probably remove the output parameter or at least
  allow defaulting it to a replacement of the extension.
- we are removing the dependencies due to duplicates in JSON keys.
- we are looking for .dia diagrams instead of .json for references.

*Previous Understanding*

We converted all of dogen's models from dia into JSON using tailor and
code-generated them to see if there were any differences.

Issues to address:

- problems with =quilt.cpp= and =yarn.dia= / =yarn.json=: the
  conversion of the model path did not work as expected - we do not
  know of the "."  separator. Fixed it manually and then it all worked
  (minus CMakeLists, see below). We could possibly fix the builder to
  automatically use the "." to separate model paths. Actually with the
  latest changes we now seem to only be looking at the first model
  module, so for =yarn.dia= we only have =yarn=.
- CMakeLists were deleted on all models for some reason, even though
  the annotations profile look correct.
- in quilt we correctly generated the forward declarations for
  registrar error and workflow error without including boost
  exception. Not sure why that is, nor why it is that we are including
  them for forward declarations.
- Missing include of registrar serialisation in
  =all_ser.hpp=. Instability in =registrar_ser.cpp=, but content is
  correct otherwise.
- =database.json= generated invalid JSON.
- references in dia diagrams have the dia extension. This means that
  they do not resolve when converted to JSON.

"Script":

#+begin_src
rm *.json
A="dia knit quilt.cpp wale yarn.json annotations formatters quilt yarn database options stitch yarn.dia"
for a in $A; do /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/bin/dogen.tailor -t $a.dia -o $a.json; done
for a in $A; do /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/bin/dogen.knitter -t ${a}.json --cpp-project-dir /home/marco/Development/DomainDrivenConsulting/dogen/projects --ignore-files-matching-regex .*/CMakeLists.txt --ignore-files-matching-regex .*/test/.* --ignore-files-matching-regex .*/tests/.* --verbose --delete-extra-files; done
#+end_src

In an ideal world, we should probably have a script that we run as
part of =knit_and_stitch= that converts to tailor and then runs
knitter on the models, so that we keep track of tailor breaks outside
of JSON test models.

*** Log file names do not have frontend                               :story:

Add extension to log file name so that we can see both Dia and JSON
logs at the same time. At present, one overwrites the other because we
do not have the frontend (e.g. the extension) on the log file name.

*** Update static strings to string views                             :story:

Now we're on C++17 we can start making use of its new features. One
low hanging fruit is string view. We use static strings quite a lot
for logging etc. We can just replace these with string views.

Links:

- [[https://www.bfilipek.com/2018/10/strings17talk.html][Let's Talk About String Operations in C++17]]

*** Add basic "diff mode"                                             :story:

We need a very simple way of checking all generated files in memory
against what's in the file system and returning a flag if they are
different. We can then use these flags to determine if tests pass. In
the future we can extend this approach to include a proper diff of the
files, but for now we just need a reliable way to run system tests
again.

Actually the right solution for this is to see the processing as part
of a chain:

- out of the generator come a set of artefacts with operations (write,
  merge, ignore)
- these get joined with a transform that reads the state of the file
  system. It then adds more operations: delete, etc. If there are no
  diffs, it marks those files as skip.
- the final step is a processor which gets that model and executes the
  operations. This can then be replaced by a "reporter" that simply
  states what the operations would be.

Diff mode is using the report to see if there are any diffs.

Merged Stories:

*Validation-only or dry-run mode*

Both stitcher and knitter could do with a "dry-run" mode in which we'd
do everything except for actually outputting.

*For Knitter*

It would be nice if one could just check if a dia diagram is valid for
code generation, e.g. =--validate= or something along those lines.

*For Stitch*

We are interested in performing the parsing. This would be useful for
example for a flymake mode in emacs.

An additional feature of dry-run would be to run, generate the model
and then produce a unified diff, e.g. tell me what you'd change. For
this we'd have to link against a diff library. We need to
automatically exclude non-overwrite files (or have an option to
exclude/include them).

Links:

- [[https://github.com/google/diff-match-patch/tree/master/cpp][google Diff Match Patch library]]
- [[https://github.com/cubicdaiya/dtl][DTL: Diff Template Library]]
- [[https://stackoverflow.com/questions/1451694/is-there-a-way-to-diff-files-from-c][SO: Is there a way to diff files from C++?]]

*Dry-run option to just diff with existing generated code*

#+begin_quote
*Story*: As a dogen user, I want to know what has changed with the
next code generation so that I can evaluate if the changes are as
expected or not.
#+end_quote

It would be useful to have an option that would do everything except
writing the files to disk; instead, it would diff them with the
existing files and report if there are any differences. This would be
useful to make sure the source code matches the latest version of the
diagram.

We could use something like the [[https://code.google.com/p/dtl-cpp/wiki/Tutorial][DTL library]].

*** Rename debian package                                             :story:

At present our package is called =dogen-applcations=. Since there will
only be one dogen application/package, this is a confusion name. We
should rename it. Names:

- masd-dogen

*** Finish adding support for clang-cl builds                         :story:

We have added preliminary support for building with clang-cl on
windows, but the build is not green. Most of the errors seem to be on
boost.

Links:

- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/6bnv6ppljlklu2ag][Release build]]
- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/45yhn8sdhexvsdmi][Debug build]]
- [[https://github.com/Kitware/CDash/issues/733][CDash reporting problems]]

*** Tidy-up dogen windows package                                     :story:

There are a few inconsistencies with the package:

- dogen components have a strange structure:
  "Dogen/runtime/dogen".
- we should probably have a top-level umbrella for MASD, under which
  dogen installs.
- package name is windows amd64. We should use the vcpkg triplets for
  simplicity (e.g. x64-windows).

*** Mapping of third-party dependencies                               :story:

System models should follow the physical structure of
dependencies. That is, we should not have a "boost" system model, but
instead a boost-test etc. Each of these can then have mappings
(e.g. vcpkg name, build2 name, etc). Users must declare these
references just like they do with user models. Dogen can then create
code for:

- cmake targets, properly linking against libraries;
- vcpkg install, at product level, by de-duplicating component
  dependencies;
- possibly distro dependencies.

We should only have a mandatory dependency, which is the STL. In
addition, we need different models for each version (e.g. c++ 03,
etc). This makes it easier to include the right types.

Note that each model must have an associated version. The version
should be part of the file name. However, maybe we need to distinguish
between TS version (11, 17, etc) from library version.

*** Rename input models directory to models                           :story:

We need to move the dogen project to the new directory layout whereby
all models are kept in the =models= directory.

*** dogen as a github integration                                     :story:

Perhaps there are some useful services dogen could provide to users in
terms of dogen integration. If, with every commit, we could regenerate
the model and read the current state in github, we could then provide
a status report:

- the model does not build; red emblem. Some changes were made to the
  model (or to dogen) that make the model invalid. User should take
  action.
- the model builds but generates files that are different from what's
  checked in on github. yellow emblem. Provide a report with the
  diffs. This can either be because the code generator has changed or
  the user changed the model.
- the model builds and generates exactly the same code; green emblem.

With this approach we have two advantages:

- we do not need to add projects as part of the dogen tests; the
  service takes its place. We can still add a few as the core tests,
  but we don't need to expand it much beyond reference implementation
  and dogen itself.
- we exercise dogen itself as well as the rest endpoint generation
  code in a way that is actually useful to end users; it would be nice
  to know immediately when something breaks.

Notes:

- we'll need some kind of way of dealing with tokens and secrets in
  order to support private GH projects.

*** Add reporting support to dogen model testing                      :story:

Dogen should have a mode which generates a report for a run rather
than code generate. The report could look like so:

:              /project_a
:                  /summary for this commit
:                  /diffs
:                  /errors
:                  /benchmark data
:                  /probing data
:                  /log

If the report was largely in HTML we could link it to the dogen docs
and save it into git. This would make troubleshooting much easier. If
the report contains the probing data it would be easier to figure out
what went wrong. We should also keep track of the model that was
generated (e.g. its location and git commit) so we can download it and
reproduce it locally.

*** Rework the tests using diff mode                                  :story:

Once we have diff mode, we need to find some kind of workflow for
tests:

- each product is composed of a git URL and a list of models.
- we git clone all repos as part of the build process.
- directories and model locations are hard-coded in each test.
- test runs against the model and hard-coded location, produces the
  diff. Test asserts of the diff being non-zero.

*** Fix the northwind model                                           :story:

There are numerous problems with this model:

- at present we have oracle support on ODB. Oracle libs are not
  distributed with debian. If we do not find oracle we do not compile
  northwind. This is not ideal. We should remove oracle support from
  northwind, and install odb support in the build machine (hopefully
  available as debs).
- the tests are commented out and require a clean up.
- the tests require a database to be up.

Notes:

- it is possible to setup [[https://docs.travis-ci.com/user/database-setup/#postgresql][postgres on travis]]

*** Simplify split configuration configuration                        :story:

At present we have two separate command line parameters to configure
the main output directory and the directory for header files. The
second parameter is used for split configurations. The problem is that
we now need to treat split configuration projects specially because of
this. It makes more sense to force the header directory to be relative
to the output path and make it a meta-data parameter.

*** Make "ignore regexes" a model property                            :story:

At present we have a command line option:
=--ignore-files-matching-regex=. It is used to ignore files in a
project. However, the problem is, because it is a command line option,
it must be supplied with each invocation of Dogen. This means that if
we want to run dogen from outside the build system, we need to know
what options were set in the build scripts or else we will have
different results. This is a problem for testing. We should make it a
meta-data option, which is supplied with each model and even more
interesting, can be used with profiling. This means we can create
profiles for specific purposes (ODB, lisp, etc) and then reuse them in
different projects.

We should do the same thing for =--delete-extra-files=.

*** Update all stereotypes to masd                                    :story:

We need to start distinguishing MASD from dogen. The profile for UML
is part of MASD rather than dogen, so we should update all stereotypes
to match. We need to make a decision regarding the "dia extensions" -
its not clear if its MASD or dogen.

*** Incorrect generation when changing external modules               :story:

When fixing the C# projects, we updated the external modules, from
=dogen::test_models= to =CSharpRefImpl=. Regenerating the model
resulted in updated project files but the rest of the code did not
change. It worked by using =-f=. It should have worked without forcing
the write.

*** Code coverage does not work for C#                                :story:

It seems that using NUnit and OpenCov does not work. The main reason
appears to be the use of shadow copying, which is no longer optional
on NUnit 3.

Links:

- https://github.com/Ullink/gradle-opencover-plugin/issues/1
- https://github.com/codecov/example-csharp/blob/master/appveyor.yml
- https://www.appveyor.com/blog/2017/03/17/codecov/

*** Improve comments on reference implementation                      :story:

At present it is very difficult to understand what each model and/or
each type does in the reference implementations. We need to add some
comments to make it more obvious.

*** Code generate C# models using msbuild                             :story:

At present we did a quick hack to code generate in C#: a simple bash
script that runs dogen. However, this is not how we expect the end
user to consume it; there should be a msbuild target that:

- detects the code generator;
- contains the configuration (e.g. options, location of models);'
- runs the code generator - possibly every time models change;
- has a tailor target to generate JSON.

*** Add project documentation                                         :story:

We should be able to create a simple set of docs following on from the
[[https://ned14.github.io/outcome/][outcome project]]. They seem to be using Hugo.

Links:

- https://github.com/foonathan/standardese
- https://github.com/ned14/outcome/tree/develop/doc/src

*** Create the =generation= model                                     :story:

Create a new model called =generation= and move all code-generation
related class to it.

We need to create classes for element properties and make model have a
collection that is a pair of element and element properties. We need a
good name for this pair:

- extended element
- augmented element
- decorated element: though not using the decorator pattern; also, we
  already have decoration properties so this is confusing.

Alternatively we could just call it =element= and make it contain a
modeling element.

Approach:

- create a new generation model, copying across all of the meta-model
  and transform classes from yarn. Get the model to transform from
  endomodel to generation model.
- augment formattables with the new element properties. Supply this
  data via the context or assistant.

Problems:

- all of the transforms assume access to the modeling element means
  access to the generation properties. However, with the introduction
  of the generation element we now have a disconnect. For example, we
  sometimes sort and bucket the elements, and then modify them; this
  no longer works with generation elements because these are not
  pointers. It would be easier to make the generation properties a
  part of the element. This is an ongoing discussion we've had since
  the days of formattables. However, in formattables we did write all
  of the transforms to take into account the formattable contained
  both the element and the formattable properties, whereas now we need
  to update all transforms to fit this approach. This is a lot more
  work. The quick hack is to slot in the properties directly into the
  element as some kind of "opaque properties". We could create a base
  class =opaque_properties= and then have a container of these in
  element. However, to make it properly extensible, the only way is to
  make it a unordered set of pointers.
- actually the right solution for this is to use multiple
  inheritance. For each modeling element we need to create a
  corresponding generation version of it, which is the combination of
  the modeling element and a generation element base class. Them the
  generation model is made up of pointers to generation elements and
  it dispatches into generation elements descendants in the
  formatter. The key point is to preserve the distinction between
  modeling (single element) vs generation (projection across facet
  space).

*** Create a =ci= folder in build                                     :story:

We should use the same approach as nupic for organising the scripts: a
top-level =ci= folder with folders per CI system. We should also
follow their naming convention for the build scripts which seem to
follow the CI events.

Links:

- https://github.com/numenta/nupic.core/tree/master/ci

*** Adding reference to itself results in resolution errors           :story:

Whilst trying to fix the JSON models we inadvertently added a
self-reference in =dogen.generation.json=:

:    "yarn.reference": "dogen.generation.json",

This resulted in some puzzling errors:

: 2018-10-18 19:15:00.861210 [ERROR] [yarn.transforms.enablement_transform] Duplicate element archetype: quilt.cpp.serialization.registrar_implementation <dogen><generation><registrar>

Ideally we should either warn and ignore or fail to process models
with self-references.

*** Handcrafted stereotype would not work on C# models                :story:

At present we do not have any tests for C# models with handcrafted. If
we did it would not work because the annotations only set the C++
facets. Add a test for this and fix the annotation profile. The same
problem will apply to all new profiles:

- pretty printable
- serialisable
- hashable

*** Rename facets                                                     :story:

We originally called our support for =std::hash= just =hash= and our
support for =boost::serialization= just =serialization=. The problem
is:

- we may want to also support =boost::hash=.
- we may want to support other serialisation types.

We should rename these. Perhaps:

- =std_hash=
- =boost_serialization=: a tad verbose, but quite explicit.

In addition, =io= is very misleading as the facet is not supposed to
do I/O proper (e.g. serialisation) but more pretty-printing or debug
dumping. So perhaps =pretty_print=.

*** Update annotation profiles and stereotypes to masd namespace      :story:

We should rename all annotation profiles and all stereotypes into the
MASD namespace.

We should also rename the artefact formatters to a compliant names,
e.g. instead of =C# Artefact Formatter= maybe
=dogen::csharp_artefact_formatter=. Note its dogen not MASD because
these are dogen specific profiles. We need to create a model for
dogen, separate from the MASD standard profile.

*** Consider adding collections at the meta-model level               :story:

A very common pattern we've observed is the creation of "container"
classes that have a member which is a container (say
=std::list<some_type>=) and a set of properties associated with that
container (name, description, etc). We then expose the underlying type
to users directly. However, perhaps it would make more sense to create
a new meta-type (collection? container?) with associated meta-data; in
C++, it would translate to the creation of a class obeying C++
collection concepts such as iterators etc.

Once this is in place we get code that expresses the high-level domain
concepts but its still pluggable into the STL machinery such as
algorithms, ranges, etc.

This also addresses somewhat the annoying issue with pretty printing
of STL containers because most of the time we would have a domain type
for the STL collection. The approach is akin to C#'s idea of
inheriting from collections, except here we use composition.

This also means we can transparently change the underlying collection;
though it would force a recompile, no code should break (e.g. from
list to vector or array).

** Deprecated
*** CANCELLED Sort out iconv on windows                               :story:
    CLOSED: [2018-10-16 Tue 11:49]

*Rationale*: no longer needed now we're using vcpkg.

Latest conan packages seem to have changed how iconv is packaged. Output:

: Get-ChildItem C:/Users/appveyor/.conan/data/libiconv/1.15/bincrafters/stable/package/f6bcf0d95fafcf303dfebe42c8562386d4cdbf69 -Recurse
:
    Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: d-----        1/25/2018   1:30 PM                bin
: d-----        1/25/2018   1:30 PM                include
: d-----        1/25/2018   1:30 PM                lib
: d-----        1/25/2018   1:30 PM                licenses
: d-----        1/25/2018   1:30 PM                share
: -a----        1/25/2018   1:30 PM            466 conaninfo.txt
: -a----        1/25/2018   1:30 PM           1308 conanmanifest.txt
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\bin
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM        1047040 iconv.exe
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\include
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM           9270 iconv.h
: -a----        1/25/2018   1:30 PM           1512 libcharset.h
: -a----        1/25/2018   1:30 PM           1319 localcharset.h
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\lib
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM            196 charset.alias
: -a----        1/25/2018   1:30 PM           8030 charset.lib
: -a----        1/25/2018   1:30 PM        1111174 iconv.lib
: -a----        1/25/2018   1:30 PM           1106 libcharset.la
: -a----        1/25/2018   1:30 PM           1100 libiconv.la
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\licenses
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM          25291 COPYING.LIB
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\share
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: d-----        1/25/2018   1:30 PM                doc
: d-----        1/25/2018   1:30 PM                man
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\share\doc
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM           6438 iconv.1.html
: -a----        1/25/2018   1:30 PM           6335 iconv.3.html
: -a----        1/25/2018   1:30 PM           4399 iconvctl.3.html
: -a----        1/25/2018   1:30 PM           2054 iconv_close.3.html
: -a----        1/25/2018   1:30 PM           8489 iconv_open.3.html
: -a----        1/25/2018   1:30 PM           3406 iconv_open_into.3.html
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\share\man
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: d-----        1/25/2018   1:30 PM                man1
: d-----        1/25/2018   1:30 PM                man3
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\share\man\man1
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM           4231 iconv.1
:     Directory: C:\Users\appveyor\.conan\data\libiconv\1.15\bincrafters\stable\package\f6bcf0d95fafcf303dfebe42c8562386d4cdbf69\share\man\man3
: Mode                LastWriteTime         Length Name
: ----                -------------         ------ ----
: -a----        1/25/2018   1:30 PM           4239 iconv.3
: -a----        1/25/2018   1:30 PM           2385 iconvctl.3
: -a----        1/25/2018   1:30 PM           1044 iconv_close.3
: -a----        1/25/2018   1:30 PM           4671 iconv_open.3
: -a----        1/25/2018   1:30 PM           1822 iconv_open_into.3

We have commented it out from CPack for now.
*** CANCELLED Rename options to transformation request                :story:
    CLOSED: [2018-10-19 Fri 14:54]

*Rationale*: this will be cleaned up as part of the work on the
product API.

These are not really "options"; it is a request made into yarn to
code-generate a model. We haven't yet got a proper name but it has to
somehow involve the word "request". The best way is to visualise this
as part of some API where may such requests can be made (and handled
concurrently).

This also means we need to split out the request from the context. We
should have an initialisation phase where we construct the context and
then we should be able to reuse the pipeline for many requests. This
also means that the right place to put the transform metrics is in the
request - not the context - given that these are request specific.

The best way to go about it may be to have two contexts:

- transformation context: const; loaded at start-up.
- request context: request specific context, including probing and the
  request itself.

Then:

- clients are responsible for setting up the transformation
  context. This ensures we do it only once.
- clients are also responsible for setting up the request context, but
  they then do it for each request.

Note also that a request should support multiple target models.
