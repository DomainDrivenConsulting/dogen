#+title: Sprint Backlog 19
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Move fabric into coding meta-model.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-11-13 Wed 18:41]
| <75>                                                      |         |       |       |       |
| Headline                                                  | Time    |       |       |     % |
|-----------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                              | *76:38* |       |       | 100.0 |
|-----------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                   | 76:38   |       |       | 100.0 |
| Active                                                    |         | 76:38 |       | 100.0 |
| Edit release notes for previous sprint                    |         |       |  3:36 |   4.7 |
| Create a demo and presentation for previous sprint        |         |       |  1:39 |   2.2 |
| Time spent producing coding videos                        |         |       |  1:56 |   2.5 |
| Sprint and product backlog grooming                       |         |       |  6:09 |   8.0 |
| Remove test model sanitizer                               |         |       |  0:20 |   0.4 |
| Remove master headers                                     |         |       |  1:12 |   1.6 |
| Fix the issues with tests in reference model              |         |       |  3:53 |   5.1 |
| Update static strings to string views                     |         |       |  1:15 |   1.6 |
| ODB include directories is incorrect                      |         |       |  1:43 |   2.2 |
| Create a "modeling" report                                |         |       |  4:56 |   6.4 |
| Move visual studio related options to top profile         |         |       |  0:17 |   0.4 |
| Fix incorrect message keyword when detecting ODB includes |         |       |  0:29 |   0.6 |
| Meta-data keys are in the inverse order                   |         |       |  0:39 |   0.8 |
| Creating reference cycles produces strange errors         |         |       |  5:32 |   7.2 |
| Error on duplicate references                             |         |       |  0:26 |   0.6 |
| Generate model dependency graph                           |         |       |  1:55 |   2.5 |
| Make the building of the relational model conditional     |         |       |  0:23 |   0.5 |
| Make all transform contexts code-generated                |         |       |  0:08 |   0.2 |
| Tracer numbering of dumped models is incorrect            |         |       |  0:26 |   0.6 |
| Fix compilation errors in c++ impl nightly                |         |       |  0:24 |   0.5 |
| Add support for meta-data overrides in Dogen              |         |       |  5:16 |   6.9 |
| Time spent fixing emacs issues                            |         |       |  1:44 |   2.3 |
| Move registrar into assets                                |         |       |  5:29 |   7.2 |
| Upgrade to boost 1.70                                     |         |       | 10:00 |  13.0 |
| Setup laptop to work on dogen                             |         |       |  0:45 |   1.0 |
| Org-mode as a carrier format for modeling                 |         |       |  5:37 |   7.3 |
| Add relational tracing support                            |         |       |  6:52 |   9.0 |
| Split generated tests from manual tests                   |         |       |  3:37 |   4.7 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2019-06-03 Mon 12:59]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 16:01]--[2019-06-03 Mon 16:30] =>  0:29
    CLOCK: [2019-06-03 Mon 12:51]--[2019-06-03 Mon 12:59] =>  0:08
    CLOCK: [2019-06-03 Mon 09:51]--[2019-06-03 Mon 10:45] =>  0:54
    CLOCK: [2019-06-03 Mon 06:47]--[2019-06-03 Mon 08:52] =>  2:05
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.18, "Estação de Comboio"

#+begin_src markdown
![Nova estação de comboio e a antiga](https://gdb.voanews.com/957FFA4D-4D6B-49D0-B3C4-C5577701EEE8_w1597_n_r1_st.jpg)

_The new CFM train station, next to the old. Moçamedes, Namibe, Angola. (C) 2018 [Armando Chicoa (VOA)](https://www.voaportugues.com/a/autoridades-falam-em-neglig%C3%AAncia-no-acidente-de-comboios-no-namibe/4559078.html)._

**DRAFT: Release notes under construction**

# Introduction

At about three quarters of the planned commitment, this sprint was slightly shorter than usual. Nevertheless, it is still packed with intense work and exciting progress. The "meta-model all things" theme continues in full flow, and we just about reached the next great refactoring battlefront: the ```fabric``` namespaces in the C# and C++ generation models. Predictably, there are not many user facing stories, as the refactoring continues to gather steam.

# User visible changes

This section normally covers stories that affect end users, with the video providing a quick demonstration of the new features. As this sprint had only a very trivial user visible change (discussed below), we took the opportunity to demo a couple of existing features instead.

[![Sprint 1.0.18 Demo](https://img.youtube.com/vi/TkYQTW_jAGk/0.jpg)](https://youtu.be/TkYQTW_jAGk)

## Data directory clean up

For the last few sprints we have been chasing an elusive target: the removal of the assortment of non-model JSON files that have long lived in our ```data``` directory. If nothing else, anything with a name like "data" triggers immediately the "code smells" part of any developer's brain. With this sprint, we have finally achieved this milestone: the text templates that we use in the C++ and C# models have now been moved into the models themselves, with the addition of the text templates meta-modeling elements.

The change gave us the opportunity to rethink the approach from first principles. As a result, the ```data``` directory is no longer, and instead we now have only the ```library``` directory under the Dogen ```shared``` folder. It too will one day cease to exist, when we implement proper support for the PDMs (Platform Description Models) - but for the next three or four sprints it will continue to house the simplified version of the PDMs as they are currently implemented.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_18.org).

## Significant Internal Stories

There were four very significant stories this sprint, which we cover briefly below.

### Use generated static configurations in transforms

 First and foremost, consuming the majority of the sprint's resourcing, was the move towards using code generated static configurations. We started this work when we moved feature templates into the meta-model; it seemed only logical to start code-generating the C++ types to represent the dynamic configurations, as well as the "deserialisation" code that converted dynamic configurations to static configurations.

With this release we removed the majority of the hand-crafted uses of static configurations, making the code more readable. As an added bonus, It also means it's much easier to add new features to the code generator now: simply create a new instance of a ```masd::variability::feature_bundle``` modeling element, and add the required feature templates. While we were at it, we also cleaned up the way bundles were modeled, meaning we now have less boilerplate to add features and bundles are now more logically consistent.

 As an example of how feature bundles are used, here's how we declare the generalisation feature bundle:

```json
    {
      "name": "features::generalization",
      "documentation": "Features related to the generalization relationship.\n",
      "stereotypes": [
        "masd::variability::feature_bundle"
      ],
      "tagged_values": {
        "masd.variability.default_binding_point": "element",
        "masd.variability.archetype_location.kernel": "masd",
        "masd.variability.template_kind": "instance"
      },
      "attributes": [
        {
          "name": "masd.generalization.is_final",
          "type": "masd::variability::boolean",
          "documentation": "Whether to mark a type as final or not.\n",
          "tagged_values": {
            "masd.variability.qualified_name": "masd.generalization.is_final",
            "masd.variability.is_optional": "true"
          }
        },
        {
          "name": "masd.generalization.parent",
          "type": "masd::variability::text",
          "documentation": "Name of the parent of the current element.\n",
          "tagged_values": {
            "masd.variability.qualified_name": "masd.generalization.parent",
            "masd.variability.is_optional": "true"
          }
        }
      ]
    },

```

Users then make use of these features in their diagrams:

```
#DOGEN masd.generalization.is_final=true
#DOGEN masd.generalization.parent=some_package::some_type
```

We've already noticed how much quicker the development of new features has been since this new functionality has been added, so this is a great win.

### Make wale templates meta-model elements

As explained above, we have been chasing the "meta-modelisation" of all configuration files that lived in the data directory for a long time. Wale text templates were one of the most annoying cases, because they **really** did not belong in the data directory; after all, text templates are internal to the model that uses them, rather than visible to all users of the code generator.

With this release, we've finished adding support for a logic-less text template meta-modeling element, which represents the text template. We then moved the templates into their respective models, under the new ```templates``` directory. The name logic-less was chosen [to be close to the domain terminology](https://en.wikipedia.org/wiki/Mustache_(template_system)) but it perhaps yet another example of "domain overfitting": it seems it's more a source of confusion rather than enlightenment, as many users (and even domain experts!) are not familiar with the term. We will probably rename it to just "text templates".

![Logic-less templates](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/logic_less_templates_modeling_elements.png)

Interestingly, _in theory_, this change should have made possible for users to create their own text templates. However, _in practice_, it is of extremely limited value because:

- we do not yet have a stable API for the meta-modeling elements;
- nor do we expose these properly to the templates;
- nor do we have a proper logic-less templating engine such as one of the mustache-like clones that exist in C++.

However, it lays an important foundation for the work to come in this space and, though long in coming, the end goal in the area is now very well defined.

### Rename the ```coding``` model

Ever since we renamed our core model to ```coding``` we've been wondering if this was the right name. We've spent a fair bit of time wading through the literature in search of a fitting name, which would simultaneously reflect the domain terminology of [MDE](https://en.wikipedia.org/wiki/Model-driven_engineering), as well as clarifying our intent. We've finally settled on ```assets```, after reading the most enlightening review article by JM Jézéquel: ["Model-driven engineering for software product lines"](http://downloads.hindawi.com/journals/isrn.software.engineering/2012/670803.pdf).

The new name is also consistent with the fact that we intend to model both products and components within this meta-model, so hopefully the rename is future-proof, and - gasp - final. We have gone through some four or five names since Dogen's inception, so take that with a grain of salt.

### Start of Fabric clean-up

One of the most anticipated tasks has been moving the fabric meta-model elements from the C++ and C# generation models into the assets model (as it is now known). This sprint fired the starting shot in this race: we have addressed the modeling of forward declarations in C++'s fabric. These have now been made consistent with the modeling ideas in Fabric. Sadly, many more items remain: some 15 or so elements need to be re-thought and re-modeled, moved into assets and then all of the associated formatting code needs to be updated.

## Resourcing

As explained on the introduction, we've had around three quarters of the usual resourcing for this sprint, which was not ideal. On the plus side, over 77% of the sprint's total ask was spent on stories directly related to the sprint's mission, and just shy of 18% on process related work - with the release notes and demo consuming over 12% of that. Finally, we spent the remaining ~4% on spikes, mainly related to investigating the (many) test failures we're experiencing on Windows. Sadly no easy answers were to be found, so the investigation continues.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_pie_chart.jpg)

## Planning

The project plan has suffered a couple of major setbacks this sprint. First, predictably, the fabric clean up was not completed this sprint. In addition, it is now clear it will be much harder than what we had estimated, so its now set to cost us the entirety of the next sprint. In addition, the PDM work is significant and it had not yet been added to the project plan.

The updated plan is now as follows.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_resource_allocation_graph.png)

# Next Sprint

We shall focus on the Fabric clean-up for the entirety of the next sprint. It is likely that there will be some overrun, but we remain optimistic.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.18_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.18/dogen_1.0.18_amd64-applications.deb)
- [dogen-1.0.18-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.18/DOGEN-1.0.18-Darwin-x86_64.dmg)
- [dogen-1.0.18-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.18-Windows-AMD64.msi)

**Note 1**: we've made some slight improvements to the build duration, but in truth we're still desperately close to our 50 minutes allocation on Travis, and as such we're getting many red builds. This is not ideal, so next sprint we will probably need to start disabling some of the generated tests to lower the build times.

**Note 2:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+end_src markdown

- [[https://twitter.com/MarcoCraveiro/status/1135567734010523648][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6541333935140458497][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    CLOSED: [2019-06-03 Mon 12:59]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 10:46]--[2019-06-03 Mon 12:25] =>  1:39
    :END:

Time spent creating the demo and presentation.

*** STARTED Time spent producing coding videos                        :story:
    :LOGBOOK:
    CLOCK: [2019-11-13 Wed 08:18]--[2019-11-13 Wed 08:22] =>  0:04
    CLOCK: [2019-11-08 Fri 14:28]--[2019-11-08 Fri 15:00] =>  0:32
    CLOCK: [2019-11-08 Fri 09:50]--[2019-11-08 Fri 10:17] =>  0:27
    CLOCK: [2019-11-05 Tue 22:43]--[2019-11-05 Tue 23:06] =>  0:23
    CLOCK: [2019-11-05 Tue 21:30]--[2019-11-05 Tue 22:00] =>  0:30
    :END:

Story that captures time spent producing coding videos but not
actually doing any development related activities.

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2019-11-13 Wed 08:36]--[2019-11-13 Wed 08:46] =>  0:10
    CLOCK: [2019-11-12 Tue 21:03]--[2019-11-12 Tue 21:22] =>  0:19
    CLOCK: [2019-11-08 Fri 15:39]--[2019-11-08 Fri 16:01] =>  0:22
    CLOCK: [2019-11-08 Fri 11:20]--[2019-11-08 Fri 11:32] =>  0:12
    CLOCK: [2019-11-06 Wed 07:40]--[2019-11-06 Wed 08:06] =>  0:26
    CLOCK: [2019-11-05 Tue 22:25]--[2019-11-05 Tue 22:42] =>  0:17
    CLOCK: [2019-11-01 Fri 11:01]--[2019-11-01 Fri 11:18] =>  0:17
    CLOCK: [2019-10-29 Tue 08:01]--[2019-10-29 Tue 08:37] =>  0:36
    CLOCK: [2019-10-28 Mon 17:40]--[2019-10-28 Mon 17:44] =>  0:04
    CLOCK: [2019-10-28 Mon 08:53]--[2019-10-28 Mon 08:58] =>  0:05
    CLOCK: [2019-10-27 Sun 12:41]--[2019-10-27 Sun 12:56] =>  0:15
    CLOCK: [2019-10-25 Fri 12:41]--[2019-10-25 Fri 13:12] =>  0:31
    CLOCK: [2019-10-24 Thu 17:30]--[2019-10-24 Thu 17:35] =>  0:05
    CLOCK: [2019-06-11 Tue 10:55]--[2019-06-11 Tue 11:15] =>  0:20
    CLOCK: [2019-06-04 Tue 09:36]--[2019-06-04 Tue 10:35] =>  0:59
    CLOCK: [2019-06-04 Tue 09:32]--[2019-06-04 Tue 09:35] =>  0:03
    CLOCK: [2019-06-03 Mon 19:46]--[2019-06-03 Mon 20:30] =>  0:44
    CLOCK: [2019-06-03 Mon 06:31]--[2019-06-03 Mon 06:46] =>  0:15
    CLOCK: [2019-06-03 Mon 06:21]--[2019-06-03 Mon 06:30] =>  0:09
    :END:

Updates to sprint and product backlog.

*** COMPLETED Remove test model sanitizer                             :story:
    CLOSED: [2019-06-03 Mon 16:51]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 16:31]--[2019-06-03 Mon 16:51] =>  0:20
    :END:

We seem to have introduced testing to the reference implementation but
left the canned tests still there. There shouldn't be any reason to
keep these so remove them.

*** COMPLETED Remove master headers                                   :story:
    CLOSED: [2019-06-03 Mon 19:05]
    :LOGBOOK:
    CLOCK: [2019-06-04 Tue 09:21]--[2019-06-04 Tue 09:31] =>  0:10
    CLOCK: [2019-06-03 Mon 18:49]--[2019-06-03 Mon 19:01] =>  0:12
    CLOCK: [2019-06-03 Mon 18:28]--[2019-06-03 Mon 18:48] =>  0:20
    CLOCK: [2019-06-03 Mon 17:56]--[2019-06-03 Mon 18:12] =>  0:16
    CLOCK: [2019-06-03 Mon 16:52]--[2019-06-03 Mon 17:06] =>  0:14
    :END:

At present we are not making use of this feature, and it could even be
argued that it is not a feature that should be used; by having a
single header that includes all files we encourage unnecessary
inclusion, increasing build times. We had a use for this, which was
related to testing model types, but since we replace that with
generated tests, we no longer required it. Remove this feature.

Notes:

- actually, we left the test model sanitizer. Not clear why.

*** COMPLETED Code-generate variability feature templates             :story:
    CLOSED: [2019-06-03 Mon 20:23]

*Rationale*: implemented in the previous sprint.

Type templates are in effect features from a feature model. We need to
add UML support for features (e.g. add meta-model elements for them),
with code generation, and link them back to annotations.

In fact, we made a mistake by binding annotations so closely to
dogen. There are two distinct concerns here:

- the annotations library. This provides "typed support" on top of KVP
  infrastructure. The idea here is that users can define "fields" with
  "types" and retrieve information from those KVPs in a structured
  way. Instead of having to create their own validation
  infrastructure, they can rely on annotations to do all the hard work
  for them. As part of the field creation, ideas such as "scopes" and
  "archetype locations" emerge. However, these do not really belong to
  the domain of annotations; these are concepts that end users create
  and give them semantics. What annotations needs to be able to do is
  to allow the creation of arbitrary notions of "scopes" and
  "hierarchy". Basically, annotations could be a completely
  self-contained project with no dependencies and usable outside of
  dogen.
- the linkage between the annotations library and dogen. Here we can
  create metamodel elements to convey the input parameters needed to
  code generate the elements for the annotations library. In this
  sense, annotations is nothing more than a platform that the
  transforms leverage; it has nothing particularly special to do with
  dogen. It just so happens that dogen itself then makes use of
  annotations to supply metadata internally, but this is a mere
  coincidence.
- the linkage between stitch and annotations. In this view, stitch is
  yet another client of annotations, via dogen. Again, there is no
  reason why stitch needs to have any dependency on dogen, other than
  annotations. In this sense, features such as licences and other
  boilerplate must be supplied as KVP parameters into stitch, without
  it directly depending in formattables. In addition, the fact that
  stitch generates c++ is also a coincidence. We could have a
  parameter that configures stitch and generate say C#.

Interestingly, in this sense we could then say that both stitch and
annotations are stand alone libraries generated using dogen, and then
in turn consumed by dogen. This could be done as packages by means of
vcpkg. And of course, stitch could then use a proper templating engine
instead of wale (another vcpkg dependency).

Finally, the logical conclusion is that dogen can use *any* of a
number of templating engines. The parameters to the engine are
supplied using KVPs (by means of annotation). There is a generic
metamodel element representing the binding to templating, and one of
its parameters is the templating engine. These are bound to the dogen
binary at compile time. End users can also make use of this mechanism,
for any of the available facets. This means that where we supply
=formatting_style=, we should really reflect the templating
engine. And then, all parameters with a known prefix, say:

: masd.templating.ENGINE.X=Y

Are supplied as parameters to the engine. These may need to take into
account facets as well, so that we can bind each facet to a different
template and supply different parameters.

Notes:

- one really useful feature would be to bind an enumeration to a
  string field, such that we'd automatically convert the string into a
  valid value of the enumeration (or throw).

*Previous Understanding*

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it. Name: =yarn::annotation_type_template=?
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** COMPLETED Fix the issues with tests in reference model            :story:
    CLOSED: [2019-06-19 Wed 16:48]
    :LOGBOOK:
    CLOCK: [2019-06-18 Tue 20:02]--[2019-06-18 Tue 23:55] =>  3:53
    :END:

It seems when we added the tests in the test model, we did not enable
them for all models: we skipped a few, probably because we started
seeing lots of compilation errors. However, now that we need to test
serialisation with the new registrar, we need those tests. We need to
go back and figure out why the tests where failing and fix them.

Notes:

- immutability issues: some tests cannot run if a type is immutable
  (e.g. assignment, etc).
- issues with the new tests facet directory and destination.

*** CANCELLED Update static strings to string views                   :story:
    CLOSED: [2019-09-05 Thu 11:05]
     :LOGBOOK:
     CLOCK: [2019-09-05 Thu 10:50]--[2019-09-05 Thu 11:04] =>  0:14
     CLOCK: [2019-09-04 Wed 19:20]--[2019-09-04 Wed 19:47] =>  0:27
     CLOCK: [2019-09-04 Wed 18:45]--[2019-09-04 Wed 19:19] =>  0:34
     :END:

 Now we're on C++17 we can start making use of its new features. One
 low hanging fruit is string view. We use static strings quite a lot
 for logging etc. We can just replace these with string views.

 Example:

 : #include <string_view>
 : constexpr std::string_view foo("abc");

 Problems:

 - cannot do XML text reader because we do not have a good way to
   convert string_view to cstr. See [[https://stackoverflow.com/questions/48081436/how-you-convert-a-stdstring-view-to-a-const-char][How you convert a std::string_view
   to a const char*?]]

 Links:

 - [[https://www.bfilipek.com/2018/10/strings17talk.html][Let's Talk About String Operations in C++17]]
 - [[https://developercommunity.visualstudio.com/content/problem/24487/constexpr-stdstring-view-from-string-literal.html][constexpr std::string_view from string literal]]
 - [[https://www.reddit.com/r/cpp/comments/cw35kk/best_practices_for_efficient_string_constants/][Best practices for efficient string constants]]

*** COMPLETED ODB include directories is incorrect                    :story:
    CLOSED: [2019-10-02 Wed 16:53]
    :LOGBOOK:
    CLOCK: [2019-10-02 Wed 15:46]--[2019-10-02 Wed 16:53] =>  1:07
    CLOCK: [2019-10-02 Wed 13:20]--[2019-10-02 Wed 13:56] =>  0:36
    :END:

With the upgrade to vcpkg ODB, we have broken ODB generation. The
problem is that we rely on the export of =ODB_INCLUDE_DIRS=, but this
no longer happens as the include directories are set by vcpkg. The
right solution is to rely only on the global includes.

In fact the right solution is to set globally a
=ODB_EXECUTABLE_GLOBAL_ARGS= and reuse that in each generated file. We
should also ensure this variable is defined and issue a message
explaining the problem.

Links:

- [[https://stackoverflow.com/questions/47475731/cmake-include-directories-for-custom-target-type/58200691#58200691][CMake include_directories for custom target type]]
- [[https://cmake.org/cmake/help/v3.3/command/target_include_directories.html][CMake manual: target_include_directories]]

*** CANCELLED Create a "modeling" report                              :story:
    CLOSED: [2019-10-26 Sat 16:02]
    :LOGBOOK:
    CLOCK: [2019-10-25 Fri 22:22]--[2019-10-25 Fri 23:40] =>  1:18
    CLOCK: [2019-10-25 Fri 15:21]--[2019-10-25 Fri 16:41] =>  1:20
    CLOCK: [2019-10-25 Fri 13:13]--[2019-10-25 Fri 14:55] =>  1:42
    CLOCK: [2019-10-24 Thu 17:50]--[2019-10-24 Thu 18:13] =>  0:23
    CLOCK: [2019-10-24 Thu 17:36]--[2019-10-24 Thu 17:49] =>  0:13
    :END:

*Rationale*: we will address this via the relational model instead.

At present when we introduce a new modeling element and things stop
working, its very difficult to understand why. The problem could be
any where in the pipeline, and looking through the logs and the
transform reports doesn't make the task easier. The information is
there but the problem is knowing where to look. The ideal scenario is
to have a relational model describing all working within dogen, but
that is a lot of work. One quicker way of getting some of this
information is to create a "modeling report". This would be in
org-mode format and have a hierarchical structure like so:

- run:
  - start time
  - command line options
- models:
  - name of the model
  - input language, output languages.
  - path to the model
  - global enablement properties
  - type: target or reference.
- dia elements:
  - dia object name
  - dia object type as tag.
  - stereotypes
  - transforms: processed and skipped. These are groups of transforms
    that processed or skipped the element.
  - assets: asset meta-model elements for this object.
    - transforms: processed and skipped.
    - artefacts
      - flag of enabled or disabled
      - path
      - transforms: processed and skipped.

Notes:

- we already have start and end transform/chain in tracer. We just
  need a way to mark a type as processed at the end of a transform,
  else it should be marked as ignored/skipped. We can use the
  qualified name for this; e.g. default the state to ignored and only
  set it to processed if called. Or maybe we can only state the
  transforms that touched it and not worry at all about
  ignored/skipped.
- we can only tell if an element was processed on a leaf transform,
  not on a chain.
- we should add the transform's GUID to the report if they are
  enabled.
- because the transforms are in order, we can see who was the last
  transform that saw a given model element.

Tasks:

- add injector id property to asset elements. Populate this property
  during transforms. Actually we probably should just call it "source
  element id" and use the same name in the extraction model.
- add reporting elements to tracing for the modeling report.

Conclusions:

- the general conclusion, after some work in modeling the data types
  required for this, is that this is a subset of the use cases of the
  relational model. It will be yet another special case for reporting,
  which will answer some questions but not all. And in the future we
  will have to create yet another set of reports to answer different
  kinds of questions. The relational model is a more general solution
  to the problem. If we need to extend it we can write stored
  procedures in postgres.

*** COMPLETED Move visual studio related options to top profile       :story:
    CLOSED: [2019-10-26 Sat 17:38]
    :LOGBOOK:
    CLOCK: [2019-10-26 Sat 17:21]--[2019-10-26 Sat 17:38] =>  0:17
    :END:

At present we are duplicating all of the visual studio related options
across a number of models. We should use a profile instead.

*** COMPLETED Fix incorrect message keyword when detecting ODB includes :story:
    CLOSED: [2019-10-28 Mon 08:47]
    :LOGBOOK:
    CLOCK: [2019-10-28 Mon 08:41]--[2019-10-28 Mon 08:47] =>  0:06
    CLOCK: [2019-10-26 Sat 18:01]--[2019-10-26 Sat 18:24] =>  0:23
    :END:

We are using the non-existent keyword =FATAL= in the ODB portion of
the CMakeLists. We need update it to =FATAL_ERROR= as per the
documentation.

Actually the right solution for this is to remove this check
altogether. We don't really know how the user is finding ODB and the
more checks we do the larger the interface between our generated cmake
file and the regular cmake files. By removing the check we pass the
work to the user.

Links:

- [[https://cmake.org/cmake/help/v3.0/command/message.html][message]]

*** COMPLETED Meta-data keys are in the inverse order                 :story:
    CLOSED: [2019-11-04 Mon 22:14]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 22:11]--[2019-11-04 Mon 22:14] =>  0:03
    CLOCK: [2019-11-04 Mon 21:35]--[2019-11-04 Mon 22:11] =>  0:36
    :END:

Whilst investigation an issue with cycles, we noticed that all lists
within meta-data appear to be in inverse order. Fix this and
regenerate all models accordingly.

*** COMPLETED Creating reference cycles produces strange errors       :story:
    CLOSED: [2019-11-04 Mon 22:51]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 22:52]--[2019-11-04 Mon 23:00] =>  0:08
    CLOCK: [2019-11-04 Mon 22:14]--[2019-11-04 Mon 22:51] =>  0:37
    CLOCK: [2019-11-04 Mon 20:50]--[2019-11-04 Mon 21:34] =>  0:44
    CLOCK: [2019-11-04 Mon 20:01]--[2019-11-04 Mon 20:09] =>  0:08
    CLOCK: [2019-11-04 Mon 17:46]--[2019-11-04 Mon 18:30] =>  0:44
    CLOCK: [2019-11-04 Mon 08:00]--[2019-11-04 Mon 08:40] =>  0:40
    CLOCK: [2019-11-01 Fri 16:36]--[2019-11-01 Fri 16:52] =>  0:16
    CLOCK: [2019-11-01 Fri 14:20]--[2019-11-01 Fri 16:35] =>  2:15
    :END:

If a model A references another model B and model B also references
model A, dogen does not detect the cycle. This results in the not very
obvious error of having duplicate types:

: std::exception::what: More than one master segment found. Last: dogen.variability.registrar

What we should do instead is to detect the cycle when loading the
models and provide a sensible error message to the user.

Notes:

- add a data structure in the injection model set to capture reference
  information.
- add a validator as part of the IMS chain to validate that there are
  no cycles.
- add a stack to the validator to provide context when cycles occur.
- create a tracing report that takes in the data structures.

Problems:

- references seem to have been processed in reverse order.

*** COMPLETED Error on duplicate references                           :story:
    CLOSED: [2019-11-04 Mon 23:21]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 23:25]--[2019-11-04 Mon 23:32] =>  0:07
    CLOCK: [2019-11-04 Mon 23:16]--[2019-11-04 Mon 23:21] =>  0:05
    CLOCK: [2019-11-04 Mon 23:01]--[2019-11-04 Mon 23:15] =>  0:14
    :END:

We need to check to see what happens if you enter the same reference
multiple times. We should error.

We should also detect references to "self".

*** COMPLETED Generate model dependency graph                         :story:
    CLOSED: [2019-11-05 Tue 18:17]
    :LOGBOOK:
    CLOCK: [2019-11-05 Tue 18:17]--[2019-11-05 Tue 18:30] =>  0:13
    CLOCK: [2019-11-05 Tue 17:41]--[2019-11-05 Tue 18:16] =>  0:35
    CLOCK: [2019-11-05 Tue 08:32]--[2019-11-05 Tue 08:45] =>  0:13
    CLOCK: [2019-11-05 Tue 08:00]--[2019-11-05 Tue 08:31] =>  0:31
    CLOCK: [2019-11-04 Mon 23:40]--[2019-11-04 Mon 23:54] =>  0:14
    CLOCK: [2019-11-04 Mon 23:33]--[2019-11-04 Mon 23:39] =>  0:06
    CLOCK: [2019-11-04 Mon 23:22]--[2019-11-04 Mon 23:25] =>  0:03
    :END:

It would be nice to generate a tracing of the model dependencies. This
may not necessarily be part of tracing.

*** COMPLETED Make the building of the relational model conditional   :story:
    CLOSED: [2019-11-05 Tue 22:24]
    :LOGBOOK:
    CLOCK: [2019-11-05 Tue 22:18]--[2019-11-05 Tue 22:24] =>  0:06
    CLOCK: [2019-11-05 Tue 22:01]--[2019-11-05 Tue 22:17] =>  0:16
    CLOCK: [2019-11-05 Tue 21:59]--[2019-11-05 Tue 22:00] =>  0:01
    :END:

We should only build the relational model if ODB support is
present. Otherwise we should ignore this model. Dogen should still
function, but all code related to the relational model should be
excluded. This includes the command line options related to database
configuration.

We should also tell the users that dogen was built without relational
support.

*** COMPLETED Make all transform contexts code-generated              :story:
    CLOSED: [2019-11-06 Wed 18:15]
    :LOGBOOK:
    CLOCK: [2019-11-06 Wed 18:07]--[2019-11-06 Wed 18:15] =>  0:08
    :END:

Try generating the engine context, it seems there is no obvious reason
for it not to work.

*** COMPLETED Tracer numbering of dumped models is incorrect          :story:
    CLOSED: [2019-11-08 Fri 11:09]
    :LOGBOOK:
    CLOCK: [2019-11-08 Fri 10:43]--[2019-11-08 Fri 11:09] =>  0:26
    :END:

We seem to be skipping numbers when dumping trace models:

: 000-injection.dia.decoding_transform-71058be5-3e36-4ca7-9e7b-10cee985a07d-input.json
: 002-injection.dia.decoding_transform-71058be5-3e36-4ca7-9e7b-10cee985a07d-output.json
: 002-injector.transforms.tagged_values_overrides_transform-d92cdc73-e5b3-4e15-9559-b430ab40040f-input.json
: 004-injector.transforms.configuration_transform-57a397a6-843a-4b4b-943c-aa66a31bd34a-input.json
: 004-injector.transforms.tagged_values_overrides_transform-d92cdc73-e5b3-4e15-9559-b430ab40040f-output.json
: 006-injection.transforms.input_technical_space_transform-dea341a4-9406-4213-b252-d88fecf2f1a2-input.json
: 006-injector.transforms.configuration_transform-57a397a6-843a-4b4b-943c-aa66a31bd34a-output.json
: 008-injection.transforms.input_technical_space_transform-dea341a4-9406-4213-b252-d88fecf2f1a2-output.json
: 008-injection.transforms.references_transform-eaf2422c-9dd2-4da0-aaf0-908688e2721d-input.json
: 010-injection.transforms.references_transform-eaf2422c-9dd2-4da0-aaf0-908688e2721d-output.json
: 011-injection.transforms.model_production_chain-2560043d-867e-43be-bd2d-2fec62d05bcc-output.json

We have rejiged the tracing numbering and it now seems ok. Instead of
trying to have the same entry number for the input and output of a
transform, we now give them distinct numbers. This makes the logic
easier to follow.

*** COMPLETED Fix compilation errors in c++ impl nightly              :story:
    CLOSED: [2019-11-08 Fri 11:14]
    :LOGBOOK:
    CLOCK: [2019-07-14 Sun 14:03]--[2019-07-14 Sun 14:27] =>  0:24
    :END:

Ever since we moved to the new PC, we are now getting weird
compilation errors:

: ../../../../projects/cpp_ref_impl.cpp_98/tests/an_enumeration_tests.cpp:100:58: error: the result of the conversion is unspecified because ‘13’ is outside the range of type ‘cpp_ref_impl::cpp_98::an_enumeration’ [-Werror=conversion]

The problem appears to be that our push for the warning is no longer working:

: BOOST_AUTO_TEST_CASE(casting_invalid_enumeration_throws) {
: #if BOOST_COMP_GNUC
: #pragma GCC diagnostic push
: #pragma GCC diagnostic ignored "-Wconversion"
: #endif
:    using cpp_ref_impl::cpp_98::an_enumeration;
:   const an_enumeration r(static_cast<an_enumeration>(13));
: #if BOOST_COMP_GNUC
: #pragma GCC diagnostic pop
: #endif

This may be related to our use of boost macros without including
=predef.h=.

This problem seems to have gone away by itself.

*** COMPLETED Add support for meta-data overrides in Dogen            :story:
    CLOSED: [2019-11-08 Fri 15:56]
    :LOGBOOK:
    CLOCK: [2019-11-08 Fri 15:28]--[2019-11-08 Fri 15:39] =>  0:11
    CLOCK: [2019-11-08 Fri 15:01]--[2019-11-08 Fri 15:28] =>  0:27
    CLOCK: [2019-11-08 Fri 11:33]--[2019-11-08 Fri 12:21] =>  0:48
    CLOCK: [2019-11-08 Fri 11:27]--[2019-11-08 Fri 11:32] =>  0:05
    CLOCK: [2019-11-07 Thu 23:15]--[2019-11-08 Fri 00:02] =>  0:47
    CLOCK: [2019-11-07 Thu 22:35]--[2019-11-07 Thu 23:15] =>  0:40
    CLOCK: [2019-11-06 Wed 23:46]--[2019-11-06 Wed 23:58] =>  0:12
    CLOCK: [2019-11-06 Wed 23:42]--[2019-11-06 Wed 23:45] =>  0:03
    CLOCK: [2019-11-06 Wed 23:15]--[2019-11-06 Wed 23:41] =>  0:26
    CLOCK: [2019-11-06 Wed 22:58]--[2019-11-06 Wed 23:14] =>  0:16
    CLOCK: [2019-11-06 Wed 22:41]--[2019-11-06 Wed 22:57] =>  0:16
    CLOCK: [2019-11-06 Wed 18:20]--[2019-11-06 Wed 18:40] =>  0:20
    CLOCK: [2019-11-06 Wed 18:16]--[2019-11-06 Wed 18:19] =>  0:03
    CLOCK: [2019-11-06 Wed 17:49]--[2019-11-06 Wed 18:06] =>  0:17
    CLOCK: [2019-11-06 Wed 08:07]--[2019-11-06 Wed 08:32] =>  0:19
    :END:

In order to support the scenario of builds with generated code we need
to be able to override the profile of a model. To do so we should
build a generic mechanism to override meta-data in a model.

Notes:

- see [[*Consider creating a test build for all facets][Consider creating a test build for all facets]]
- create a new command line flag: =variabulity-override=
- it takes a triplet in the form of

: MODEL_NAME,ELEMENT_NAME,ATTRIBUTE_NAME,KEY,VALUE

- we parse these tuples into a container and then use it in the
  variability transforms.
- we need a data structure that reflects the topology: global,
  element, property. It must be keyed by model name, element name,
  attribute name.
- the command line option is parsed and expanded into this new data
  structure. The data structure is kept in injection context. Just
  before calling the configuration factory, we need to locate the
  appropriate overrides. We supply these to the configuration factory.
- the factory takes the appropriate decision:
  - for scalar value types we merely override the value.
  - for collections we push back.
- we should also mark unused overrides and throw if there are any. We
  should record the original override string.
- the remainder of variability processing will work as at present. We
  just must ensure we override prior to any profile merging/expansion.

*Previous understanding*

In the past we had enabled a lot of facets on the dogen models to
serve as part of the testing infrastructure. However, its no longer
feasible to do this because the build is taking too long. This is not
ideal as the reference models just can't capture all of the complexity
of a codebase like dogen's so we lost some testability with this
move. What would be really nice is if we could create "test builds":

- given a set of test models, copy them somewhere, generate a product
  configuration with some kind of override that enables all facets
  everywhere. some will just not come through like ORM.
- build the product. all handcrafted code is now blank but all facets
  are coming though.
- this could be part of the ctest script, as a "mode" - product
  generation test. Every time there is a commit to a product the build
  kicks in.

Notes:

- one way to achieve this would be to force the profile of the
  model. However, we are moving away from profiles, and in the future
  there will be a list of stereotypes associated with the model. Then
  it will be much harder to figure out what stereotypes do what and to
  overwrite them.
- an alternative would be to have some kind of "test mode"; when
  handling enablement, we'd check the "mode". If we're in test mode,
  we simply enable all and ignore any other settings. We could have a
  "force enable" flag or some such like we do for
  overwriting. However, we may then hit another problem: enabling all
  facets may result in non-buildable models:
  - facets may be incompatible. This is not a problem at present.
  - handcrafted classes may result in code that does not
    compile. Shouldn't though because we are still checking the status
    of the attributes.

Conclusions:

- create a new flag: =force-enablement=. When set to true, we ignore
  all enablement settings and generate all facets. We do not generate
  all kernels though (e.g. the kernel must be on in the model).
- create a script that copies the models to a new product and
  generates them with fore-enablement. This will only work when we can
  generate products.
- as facets are enabled, tests are automatically generated for them.
- build the result and run all tests.

Merged stories:

*New approach to model testing*                                    :story:*

In the beginning we generated all models with all facets, even the
dogen core models. The idea was to test the generator even though
these facets were not useful for the product. This was really useful
because the dogen models are much more realistic than the test models
and due to this we picked up a number of bugs. However, we have now
hit the maximum build times on travis and we need to start removing
all ballast. This will mean we lose these valuable tests. The
alternative is to create these tests on the fly:

- create a new override flag that forces all facets to be emitted.
- create a new test facet with templates that are dependent on the
  enabled facets; each test tests the dependent facet.
- create a ctest nightly build that generates code using these new
  facets, compiles it and runs all tests.
- we need some meta-data to "ignore" some modeling elements for
  certain facets such as composition which are known to be broken. Or
  maybe we should just leave the tests as red so we know.
- the tests should be designed not to use templates etc to make the
  debug dumps really obvious (unlike the existing tests). It may even
  make more sense to test each type individually so that when the test
  fails its really obvious:

: MY_TYPE_serialisation_roundtrips_correctly

  this way when we look at CDash we know exactly which types failed to
  serialise.

During the transition phase, we will remove all of the existing tests.

*** STARTED Time spent fixing emacs issues                            :story:
    :LOGBOOK:
    CLOCK: [2019-11-08 Fri 10:18]--[2019-11-08 Fri 10:43] =>  0:25
    CLOCK: [2019-11-07 Thu 21:15]--[2019-11-07 Thu 22:34] =>  1:19
    :END:

- fix issue with smartparens: For some random reason emacs on our
  desktop PC is behaving very strangely. It seems that the problems
  stem from smartparens. Try to disable this in prelude.
- fix issue with treemacs: we are getting a lot of weird errors in
  treemacs since the last update, which happened because we were
  trying to fix smartparens. See issue: [[https://github.com/Alexander-Miller/treemacs/issues/562][When tag follow mode is
  enabled, the message buffer is flooded with: {Treemacs} Encountered
  error while following tag at point: (wrong-type-argument listp
  DimCounter) #562]]

*** STARTED Move registrar into assets                                :story:
    :LOGBOOK:
    CLOCK: [2019-10-24 Thu 08:20]--[2019-10-24 Thu 08:43] =>  0:23
    CLOCK: [2019-06-12 Wed 15:08]--[2019-06-12 Wed 17:09] =>  2:01
    CLOCK: [2019-06-11 Tue 21:31]--[2019-06-11 Tue 22:52] =>  1:21
    CLOCK: [2019-06-11 Tue 11:57]--[2019-06-11 Tue 12:20] =>  0:23
    CLOCK: [2019-06-11 Tue 11:52]--[2019-06-11 Tue 11:56] =>  0:04
    CLOCK: [2019-06-11 Tue 11:16]--[2019-06-11 Tue 11:51] =>  0:35
    CLOCK: [2019-06-03 Mon 19:34]--[2019-06-03 Mon 19:45] =>  0:11
    CLOCK: [2019-06-03 Mon 19:27]--[2019-06-03 Mon 19:34] =>  0:07
    CLOCK: [2019-06-03 Mon 19:02]--[2019-06-03 Mon 19:26] =>  0:24
    :END:

Move the registrar type into assets, in the quickest way possible.

Notes:

- In order to avoid blocking due to lots of analysis, we need
  to split this story into three:
  - first, we need to just move the registrar as is into assets.
  - a second story is to clean up the existing registrar code to have
    less templates and possibly address the existing registration
    bugs. We could also look into calling the registrars for
    referenced models automatically as part of this work (at present
    we are doing this manually).
  - finally, we need some meta-level refactoring to figure out if the
    pattern can be generalised to include initialisers, etc.
  In general that should be our approach: try to split out the
  capturing of patterns into as many steps as possible, to make sure
  we don't get overwhelmed as we implement things.
- we need to keep track of all type registrars on referenced models,
  not on the referenced models themselves. We need to know which
  models we referenced directly, and then find the registrars for
  those models.
- leaves need to know of the registrar. This is so that we can call it
  in their generated tests. We could use the registrar transform to go
  and find all leaves and populate their registrar name.
- current state is that we cannot generate the registrar for some
  reason.
- test model with registrar is C++ model. Type is called
  registrar. Its probably not a good idea to also call it registrar -
  wouldn't that clash with the existing type?
- we should have a warning/error: if using boost serialisation with a
  model that has inheritance, the registrar should be present. Added
  to warnings story.

*** STARTED Upgrade to boost 1.70                                     :story:
    :LOGBOOK:
    CLOCK: [2019-09-10 Tue 13:05]--[2019-09-10 Tue 16:53] =>  3:48
    CLOCK: [2019-09-10 Tue 10:15]--[2019-09-10 Tue 12:39] =>  2:24
    CLOCK: [2019-09-10 Tue 08:40]--[2019-09-10 Tue 10:14] =>  1:34
    CLOCK: [2019-09-05 Thu 11:07]--[2019-09-05 Thu 11:17] =>  0:10
    CLOCK: [2019-07-14 Sun 14:34]--[2019-07-14 Sun 16:33] =>  1:59
    CLOCK: [2019-07-14 Sun 14:28]--[2019-07-14 Sun 14:33] =>  0:05
    :END:

We should try to upgrade to latest boost.

Notes:

- the problem appears to be that with OSX we do not have a compiler
  installed that can compile vcpkg. It is not clear how we did it
  before. The installed XCode compiler is too old and we do not have
  homebrew for gcc.
- installed LLVM 7. Ninja then went on a strange loop, regenerating
  CMake files. This was because NTP had not been working on OSX for
  some reason, and the clock was in the past.
- compiling with clang 7 causes the =-lc++fs= linking error. Tried
  compiling with clang 8.
- Compilation required setting LDFLAGS -L to point to the lib
  directory of the download, else the static library for filesystem
  could not be location.
- We may have linking problems now that we are using XCode 10 in
  travis and clang 8 to build vcpkg dependencies.
- ODB 2.5 no longer works due to a git ref mismatch. Not clear why
  that would be but the object we were referencing no longer exists in
  code synthesis git repo.
- the ref for ODB SQL lite 2.5.0-b.9 does not seem to exist in their
  repo any longer. Due to this, the OSX build is failing. For now we
  shall try to update excluding that dependency, given we are not even
  using it.
- boost regex fails to build. The problem is that we are picking up
  the system compiler instead of CXX. It is not clear why that
  is. Maybe we got lucky in the past because we were using c++14 but
  now with c++17 system clang fails to compile because it does not
  have c++ 17 support.
- nightlies are now failing with a missing reference to SQL lite.

*** STARTED Setup laptop to work on dogen                             :story:
    :LOGBOOK:
    CLOCK: [2019-10-28 Mon 08:25]--[2019-10-28 Mon 08:41] =>  0:16
    CLOCK: [2019-10-28 Mon 08:19]--[2019-10-28 Mon 08:25] =>  0:06
    CLOCK: [2019-10-28 Mon 08:13]--[2019-10-28 Mon 08:19] =>  0:06
    CLOCK: [2019-10-24 Thu 08:03]--[2019-10-24 Thu 08:20] =>  0:17
    :END:

We haven't used the laptop for dogen for quite a bit so its behind the
main machine. Get it in a shape to do development again.

Items missing:

- consolas font. done.
- dir locals for projects
- polymode
- build2
- odb

*** STARTED Org-mode as a carrier format for modeling                 :story:
    :LOGBOOK:
    CLOCK: [2019-06-05 Wed 14:17]--[2019-06-05 Wed 18:02] =>  3:45
    CLOCK: [2019-06-05 Wed 12:17]--[2019-06-05 Wed 12:42] =>  0:25
    CLOCK: [2019-06-05 Wed 10:50]--[2019-06-05 Wed 12:17] =>  1:27
    :END:

This is a bit of a weird idea, but may just work; this story is a
placeholder to capture ideas in this space. Consider a org-mode
file as a model. Ideas:

- the top-level properties are all model properties. For example, if
  you add text at the top, that is a model comment.
- we can also make use of the exact same format for Dogen comments as
  we do in Dia, with =#DOGEN= markers.
- stereotypes and other meta-data can be conveyed using org-mode
  properties. In addition, due to org-babel, we can include code
  snippets on any programming language, with some (minimal) IDE-like
  integration.
- we could also include the GUIDs for merging as org-mode properties.
- once we create a C++ stand-alone product to represent org-mode
  documents, we can just create an adapter for it as an injector.
- there already is some support for creating state-machines in
  org-mode: [[https://orgmode.org/worg/org-tutorials/org-dot-diagrams.html][Org tutorial on generating simple process diagrams using
  dot and tables]]

Links:

- [[https://github.com/mirkoboehm/OrgModeParser][OrgModeParser]]: requires QT.
- [[https://www.reddit.com/r/emacs/comments/bciwiz/does_orgmode_have_a_formal_grammar_or_some_subset/][Does orgmode have a formal grammar, or some subset of it?]]
- [[https://orgmode.org/worg/dev/org-syntax.html][Org Syntax (draft)]]
- [[https://orgmode.org/worg/dev/org-element-api.html][Org Element API]]
- [[https://github.com/ngortheone/org-rs][org-rs]]: rust library for org-mode.
- [[https://github.com/felipealmeida/orgmode-parsers][orgmode-parsers]]

*** STARTED Add relational tracing support                            :story:
    :LOGBOOK:
    CLOCK: [2019-11-07 Thu 20:11]--[2019-11-07 Thu 20:40] =>  0:29
    CLOCK: [2019-11-07 Thu 19:55]--[2019-11-07 Thu 20:10] =>  0:15
    CLOCK: [2019-11-07 Thu 18:31]--[2019-11-07 Thu 18:33] =>  0:02
    CLOCK: [2019-11-07 Thu 17:41]--[2019-11-07 Thu 18:30] =>  0:49
    CLOCK: [2019-11-07 Thu 08:31]--[2019-11-07 Thu 08:55] =>  0:24
    CLOCK: [2019-11-07 Thu 08:14]--[2019-11-07 Thu 08:19] =>  0:05
    CLOCK: [2019-11-01 Fri 11:18]--[2019-11-01 Fri 11:54] =>  0:36
    CLOCK: [2019-10-29 Tue 18:07]--[2019-10-29 Tue 18:30] =>  0:23
    CLOCK: [2019-10-29 Tue 17:46]--[2019-10-29 Tue 18:06] =>  0:20
    CLOCK: [2019-10-29 Tue 08:55]--[2019-10-29 Tue 09:03] =>  0:08
    CLOCK: [2019-10-29 Tue 08:38]--[2019-10-29 Tue 08:48] =>  0:10
    CLOCK: [2019-10-28 Mon 18:55]--[2019-10-28 Mon 19:10] =>  0:15
    CLOCK: [2019-10-28 Mon 17:45]--[2019-10-28 Mon 18:20] =>  0:35
    CLOCK: [2019-10-28 Mon 17:29]--[2019-10-28 Mon 17:39] =>  0:10
    CLOCK: [2019-10-28 Mon 08:48]--[2019-10-28 Mon 08:52] =>  0:04
    CLOCK: [2019-10-27 Sun 12:05]--[2019-10-27 Sun 12:40] =>  0:48
    CLOCK: [2019-10-27 Sun 08:57]--[2019-10-27 Sun 09:04] =>  0:07
    CLOCK: [2019-10-27 Sun 08:29]--[2019-10-27 Sun 08:56] =>  0:27
    CLOCK: [2019-10-26 Sat 17:54]--[2019-10-26 Sat 18:00] =>  0:06
    CLOCK: [2019-10-26 Sat 17:47]--[2019-10-26 Sat 17:53] =>  0:06
    CLOCK: [2019-10-26 Sat 17:40]--[2019-10-26 Sat 17:46] =>  0:06
    CLOCK: [2019-10-26 Sat 16:01]--[2019-10-26 Sat 16:41] =>  0:40
    :END:

Whenever we bump into a problem we seem to spend a lot of time going
through the log files and trace files trying to figure out where the
problem is happening. Have a quick go in trying to implement a
relational model for tracing to see if we can transfer the bulk of the
data into a relational format which we can query via SQL.

We've created a basic relational model for tracing. The relational
part of it seems straightforward (ish); the problem is the integration
of the tracer with the relational model. At present we rely on the
fact that all traceable objects have IO enabled; this works because
the code generator creates the IO facet, which is then used by the
write method in utility to convert any model type into a
string. However, we now need to change the approach: we need multiple
tracing backends:

- file tracer
- database tracer.

The file tracer is more or less the current tracer. The database
tracer needs to decompose the objects in existing models into a
relational representation. In an ideal world, the user would configure
the tracer to use one of the two backends and the remaining usage
would be transparent. However, we cannot have an interface for the
tracer backend that uses template methods because then we'd need
virtual template functions, it seems.

Another alternative is to make the tracer aware of the model objects
it is tracing. This is also not ideal because we would create cycles
int he design.

In effect we need to somehow implement a similar approach to the existing
tracer: rely on global template functions a-la =operator<<= to
decompose objects into their relational representations and then
supply those to the backend. It is not very clear how this would
work. For now we've postponed this approach as it seems its not going
to be a quick win.

We should approach this incrementally. Next time we have a bit of
spare time, we need to generate the model and then create the adapters
from existing models. Finally we can look at how it will be integrated
with tracing.

Notes:

- compilation generates an ODB error:

: FATALODB include directories not defined.

- the key difference between northwind and tracing is that we have a
  namespace. The application of the schema pragma is probably not
  working due to this. We need to look into the transform to see how
  that pragma propagates.
- the problem arises because we are only populating the primitive's
  properties if there is a top-level pragma. As the schema is not
  populated for the namespace, there isn't one. It is not clear why
  one would want to skip properties such as DB member if there isn't a
  schema, but perhaps this is due to some ODB error. We should
  probably issue an error or warning if we cannot generate code
  without a schema name.
- with regards to the relational model, the problem is that we can't
  really create a schema for each namespace in a model because schemas
  are not really like namespaces. The entities in a schema should
  really be self-contained and not refer to other schemas or else the
  database will be confusing to use. For example in postgres we will
  need to set the schema path, etc in order to see the different
  tables. One possible solution is to set the schema name to the same
  value for all namespaces (e.g. =dogen=). This would then allow us to
  have namespaces in C++ but not in the database.
- it seems foreign keys are not supported at present. We probably need
  support for this in order to query quickly or else we will have to
  manually setup indices for each of these joining fields.
- we need a command line option to choose the tracing backend
  (e.g. file or database). We also need the database configuration
  parameters: hostname, port, database, user.
- we need to refactor tracer as follows:
  - update the tracer interface to take actual types rather than
    templates.
  - create a top-level interface for the notion of a backend.
  - create two implementations of the backend: file and relational.
  - move all the file related code to the file backend.
  - implement adapters for each model to convert them into relational
    model types.
  - implement the relational backend.

Merged stories:

*Scripts for loading traces into postgres*

- rationale: this story is superseded by having a relational model.

It would be really nice if as part of the tracing generation we also
generated a set of SQL scripts that:

- created a number of tables
- copied all of the generated data into the database
- added a number of utility functions such as get elements in model, etc.

Over time we could build up functionality but to start off with we
just want something really simple that copies all of the
files. Interestingly this "looks" like a job for dogen. It would be
nice to have a meta-model element for this etc.

In the future it would be nice to have a think about the schema so
that we could do joins etc. For example:

- show me all transforms with element of type X (the state of the
  element at each transform).

We should also take into account multiple runs. Perhaps a more
adequate solution is to create a dogen library that has the ORM
support for this. Once we have proper JSON serialisation we can store
the objects as JSON serialisable, allowing us to re-run transforms,
etc.

Notes:

- ensure we upload the file name or at least the coordinates to the
  transform graph with the data so that we know what it refers to.
- rename relational database enum to just database
- rename hostname to just host

*Improved understanding*

Better than uploading a whole load of JSON blobs and then having to do
a number of really complex queries, is to have a ORM schema that is
designed to capture the data in the format we're interested in. Then
we could do very simple queries. What we really care about is
capturing all attributes of the model as it changes across the
transformations. We also care about the relationships between
transformations. We also need a way to uniquely identify elements
across their entire lifecycle. A simple way would be to create a hash
of the file name of the model, column and line number. We can then
associate other IDs to this one such as dia ID, etc.

We need to create a set of adaptors that convert an existing model
(injection, coding, etc) into the ORM model and then write the ORM
model into the database. The ORM model does not need as much detail
and structure as a regular model; for example, names can be flattened
or linked into IDs (e.g. name table), etc. Whatever makes sense from a
relational perspective.

It would also be nice to dump the log into the database so that we
could do simple correlations such as "what was logged between the
start and end of this transform?"

Interestingly, this would also allow us to compare things between
runs. The schema should be designed with this in mind.

*** STARTED Split generated tests from manual tests                   :story:
    :LOGBOOK:
    CLOCK: [2019-11-13 Wed 18:24]--[2019-11-13 Wed 18:41] =>  0:17
    CLOCK: [2019-11-13 Wed 18:10]--[2019-11-13 Wed 18:23] =>  0:13
    CLOCK: [2019-11-13 Wed 17:44]--[2019-11-13 Wed 18:09] =>  0:25
    CLOCK: [2019-11-13 Wed 08:22]--[2019-11-13 Wed 08:36] =>  0:14
    CLOCK: [2019-11-13 Wed 08:00]--[2019-11-13 Wed 08:17] =>  0:17
    CLOCK: [2019-11-12 Tue 21:23]--[2019-11-12 Tue 22:39] =>  1:16
    CLOCK: [2019-11-12 Tue 19:25]--[2019-11-12 Tue 19:55] =>  0:30
    CLOCK: [2019-11-08 Fri 16:35]--[2019-11-08 Fri 17:00] =>  0:25
    :END:

We need to isolate the generated tests into its own binary, to allows
to run them in isolation from manual tests.

Notes:

- rename the facet to something like =generated_tests=, splitting out
  the manual tests from it.
- add CMake options to enable the different types of tests:
  - =WITH_MANUAL_TESTS=
  - =WITH_GENERATED_TESTS=
- create different top-level targets:
  - run all manual tests
  - run all generated tests
- make run all tests depend on both targets. With this it should now
  be possible to run only the generated or the manual tests.
- nightly:
  - create two clones of dogen, one just for the purposes of running
    the tests. Set the env var to point to this clone. Give it a
    meaningful name (e.g. "clean_dogen"?)
  - at present we only have =ctest_build=. However, we need a two step
    build process. First we enable overrides, then we run =gad= then
    we build all.
  - the right solution is to have a separate process that checks out
    dogen from git, builds CLI, rungs =gad= with overrides and then
    commits and pushes the result. The problem though is that we need
    to always start from master, and disregard any previous state. For
    this we have several possibilities:
    - create a new branch every day, using the same name. The downside
      is that we cannot easily see previous state. However, it should
      always be reproducible because we started from a known
      commit. On the plus side, branch management is easy. We just
      need to make sure appveyor and travis are ignoring this branch.
    - create a new branch per day. This way we can easily compare
      today with yesterday for example. However, we now need a way to
      manage the branches - i.e. if we reach day 7 go back to day 1,
      etc.
    - find a way to do this using a single branch. Maybe there is a
      way in git to "merge" branches in a way that we always start
      with the current commit in master. However, it must be based on
      some kind of "force" approach because we can't merge or rebase
      (we do not want to carry state from previous commits). We need
      to investigate what can be done within git. For example, "undo"
      previous commit if commit has string XYZ, merge
      master.
    - do not commit the state. Simply run a script that pulls latest,
      builds CLI and calls gad with overrides. Then do a regular
      nightly. The git pull will do nothing (so we can't see what
      commits we have in the nightly).
- check in the script for nightly builds.

*Previous understanding*

When we created the tests facet, we did not exactly follow the
existing framework. Normally we have the concept of a project
(e.g. say "directory settings"), which contains what is now understood
as "destinations". In C++ these can be:

- include
- source (=src= folder)

Destinations are just folders in the filesystem. These can be mapped
to any name defined by the user. Normally, we then place facets inside
of destinations. For example, the facet "types" can be projected into
the include and source destinations:

- =include/types=
- =src/types=

Some facets do not have a facet folder, and as such are projected
directly onto the project directory (CMakeLists, etc).

 When we added tests, we created a destination called =tests= but we
 also created a facet called =tests=. If all had gone according to the
 existing logic, we would have ended up with:

- =tests/tests=

However, we glossed over all of the above and when setting the file
paths we used the facet directory, but when setting other things (such
as say the CMakeLists directories), we used the destination. The net
result is that, for all intents and purposes, it looked as if we were
following the above setup, but somehow had ended up with:

- =tests=

When we recently updated the directory settings model, it all came
undone. As a hack, we did:

: #DOGEN masd.generation.cpp.tests.directory=tests_dir
: #DOGEN masd.generation.cpp.tests_directory_name=tests_dir

To maintain the existing logic, but in truth we need a proper fix. The
right solution is to give a name to the facet which is not =tests=. It
should really be something evocative of its functionality:

- generated tests? but then we don't partition generated code anywhere
  else. It would be useful though, so that we can easily locate
  handcrafted tests - and even exclude them too. If we also had
  separate CMakeFiles we could have a top-level variable that excludes
  generated tests for compilation.
- tests for code generated code?
- structural tests?

Basically we need a fitting name for this facet. Once found, we need
to move all tests into this directory.

Note that we cannot do the locator clean up until we sort out this mess.

*** Create a "manual tests" stereotype with profiles                  :story:

At present we have a =tests= facet that contains only the generated
tests, and writes to the =generated_tests= directory. We also have
created folders for manual tests under the =tests= directory. However,
the problem is that we still have no way to tell dogen about the
manual tests. This means we must use a regex to ignore the contents of
the folder. A better approach is:

- create two distinct test facets:
  - manual tests
  - generated tests
- create a profile that enables manual tests. When enabled, we simply
  create a skeleton boost test file. We must set it to override so
  that we update the contents of the file manually with real tests.

Tasks:

- rename tests facet to =generated_tests=
- create a new facet =manual_tests=. Copy most of the contents of the
  existing facet (main, cmake). Make the directory name =tests=.
- create a template for tests with associated meta-model entity
  (e.g. =masd::test=). It probably will also need its own namespace
  (=test=).
- create a stereotype that enables manual tests in the dogen model.
- update all models, adding =masd::test= for each manual test, with
  the new stereotype.

*Previous understanding*

*Rationale*: the right solution for this is to split generated tests
from manual tests such that we do not have to mix and match the two
types of code.

At present we are ignoring all of the contents of =tests=. This means
whenever we delete a type we are left behind with its tests. A better
solution is to create model elements for each handcrafted test marked
as "masd::handcrafted_test". This disables all facets except for
tests. We can then remove the regex.

Whilst variability overrides will address the underlying issue in a
more maintainable way (e.g. the deletion of generated tests), we
should still create a profile and model all tests as proper entities
instead of bypassing the modeling system. We want to move to a world
were *all* files in the system can be attributable to modeling
entities.

*** Generated tests fail when model has nothing to test               :story:

In cases where we are generating tests for a model which has nothing
to test - from a dogen generated code perspective - the tests will
fail with an error. This is because boost test expects to have at
least one test registered. In the past we have solved this by adding
"fake tests" which kept the test suite green until we added real
tests. However, for generated code, we need to somehow determine when
the fake tests should be injected.

This is not a trivial thing to do because we need to ensure that the
test template did not emit a single test for a given entity, and then
look at all entities and see if there is at least one test or not.

Example fake tests:

#+begin_src c++
#include <boost/test/unit_test.hpp>
#include "dogen.utility/types/test/logging.hpp"

namespace {

const std::string empty;
const std::string test_module("dogen.tests");
const std::string test_suite("fake_tests");

}

BOOST_AUTO_TEST_SUITE(fake_tests)

BOOST_AUTO_TEST_CASE(test) {
    SETUP_TEST_LOG("test");
}

BOOST_AUTO_TEST_SUITE_END()
#+end_src

*** Consider creating a test build for all facets                     :story:

We can't afford to generate test code in Dogen for the continuous
builds because we don't have enough build time to compile all of the
generated code. This is true even when we are just generating a few
facets (=test_data=, =types=, =tests=). However we definitely want to
test the generated code in real models.

The solution for this is to allow "variability overrides" (see [[*Add support for meta-data overrides in Dogen][Add
support for meta-data overrides in Dogen]]). Once we have this
functionality in place, we can then update our nightly builds.

Notes:

- create a test profile that enables all facets.
- in CMake, add an override to this profile for nightlies. This could
  be a parameter passed in from CTest.
- run the =gad= target in CTest first, then build then run all tests.
- because the nightly is running under our control, we can easily
  check CDash for errors and look at the generated source to
  investigate the problem.
- we should setup nightlies for Windows and OSX as well.
- remove all of the test facets from the main repo (e.g. =test_data=,
  =tests=).
- note that this approach will also resolve the problem with ignoring
  tests because we don't need to have them in the version control
  system any longer. We should remove all of the regexes ignoring
  tests as part of this work.
- this approach could be extended to conversion: once we fix all of
  the issues with JSON conversion, we don't need to have the JSON
  models in version control. We can generate them on the fly for
  nightlies only. It will require a bit of thinking because the tests
  are hard coded.
- the key thing though is the overall build time must be below the
  threshold. Maybe we can have this on a nightly, running on our own
  hardware.
- manually created generation tests at present do not use the
  overrides; this means that we will now have a lot of spurious
  differences in code generation due to this. As a result we must not
  run the generation tests whenever we run the generated tests. Which
  means we must somehow split these two kinds of tests and make them
  mutually exclusive. This is best achieved by having two different
  nightly builds:
  - "manual" build: as is at present minus all the generated tests. We
    do not need to ignore generated tests because there will be none
    in git.
  - "generated" build: only runs the generated tests. Must not run the
    manully generated tests.

*** Use clang9 and GCC 9 in nightly and CI                            :story:

We seem to still be in clang 8 and gcc 8 in some places. Update the
compilers.

*** Create tests for variability overrides                            :story:

We've added all the functionality needed to override meta-data, but we
did not update any of the test models to exercise all of its
permutations:

- update model, element, attribute

*** Consider creating top level exceptions                            :story:

There are a number of exceptions that have been repeated across
projects. For example:

-  transform error
- building error

We should consider having these in the dogen API and removing them
from each project.

*** Create a chain to encapsulate variability transforms              :story:

At present we are using individual variability transforms in the
engine, and interspersing those with other transforms. A nicer way is
to have a chain in variability that takes in a configuration model set
and runs a chain against it.

Actually we can only encapsulate two transforms:

- profile_binding_transform
- profile_repository_transform

Still, its worthwhile doing it.

*** Generate ORM tests                                                :story:

We do not seem to be testing the generated ODB code. We don't need to
test ODB per se, but we should at least have some sanity checks that
test CRUD functionality.

Notes:

- for this we need a "masd database".
- tests should only trigger if postgres or some other relational
  database is detected.
- if foreign keys are used we need to detect them and ensure we
  populate the data accordingly.

*** Schema name in ORM should be transitive                           :story:

At present when we define the schema name on a top-level namespace, we
don't "inherit" it from child namespaces. The problem is compounded by
the fact that we need the schema name in order to output ODB pragmas
(separate bug). It seems more logical to propagate the schema name to
child namespaces.

*** ODB pragmas not populated when schema name is not set             :story:

At present we have a bug whereby not setting the schema name results
in not having most ODB pragmas set. We should always populate them
even if the schema name is not set. To be precise, the problem is not
directly related to the schema name - we just require some ORM
property to be set. AS it happens, it normally tends to be the schema
name, because it makes sense to set it when defining a relational
model. This is why we never bumped into this problem before.

*** Consider renaming =origin_types=                                  :story:

We created an enumeration called =origin_types= to distinguish between
models of different types:

- target model
- reference model:
  - proxy reference: a PDM really.
  - non-proxy reference: a regular dogen model we reference.

However, this is not really the model's "origin". It is more like "the
model's purpose in this context". We need to think more about the
meaning of this enumeration.

*** Make =scoped_tracer= header only                                  :story:

At present we are generating the cpp for this file for no reason, use
the correct profile for header only.

*** Replace =operator<= for sorting with lambdas                      :story:

We have used =operator<= a lot for sorting lists. We don't really need
this since c++ 11, we can just create a simple inline lambda.

*** =CMakeFiles= do not reference dogen models                        :story:

At present we cannot test cross-model referencing because our
CMakeFiles are not adding the linking references to these models. This
needs to be fixed before we can test cross model serialisation.

Notes:

- in order to map references to models, we need to create a modeling
  element for a reference. For this we have two cases: for proxy
  models/PDMs, we need to read from the meta-data the name of the lib
  the model generates. For dogen models we can create it from the
  model name.
- this is a variation of the "exports and imports" pattern: we import
  a set of libraries (these can either be macros or actual library
  names) and we export (for now) a single library. When we support
  facets in libraries, we may need to export more than one, so we
  should cope with this scenario now. We need to keep track of the
  exports for a reference, and then use those as the imports for the
  model.
- in an ideal world, all imports come via this mechanism. However,
  this means we now have to create PDMs/proxies just to setup the
  imports. For example, for LibXML we will not need to define any of
  the types, but we need the import. However, If we do force the
  definition of the PDM, the advantage is that we now have the right
  place to put the definition, and is done only once and shared by all
  models.

*** Mask sensitive fields in io                                       :story:

Certain types contain fields that should not be logged by default. For
example, passwords, salt/seeds, etc. It should be possible to mark
these fields as "sensitive" such that when one dumps an object to the
logger the fields are masked out with say =****=. It should also be
possible to set an environment variable such as
=MASD_DO_NOT_MASK_SENTIVE= and get the actual values printed.

To implement this we need:

- a feature for marking fields as sensitive. Add a sensitive default
  for each primitive type, e.g. =****= for strings, =1234= for
  numbers, etc.
- a new manipulator in the shared library: =masd::unmask_sensitive=.
- update io for fields marked as sensitive; by default output the
  sensitive default unless =masd::unmask_sensitive= - in which case
  output the real value.

Notes:

- consider adding a warning for fields with certain names such as
  "password": mark this field as sensitive?

Merged stories:

*Consider adding a global configuration for io*

It would be nice to have some kind of configuration for IO that could
be accessed globally for the current process. There we could set
things such as floating point display, etc.

Actually maybe the right thing to do is to have masd specific
manipulators that you can check for in the streaming functions. We
need to read up on manipulators.

Links:

- [[http://www.two-sdg.demon.co.uk/curbralan/papers/WritingStreamManipulators.html][Writing your own stream manipulators]]

*** Add meta-data to "force" parent                                   :story:

At present we can force a class not to be final:

: #DOGEN masd.generalization.is_final=false

However, this still does not create the methods for a parent such as
virtual destructor, equals etc. We need something to trigger those
methods as well.

*** Add string view to dogen exception constructor                    :story:

At present we cannot build an exception if the string passed in is a
string view.

*** Move fabric types into coding                                     :story:

Fabric types need to be tidied up and moved into coding as regular
meta-model elements. We need to try to make them as technical space
agnostic as possible.

*Previous understanding*

Move fabric types into generation.

- copy across the fabric types from cpp and csharp into generation.
- update formatters to use the types from generation.
- delete them from original models.

At present we are always generating the fabric types via the injctor
and then asking the user to disable them as required via the
enablement settings. This is very silly. The approach should now be
that we look for elements with the correct stereotypes,
e.g. =masd::cmakelists= and so forth and use those to generate these
elements. This must be done as part of the work to move fabric types
into the metamodel. We should also take this opportunity to merge
common types between C# and C++, if any exist.

Notes:

- this will also address the naming of types such as registrar.
- we need to remove all top-level knobs that are controlling the
  enablement of meta-types such as visual studio, etc. In addition, at
  present when we enable say ODB we automatically get ODB options,
  etc. In this world, we would need to create the element in the
  model. This is a bit confusing because users won't know this is a
  requirement. Perhaps we need to have a combination of implicit and
  explicit types?

*** Fabric generates forward decls with no path                       :story:

The following looks strange:

: 2019-03-06 17:30:20.074618 [DEBUG] [quit.cpp.formatters.workflow] Procesing element: <dogen><hello_world><transformation_error>
: 2019-03-06 17:30:20.074627 [DEBUG] [quit.cpp.formatters.workflow] Meta name: <dogen><generation><cpp><fabric><forward_declarations>
: 2019-03-06 17:30:20.074636 [DEBUG] [quit.cpp.formatters.workflow] Using the stock formatter: masd.extraction.cpp.serialization.forward_declarations
: 2019-03-06 17:30:20.074647 [DEBUG] [generation.cpp.formatters.assistant] Processing element: <dogen><hello_world><transformation_error> for archetype: masd.extraction.cpp.serialization.forward_declarations
: 2019-03-06 17:30:20.074659 [DEBUG] [quit.cpp.formatters.workflow] Formatted artefact. Path: ""

This could help explain the problems we're having with empty
artefacts. This should be fixed with the new approach to forward
declarations.

Another related problem is that we are not setting the path when
creating stitch templates for the first time in the stitch formatter.

We should add checks for empty path and see what breaks, now that we
are using the new implementation of forward declarations.

*** Create metamodel elements for =entry_point= and =interface=       :story:

These have been incorrectly added as configurations and/or fabric
types. This should be looked at after merging the fabric types.

*** Move ORM camel-case and databases into yarn                       :story:

We should handle this property at the ORM level, rather than at the
ODB level.

Similarly, we should move the ODB databases into yarn and make that a
ORM-level concept.

*** Consider renaming logic-less templates                            :story:

Originally we though this was a good name because it was used by some
domain experts, but it seems it generates more confusion than
anything. It may just be a term used by mustache and other niche
template groups. We should probably rename it to text templates given
most domain experts know what that means,

*** Windows clang-cl release build is failing 4 tests                 :spike:

This has been going on for a fair bit, and we've ignored it so far but
its a bit annoying. It also makes it likely that we break something
without noticing because we are getting used to seeing red.

The problem started at build [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/23959333/job/r34e67jyjk6s8x66][1771]]. It happened with commit
[[https://github.com/MASD-Project/dogen/commit/2eca4e92de08cd3a84944abc9cf26e7e117e7144][2eca4e92de08cd3a84944abc9cf26e7e117e7144]]. Everything was fine up to
commit [[https://github.com/MASD-Project/dogen/commit/655b56cd32b94b7091e79c4cc76f6a2db5458416][655b56cd32b94b7091e79c4cc76f6a2db5458416]]. However, manually
checking the commits in this interval did not reveal anything obvious.

Failing tests:

- masd.dogen.coding.tests/object_templates_transform_tests/model_with_object_template_that_parents_missing_object_template_throws (Failed)
- masd.dogen.coding.tests/object_templates_transform_tests/model_with_object_that_models_missing_object_template_throws (Failed)
- masd.dogen.coding.tests/object_templates_transform_tests/model_with_object_with_missing_parent_throws (Failed)
- masd.dogen.coding.tests/stereotypes_transform_tests/visitable_object_with_no_leaves_throws (Failed)

All failing tests are related to exceptions that should be thrown. All
work on all other builds (debug and release) except this one,
including MSVC release. However, they were previously working fine on
this build (over 10 successful runs).

Interestingly, on failure we do not seem to get any output at all.

Notes:

- History of builds available [[https://my.cdash.org/index.php?project=MASD+Project+-+Dogen&filtercount=4&showfilters=1&filtercombine=and&field1=site&compare1=61&value1=appveyor&field2=buildname&compare2=61&value2=clang-cl-Windows-AMD64-Release&field3=buildtype&compare3=61&value3=Continuous&field4=buildstarttime&compare4=0&value4=][here]].
- it would be nice to be able to enable debug logging for these tests
  test and have the CI dump the log files into the main build
  log. That is, what we really need is to dump the log to the console
  for a specific set of tests. We could create a different macro that
  does this and manually replace it just for these tests.
- seems like we've managed to fix the clang-cl errors that have been
  traffic-lighting of late. This was a result of the assets changes on
  enumerations, primitives and parsing. Changes are between commits:
  - end: [[https://github.com/MASD-Project/dogen/commit/c629048f0c873f76f576200073ee647acbfbfcea][c629048f0c873f76f576200073ee647acbfbfcea]]
  - start: [[https://github.com/MASD-Project/dogen/commit/166110a944587b0dfb2a53794fd71b504da89065][166110a944587b0dfb2a53794fd71b504da89065]]
- started again with next build. Its traffic lighting, but the pattern
  is not yet obvious.

*** Make explicit all implicit modeling elements                      :story:

At present we have a number of modeling elements that can be
configured (enabled/disabled) but do not have a representation within
a model. Example:

- cmake
- visual studio
- odb
- etc.

This means we cannot associate any configuration with these elements
such as licences, modelines etc. This is one reason why there are
hacks to hard-code the modeline of CMake files. A better way is to
force users to create a modeling element (with the appropriate
meta-model stereotype, e.g. =masd::visual_studio::project=) and then
have them configured via named configurations. This means that for
each archetype we must have a distinct modeling element. It also means
that some modeling elements are language specific, but the metamodel
will merge them all into one space. We should also have them inherit
from common base classes where possible.

Note: not all meta-model elements will be available on all technical
spaces. We need a way to make sure they are compatible. Perhaps the
element could have a list of compatible TSs.

This approach follows the unwritten rule of "no black box injection of
modeling elements". We should formalise this rule somewhat and explain
the rationale for it.

Note that the handling of =invalid= in enumeration also falls under
this remit. At present we are injecting the invalid enumerator
transparently via meta-data switches. This is not a good idea. Users
should instead have some kind of "enumeration template" from which
they can inherit which will give them the required enumerators. We
should not do anything special for invalid.

Merged Stories:

*Consider allowing renaming of "internal" types*

Users may want to change the =_visitor= postfix for visitors or the
boost serialisation registrar name. This could be achieved via
meta-data.

*Consider renaming registrar in boost serialisation*

At present we have a registrar formatter that does the boost
serialisation work. However, the name =registrar= is a bit too
generic; we may for example add formatters for static registrars. We
should rename this formatter to something more meaningful. Also the
name registrar is already well understood to mean static registrar.

This is a big problem now that we cannot add a type with the name
registrar to the main model as it clashes with the serialisation
registrar.

We could simply name it serialisation registrar or some such name that
is very unlikely to clash. We should then have a validation rule that
stops users from defining types with that name.

We need to go through all of the renamed registrars and fix them.

Another option is to allow users to supply a name via meta-data to
avoid name clashes. We could error when the user has defined a type.

Actually, since the clash is only internal - the names we are
generating on the fly are clashing with the user defined names - we
should probably have a "postfix" that can be added in case of
clashes. The generated code will not cause problems, its just the
formattables pipeline.

*Allow renaming of visitor*                                         :story:*

At present the visitor is named by dogen. There is nothing stopping us
from allowing users to rename it via meta-data. We don't have a use
case yet.

*Handcrafted support for fabric types*

At present we can either disable fabric types or enable them
(CMakeLists, etc). However, there is a third common use case: to
handcraft them. To do this we normally disable them and then add the
file to the ignore list:

:  --ignore-files-matching-regex .*/CMakeLists.txt)

One could conceive of some meta-data support that would make this
process a tad easier and more generic:

: quilt.cpp.cmakelists.stereotypes=handcrafted

Then hopefully the existing pipeline would take over and we'd generate
the files for the first time but then let the user overwrite it. This
would also be applicable to all fabric types (registrar, etc) but we'd
have to manually read each stereotype on each factory.

Merged stories:

*Make visitor an explicit type*

Instead of automatically generating visitors via the visitable
stereotype, we should:

- create a new stereotype =masd::visitor=. It triggers the creation of
  the visitor meta-model element.
- visitor must have a target via meta-data. This points to the element
  to visit.

We need to make sure we don't break cross model visitation with this change.

*** Fix issues with nightly build and CI                              :story:

Time spent fixing build issues with either nightlies and/or CI.

- make space for builds in CDash.

*** Read variability papers                                           :story:

Time spent reading the literature on variability.

*** Element extensions considered harmful                             :story:

When we implemented forward declarations we created them as "element
extensions"; that is, some kind of hack where we'd have two model
elements stuck together (the main model element and its "extension",
the forward declaration). In reality, they are just projections of the
same model element. We need to handle them just as we handle class
header / implementation. We just need to use the formatter specific
postfix to distinguish between files.

The problem with this approach, of course, is that we now need to
create many formatters (per element type). A possible solution is to
factor them out into a formatting helper function that they call. We
still need all of the common machinery to formatters
though. Nevertheless, this is a price worth paying in order to keep
the meta-model simple (e.g. none of the hacks we introduced for
element extensions).

Notes:

- add forward declaration formatters for each type. Create common
  formatting function.
- remove forward declaration element in fabric.
- remove element extensions across the code base. Actually this is not
  possible at present as it is used by ODB options. We need to first
  move them into assets before this can be done.

Merged stories:

*Remove element segmentation*

We need to remove the idea of forward declarations being handled as
"element segmentation". They should just be different facets of the
same elements. There is another story for this which should be merged
with this one.

*Move element segmentation into yarn*

We've added the notion that an element can be composed of other
elements in quilt, in order to handle forward declarations. However,
with a little bit of effort we can generalise it into yarn. It would
be useful for other things such as inner classes. We don't need to
actually implement inner classes right now but we should make sure the
moving of this feature into yarn is compatible with it.

Notes:

- seems like we have two use cases: a) we need all elements, master
  and extensions and we don't really care about which is which. b) we
  only want masters. However, we must be able to access the same
  element properties from either the master or the extension. Having
  said all that, it seems we don't really need all of the element
  properties for both - forward declarations probably only need:
  decoration and artefact properties.
- we don't seem to use the map in formattables model anywhere, other
  than to find master/extension elements.
- Yarn model could have two simple list containers (masters and
  all). Or maybe we don't even need this to start off with, we can
  just iterate and skip extensions where required.
- so in conclusion, we to move decoration, enablement and dependencies
  into yarn (basically decoration and artefact properties) first and
  then see where segmentation ends.

Tasks:

- add a concept for element extensions: =Extensible=. Contains a list
  of element pointers.
- populate it with the extensions.
- change enablement to merge all element properties of extensible
  elements.

*** Validate feature template names                                   :story:

We need to ensure the template names are valid identifiers in C++.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the SO, allowing users to consume it. In
the Dogen case, it means users can create plugins for Dogen. In the
PDM case, it means users can make use of the PDM in their own models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.

*** Emacs maintenance and exploration work                            :story:

Any time spent improving emacs, exploring new modes, fixing snags,
etc.

- add support for indent guides. [[https://github.com/DarthFennec/highlight-indent-guides][highlight-indent-guides]], [[https://stackoverflow.com/questions/1587972/how-to-display-indentation-guides-in-emacs/56144459#56144459][SO question]].
- treemacs issues: when blank type g to refresh.
- lsp seems to update with every character we type. It would be nice
  to update on save only.

** Deprecated
*** CANCELLED Reactivate injection.dia tests                          :story:
    CLOSED: [2019-06-03 Mon 20:01]

*Rationale*: these tests have now been removed when serialisation
support was removed.

We seem to have a number of tests commented out in
injection.dia. Investigate why and if possible, reactivate them.
