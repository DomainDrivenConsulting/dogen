#+title: Nerd Food: Extreme Incrementalism: A Coding Religion
#+options: date:nil toc:nil author:nil num:nil title:nil

A lot of interesting lessons have been learned during the development
of Dogen and I'm rather afraid many more are still in store. As it is
typical with the agile process, I'm constantly reviewing what went
right and what went wrong. I thought I'd put pen to paper with some of
these, In the vain hope of them being of general interest.

One of the major wins in terms of development process was to think
incrementally. Of course, agile already gives you a mental framework
for that, and we had a perfectly functioning agile process during our
start up days - the time of the inception of Dogen. However, with the
tearing down of the start up and the return to gainful employment,
agile soon revealed itself to be far too coarse grained.

When I decided to continue developing Dogen on my spare time, I had
two conflicting requirements: very little development resources and
very ambitious ideas that required lots of work. The development
resource was, well, just me. With a family and a full time job there
isn't a lot of spare cycles left. In fact, after some analysis, I
realised I was in a conundrum. Whilst there is was a lot of
"dead-time" in the average week, it was mostly "low-quality grade
time": lots of discontinued segments of varying and unpredictable
lengths. Summed together in a naive way it seemed like a lot, but - as
every programmer knows - six blocks of ten minutes do not one solid
hour make.

Nevertheless, one has to play the game with the cards that were dealt.
Soon I realised that the correct question to ask was: "what kind of
development environment would be required such that one can be
productive given these conditions?"

As it turns out, the answer is multi-part - just like MIME.

* DVCS is at the core

We had already started to use git during the start-up days, and it had
proved to be a major win at the time. After all, one never quite knows
where one will be coding from - and whether internet access is
available or not - so its important to have a self-contained
environment. In the end we found out it brought many, many more
advantages such as great collaborative flows, good managed web
interfaces/hosting providers ([[http://www.github.com][GitHub]] and, to some extent, [[http://www.bitbucket.com][BitBucket]]),
amazing raw speed even on low-powered machines, and many other
wins. Since most of it is covered elsewhere, I won't bore you with it.

However, one decision that turned out to be a major win is still not
the done thing. Ever the trailblazers, we decided to put everything
related to the project in version control. And by "everything" I do
mean *everything*: documentation, bug reports, agile process, blog
posts, the whole lot. It did seem a bit silly not to use GitHub's Wiki
and Issues at the time, but, on hindsight, having everything in one
versioned controlled place proved to be a major win:

- searching is never further than a couple of greps away, and its not
  sensitive to connectivity;
- updates by other people come in as commits and can be easily
  reviewed as part of the normal push/pull process;
- changes can easily be diffed;
- history can be checked using the familiar version control interface,
  which is available wherever you go.

When you have little time, these little advantages are life-savers.

The second and very important lesson learned was to commit early and
commit often. Its rather obvious in hindsight, really. After all, if
you have very small blocks of time to do work, you want to make sure
you don't break anything; last thing you need is to spend a week
debugging a tricky problem, with no idea of where you're going or how
far you still have to travel. So its important to make your commits
/very small/ and /very focused/ such that a bisection would almost
immediately reveal a problem - or at least provide you with an obvious
rollback strategy. This has proved itself to be invaluable, far too
many times to count. The gist of this approach it is to split changes
in an almost OCD sort of way, to the point that anyone can look at the
commit comment and the commit diff and make a judgement as to whether
the change was correct or not. To be fair, its not quite always that
straightforward, but that has been the overall aim.

* Continuously Integrated

After the commit comes the build, and the proof is in the pudding, as
they say. When it comes to code, that largely means CI; granted, it
may not be a very reliable proof, but nevertheless it is the best
proof we've got. One of the major wins from the start up days was to
setup CI, and to give it as wide a coverage as we could muster. We
setup multiple build agents across compilers and platforms, added
dynamic analysis, code coverage, packaging and basic sanity tests on
those packages.

All of these have proven to be major steps in keeping the show on the
road, and once setup, very trivial to maintain. We've had a couple of
minor issues with CDash - and certainly our setup could do with some
improvements - but on the main, it just chugs along.

The packaging story was also a very good one - after all, most users
will probably rely on those - but it turned out to be much harder than
first thought. We spent quite a bit of time integrating with the
GitHub API, uploading packages into their downloads section,
downloading them from there, testing, and then renaming them for user
consumption. And, while it lasted, it was very useful. Unfortunately
it didn't last very long. GitHub decommissioned their downloads
section. Since most of the upload and download code was GitHub
specific, we could not readily move over to a different location. The
lesson here was that this sort of functionality is extremely useful,
and its worth dedicating time to it, but one should always have a plan
B - and even a plan C.

The end result is that, at the moment, we don't have any downloads
available at all - not even a stale ones - nor do we have any sanity
checks on packages we produce; they basically go to =/dev/null=. But
the remaining aspects of CI are still in place, and we rely on it
daily.

* Loosely Coupled

Another very useful lesson learned was to keep the /off-distro/
dependencies as low as possible. We started off by requiring a C++
compiler with good C++ 11 support, and a Boost library with a few
off-tree libraries - mainly Boost.Log, really. This meant we had to
have our own little "chroot" with all of these, and we had to build
all of these by hand, sprinkled with plenty of helper scripts. It was
workable when we had time, but this is really not the sort of thing
you want to spend time maintaining if you are working on a project on
your spare time.

To be fair, we always intended to move to distro-supplied packages
when they caught up, and as it happens that is soon to happen: both
default GCC and Clang are more than good enough in Debian unstable,
and Boost 1.55 should soon be hitting it too. This means we can start
thinking of creating Debian packages for inclusion in Debian - rather
than the stand-alone packages we've been doing up to now.

Going back, it seems to me we took the right decisions as both C++ 11
and Boost.Log have proven quite useful; but in the future I certainly
will think twice when adding an off-distro library.

* Slow Motion Agile

During our start-up days, we had a "normal" agile process: daily
stand-ups, bi-weekly iterations, pre-iteration planning,
post-iteration reviews and all of that good stuff. It worked really
well, and keep us honest. We used a very simple org-mode file to keep
track of all the open stories, and at one point we even built a simple
burn-down chart generator to allow us to measure velocity - all in the
name of keeping everything in git.

Granted, when you are working by yourself and on your spare time, a
large chunk of agile makes no sense at all; after all, providing
status updates to yourself may not be the most productive use of
scarce time. But, surprisingly, quite a bit of it is vital. I've kept
the bi-weekly iteration cycle, the iteration logs, the backlog and the
time-tracking we had originally setup and found them *extremely*
useful - quite possibly the thing that has kept me going for such an
extended period of time, to be honest. After all, when you are working
on an open source project its very easy to get lost in its
open-ended-ness and end up giving up altogether, in particular if you
are not getting (or expecting) any user feedback. Even Linus himself
has said many times he would have given up the kernel if it wasn't for
other people bringing him problems to keep him interested.

Lacking Linus' ability to attract crowds of interested developers, I
went for the next best thing: I made them up. Well, at least in
metaphorical way, I guess, as this is what user stories are when you
have to external users to drive them. As I am using the product in
anger, I find it very easy to put myself in the head of a user and
come up with requirements that drive development forward. These
stories really help, because they transform the cloud of possibilities
into concrete, simple, measurable deliverables that one can choose to
deliver or not. Once you have a set of stories, you have no excuse to
be lazy because you can visualise in your had just how much effort it
would require you to implement a story - and hey, since nerds are
terrible at estimating, its never that much effort at all. Of course,
it's never quite that easy in the end; but once you've started, you
get the feeling you have to at least finish the task at hand, and so
on, one story at a time, one iteration at a time, until a body of work
starts building up. Its slow, excruciatingly slow, but it's
steady. Its like water working in geological time; when you look back
5 iterations, you cannot help but be amazed on how much can be
achieved in such a incremental way. And how much is still left.

And then you get hooked into measurements. I now love measuring
everything, from how long it takes me to complete a story, to where
time goes in an iteration, to how many commits a day, to, well,
everything that can easily be measured without adding any overhead.

* Make It Work Then Make It Better

Many moons ago there was a discussion in kernel-land about the order
of things: when faced with a problematic code base, should one first
"make it better" or "make it work"? For me, Alan Cox won the day on
that debate - you should always make it work first. The gist of the
argument was that when you inherit a broken code base, you should try
to get in the head of the previous developers and take their ideas to
a logical consequence - the point at which the code is actually doing
/something/ useful; and after that you can start refactoring, very
carefully, until the code is in a good shape (as defined from the
perspective of the maintainer).

This debate made a big impression on me, because the core ideas apply
to more than inherited code bases; it actually touches in a lot of the
fundamental principles of development. Of course, Agile then came
around and formalised a lot of ideas around this topic; but as with
everything in coding, the ideas are easy to understand but very
difficult to put in practice, and one only gets right after making a
lot of mistakes.

In truth, it is very easy to have big ideas when it comes to coding,
but for every N big ideas - where N is very large - only a tiny
fraction ever actually makes it into a successful project. One reason
is that people often start aiming their code directly at the big idea,
adding frameworks, services, layers of abstraction and so on well
before the system does anything at all. You would have thought that in
an Agile world this wouldn't happen, but it still often does. I guess
Agile reduced the scope and frequency of these catastrophes because it
gives much more visibility to what developers are up to. Having said
that, I've been involved in a number of Agile projects where the
"frameworkisation" was still alive and well, and given enough
resources, there is always something user-facing to demo.

After many years of experiencing this first hand - many times of my
own making - I came to believe on austere coding. Every project should
start with a =main= with some console output: "hello world" would
do. Does that compile and run?  Great, check it in. Then organically
add the simplest possible bit of code that does something related to
what you want to do. Eventually it will build up to a point where it
has some tiny bit of functionality related to your big idea; at that
point at a "system" or "end-to-end" test. For instance, create a text
file that contains the string "hello world" and make sure that the
output is equal. Don't worry too much about internal unit tests -
that's for when the structure of the program has been proven. Don't
take me wrong - unit tests are not optional - but "premature testing"
is at least evil as "premature optimisation" if not more. Just like
with frameworkisation, "unittestisation" is very common and people
write endless unit tests that prove that some internal class or other
works, without any regard for what that class contributes in terms of
end to end functionality.


I even gave it a name: /Extreme Incrementalism/ or EI for
short. As you can see, the name fits well with the spirit of
"overreaching naming conventions", typically used for development
methodologies. Opportunistic coding.
