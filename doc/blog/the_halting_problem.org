#+title: Nerd Food: The Halting Problem
#+options: date:nil toc:nil author:nil num:nil title:nil

[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_07.org][The latest Dogen sprint]] turned out to be a really long and tortuous
one, which is all the more perplexing given the [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_06.org][long]] [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_05.org][list]] of [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_04.org][hard]]
[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_03.org][sprints]] that preceded it. Clearly, the slope is steepening
unrelentingly. Experience teaches that whenever you find yourself over
these terrains, its time to stop and gather your thoughts; more likely
than not, you are doing something wrong.

Thus, for Dogen, this a post of reflection. To the casual reader - if
nothing else - it will hopefully serve as a cautionary tale.

* Plus Ã§a change

If you are one of the lucky few internauts who follows our release
notes, you may recall that the [[https://github.com/DomainDrivenConsulting/dogen/releases/tag/v1.0.06][previous sprint]] had produced a moment
of enlightenment where we finally understood =yarn= as /the/ core of
Dogen. At the time, it felt like an /eureka/ moment, and it did seem
this it was "the one last great change to the architecture";
afterwards, all would be light. On the face of it, the changed seemed
logical and made perfect sense in the historical context. This could
be briefly summarised as follows: Dogen started out as split into
three very distinct parts: the frontends, the middle-end and the
backends; over time, we moved more and more functionality that was
originally on the frontends and backends into the middle-end (that is,
=yarn=) because it meant that it could easily be reused. You could say ww


place all meta-models and transforms in one place, and the
code to hook things. Everything is either a meta-model element or a
transform, as per the literature. Yet, just as we were about to
complete this work, it quickly became apparent that there were some
downsides, which were not understood when we started it. This is a
disadvantage of working alone and in your spare time: without feedback
from other developers, its easy to convince yourself and then enter a
tunnel of self-reinforcement.

The problem of centralising all responsibilities into ```yarn```,
obvious on hindsight, is that we start to create "circular"
dependencies. Now, these are not "canonically circular" - and this is
probably why the problem was not picked up in the first place -
because ```yarn``` provides interfaces for other models to
implement. Thus, due to the magic of (hand-crafted) dependency
injection, at run time we assemble ```yarn``` with all of its
dependencies. [Lakos](https://www.amazon.co.uk/Large-Scale-C-Software-Design-APC/dp/0201633620),
is very helpful here in explaining what is going on: our _logical_
design had no cycles, but the _physical_ design had them.

* From a Pipeline to a Circle

Originally, we had used the "pipeline" metaphor to guide our physical
and logical design because we saw Dogen very much like a compiler,
with distinct frontend, middle-end and backend stages. This was very
handy because it meant we could test all elements of the pipeline in
isolation. Composition was done by orchestrating frontend, middle-end
and backends, at a higher level. This architecture had very good
properties when it came to debugging: we'd start by running the entire
pipeline and finding the problem; then, one could easily isolate the
issue to a specific component either by looking at the log file, or by
dumping the inputs and outputs of the different stages and sifting
through them. As a result, bug reproduction was very straightforward
since we just needed to record the inputs and create the test at the
right level (e.g. frontend, middle-end, backend).

The downside of this approach was that it resulted in a number of very
small models - "modelets", we named them. These are models with very
little responsibility other than gluing together a few things. The
overhead of maintaining a physical component (e.g. static or dynamic
library) for the sake of one or two classes was deemed to be too high,
which led us to revisit the architecture and centralise orchestration
in ```yarn```, in addition to its middle-end responsibilities. Even
though frontends and backends still had their own models, the net
result of this change was that we jumbled up all of the elements of
the pipeline into a single model. This meant for example that
explaining the code to a new developer now meant saying things such as
"ah, don't worry about that bit for now, it belongs to the
middle-end". [Reasonability](https://the-whiteboard.github.io/coding/debugging/2016/04/07/reasonable-code.html),
once a property of the architecture itself, now had to be explained in
words; our pursuit of clarity muddied up the waters.

## Refactoring _Ad Inifinitum_

To make matters worse, an even more pertinent question arose: just
when exactly should you stop refactoring? In my two decades of
professional development I had never encountered this question. In the
real world, you are lucky to get a tiny amount of time allocated to
refactoring - most of the time you need to somehow sneak it in into
some overall estimate and hope no one notices. Like sharks, Project
Managers are bred to smell refactoring efforts from a mile a way and
know how to trim estimates down to the bone. Even when you are lucky
and have an enlightened manager who will bat for you, he or she will
still have to contend with the realities of corporate development. No
one gets away with refactoring forever. No one, that is, other than
the Free and Open Source Software Developer.

Like many a spare time project, Dogen is my test bed of ideas around
coding and coding processes; a general sandbox to have fun outside of
work. As such - and very much by design - the traditional feedback
loops that exist in the real world need not apply. I wanted to see
what happened if you coded without _any_ constraints and, in the end,
what I found out was that if you do not self-impose some kind of
halting machinery, _you will refactor on forever_. In practice,
physics still apply, so your project will eventually die out because
its energy will dissipate across the many refactoring efforts and
entropy will, as always, triumph. To keep it at bay, at least for a
little while, one has to have one single, consistent vision - "wrong"
as it may be according to some metric or other. Voltaire, of course,
had warned about it a long time ago: "Le mieux est l'ennemi du bien".
Like the sisyphian creatures that we are, we are condemned to repeat
history.

On reflection, the problem is that refactoring is made up of a set of
engineering trade-offs, and when you optimise for one thing you'll
inevitably make something else worse. Looking for a global minima in
such a gigantic multidimensional space is impossible so you need to
make do with local minima. But how do you known you reach a "good
enough" point in that space? How do you halt the refactoring
algorithm?

## Solving the Halting Problem

To solve this conundrum, we want back to basics and asked: given what
we now know about the domain and its implementation, what are the most
important characteristics of an idealised physical and logical design?
The answer was deceptively simple:

1. the entities of the logical design (models, namespaces, classes,
   methods and the like) should reflect what one reads in the
   literature of Model Driven Engineering (MDE). That is, a person
   competent on the field should find a code base that talks his or
   her language.
2. logical and physical design should promote reasonability and
   isolation, and orchestration should be performed via composition
   rather than by circular physical dependencies.

These are the two fundamental pillars of Dogen architecture, and any
engineering trade-offs to be made must ensure these dimensions take
precedence. In other words, we can only optimise away any "modelets"
only if they do not impact negatively either of these two
dimensions. If they do, then we must discard the refactoring
option. More generally, it is now possible to classify all refactoring
activity - a refactoring gradient descent if you'd like; it either
brings us closer to the local minima or further away. We have a sieve
for the product backlog.

So it was that we then came up with a "final" set of changes to the
architecture to get us closer to the local minima:

- **move away from sewing terms**: this was a painful decision but
  clearly required if one is to comply to point one above. We need to
  replace all sewing terms with domain specific vocabulary.
- **reorganise the models into a pipeline**: however, instead of
  simply going back to the "modelets" of the past, have a deep think
  as to what responsibilities belong at what stage of the
  pipeline. Perhaps the "modelets" were warning us of design failures.

Fragmenting the ```yarn``` model
