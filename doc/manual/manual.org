#+title: Domain Driven Development with Dogen
#+options: author:nil

Copyright (c) 2012, 2013, 2014 Marco Craveiro

Permission is granted to copy, distribute and/or modify this document under the
terms of the GNU Free Documentation License, version 1.3; with no Invariant
Sections, no Front-Cover Texts and no Back-Cover Texts.

Revision *DRAFT*, May 2014

This revision of the manual describes Dogen *master* and is available
in the following formats: HTML and PDF.

#+toc: headlines 2
#+toc: listings
#+toc: tables

* Preface

** About This Document

This document is the official manual for Dogen. Dogen - the domain
generator - is a code generation tool designed specifically to target
domain models. Dogen was created to make the modeling process simpler:
the user creates a domain model using a UML tool and Dogen uses it to
generate its source code representation. The generated code contains
most of the services required from a typical C++ domain object such as
serialisation, hashing, streaming and so on.

If you are reading a printed copy of this manual, you can always
access the latest version online:

- https://github.com/kitanda/dogen/blob/master/doc/manual/manual.org

** More Information

You can find the latest source code for Dogen at the official
repository in GitHub:

- https://github.com/kitanda/dogen

There is also a mirror in BitBucket:

- https://bitbucket.org/marco_craveiro/dogen/overview

You can find details on the ongoing work in the Agile folder (look for
the latest sprint):

- https://github.com/kitanda/dogen/tree/master/doc/agile

Continuous builds are available via CDash:

- http://hedgr.co.uk/cdash/index.php?project=dogen

* PART I - THEORY

In this part we describe the philosophical underpinnings behind Dogen,
its internal architecture and the development model.

** Introduction

Dogen is largely the result of exploring a simple question: what
portion of the objects required to model a problem domain are
generatable by a program - such that the generated code is as good
as or even better than code crafted by humans?

The question stems from many years of looking at object models and
their limitations. For instance, one of the main problems one often
finds in production C++ code is a lack of a large number of useful
"facilities" that all objects on all domain models should have; for
instance, a simple way of dumping current state to a stream, like
/toString/ in Java. Developers tend to add facilities like these on a
haphazard sort of way, because it is laborious and not particularly
exciting functionality to work on; by the time the domain model has
matured, its too late to find time for these sort of fundamental
activities.

We further observed that a large portion of the objects required to
model a problem domain have fairly straightforward behaviours; in many
cases they are but glorified structs with a few trivial behaviours
bolted on such as serialisation and hashing. A lot of programmer time
is taken on generating getters, setters, serialisation code and so
on. This is also code that is easy to get sloppy with, because its so
repetitive.

Dogen aims to code generate all such code - and /only/ such code;
anything which is deemed non-trivial is expected to be done by
humans. For everything else, we aim to add support in Dogen.

*** Models and modeling

Programming is the art of refining abstractions. In general, the
programmer's job is to create a set of constructs that represent
entities in some problem domain or other; and to get those entities to
cooperate successfully in producing work that is defined as useful by
something or someone. Together, these entities form /a model/ because
they are representation inside of the computer of a subset of the
problem domain.

/Domain modeling/ is then the activity of finding a set of domain
types that describe the domain in question via their properties and
their relationships.

Many programmers don't dwell too much on the fact that they are
modeling - its just an implicit activity done as part of
coding. However, as we shall see, the modern way of looking at
programming puts a strong emphasis in understanding and introspecting
about this activity.

*** Code generation and meta-models

The concept of programs that generate programs is probably as old as
computer science itself: it certainly was a common feature in the days
of machine code and assembler code programming. These ideas were
incorporated in early languages such as LISP, where there was a
blurring of the lines between hand crafted source code and machine
generated source code. Sadly, these progressive thoughts faded into
the foreground as the C family of languages took front stage.

It's not as if code generation disappeared - it just went into
hiding. In fact, there are many widely used tools in the Open Source
ecosystem that generate code:

- [[https://developers.google.com/protocol-buffers/][Google Protocol Buffers]]
- [[http://www.codesynthesis.com/products/odb/][ODB]]: C++ Object-Relational Mapping (ORM)
- [[http://www.codesynthesis.com/products/xsde/][eXSD]]: XSD/e: XML for Light-Weight C++ Applications
- [[http://msdn.microsoft.com/en-us/library/windows/desktop/aa367300(v%3Dvs.85).aspx][MIDL]]: COM IDL compiler
- and many more.

Each of these tools are designed to do a specific task and to do it
well, hiding as much as possible of the code generation details from
the end user. We call these are special-purpose code generators -
although, as we shall see, in a sense all code generators are special
purpose. The code generated by these tools contains both the data
structures they require as well as hard-coded behaviour associated
with them: how to read and write them from raw storage (in the case of
Protocol Buffers), how to read and write them from the database (ODB),
and so on.

All code generators have an internal set of data structures that
represent the entities to generate - explicitly or implicitly. These
data structures are known as the /meta-model/. Meta-models are a class
of models that focus on describing models themselves. They allow code
to introspect and to think about code; to reflect. In this form, code
generation is simply the transformation of a model, described in one
such representation, into another representation - source code -
following the rules laid out by the grammar of a programming
language. The richer the meta-model, the more expressive the generated
code can be - and vice-versa. It is in this light that we call certain
classes of code generators /special purpose/, because they have
meta-models that are very focused, designed only for the task at
hand. Don't think of this as a disadvantage though: there is a price
to pay in complexity for every ounce of flexibility, so its best to
have simple code that does one thing and does it well.

Nevertheless, meta-models can be useful in a more general form when
designing software applications: they can allow one to reason about
the structure of the code. One of the most common meta-models in
existence is [[http://en.wikipedia.org/wiki/Unified_Modeling_Language][UML]]. UML is used widely in the industry and there are
many tools that can be used to generate source code from UML
diagrams. It is simultaneously ubiquitous - that is, available
everywhere - and complete - that is, as a meta-model, it defines a
extensive list of concepts for pretty much any aspect of
programming. Thus it is common for tools to take a UML representation
and use it to generate source code; as examples of Open Source tools
that can generate source code from a UML diagram see:

- [[http://dia2code.sourceforge.net/][dia2code]]
- [[http://umbrello.kde.org/][Umbrello]] (see [[http://docs.kde.org/development/en/kdesdk/umbrello/code-import-generation.html][this]] for code generation)

In a sense one, may think of these as /general purpose/ code
generators because they output code that is not tied up to any
specific purpose, other than to model the problem domain. Unlike the
special purpose tools, the generated code is very much skeleton code,
code that adds little in terms of behaviour. This is all as it should
be: the more specific your intent is, the more the code generator can
do for you and, conversely, the less specific your intent is, the less
helpful the code generator can be.

The astute reader would have already devised a simple solution to the
behaviour conundrum: nothing stops us from modeling the signatures of
methods in the meta-model - after all UML provides us with all the
required machinery - and then hand-craft an implementation for these
methods. Indeed there are code generators which permit such workflows;
they are known as /merging code generators/. The merging aspect comes
from the fact that the code generator must be able to distinguish
between the hand-crafted code and the machine generated code in order
to handle meta-model updates.

So these are three key themes for Dogen: special purpose code
generation, general purpose code generation and merging code
generation. But before we can proceed, we need to add one more actor
to the scene.

*** Domain Driven Design

One of the main problems facing software engineers working on large
systems is the need to clearly separate business rules from
scaffolding code. In many ways, this need originates from the long
forgotten days when the word /Application/ was coined: the use of
computer science /applied/ to a specific problem to provide an
automated solution to the set of people with the problem - the
/users/. During the process of development, users will provide all
sorts of insights into what it is they want solved, and these are
ultimately captured in code. Code will also be made up of reading and
writing records to a database, socket communication, reading and
writing to file and so on; the challenge then is to avoid obscuring
the former while dealing with the latter.

Many people have thought deeply about this dichotomy. Arguably, the
most significant advance was made by Eric Evans with his seminal book
[[http://www.amazon.co.uk/Domain-driven-Design-Tackling-Complexity-Software/dp/0321125215][Domain-Driven Design]]: Tackling Complexity in the Heart of
Software. Domain Driven Design (DDD) is a software engineering
methodology that places great emphasis on understanding the problem
domain and, coupled with Agile, it provides a great platform for
iterative improvements both to the understanding and to its expression
in code. DDD places great emphasis in defining a clear and concise
domain model - a set of classes and relationships that model the
insights provided by the users and domain experts in general. It also
explains the difference between the conceptual domain model and myriad
of representations: UML diagrams, specification documents, oral
conversations and, most importantly, source code.

*** Adding It All Together

The key idea behind Dogen is that all of the aspects we described up
til now are deeply interrelated. That is to say that we store deep
knowledge about the domain in meta-models, which tend to be
represented graphically - say in UML class diagrams; and we do so
because these representations provide a quick and yet expressive way
to communicate domain knowledge. But those very same documents are -
or can be made - sufficiently complete to be used as a basis for the
code generation of skeleton code by some general purpose code
generation tool. Furthermore, there are a large number of services
that are required of most domain models, and these can be thought of
as special purpose extensions to such a general purpose tool; and,
finally, that which cannot be code generated can be manually added and
merged in.

Lets return to the "basic services" required by all domain
models. What do we mean exactly? Well, ODB and the like already hinted
at some of the things one may wish to do with C++ objects - persist
them in a database - but there are other even more fundamental
requirements:

- the ability to support getters and setters, hashing, comparisons,
  assignment, move construction and many other fundamental behaviours;
- the ability to dump the current state of the object to a C++ stream
  in a format that is parsable by external tools (like say JSON);
- the ability to generate [[http://stackoverflow.com/questions/5140475/how-to-write-native-c-debugger-visualizers-in-gdb-totalview-for-complicated-t][debugger visualisers]];
- the ability to serialise and deserialise objects using a multitude
  of technologies such as [[http://download.oracle.com/otn_hosted_doc/coherence/353CPP/index.html][POF]], [[http://www.boost.org/doc/libs/1_55_0/libs/serialization/doc/index.html][Boost Serialisation]], [[https://github.com/hjiang/jsonxx][JSON]], [[http://libxmlplusplus.sourceforge.net/][XML]] and many
  others;
- the ability to generate objects populated with random data for
  testing;
- ...

And on and on. The more we looked, the more boilerplate code we
found - code that could easily be generated for the vast majority of
the cases. Of course, there are quite a few corner cases which are
just too hard to automate but they can easily be manually coded.

The picture that emerges from this [[http://en.wikipedia.org/wiki/Thought_experiment][gedankenexperiment]] is some kind of
"cyborg" coding - a type of programming where any and all aspects that
can be reduced to a set of rules inferable from the structure of the
domain model, are implemented as extensions of the code
generator. Dogen is an attempt to create such a tool. As we are C++
developers we started off by trying to implement the vision as a C++
tool; but the notions are general enough that they would apply to any
programming language.

** The Dogen Architecture

Almost all code in Dogen is implemented as Dogen domain models; that
is, we use Dogen to generate the vast majority of Dogen itself, and we
do so for several reasons:

- dog-fooding: using your own tool frequently is a great way of making
  sure the tool does what it is meant to do and does so in a workable,
  pragmatic manner.
- keeping our feet on the ground: if we have some crazy ideas and
  break Dogen, we can no longer develop Dogen. Thus Dogen must always
  be able to code-generate itself at all points in the development
  cycle, which forces one to think "extremely" incrementally.
- code faster and test our theoretical underpinnings: if our ideas
  around code generation are correct, Dogen should significantly
  speed-up development.

Dogen is made up of a large number of domain models. These fall into
two broad categories: /test models/ and /core models/. Test models are
models we created specifically to test some aspect of code
generation - such as say inheritance - and whose code is not used by
the main binary. The core models are what really makes up the
application and that is what is of interest for this chapter.

The core models are hooked together in a fashion similar to that of
the internals of a compiler, and so core models belong to one of three
groups: the front-end, the middle-end and the back-end. The front-end
group of models allows for different sources of domain information to
be plugged into Dogen. The middle-end model - as there is only one -
is where all the language neutral transformations take place; It can
be thought of as a bridge between domain modeling and code
generation. Finally, the back-end group of models are responsible for
expressing SML as code according to the grammar of a programming
language like C++.

Lets look at each of these in more detail.

*** The Front-end

When we started developing Dogen, we chose Dia as our main input
format. Dia is a simple yet very powerful tool for drawing structured
diagrams that focuses almost exclusively on diagram editing, and
leaves other use cases to external tools. To their credit, a number of
tools have sprung up around Dia, in no small part due to the
simplicity and stability of their XML file format. Some of these tools
generate code from Dia XML, others convert code into diagrams; we
aimed for Dogen to be another chain in that tooling ecosystem.

At the same time, Dogen has been developed from the start with the
intention to support multiple input formats. We knew that different
people would have different needs and for some Dia would not be
sufficient. So we imagined a pipeline that was made up with a pair of
models: one to model closely the input model and a /transformation/
model responsible for converting the input model into the middle-end

Dogen. Furthermore, all of Dogen's own models have been created and
are being maintained using this application, so it is very core to the
code generation experience.

There are two models responsible for implementing this use case:

- =dia=
- =dia_to_sml=

The =dia= model has a representation of the Dia XML types, and tries
to do so as faithfully as possible. It was created to avoid having a
direct dependency with Dia's code base. Since Dia XML changes very
infrequently and since we use such a small part of Dia's
functionality, this turned out to be a good decision.

**** Meta-data and tags

In certain cases we had the need to pass certain information to SML
for which there was no available equivalent in Dia. In some cases
these were just shortcomings of the application and could be solved by
patching it; in some other cases, it just made no sense at all to
convey this kind of information in Dia. To solve this problem in a
general manner, we created a set of special "instructions" that are
interpreted by Dogen. These instructions are passed in to Dogen via
UML Comments, with a special form:

: #DOGEN KEY=VALUE

All lines starting with the well-known prefix =#DOGEN= are considered
special instructions. They must follow the key-value-pair form defined
above.

Initially this was done to fix a couple of minor problems with Dia,
but this infrastructure has taken a life of its own, and its now used
through Dogen. Each sub-system takes responsibility of its own keys -
it defines them and validates to ensure the values for a key are
valid. In the remainder of this manual you will find sections with a
name similar to this one, where we will define the tags available for
that component and their semantics.

These keys within Dogen are known as /tags/ and they are part of the
meta-data processing sub-system.

Dia defines the following tags:

- =dia.comment=: Comment provided by user when dia does not allow for
  it.
- =dia.identity_attribute=: Attribute that provides this entity its
  identity.
- =is_final=: If true, the type cannot be inherited from.

*** The middle-end

We store the domain model internally as SML - a /meta-model/ largely
based on Domain Driven Design. A meta-model is simply a model whose
sole purpose is to describe other models. SML is designed to capture
all the details of the domain model that are required for code
generation. SML is not designed for anything else, so it is very terse
and not a particularly obvious model.

*** The back-ends

We need to express the meta-model as code. That is, we need to
generate a /representation/ of the different "parts" the domain type,
according to the rules of some well-known /grammar/: that is, it must
obey to a set of rules defined somewhere. Typical grammars are
programming languages such as C++ or SQL, but they can also be more
esoteric such as a Dia diagram; it uses the Dia XML grammar.

A /representation/ in this context is understood to be a physical
expression of a domain type - e.g. as zeroes and ones stored in a file
somewhere.

Representations have two related concepts: facets and aspects. These
are best explained by way of an example. The most fundamental facet is
the @e types facet. This is the class definition itself. A facet is
made up of @e aspects - for example in C++ there is a header file and
an implementation file. However, an aspect need not map directly to a
file - its perfectly possible to have more than one aspect in a file.

* PART II - PRACTICE

In this part we describe how to build and install Dogen, and how to
use it effectively - from very simple use cases all the way to the
more complex setups. We also explain how Dogen can be integrated with
a build system, how to manage the multitude of diagrams that soon get
created and many other such practical aspects.

** Obtaining Dogen

There are two ways of obtaining Dogen: you can either install one of
the available binary packages or compile it yourself from source.

*** Installing Dogen Using the Binary Packages

Dogen uses Continuous Integration (CI) and Trunk Development. We use
CDash for CI. In practice, this means that it should always be safe
(and preferable) to install the most recent packages available.

You can monitor the build status [[http://hedgr.co.uk/cdash/index.php?project%3Ddogen][here]]. When the build is green, latest
is always greatest; when the build is not green, it is our top
priority to make it green again.

We have build agents for the following Operative Systems:

- Linux: 32-bit and 64-bit with Clang and GCC.
- Mac OS X: 64-bit with GCC.
- Windows: 32-bit using MinGW (GCC for Windows).

The generated packages are named after the build agents, and contain
the Operative System name and bitness (e.g. 64-bit or 32-bit) in their
names.

#begin_quote
IMPORTANT: Installable packages generated off of CI used to be available at
github [[https://github.com/DomainDrivenConsulting/dogen/downloads][here]], but since they decommissioned the downloads section, we
found no place to upload them to. So, at present, there is no way of
downloading the packages generated by the build agents. We are trying
to find a new location to upload the packages to.
#end_quote

*** Building Dogen from Source

We officially support Linux, Mac OS X and Win32 since we have build
agents for these platforms. However, any platform that meets the
dependencies below should be able to build Dogen.

**** Dependencies

In order to compile Dogen you need:

- a fairly recent version of [[http://gcc.gnu.org/][GCC]] (> [[http://gcc.gnu.org/gcc-4.7/][4.7]]) or [[http://clang.llvm.org/index.html][Clang]] (> [[http://llvm.org/releases/3.0/docs/ClangReleaseNotes.html][3.0]]) or any
  compiler with good C++-11 support;
- [[http://www.cmake.org/][CMake]] [[http://www.kitware.com/news/home/browse/CMake?2013_05_22&CMake%2B2.8.11%2BNow%2BAvailable][2.8]] or later;
- Boost [[http://www.boost.org/users/history/version_1_55_0.html][1.55]];
- for portable serialisation, you need [[http://epa.codeplex.com/][EOS]] support (optional);
- for relational database support you need [[http://www.codesynthesis.com/products/odb/][ODB]] support (optional);

**** Building Instructions

Once all dependencies have been installed, and placed in the
appropriate =INCLUDE= and =LIB= paths (or the equivalent for your
operative system), follow the following steps:

: git clone git://github.com/kitanda/dogen.git
: mkdir output
: cd output
: cmake ../dogen -G "Unix Makefiles"
: make -j5 # number of cores available

The dogen binary will be in =output/stage/bin/dogen_driver=.

If you are on a non-Unix platform you need to use the appropriate
CMake generator (the =-G= parameter above). At present the Ninja
generator is known not to work. No other generator has been used by
the Dogen team.

Once the build has completed successfully, you should run the unit
tests to make sure your system is fully supported.

**** Running Unit Tests

In order to ensure your platform is properly supported by Dogen, you
should run the test suite and ensure that all tests pass.

If you have setup ODB support, you will need to do the following steps
first:

- install and configure [[http://www.postgresql.org/][PostgreSQL]];
- [[https://kb.mediatemple.net/questions/1237/How%2Bdo%2BI%2Benable%2Bremote%2Baccess%2Bto%2Bmy%2BPostgreSQL%2Bserver%253F#dv][configure]] access to local and remote users;
- create a database called =musseque= and a user called =build= with a
  password of your choice;
- create a =.pgpass= file as described in [[http://wiki.postgresql.org/wiki/Pgpass][here]] (more details in the
  Postgres manual, section [[http://www.postgresql.org/docs/current/static/libpq-pgpass.html][The Password File]]). Test access to the
  database before proceeding.

Finally, run:

: make run_all_specs

If there are no failures, you are good to go. If there are failures,
you should report them to help improve Dogen.

**** Submitting bug reports

If you have a failure building Dogen or running its unit tests, please
submit a bug report that includes:

- the error messages;
- the compiler version;
- the Operative System.

If you find a bug whilst using Dogen, please send the log file as
well; it is located under the directory where you executed Dogen and
named =dogen.log=.

Bugs can be submitted using [[https://github.com/kitanda/dogen/issues][github Issues]].

** Running Dogen

Once you got access to Dogen, either by installing it or building it,
the next logical step is to try to use it. This section provides an
overview of common use cases. Note that Dogen is a command line tool,
and as such there is a presumption that the user has at least a
rudimentary knowledge of the shell of his or her operative system.

This section is dedicated to understanding the command line tool,
rather than the code it generates or the diagrams it receives as an
input; latter sections will deal with these topics exclusively.

*** Validating the Setup

The first thing one should do is to make sure Dogen is operational. To
do so, run:

: $ dogen_driver --version

If you are running it from the build directory =stage/bin= and on
UNIX, you may need to refer to the current directory:

: $ ./dogen_driver --version

Alternatively, you may find yourself in a sub-directory of the build
directory; in that case you should use a relative path to the binary:

: $ ../dogen_driver --version

If you are in any of these cases, from now on you will have to add the
required relative path to all of the following examples - e.g. =./=,
=../=, etc. Note that you *should not* try to copy the binary around,
as it must be setup properly in order to work; this is done by the
build system for both binary packages and builds. You should always
use relative paths if the binary is not on the path.

If all is well, you should see something along the lines of:

: dogen v0.0.2233
: Copyright (C) 2012 Kitanda.
: License: GPLv3 - GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.

The first line indicates the version of Dogen, the last digit of which
is the number of commits done in master. To know how recent this
version is, go to the [[https://github.com/DomainDrivenConsulting/dogen][project page]] in GitHub and look up the number of
commits there. Ideally you want the most recent version.

Now that we have confirmed Dogen is operational, lets have a look at
all the available options. Run:

: $ dogen_driver --help

A text similar to the below will come up:

: Dogen - the domain generator.
: Generates domain objects from a Dia diagram.
: 
: 
: General options:
:  -h [ --help ]         Display this help and exit.
:  --version             Output version information and exit.
: ...

We will cover all of these options in more detail later, but for now
it suffices to say that command line options belong to option groups,
which attempt to aggregate related functionality. For instance,
/General options/ are those that are not directly related to
operational aspects, but provide information about the application. We
have already seen both =help= and =version=, the most important of
this group.

At this point we now know our Dogen setup is operational so lets make
use of it.

*** Generating Hello World

Before we can generate any code, we need a model. It also helps if we
keep all files isolated so we know what Dogen has been up to. We will
meet both of these conditions by placing ourselves in a new directory
and copying across the "Hello World" model from the Dogen git
repository:

: mkdir dogen_examples
: cd dogen_examples
: mkdir source
: cd source
: wget FILE
FIXME - path to dia file

The source directory is created so we can separate our source code
from the build files, as you'll see in a moment. All that is left is
to code generate:

: $ dogen_driver --target hello_world.dia --cpp-enable-facet domain

We use the =--target= command line option to tell Dogen about the
"Hello World" model. The target file must be a UML Dia diagram,
crafted according to the rules stipulated by Dogen. Notice that the
diagram's file name does not contain any spaces, camel case, and so
on. This is important because it will be used as the name of the
project and as the top-level namespace, so it must be valid as a C++
identifier and it should follow the conventions of the other C++
identifiers.

The second point of note is the =--cpp-enable-facet= command line
option. It ensures that only the select /facets/ are on. A facet is
just Dogen-speak for a logically distinct portion of code generation,
which can be switched on or off independently of other such
portions. In this particular case we asked for the core facet =domain=
to be on - it makes no sense to code generate otherwise, really. All
other facets are thus switched off, so there are no requirements for
third-party libraries. This is done because it is possible you do not
have a development environment set up with all of the third-party
libraries and tools that are supported by Dogen by default, such as
EOS, ODB and so on.

After generation, your directory should look like so:

: ls -l
: total 24
: drwxr-xr-x 4 marco marco 4096 Mar 13 07:56 hello_world
: -rw-r--r-- 1 marco marco 7315 Mar 12 18:51 hello_world.dia
: drwxr-xr-x 2 marco marco 4096 Mar 12 18:51 log

The =log= directory is where the log file is stored; it is named
=dogen.log=. By default Dogen is not particularly expressive, so there
won't be much in the log file to look at. If you wish to increase the
verbosity of the logging, you can do so using =--verbose=:

: $ dogen_driver --target hello_world.dia --cpp-enable-facet domain --verbose

See the Advanced Command Line Options section for more details on
=--verbose=.

The other directory of interest is =hello_world=. This is where the
generated C++ code is stored. To understand the meaning and the
rationale of the directory structure you should read sections [[*File%20and%20Directory%20Standards][File and
Directory Standards]] and also [[*Physical%20Layout][Physical Layout]].

For now we'll just have a quick peek at one of the generated files,
the class =hello_world= itself:

: $ grep -e class -B5 -A5  hello_world/include/hello_world/types/hello_world.hpp
: namespace hello_world {
: /**
:  * @brief Welcome to Dogen!
:  *
:  * This is one of the simplest models you can generate, a single class with one
:  * property. You can see the use of comments at the class level and property
:  * level.
:  */
: class hello_world final {
: public:
:    hello_world() = default;
:    hello_world(const hello_world&) = default;
:    hello_world(hello_world&&) = default;
:    ~hello_world() = default;

As you can see, a C++ 11 class was generated. At this point it is
recommended you look at the =hello_world.dia= using Dia, and the
generated sources using your preferred text editor.

*** Supporting Infrastructure

In order to compile the generated code, we need two additional bits of
infrastructure: a CMake file and a main.

Dogen models are designed to be integrated with an existing CMake
build, so we have to generate a minimal =CMakeLists.txt=. Something as
simple as this would do:

: cmake_minimum_required(VERSION 2.8 FATAL_ERROR) # 2.6 should work too
: project(hello_world)
: set(CMAKE_CXX_FLAGS "-std=c++11") # Dogen requires C++ 11 or greater
: include_directories(${CMAKE_SOURCE_DIR}/hello_world/include)
: add_subdirectory(${CMAKE_SOURCE_DIR}/hello_world)
: add_executable(main main.cpp)
: target_link_libraries(main hello_world)

Take the above code and slap it on a =CMakeLists.txt= in your =source=
directory; granted, you could get much fancier, but this suffices for
the purposes of our minimalist example. The contents of the file
shouldn't be that surprising, unless you are unfamiliar with CMake. If
that is the case, I'm rather afraid that an introduction to CMake is
outside of the scope of this manual. On the plus side, there are
plenty of good articles on the subject.

We also need to create a basic =main.cpp= to make use of the genrate
code. It is equally straightforward - a few lines over the traditional
C++ "Hello World":

: #include <iostream>
: #include "hello_world/types/one_property.hpp"
: 
: int main() {
:     hello_world::one_property op("hello world!");
:     std::cout << op.property() << std::endl;
:     return 0;
: }

We are making use of the full constructor that the =domain= makes
available; because the property is of type =std::string= we can stream
it directly into the console.

At this point in time, your directory should look roughly like this:

: $ ls -l
: total 24
: -rw-r--r-- 1 marco marco  284 Mar 13 18:14 CMakeLists.txt
: drwxr-xr-x 4 marco marco 4096 Mar 13 18:34 hello_world
: -rw-r--r-- 1 marco marco 7316 Mar 13 18:19 hello_world.dia
: drwxr-xr-x 2 marco marco 4096 Mar 13 18:20 log
: -rw-r--r-- 1 marco marco  230 Mar 13 18:55 main.cpp

It is time to compile.

*** Compiling and Running

The compilation steps are fairly simple. We need to create a folder to
house the build paraphernalia, to avoid getting it all mixed with the
source code. There we shall build and run our main. The following
achieves that (assuming you are currently in =source=):

: $ cd ..
: $ mkdir output
: $ cd output/
: $ cmake ../source
: <lots of cmake output>
: $ make
: <lots of make output>
: $ ./main
: hello world!

And with that, we have built and instantiated our simple Dogen model.

*** Version Controlling the Models

We strongly recommend you store all of the code generated by Dogen in
the version control system (VCS) of your choice. This may sound
counter-intuitive at first. After all, you wouldn't want to store
Protocol Buffers code in version control, or the output of an IDL
compiler. However, the same logic doesn't /quite/ apply to Dogen. As
you will see later, we strive to allow intermixing of manually crafted
code with generated code, and we also want the generated code to look
as if it was generated by humans; granted, some rather boring,
robot-like humans, but still. Finally, we want you to actively
distrust Dogen - every time you code generate, you should inspect the
output and make sure it looks exactly the way you want it to look. The
best way to do that is to validate diffs. At any rate, if none of
these arguments convince you, please suspend disbelief for a second
and humour us in thinking that the rightful place of the code
generated by Dogen is in version control.

Git is our preferred VCS - it is, in fact, a distributed VCS, so DVCS
would be the right term, but it's distributed nature is not relevant
for the current argument. Anyway, we shall use git to demonstrate how
VCS in general can be used to /see/ what Dogen is up to. If
=${VCS_OF_CHOICE}= is not git, feel free to do the equivalent commands
in =${VCS_OF_CHOICE}= instead.

To start off with, we need to initialise a repository in our source
folder:

: $ git init .
: <git output>
: $ echo log > .gitignore
: $ git add -A
: $ git commit -m "initial import"
: [master (root-commit) a6b706a] initial import
:  13 files changed, 581 insertions(+)
:  create mode 100644 .gitignore
:  create mode 100644 CMakeLists.txt
:  create mode 100644 hello_world.dia
:  create mode 100644 hello_world/CMakeLists.txt
:  create mode 100644 hello_world/include/hello_world/types/all.hpp
:  create mode 100644 hello_world/include/hello_world/types/one_property.hpp
:  create mode 100644 hello_world/include/hello_world/types/one_property_fwd.hpp
:  create mode 100644 hello_world/src/CMakeLists.txt
:  create mode 100644 hello_world/src/types/one_property.cpp
:  create mode 100644 main.cpp

Now that we have committed our changes, we can use =git diff= and =git
status= to see the results of all Dogen commands. For example, lets
say we decide to add more comments to the class using Dia. After
saving, git tells us the following:

: $ git diff
: diff --git a/hello_world.dia b/hello_world.dia
: index dfbb93b..46074d7 100644
: --- a/hello_world.dia
: +++ b/hello_world.dia
: @@ -182,7 +182,9 @@ level.#</dia:string>
:              <dia:string>##</dia:string>
:            </dia:attribute>
:            <dia:attribute name="comment">
: -            <dia:string>#This is a sample property.#</dia:string>
: +            <dia:string>#This is a sample property.
: +
: +This is an additional comment.#</dia:string>
:            </dia:attribute>
:            <dia:attribute name="visibility">
:              <dia:enum val="0"/>

Because we chose to save the diagram in text format, its very easy to
see what the changes are. We can now code generate, very much the same
way as we did before:

: $ dogen_driver --target hello_world.dia --cpp-enable-facet domain

Other than the diagram file itself, one would expect to see exactly
one modified file; and for that file to be =one_property.hpp=. And
this is what =git status= tells us:

: $ git status
: On branch master
: Changes not staged for commit:
:   (use "git add <file>..." to update what will be committed)
:   (use "git checkout -- <file>..." to discard changes in working directory)
: 
:  modified:   hello_world.dia
:  modified:   hello_world/include/hello_world/types/one_property.hpp
: 
: no changes added to commit (use "git add" and/or "git commit -a")

But are these the expected changes? Again, =git diff= comes to the
rescue:

: $ git diff hello_world/include/hello_world/types/one_property.hpp
: diff --git a/hello_world/include/hello_world/types/one_property.hpp b/hello_world/include/hello_world/types/one_property.hpp
: index 1759275..b0759ec 100644
: --- a/hello_world/include/hello_world/types/one_property.hpp
: +++ b/hello_world/include/hello_world/types/one_property.hpp
: @@ -50,6 +50,8 @@ public:
:  public:
:      /**
:       * @brief This is a sample property.
: +     *
: +     * This is an additional comment.
:       */
:      /**@{*/
:      const std::string& property() const;

As you can see, Dogen did exactly the modifications we expected it to
do and no more than those, and git provided us with a quick and
deterministic way of validating that.

Just for good measure, we'll commit these changes:

: $ git add -A
: $ git commit -m "add comment to property"

Now we're ready to start working on the next set of changes. Two key
points emerge from here:

- VCS are really useful to keep up with what Dogen is doing. But in
  order for it to work, you should save your diagrams in Dia as plain
  text rather than in compressed form.
- you should commit early and commit often, probably even more so than
  what you are used to. A very large diff is hard to parse,
  particularly when we start mixing generated code with non-generated
  code. We tend to do a large number of local commits and then do a
  single large push to =origin= to trigger builds in the Continuous
  Integration.

*** Integrating Dogen with the Build

You will soon tire of running the same Dogen commands every time you
want to change your model. The easiest thing is to integrate it with
the build system, so that you have a target for code generation. This
can easily be accomplished with CMake. In your top-level
=CMakeLists.txt=, add the following at the end:

: add_custom_target(codegen_hello_world
:     COMMENT "Generating Hello World model" VERBATIM
:     WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}
:     COMMAND ../../dogen_driver
:     --target ${CMAKE_SOURCE_DIR}/hello_world.dia
:     --cpp-enable-facet domain)

We called the target =codegen_hello_world= but it can be named
whatever you choose. To avoid any confusion, we should check these
changes in:

: git add -A
: git commit -m "add target for code generation"

Now, in your output directory you can simply do:

: $ cmake ../source # just in case, shouldn't be necessary
: <cmake output>
: $ make codegen_hello_world
: Scanning dependencies of target codegen_hello_world
: [100%] Generating Hello World model
: [100%] Built target codegen_hello_world

When you go back to your source directory, git status should show you
the following:

: On branch master
: nothing to commit, working directory clean

As expected, no changes were done. But how do we know the code
generator actually executed at all? This is where the log file comes
in handy:

: $ date
: Fri 14 Mar 08:32:33 GMT 2014
: $ tail -n 1 log/dogen.log
: 2014-03-14 08:32:29.763230 [INFO] [engine.workflow] Workflow finished.

As you can see, the timestamp of the last thing Dogen wrote to the log
is very close to now, so we know it executed the code generation.  As
there was nothing to change, nothing was changed.

The more advanced CMake - and make users in general - may, at this
juncture, be tempted to add a dependency between the diagram and code
generation. In such a setup, if the Dia diagram has been modified, a
code generation would take place when you build. From experience, we
do not recommend this approach. This sounds like a great idea in
theory, but in practice it actually doesn't work that well. When you
start using Dogen in anger, you will find yourself many a time with
"work-in-progress" changes; you will be speculating with the design
for quite a bit until it makes sense. At the same time, you or other
team members may also be doing unrelated code changes. This will put
in a bind: either you don't check-in the diagram changes, or your
create a branch for them (which is not always a bad idea, to be fair)
or you check them in and break everyone else's code.

The other reason why this is a bad idea is that if someone checked in
a diagram but forgot to run the code generator, the build machine
could break in mysterious ways. The code that is building is not the
code that was checked in, and this can result in a lot of wasted time
investigating strange issues.

In conclusion, its better to code generate and check in manually, as
and when you are ready to do so, and to make sure the build machine is
as dumb as possible.

*** Deleting Extra Files

As you start adding and removing classes from your diagram, you may
find that Dogen starts leaving a lot of artefacts behind. You may even
conclude that the best way is to manually delete the code generation
directory before code generation to ensure you're in a good state. In
fact, there is a better way of handling this situation.

Let's imagine a fairly simple but common use case: you just added a
brand new class to your model - =two_properties= say - and you code
generated it. It all looks fine from git:

: $ git status
: On branch master
: Changes not staged for commit:
:   (use "git add <file>..." to update what will be committed)
:   (use "git checkout -- <file>..." to discard changes in working directory)
: 
:   modified:   hello_world.dia
:   modified:   hello_world/include/hello_world/types/all.hpp
: 
: Untracked files:
:   (use "git add <file>..." to include in what will be committed)
: 
:   hello_world/include/hello_world/types/two_properties.hpp
:   hello_world/include/hello_world/types/two_properties_fwd.hpp
:   hello_world/src/types/two_properties.cpp
: no changes added to commit (use "git add" and/or "git commit -a")

Alas, after much soul searching you decide that =two_properties= was a
mistake: it doesn't reflect the domain you intend to model at all. So
you remove it from the diagram. What is Dogen to do? Well, lets look
at the git output after we removed the new class:

: $ git status
: On branch master
: Untracked files:
:   (use "git add <file>..." to include in what will be committed)
: 
:   hello_world/include/hello_world/types/two_properties.hpp
:   hello_world/include/hello_world/types/two_properties_fwd.hpp
:   hello_world/src/types/two_properties.cpp
: 
: nothing added to commit but untracked files present (use "git add" to track)

Dogen got rid of all the changes to the /existing/ files, but left the
new files lying around! This is because Dogen does not consider these
files to be its responsibility any longer; after all, there is no
matching class that "owns" them in the diagram, so they are totally
ignored. This may not be the ideal behaviour - after all you wanted to
get rid of the class altogether. To do so you need to instruct Dogen
to delete all files that it thinks are "unnecessary". This can be done
via the =--delete-extra-files= option. We can add it to the top-level
=CMakeLists.txt= like so:

: $ git diff CMakeLists.txt
: diff --git a/CMakeLists.txt b/CMakeLists.txt
: index 0f6e9c3..2db53b8 100644
: --- a/CMakeLists.txt
: +++ b/CMakeLists.txt
: @@ -12,4 +12,5 @@ add_custom_target(codegen_hello_world
:      COMMAND ../../dogen_driver
:      --target ${CMAKE_SOURCE_DIR}/hello_world.dia
: -    --cpp-enable-facet domain)
: +    --cpp-enable-facet domain
: +    --delete-extra-files)

As usual we'll commit this change:

: $ git add -A
: $ git commit -m "add delete extra files"
: <git output>

When we code generate again, the result is quite different:

: $ cd ../output
: $ make codegen_hello_world
: -- Configuring done
: -- Generating done
: -- Build files have been written to: YOUR_PATH/dogen_examples/output
: [100%] Generating Hello World model
: [100%] Built target codegen_hello_world
: $ cd ../source/
: $ git status
: On branch master
: nothing to commit, working directory clean

Dogen has now deleted all the files we're no longer interested in.

*** Ignoring Extra Files

It is not always appropriate to delete /all/ files that Dogen knows
nothing of. Imagine a second use case: you decide to manually create a
file with a stand alone function =my_function.cpp=. This file needs to
be part of the model, but it cannot be code generated by Dogen. If you
attempt to use =delete-extra-files=, this file would be removed by
Dogen as the following example shows. First we'll create the file and
commit it:

: echo "void my_function() { }" > hello_world/my_function.cpp
: $ git add -A
: $ git commit -m "add my function"
: [master 9fbc00a] add my function
: 1 file changed, 1 insertion(+)
: create mode 100644 hello_world/my_function.cpp

Then we'll code generate and check git:

: $ make codegen_hello_world
: [100%] Generating Hello World model
: [100%] Built target codegen_hello_world
: $ cd ../source/
: $ git status
: On branch master
: Changes not staged for commit:
:  (use "git add/rm <file>..." to update what will be committed)
:  (use "git checkout -- <file>..." to discard changes in working directory)
: 
: deleted:    hello_world/my_function.cpp
: 
: no changes added to commit (use "git add" and/or "git commit -a")

Again you can see the usefulness of committing early and often:
instead of losing all our work, all we need to do is to checkout the
file to restore it:

: git checkout hello_world/my_function.cpp

Our file got deleted as it is an "extra" file as far as Dogen is
concerned. The simplest way to avoid this is to use the command
=--ignore-files-matching-regex=. We can add it to the CMake file like
so:

: $ git diff CMakeLists.txt
: diff --git a/CMakeLists.txt b/CMakeLists.txt
: index ae3c12c..518f99c 100644
: --- a/CMakeLists.txt
: +++ b/CMakeLists.txt
: @@ -12,4 +12,5 @@ add_custom_target(codegen_hello_world
:      COMMAND ../../dogen_driver
:      --target ${CMAKE_SOURCE_DIR}/hello_world.dia
:      --cpp-enable-facet domain
: -    --delete-extra-files)
: +    --delete-extra-files
: +    --ignore-files-matching-regex .*/my_function.*)

If we repeat the code generation steps again, the result is a bit more
sensible:

: $ cmake ../source/ # should't really be necessary
: -- Configuring done
: -- Generating done
: -- Build files have been written to: YOUR_PATH/dogen_examples/output
: $ make codegen_hello_world
: [100%] Generating Hello World model
: [100%] Built target codegen_hello_world
: $ cd ../source/
: $ git status
: On branch master
: Changes not staged for commit:
:   (use "git add <file>..." to update what will be committed)
:   (use "git checkout -- <file>..." to discard changes in working directory)
: 
: modified:   CMakeLists.txt
: 
: no changes added to commit (use "git add" and/or "git commit -a")

The file was not deleted this time round.

This is not the only way to ignore files as we shall see, but its a
quick way of doing so, and is particularly suitable for files which
do not have a clear representation in the model. For example, this is
a good solution for adding unit tests to a model:

: --ignore-files-matching-regex .*/test/.*

This would ignore all files in a directory called test. The regular
expressions can be as complex as desired, as they internally use C++
11's regular expression library.

*** Referring to Other Models

Soon in your modeling life you will outgrow a single model - e.g. a
single Dia diagram. This could happen for many reasons: perhaps a
model is becoming too crowded and there are so many classes it has
lost its cohesiveness; or there is an obvious logical split between
two sets of classes, and just does not make sense to keep them in the
same model.

As soon as there several models, its highly likely that relationships
between models will emerge: model A will make use of model B and C,
and so on. Dogen supports this use case via the command line option
=--reference=. You can have as many instances of this option as there
are dependencies for the target model you are building. For example,
lets say create a second model which uses the class we defined in the
"Hello World" model; to generate this model one would invoke Dogen as
follows:

: $ dogen_driver --target hello_references.dia --reference hello_world.dia \
:   --cpp-enable-facet domain

"Hello References" can now make use of all the types available in
"Hello World", provided they are qualified with the source model:
=hello_world::one_property= in this particular case.

*** The External Module Path
    
It is possible to define modules inside a models to aggregate related
sub-functionality. As we shall see later on, these are defined as UML
packages in Dia and get translated into namespaces at the C++ code
generation level. However, sometimes there are top-level modules for
which a UML representation would be counterproductive. A common use
case is when the models all belong to some umbrella project, which may
have one or more top-level namespaces common to all models. For
instance, in Dogen, all models are inside the =dogen= namespace; it
really adds no value to create a UML package in every model under
these circumstances.

This is where the =--external-module-path= command line option comes
in handy. This is a way to inject information directly into SML which
is not obtained via the UML diagram. You can provide as many modules
as required, separated by =::=. For example:

: $ dogen_driver --target hello_references.dia --external-module-path a::b::c \
:   --cpp-enable-facet domain

All types in "Hello World" would now be encased inside of namespaces
=a=, =b= and =c=. Note that the external module path does not affect
references in diagrams: we should still refer to the types /without/
it. However, the =--reference= parameter must then be augmented with
it so that Dogen places the types in the correct modules:

: $ dogen_driver --target hello_references.dia \
: --reference hello_world.dia,a::b::c \
:   --cpp-enable-facet domain

Notice the comma followed by the external package path in the "Hello
World" reference. It ensures that the code generated for "Hello
References" makes use of the fully-qualified names when referring to
"Hello World" types, even though in the diagram they are partially
qualified - e.g. =hello_world::one_property=. Without this the
generated code would not compile.

*** Disabling the Model Module

On very rare cases, it may be required that the types of the model are
not placed inside of a namespace with the model name. We do not
particularly like this use case, and are likely to make it obsolete
unless we find good reasons not to do so, but it is available at
present.

The =--disable-model-module= command line option is used to trigger
this functionality.

*** Advanced Command Line Options

There are a number of options that are backend specific and which we
shall cover later, in a section dedicated to exploring the available
facets for each backend. The remaining command line options tend to be
useful for troubleshooting, and these are the ones we shall cover in
this section.

These options are really meant to be used by advanced Dogen users, but
its good to know they exist because you may need to provide
information generated by them in order to help troubleshoot problems
with your model.

As explained, Dogen generates a log file under =log/dogen.log= when
operating. This directory is always generated in the current
directory. You can control the verbosity of the log file with the
=--verbose= option, but be warned that in verbose mode log files will
grow dramatically in size. This mode is meant mainly for Dogen
developers, but it is also an instructive way to learn about the
application because it provides such a high-level of detail of its
inner-working.

You are required to do this when you attach the
log file to any bug report you submit. For the moment, we'll just
ignore the log, but it will come in handy later on.


Other useful options that can be used in conjunction with =--verbose=
are =--stop-after-merging= and =--stop-after-formatting=. If there is
a problem with a particular part of Dogen it may not make sense to run
a complete code generation every time whilst investigating it, as it
would just increase investigation time with no benefit. This is where
these two options come in: they force Dogen to halt at different
stages of its processing pipeline. On the first case, Dogen builds a
combined model and validates all dependencies and then stops; on the
second case, Dogen does everything except outputting files.

It is also possible to dump some of the intermediate state into file:
this can be achieved using =--save-dia-model= and
=--save-sml-model=. The first saves a processed representation of the
dia models loaded, and the second saves their equivalent in SML. These
files can be dumped into a directory of choice via =--debug-dir=. By
default they come out in the current directory.

** The C++ Facets

In this section we do a high-level inspection of all the services
provided by Dogen via its facets.

*** Types
*** Streaming
*** Hashing
*** Serialisation
*** Test Data
*** ODB
*** C++ backend options

  --cpp-disable-backend                 Do not generate C++ code.
  --cpp-disable-complete-constructor    Do not generate a constructor taking as
                                        arguments all member variables
  --cpp-disable-cmakelists              Do not generate 'CMakeLists.txt' for
                                        C++.
  -y [ --cpp-split-project ]            Split the model project into a source
                                        and include directory, with
                                        individually configurable locations.
  -x [ --cpp-project-dir ] arg          Output directory for all project files.
                                        Defaults to '.'Cannot be used with
                                        --cpp-split-project
  -s [ --cpp-source-dir ] arg           Output directory for C++ source files.
                                        Defaults to '.'Can only be used with
                                        --cpp-split-project.If supplied,
                                        include directory must be supplied too.
  -i [ --cpp-include-dir ] arg          Output directory for C++ include files.
                                        Defaults to '.'Can only be used with
                                        --cpp-split-project.If supplied, source
                                        directory must be supplied too.
  --cpp-enable-facet arg                If set, only domain and enabled facets
                                        are generated. By default all facets
                                        are generated. Valid values: [io | hash
                                        | serialization | test_data | odb].
  --cpp-header-extension arg (=.hpp)    Extension for C++ header files,
                                        including leading '.'.
  --cpp-source-extension arg (=.cpp)    Extension for C++ source files,
                                        including leading '.'.
  --cpp-disable-facet-includers         Do not create a global header file that
                                        includes all header files in that
                                        facet.
  --cpp-disable-facet-folders           Do not create sub-folders for facets.
  --cpp-disable-unique-file-names       Do not make file names unique. Defaults
                                        to true. Must be true if not generating
                                        facet folders.
  --cpp-domain-facet-folder arg (=types)
                                        Name for the domain facet folder.
  --cpp-hash-facet-folder arg (=hash)   Name for the hash facet folder.
  --cpp-io-facet-folder arg (=io)       Name for the io facet folder.
  --cpp-serialization-facet-folder arg (=serialization)
                                        Name for the serialization facet
                                        folder.
  --cpp-test-data-facet-folder arg (=test_data)
                                        Name for the test data facet folder.
  --cpp-odb-facet-folder arg (=odb)     Name for the ODB facet folder.
  --cpp-disable-xml-serialization       Do not add NVP macros to boost
                                        serialization code. This is used to
                                        support boost XML archives.
  --cpp-disable-eos-serialization       Do not add EOS serialisation support to
                                        boost serialization code.
  --cpp-use-integrated-io               Add inserters directly to domain facet
                                        rather than using IO facet.
  --cpp-disable-versioning              Do not generate entity versioning code
                                        for domain types.

[marco@erdos bin]$

** Authoring Diagrams in Dia

*** Properties
*** Comments
*** Packages
*** Association
*** Inheritance
*** Enumerations
*** System Models
**** Standard C++ System Model
**** Boost System Model
** Frequently Asked Questions

*Q*: When I tried running Dogen I get the following error message:

: Error: File not found: Could not find data directory.
: Base directory: /a/full/path. Locations searched: ./data ../data ../share/data
: ../share/dogen-0.0.2233 ../share/dogen . See the log file for details.

*A*: Your installation of Dogen is faulty. This could happen for
example if you copy the Dogen binary from a build directory into
another location without copying all the associated
infrastructure. The correct way is to run the binary using a relative
path to the build directory: =../../dogen_driver=.

This error should not occur if you are using a binary package as the
binary should be in the system path.

* PART III - SPECIFICATIONS

This part is made up of a set of specifications on different aspects
of Dogen, such as project structure, coding standards, and so on. The
objective is to create norms as we go along so that new developers
understand the reasons behind historical decisions. These norms are
not set in stone, of course. They are expected to change whenever the
rationale behind them no longer applies, or if there are better
options available - options that, for whatever reason, were not
considered originally.

Its important to notice that these specifications will never be
complete - in the sense that we will never cover all aspects of the
development of Dogen. There will always be things that will remain
unspecified. We shall try to at least cover those we consider more
important. But the document is organic, and will be constantly
evolving over time.

The final reason behind the creation of these standards - and their
inclusion in the main application manual - is that Dogen is a /code
generator/; that is, we are making a large number of decisions about
code that other people will use and in a very real sense, will be
stuck with. Due to this we think there is a need to explain to users
the reasons behind these choices.

** RFC 2119

The definitions in RFC 2119 apply to the text of this part. In
particular:

#+begin_quote
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
#+end_quote

** File and Directory Standards

In this section we specify aspects related to the layout of Dogen in
version control, folder structure, file names and so on. The words
"folder" and "directory" are used interchangeably.

*** Content type

1. All files added to the git repository must be in plain
   text. /Rationale/: most of the tools we use produce textual
   representations of their data files, and they should be preferred
   whenever possible. /Exceptions/: Bitmap images and other content
   type which has only binary representation is acceptable, but must
   be used only as last resort.

*** Naming

1. All files and folders must have lower-case names. /Rationale/:
   avoid as much as possible case-sensitive issues in platforms where
   casing is not that well designed, such as Windows. /Exceptions:/
   Files or folders that follow well-known naming conventions take
   precedence, such as the GNU files (e.g. =README=, =INSTALL=, etc.)
   and cmake files (=CMakeLists.txt=).

2. The dot character =.= shall only be used to separate the file name
   from its extension (e.g. =file.txt=). /Rationale/: For some reason,
   dots in the middle of the name seem to confuse cmake and force
   rebuilding targets that are up-to-date.

3. Multi-word file and directory names must make use of the underscore
   character =_= to separate words (e.g.: =folder_name=). /Rationale/:
   helps readability; since we can't use casing to separate words,
   this is our only option.

4. In cases where there is a need to distinguish between several
   multi-word components in a file name, the dash character =-= must
   be used (e.g. =some_word-some_other_word.extension=). /Rationale/:
   helps readability.

5. Names must contain only lowercase ASCII letters (=a-z=), numbers
   (=0-9=), underscores (=_=), hyphens (=-=), and periods
   (=.=). Spaces must not be used. /Rationale/: Spaces and other
   special characters cause cause obscure issues, both in Windows and
   on UNIX. These problems are often very hard to root-cause.

6. The first and last character of a file name must not be a period
   (=.=). /Rationale/: POSIX has special rules for names beginning
   with a period. Windows prohibits names ending in a period (source:
   [[http://www.boost.org/development/requirements.html][boost]]). /Exceptions:/ Control files such as =.gitignore=.

7. The first character of names must not be a hyphen =-=. /Rationale/:
   It would be too confusing or ambiguous in certain contexts (source:
   [[http://www.boost.org/development/requirements.html][boost]]).

8. The maximum length of directory and file names must be 31
   characters. /Rationale/: Long file names cause obscure issues.

9. The total path length must not exceed 207 characters. /Rationale/:
   Dictated by ISO 9660:1999 (source: [[http://www.boost.org/development/requirements.html][boost]]). In addition, many file
   systems don't really like paths longer than 255 characters, so its
   best to keep well below this limit.

10. Files have additional naming conventions that are dependent on the
    file type. See language specific policies for details.

*** Root folder

1. The top-level Dogen folder is known as the /root folder/. The
   remainder of this section deals with all the sub-folders of the
   root folder, as well as its key files. /Notes/: The root folder is
   the directory you create when you clone the project from git -
   i.e. it contains the =.git= folder. The folder should be named
   =dogen= (in lower-case).

2. The =build= folder contains all the scripts, configuration and
   utilities required to perform a build and to package it for
   end-user consumption. /Notes/: In general, =build= should contain
   either files used directly by the makefiles or files that get
   copied over to the build's output directory for further
   processing. Examples of artefacts contained in the =build=
   directory: extensions to cmake (both third party and our own),
   packaging scripts, ctest scripts for Continuous Integration,
   templates (code templates, configuration file templates, etc) and
   configuration for code-quality tools (valgrind, etc).

3. The =data= directory contains the data files used by Dogen at run
   time; these are expected to be part of an installation.

4. The =diagrams= directory contains all of the Dogen diagrams used to
   generate Dogen source code. /Notes/: At present it does not contain
   the test models. There is no good reason for this; a story has been
   added to the backlog to fix it.

5. The =doc= folder contains manually crafted documentation. /Notes/:
   The types of artefacts contained are manuals, illustrative diagrams
   (not for code generation purposes), project plans, screen-shots,
   etc.

6. The =images= folder contains all the graphical artefacts required
   by the project. /Notes/: Artefacts such as icons, logos etc should
   be housed here.

7. The =patches= directory contains ongoing work that is not yet ready
   to be committed. /Notes/: We use this directory as a poor-man's
   distributed branch. The reason why is that we often use extremely
   short-lived branches (e.g. less than 2 days) where the code is not
   yet ready to be pushed, but we may require access to it from
   multiple machines. It seems a bit heavy-weight to create and
   destroy remote branches for this case, so instead we save work down
   as patches. This directory has very short-lived files, and is
   expected to be empty most of the time.

8. The =projects= folder contains the Dogen source code, in any of the
   supported programming languages. It's sub-folders must be named
   after each individual project. /Rationale/: This directory could
   have been called =src= for source code, but we reserved this for
   the directory of the implementation files in C++. We settled on
   projects as it contains all of the projects that make up
   Dogen. /Notes/ The word "project" here is used in a rather vague
   sense, but it can be understood to mean anything which generates a
   shared library, static library or executable or any other such
   cohesive unit work. For example, a set of ruby scripts around a
   given theme (sanity) are a project. We freely mix projects which
   exist solely to test dogen with projects that provide end-user
   functionality.

9. The =sql= directory contains sql configuration files and other
   bits and pieces. /Notes/: we should either deprecate this directory
   or move it to projects.

10. The =test_data= directory contains all the files required to run
    the Dogen unit tests. /Notes/: This directory should really only
    contain the data portion of the expectations for the unit tests,
    but also contains input files.

*** Output folder

1. The folder under which the build is performed is called the
   /output/ folder. /Notes/: This is the location where all artefacts
   generated by the build are placed, such as binaries, shared
   objects, automatically generated documentation, instantiated
   templates, etc.

2. The output folder must not be the same folder as the root
   folder. /Rationale/: In-source (or in-tree) builds are not
   permitted, as they mix source code with build artefacts and can
   result in very messy situations, e.g.: a) complex ignore files that
   are never maintained properly b) when mistakes are made with ignore
   files, the checking in of build artefacts c) the deletion or
   modification of source control items by mistake and subsequent
   check-in. /Notes/: CMake files do not allow in-source builds.

3. The output folder is recommended to be located under the same
   parent directory as the root folder. /Notes/: The directory can be
   located anywhere else, but this is a convenient location.

4. The /output/ folder is recommended to be named =output= but there
   must be no dependencies on its name. /Rationale/: A well-known name
   increases interoperability with other developers.

5. Developers are recommended to partition the output folder by build
   type, storing there all variations of Dogen builds. /Rationale/
   Keeping all the builds within a single folder makes it easy to
   start from scratch by deleting the top-level folder. Whilst
   perfectly supported, multiple top-level output directories tend to
   become messy very quickly. /Notes/: Developers should create
   sub-directories for each supported platform, compiler, debug and
   release, etc - as required for their particular setup. For complex
   setups, one suggestion is to use [[http://wiki.debian.org/Multiarch/Tuples][GNU triplets]] - including the
   compiler version. Unfortunately, these directories must be setup
   manually because target folders contain a full-blown cmake
   environment, independent from the others. Thus, it is not possible
   to generate this setup directly from CMake. Example complex folder
   structure:

#+begin_example
output/linux-amd64-gcc-4_6_1-debug
output/win32-x86-clang-3_0-release
#+end_example

6. For each build, the sub-folder =stage= shall contain all the
   binaries, documentation and other artefacts that will be
   packaged. /Rationale/: "stage" was chosen as it implies some
   intermediate phase before the final packaging.

*** Third party content

1. The project's git repository is expected to only contain code owned
   by dogen; all the external dependencies must be installed by the
   user as a build prerequisite (see doc/BUILD for details).

2. In exceptional cases where the third party dependency is both small
   and not readily available in packaged form, it is acceptable to add
   it to the repository. This is the case with CMake extensions and
   with the boost portable serialisation library. Once these projects
   are packaged they shall be removed from the repository.

** C++ Coding Standards

In general, we follow the [[http://www.boost.org/development/requirements.html][Boost Library Requirements and Guidelines]]
document. In a few rare cases we may make choices that contradict it,
but we shall try our best to explain the rationale behind the
decisions.

These coding standards are not to be understood as "classic" coding
standards such as [[http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml][Google's]]. Rather, we intend to capture common
practice in our code base and explain the rationale behind it, so that
it is justified and the justification is not forgotten. Thus you may
find the standards somewhat sparse - silent in areas where other
standards are very verbose and vice-versa.

*** Physical Layout

The physical layout deals with files, directories, binaries and any
other artefacts that live in storage. We concern ourselves with only
those physical artefacts that are part of or directly related to the
C++ domain; the generic aspects of physical layout are dealt with in
[[File%20and%20Directory%20Standards][File and Directory Standards]].

1. For the purposes of this document, a /project/ is understood to be
   FIXME: look for the definition of solution and project in visual
   studio. A component is some logical partitioning of the
   project. Components manifest themselves as folders and namespaces
   within the project.

2. Header files must have the extension =hpp=. /Rationale/: this is as
   used by boost.

3. Implementation files must have the extension =cpp=. /Rationale/:
   this is as used by boost.

4. Files are named according to their content:

- files containing a /class/ shall be named after the class;
- files containing nothing but /namespace/ shall be named after the
  namespace, and should be used only to document the namespace;
- files containing one or more free-functions shall be named according
  to the logical module that these functions constitute
  (e.g. =utility=, =pricing=, etc).

5. Projects are composed of three top-level directories: =include=,
   =src= and =spec=. =include= shall house all the header files. =src=
   shall house all of the implementation files. =spec= contains all of
   the unit tests for the project. See [[http://skillsmatter.com/podcast/java-jee/kevlin-henney-rethinking-unit-testing-in-c-plus-plus][Specifications]] for details on
   the spec folder.

6. The =include= folder must have the following directory structure:
   =dogen/PROJECT_NAME/COMPONENT_NAME=. The =src= directory must have
   the following directory structure: =COMPONENT_NAME=. /Rationale/:
   by placing the =dogen/PROJECT_NAME= folder in the include directory
   we can lift it directly into a package installing the development
   header files. This is not required for the implementation files as
   they are private. /Examples/:
   =utility/include/utility/exception/...= and
   =utility/src/exception/...=.

7. All consumers of the API - internal or external - must put the
   =include= folders of each project in the compiler's include path,
   resulting in includes of the form
   =dogen/PROJECT_NAME/COMPONENT_NAME...=.

8. Projects have the following standard CMake files: the top-level
   file includes files in =src= and =spec=; The CMake files in =src=
   and =spec= are responsible for building these components.

10. The following are a set of well-defined component names that must
    be used only with this meaning. These are both the folder names
    and the namespace names:

    - *types*: houses the domain object definitions
      themselves. /Exceptions/: it is acceptable to rename this
      component where there is a more naturally fitting name.
    - *io*: streaming operators for domain types.
    - *serialization*: boost serialisation support for the domain
      types. /Rationale/: Please note the American spelling. This in
      keeping with the spelling in boost and avoids spurious spelling
      differences.
    - *odb*: database support for the domain types using ODB.
    - *hash*: standard C++-11 hash support for the domain types.
    - *test_data*: test data generators - produce pseudo-random
      instances of domain types.
    - *test*: support code needed by the unit tests such as mock
      factories. /Notes/: we should really merge this with test_data
      and find a good naming convention for the manual and code
      generated test data generators.

11. All binaries must have names of the form
    =dogen_PROJECT_NAME=. Specs must, in addition, have the post-fix
    =spec=. /Examples/: =libdogen_utility.a=, =libdogen_utility.so=,
    =dogen_utility_spec=, =dogen_utility_spec.exe=, =dogen_knitter=,
    etc. /Rationale/: this namespacing allows us to choose whatever
    names we desire without fears of clashes.

*** Specifications

1. All components in the system have a set of codified behaviours: the
   /component specification/.

2. In addition, the system itself may have additional constraints,
   which live in the /system specification/.

3.

 In a more general form:
   there are only two types of behaviour within the
system, specified or unspecified. Specified behaviour is that which is
covered by a spec; unspecified behaviour is that which is not.  With
regards to specs, components may chose to implement it in one of two
ways:

- by creating a sub-folder called /spec/; or
- by adding the spec classes directly to the main
  component folder.

The size of the spec should determine the approach.

*** Language
**** Preamble

All C++ files shall start with a descriptive text stating copyright
information and a pointer to the file where the complete licence
document can be located. This section is called the /preamble/. The
contents of the preamble are as follows:

#+begin_example
/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * Copyright (C) 2012 Kitanda
 *
 * This file is distributed under the Kitanda Proprietary Software
 * Licence. See doc/LICENCE.TXT for details.
 *
 */
#+end_example

The preamble is available as the [[file:infrastructure/emacs.org][yas snippet]] =preamble=.

**** Header guards

All header files shall have header guards after the [[Preamble][preamble]]. The name
of the guard shall be composed of the following structure:
=NAMESPACE_FILENAME_EXTENSION=. Each of these fields has the following
meaning:

- *NAMESPACE*: List of namespaces contained on that file, in upper
  case, separated by an underscore. Namespaces with multi-word names
  also use underscores as separators. Example:
  =NAMESPACE_ONE_NAMESPACE_TWO=;
- *FILENAME*: Name of the file, in upper case, separated by
  underscores when multi-word. Example: =THIS_IS_A_FILENAME=;
- *EXTENSION*: Always set to =HPP=.

The traditional header guard check is followed by the Microsoft one as
it has been proved in the boost mailing list that #pragma once
produces a speed boost.

Example of a complete header guard:

#+begin_example
#ifndef KITANDA_KITANDA_HPP
#define KITANDA_KITANDA_HPP

#if defined(_MSC_VER) && (_MSC_VER >= 1200)
#pragma once
#endif

...

#endif
#+end_example

Header guards can be generated in emacs via the [[file:infrastructure/emacs.org][yas snippet]] =once=.

**** Tabs vs Spaces

All C++ source code shall be indented with spaces as this ensures the
same indentation regardless of editor. Tabs shall not be used.

*Note*: This is enforced in emacs via the both whitespace mode and the
settings in the [[Preamble][preamble]] of every file.

The boost [[http://www.boost.org/development/requirements.html][Tabs rationale]] states:

#+begin_quote
Tabs are banned because of the practical problems caused by tabs in
multi-developer projects like Boost, rather than any dislike in
principle. See mailing list archives. Problems include maintenance of
a single source file by programmers using tabs and programmers using
spaces, and the difficulty of enforcing a consistent tab policy other
than just "no tabs". Discussions concluded that Boost files should
either all use tabs, or all use spaces, and thus the decision to stick
with spaces.
#+end_quote

For a detailed analysis see [[http://www.jwz.org/doc/tabs-vs-spaces.html][Tabs versus Spaces: An Eternal Holy War]] by
jwz.

**** Column width

Source code must not exceed 80 columns. Continuations on subsequent
lines should be avoided - the line should read as a complete
statement. /Rationale/: the human brain is used to scan one line at a
time in rapid eye movements; making long lines or multi-line
statements short-circuits this process.

This also has the beneficial side-effect of making the code readable
even on text-mode consoles at 80x25, and allowing the editing/viewing
of two files on a high resolution monitor.

*Note*: This is enforced in emacs via the whitespace mode.

**** Function length

Functions should not exceed a screen full, measured at around 25 lines
of code. Functions become more and more difficult to understand as
they become larger.

**** Documentation

We use [[http://www.stack.nl/~dimitri/doxygen/][doxygen]] in our literate programming model. Most identifiers
should be documented, except in trivial cases. In particular:

- *namespace documentation*: Every namespace generating folder must
  have a namespace documentation header; this is a header file whose
  name matches the namespace and contains documentation explaining its
  purpose.

**** General Identifier naming

Identifier names in general follow the STL and boost conventions, with
a few kitanda peculiarities:

- *classes, structs, namespaces, functions and variables* are
  lower-cased and separated by underscores when multi-word;
  e.g. =this_is_a_class_name=. They should not be longer than four to
  five words at the extreme - with the exception of specification
  methods. These are always propositions and can be as long as
  necessary - provided the entire line does not exceed the maximum
  line size. FIXME point to specification section.
- *member variables*, in addition to the above, shall be post-fixed
  with an underscore, e.g. =member_variable_=. /Rationale/: underscore
  prefixes are reserved by standard C++.
- *concepts* are camel cased with a leading capital,
  e.g. ThisIsAConcept.

**** Indentation

In general, all source code must be indented using the /Stroupstrup/
style (FIXME add link to Wikipedia); there are a few kitanda
modifications, as follows.

*Namespaces* are not indented; the =namespace= keyword must always
start at column zero. Namespace closing brackets must always close all
the enclosing namespaces in a single line. Namespace closing brackets
are never followed by comments such as the name of the namespace being
closed. /Rationale/: all major editors tell you which namespace the
bracket is closing so this just adds more maintenance work for no
reason.

#+begin_example
namespace A {
namespace B {

} }
#+end_example

**** Literate programming

Almost all identifiers should have some form of doxygen comments;
however, we should try to avoid documenting too much directly in the
source code. Instead, org articles should be used for detailed
descriptions, specifications, etc.

Rules for comments:

1. The value of a comment is directly proportional to the distance
   between the comment and the code. Good comments stay as close as
   possible to the code they're referencing. As distance increases,
   the odds of developers making an edit without seeing the comment
   that goes with the code increases. The comment becomes misleading,
   out of date, or worse, incorrect. Distant comments are unreliable
   at best.

2. Comments with complex formatting cannot be trusted. Complex
   formatting is a pain to edit and a disincentive to maintenance. If
   it is difficult to edit a comment, it's very likely a developer has
   avoided or postponed synchronizing his work with the comments. I
   view complex comments with extreme skepticism.

3. Don't include redundant information in the comments. Why have a
   Revision History section-- isn't that what we use source control
   for? Besides the fact that this is totally redundant, the odds of a
   developer remembering, after every single edit, to update that
   comment section at the top of the procedure are.. very low.

4. The best kind of comments are the ones you don't need. The only
   "comments" guaranteed to be accurate 100% of the time-- and even
   that is debatable-- is the body of the code itself. Endeavor to
   write self-documenting code whenever possible. An occasional
   comment to illuminate or clarify is fine, but if you frequently
   write code full of "tricky parts" and reams of comments, maybe it's
   time to refactor.

5. [[http://www.codinghorror.com/blog/2006/12/code-tells-you-how-comments-tell-you-why.html][Code tells you how, comments tell you why]].

Source of 1-4: [[http://www.codinghorror.com/blog/2004/11/when-good-comments-go-bad.html][When Good Comments Go Bad]]

**** Operators

- Always on the same namespace as the class.

FIXME: clean this up.

How to decide if operator should be global or in a class?

My thoughts are this:

- Do you have two classes A & B that you want to make comparable with operator==?
- If it is just one, the choice is obvious to me: make it a member.
- But for two different classes, there is a symmetry between A==B &
  B==A, yet there isn't in the declaration of the fn.
- You need both "bool A::operator==(B const &) const" & "bool
  B::operator==(A const &) const". A solution is to have one as a
  member, the other as a free fn, calling the member.
- That means you have to have unsymmetric class declarations. Which is
  simply a matter of taste.
- If there are suitable getters already, then that is fine, one can
  have two free operator== fns, one as "bool operator==(A const &, B
  const &)" and the other for the commutative operation. Where you
  place the decls and the definitions is up to you.
- Probably best to distribute the decls across the two header files,
  according to the first parameter, forward declare the second, and
  place the definition in the equivalent, related source files.
- This nicely maintains symmetry, allows you to make the declaration
  of "bool operator==(A const &, B const &)" a friend of both classes,
  as can the decl "bool operator==(B const &, A const &)" be made a
  friend of both, avoid getters if they are ugly.

On operator==:

always check for =this= equality (e.g. self).

**** Swap function

- [[http://stackoverflow.com/questions/1998744/benefits-of-a-swap-function][why we should define it]].

**** Referential transparency

A function is referentially transparent when it is guaranteed to
return the same value every time it is called with a given set of
arguments. This is a great guarantee to be able to make about your
functions as it reduces the number of things you need to keep in your
head. Referentially transparent functions really can be thought of as
black boxes: always deterministic and predictable. But a function that
operates on mutable objects cannot be referentially transparent—it
cannot make any guarantees that future calls involving that same
object will result in the same value since that object could have a
different value at any time.

- [[http://technomancy.us/159][In which we plot an escape from the quagmire of equality]]

**** Make invalid states unrepresentable

In the functional world, programmers try to make it impossible to
express invalid states, such that the type system itself stops
programming errors. For example, say function =F= takes an enumeration
of type =E= as its first argument and a second argument which is only
valid for value =e= of the enumeration. Following the "make invalid
states undefined" principle, the second argument of =F= should not be
visible when the enumeration has values other than =e=.

In a more static world, this probably means falling down to MPL to
define the enum.

**** Constructors and Destructors

- [[http://www.developerfusion.com/article/133063/constructors-in-c11/][Constructors in C++11]]
- [[http://www.codesynthesis.com/~boris/blog/2012/04/04/when-provide-empty-destructor/][When to provide an empty destructor]]

1. The non-copyable idiom shall be implemented by deleting the
   required constructors:

#+begin_example
struct non_copyable {
   non_copyable & operator=(const non_copyable&) = delete;
   non_copyable(const non_copyable&) = delete;
};
#+end_example

2. The non-default constructible idiom shall be implemented by
   deleting the default constructor:

#+begin_example
   NonCopyable() = delete;
#+end_example

3. All regular constructors with a single parameter shall be marked as
   explicit. See [[http://www.parashift.com/c%2B%2B-faq-lite/ctors.html#faq-10.22][What is the purpose of the explicit keyword?]]

4. Move constructors and copy constructors shall never be marked as
   explicit. See [[http://stackoverflow.com/questions/6758717/explicit-move-constructor][Explicit move constructor?]]

5. Abstract classes shall be enforced to be abstract via the pure
   virtual destructor. See [[http://www-304.ibm.com/support/docview.wss?uid%3Dswg21165191][Creating a Pure Virtual Destructor for a
   C++ Abstract Class]].

#+begin_example
struct abstract {
   virtual ~abstract() noexcept = 0;
};

abstract::~abstract() { }
#+end_example

**** Enumerations

#+begin_quote
An enumeration is a user-defined type consisting of a set of named
constants called enumerators. Enumerators have values.
#+end_quote

1. All enumerations shall be defined as C++-11 strong enumerations
   with the correct underlying type.

  - [[http://www.codeguru.com/cpp/cpp/article.php/c19083/C-2011-Stronglytyped-Enums.htm][C++ 2011: Strongly-typed Enums]]
  - [[http://www.cprogramming.com/c%2B%2B11/c%2B%2B11-nullptr-strongly-typed-enum-class.html][Better types in C++11 - nullptr, enum classes (strongly typed enumerations) and cstdint]]
  - [[http://nic-gamedev.blogspot.co.uk/2012/04/c11-strongly-typed-enum.html][C++11: Strongly-typed enum]]
  - [[http://www.nullptr.me/2012/01/04/enum-classes/#.T7aeNvHgYhY][C++11: enum classes]]

2. The enumeration name shall always be plural.

#+begin_example
enum class node_types : unsigned int {
...
#+end_example

   /Rationale/: This is a very difficult decision to make, mainly
   because an enumeration identifier is actually doing two jobs: a)
   the declaration of a domain of valid values - as such its the name
   of the set containing those valid values, so it makes sense to be
   plural (e.g. =player_types= is the set of valid =player_type=); b)
   its also used as a type for variables that take a single value of
   that domain, in which case it should be singular.

   The declaration of the enum occurs only once, whilst its usage is
   everywhere so at first sight it seems that the singular usage
   should win. This is the approach taken by Microsoft for
   C#. However, another /very/ typical use case is to declare getters
   and setters for a value of a type of an enumeration. This is where
   we reach the limits of C++ compiler intelligence because if we have
   both an enumeration and a member variable declared with the same
   name, the compiler thinks that we are hiding one of the names. Thus
   we end up with all sorts of imaginative getter/setter names to get
   around this problem. Due to this, the final decision is to use a
   plural name for the enumeration so that the singular name can be
   reserved for getters, setters and variables of that type.

3. Doxygen documentation shall be used to document the enumeration
   itself and all the enumerators.

#+begin_example
/**
 * @brief A given set of XML data is modeled as a tree of
 * nodes. This enumeration specifies the different node types.
 */
enum class node_types : unsigned int {
    none = 0, ///< Read method has not yet been called
...
#+end_example

4. Inserters shall be provided for each enumeration, dumping a textual
   representation of a given instance of an enumeration. The textual
   representation shall take the form =ENUMERATION::ENUMERATOR=, e.g.:
   =node_types::none=.

#+begin_example
std::ostream& operator<<(std::ostream& stream, object_types value);
#+end_example

5. It is possible for a variable of an enumeration type to have a
   value which is not part of the enumeration's domain. The exception
   =invalid_enum_value= (in =kitanda::utility::exception::=) shall be
   thrown at the point of detection.

6. When a given value of an enumeration which is part of its domain is
   not valid in a given context, the exception =invalid_enum_value=
   shall be thrown.

7. A function to convert from an instance of the underlying type to an
   enumerator may be provided. The function name shall take the form
   =to_ENUM_NAME=, where =ENUM_NAME= takes the singular form (e.g. for
   =object_types= its =object_type=). The function shall have a single
   parameter called =value=, of the same type as the underlying type
   of the enumeration. The function shall be defined in the utility
   namespace.

#+begin_example
object_types to_object_type(unsigned int value);
#+end_example

8. A function to convert a string representation of a value in the
   enumeration's domain to an enumerator may be provided. The function
   name shall take the form =parse_ENUM_NAME=, where =ENUM_NAME= takes
   the singular form (e.g. for =object_types= its =object_type=). The
   function shall have a have a single parameter of type =std::string=
   named =value=.

   The function shall be defined in the utility namespace. The format
   of the input string is enumeration dependent and may not be related
   to the streaming of the enumeration. /Rationale/: the prefix
   =parse= is used to imply that the conversion is more elaborate than
   the underlying type conversion. This is used by Microsoft for C#.

#+begin_example
object_types parse_object_type(std::string value);
#+end_example

9. A function to convert from an instance of the enumeration to the
   underlying type shall not be provided. Users are expected to use
   the =static_cast= operator.

**** Classes

1. Class names shall not use a prefix or post-fix to indicate their
   "meta-type". For example =IInterface= or =base_class= are bad
   names. /Rationale/: this is just a variation of Hungarian
   notation. For a good explanation of why the =base= suffix is a bad
   idea see Cwalina's [[http://blogs.msdn.com/b/kcwalina/archive/2005/12/16/basesuffix.aspx][blog post]].

2. Virtual destructors must be implemented due to a problem with
   clang - e.g. we cannot rely on using =default=. The canonical form
   for the virtual destructor:

#+begin_example
    virtual ~CLASS_NAME() noexcept {}
#+end_example

  Note that there is no space between ={}=.

*** References

This section lists books, sites and other material that we have read,
perused or have been otherwise in contact with, and provide useful
information with regards to C++ standards.

**** Coding standards and guidelines

- [[http://www.amazon.co.uk/Coding-Standards-Rules-Guidelines-Practices/dp/0321113586/ref%3Dsr_1_1?s%3Dbooks&ie%3DUTF8&qid%3D1393685150&sr%3D1-1&keywords%3DC%252B%252B%2BCoding%2BStandards%2B%253A%2BRules%252C%2BGuidelines%252C%2Band%2BBest%2BPractices][C++ Coding Standards : Rules, Guidelines, and Best Practices]]: The
  book to read on coding standards, especially for C++. Legislate what
  needs to be legislated and leaves alone what should be left alone.
- [[http://www.boost.org/development/requirements.html][Boost coding standards]]: The standards and recommendations of the
  boost project, on the whole useful.
- [[http://www.amazon.co.uk/Large-Scale-C-Software-Design-APC/dp/0201633620][Large Scale Software Development With C++]]: This is a classic book
  that discusses many aspects which are normally not considered such
  as minimising compilation times, etc. Unfortunately it is dated on
  some parts, making the read a bit more difficult. We need to do a
  second parse of this book.
- [[http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml][Google C++ Style Guide]]: Good in some bits, although very bad in
  others (for example, bans exceptions).
- [[https://github.com/holtgrewe/linty][Linty]]: Style Checking with Python & libclang. Requires v3.0.
- [[http://www.parashift.com/c%2B%2B-faq-lite/][C++ FAQ]]: large resource on how to use the language
- [[http://www.doc.ic.ac.uk/lab/cplus/c%2B%2B.rules/][Programming in C++, Rules and Recommendations]]

**** C++ Reference

- [[http://en.cppreference.com/w/cpp][C++ Reference wiki]]

**** C++-11

- [[http://msdn.microsoft.com/en-us/library/hh279654%2528v%3Dvs.110%2529.aspx][Welcome Back to C++ (Modern C++)]]
- [[http://herbsutter.com/elements-of-modern-c-style/][Elements of Modern C++ Style]]
- [[http://www2.research.att.com/~bs/C%2B%2B0xFAQ.html][C++11 - the recently approved new ISO C++ standard]]

**** Videos and Talks

- [[http://vimeo.com/23975522][C++0x Lambda Functions]]

**** Useful Libraries

- [[http://boost-sandbox.sourceforge.net/libs/time_series/doc/html/time_series/user_s_guide.html#time_series.user_s_guide.series_containers][Boost Time Series]]: Library that didn't make it into boost but seems
  like a very good foundation for all the time series code. See also
  the [[http://lists.boost.org/boost-announce/2007/08/0142.php][list of problems with the library.]]
- [[http://www.craighenderson.co.uk/mapreduce/][Boost.MapReduce]]: [[http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/mapreduce-osdi04.pdf][Map reduce]] implementation for single process.
- [[http://blog.quibb.org/cppbench/][CPP Bench]]: benchmarking in C++.
- [[https://bitbucket.org/maghoff/jsonxx/wiki/Home][JSONXX]]: JSON library for C++.
- [[http://calvados.di.unipi.it/dokuwiki/doku.php?id%3Dffnamespace:about%20][FastFlow]]: a skeletal multi-core programming framework
- [[http://lwn.net/Articles/370307/#Comments][0MQ: A new approach to messaging]]
- [[http://www.globus.org/toolkit/about.html][Globulus Toolkit]]: grid toolkit for C++
- [[http://www.dre.vanderbilt.edu/~sutambe/files/LEESA/LEESA.pdf][LEESA]]: XML in C++
- [[http://www.codesynthesis.com/products/xsde/][XSD/e]]: Light-weight XML in C++
- [[https://github.com/consultomd/json_spirit][JSON Spirit]]
- [[https://github.com/d5][Node.Native]]: C++ Node.js implementation
- [[http://svn.boost.org/svn/boost/sandbox/monotonic/libs/monotonic/doc/html/index.html][Boost.Monotonic]]: Special purpose allocators. Discussed on mailing
  list [[http://lists.boost.org/Archives/boost/2012/03/191798.php][here]].
- [[http://code.google.com/p/ceres-solver/][Ceres Solver]]: Library for solving nonlinear least squares
- [[http://code.google.com/p/dtl-cpp/][DTL]]: The diff template library.

**** Other C++ articles

- [[http://altdevblogaday.com/2011/11/09/a-low-level-curriculum-for-c-and-c/][A Low Level Curriculum for C and C++]]
- [[http://altdevblogaday.com/2011/11/24/c-c-low-level-curriculum-part-2-data-types/][C / C++ Low Level Curriculum part 2: Data Types]]
- [[http://altdevblogaday.com/2011/12/14/c-c-low-level-curriculum-part-3-the-stack/][C / C++ Low Level Curriculum Part 3: The Stack]]
- [[http://smellegantcode.wordpress.com/tag/c11/][Linq in C++]] See also the fix [[http://boost.2283326.n4.nabble.com/range-cannot-use-lambda-predicate-in-adaptor-with-certain-algorithms-td3560157.html][here]].
- [[http://confluence.jetbrains.net/display/TW/Cpp%2BUnit%2BTest%2BReporting][Changing the output of Boost.Test]]
- [[http://software.intel.com/en-us/articles/extending-stl-for-games/][Extending STL for Games]]
- [[http://en.wikibooks.org/wiki/Optimizing_C%252B%252B][Optimising C++]]: free ebook
- [[http://google-opensource.blogspot.co.uk/2010/10/integrating-r-with-c-rcpp-rinside-and.html][Integrating R with C++: Rcpp, RInside, and RProtobuf]]
- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1476.html][Iterator facade and adaptor]]
- [[http://blog.rethinkdb.com/improving-a-large-c-project-with-coroutines][Improving a large C++ project with co-routines]]
- [[http://www.ibm.com/developerworks/aix/library/au-ctools1_boost/%20][Get to know the Boost unit test framework]]
- [[http://drdobbs.com/parallel/232400273][Welcome to the Parallel Jungle!]]
- [[http://www.radmangames.com/programming/how-futures-aid-task-based-multithreading][How futures aid task based multi-threading]]
- [[http://www.viva64.com/en/l/full/][Lessons on development of 64-bit C/C++ applications]]
- [[http://boost-spirit.com/home/2010/05/11/a-framework-for-rad-spirit/][A framework for RAD]]: boost spirit
- [[http://arma.sourceforge.net/][Armadillo: C++ linear algebra library]]
- [[http://nerds-central.blogspot.co.uk/2012/05/c11-future-chaining-for-easy-highly.html][C++11: Future Chaining For Easy, Highly Threaded Execution]]
- [[http://www.altdevblogaday.com/2012/04/26/functional-programming-in-c/][Functional Programming in C++]]

**** Videos

- [[http://www.corensic.com/Learn/Resources/ConcurrencyTutorialPartOne.aspx][C++ Concurrency Series]]: Part 1
- [[http://skillsmatter.com/podcast/java-jee/kevlin-henney-rethinking-unit-testing-in-c-plus-plus][Rethinking Unit Testing in C++]]

* APPENDIX

** Appendix A - Related Work

This section is a bit of a general research bucket. It contains a set
of links to the C++ code generators we have found on our wanderings on
the internet, as well as other interesting projects in this space -
including those in other programming languages. It also contains books
and papers on the subject we have read, or intend to read.

- [[http://www.amazon.co.uk/Domain-Driven-Design-Tackling-Complexity-ebook/dp/B00794TAUG/ref%3Dsr_1_2?ie%3DUTF8&qid%3D1368380797&sr%3D8-2&keywords%3Dmodel%2Bdriven%2Bdesign][Domain-Driven Design: Tackling Complexity in the Heart of Software]]:
  The Eric Evans book from which we tried to steal most concepts in
  Dogen. A must read for any developer.
- [[http://www.amazon.co.uk/EMF-Eclipse-Modeling-Framework-ebook/dp/B0013TPYVW/ref%3Dsr_1_2?s%3Dbooks&ie%3DUTF8&qid%3D1368380262&sr%3D1-2&keywords%3DEclipse%2BModeling%2BFramework%2B%255BPaperback%255D][EMF: Eclipse Modeling Framework]]: The original EMF book. Useful read
  for anyone interested in code generation.
- [[http://www.scribd.com/doc/78264699/Model-Driven-Architecture-for-Reverse-Engineering-Technologies-Strategic-Directions-and-System-Evolution-Premier-Reference-Source][Model Driven Architecture for Reverse Engineering Technologies]]:
  Preview of a potentially interesting MDA book.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: This paper will be of
  interest if we decide to support multiple languages.
- [[http://www.vollmann.com/pubs/meta/meta/meta.html][Metaclasses and Reflection in C++]]: Some (early) ideas on
  implementing a MOP (Meta Object Protocol) in C++.
- [[https://code.google.com/a/eclipselabs.org/p/cppgenmodel/][cppgenmodel - A model driven C++ code generator]]: This seems more
  like a run time / reflection based generator.
- [[https://code.google.com/p/emf4cpp/][EMF4CPP - Eclipse Modeling Framework]]: C++ port of the EMF/eCore
  eclipse framework. As with Java it includes run time support. There
  is also [[http://apps.nabbel.es/dsdm2010/download_files/dsdm2010_senac.pdf][a paper]] on it.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: Describes a meta-model
  designed to model Java and C++.
- [[http://marofra.com/oldhomepage/MetaCPlusPlusDoc/metacplusplus-1.html][The Meta-C++ User Manual]]: Another early C++ meta-modeling
  tool. Contains interesting ideas around C++ meta-models.
- The Columbus C++ Schema: Useful tool for re-engineering large C++
  code bases. Contains a meta-model for C++. A number of papers have
  been written about it:
  - [[http://www.inf.u-szeged.hu/~beszedes/research/tech27_ferenc_r.pdf][Columbus – Reverse Engineering Tool and Schema for C++]]
  - [[http://journal.ub.tu-berlin.de/eceasst/article/download/10/19][Third Workshop on Software Evolution through Transformations]]:
    Embracing the Change
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_schema.ppt.pdf][Towards a Standard Schema for C/C++]]
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_columbus_schema_cpp.pdf][Data Exchange with the Columbus Schema for C++]]
- [[http://www.cpgf.org/][CPGF]]: An open source C++ library for reflection, script binding,
  serialisation and callbacks.
- [[http://www.artima.com/articles/dci_vision.html][DCI]]: The DCI Architecture: A New Vision of Object-Oriented
  Programming. Some fundamental insights on the nature of OO.
- [[http://www.ischo.com/xrtti/index.html][xrtti]]: Extending C++ with a richer reflection.
- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3883.html][Code checkers and generators]]: adding AngularJS-like capabilities to
  C++.
- [[http://stackoverflow.com/questions/355650/c-html-template-framework-templatizing-library-html-generator-library][Text Template libraries for C++]]: T4 like implementations for C++.
