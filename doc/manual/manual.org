#+title: Domain Driven Development with Dogen
#+options: author:nil

Copyright (c) 2012, 2013, 2014 Marco Craveiro

Permission is granted to copy, distribute and/or modify this document under the
terms of the GNU Free Documentation License, version 1.3; with no Invariant
Sections, no Front-Cover Texts and no Back-Cover Texts.

Revision *DRAFT*, May 2014

This revision of the manual describes Dogen *master* and is available
in the following formats: HTML and PDF.

#+toc: headlines 2
#+toc: listings
#+toc: tables

* Preface

** About This Document

This document is the official manual for Dogen. Dogen - the domain
generator - is a code generation tool designed specifically to target
domain models. Dogen was created to make the modeling process simpler:
the user creates a domain model using a UML tool and Dogen uses it to
generate its source code representation. The generated code contains
most of the services required from a typical C++ domain object such as
serialisation, hashing, streaming and so on.

If you are reading a printed copy of this manual, you can always
access the latest version online:

- https://github.com/kitanda/dogen/blob/master/doc/manual/manual.org

** More Information

You can find the latest source code for Dogen at the official
repository in GitHub:

- https://github.com/kitanda/dogen

There is also a mirror in BitBucket:

- https://bitbucket.org/marco_craveiro/dogen/overview

You can find details on the ongoing work in the Agile folder (look for
the latest sprint):

- https://github.com/kitanda/dogen/tree/master/doc/agile

Continuous builds are available via CDash:

- http://hedgr.co.uk/cdash/index.php?project=dogen

* PART I - THEORY

In this part we describe the philosophical underpinnings behind Dogen,
its internal architecture and the development model.

** Introduction

Dogen is largely the result of exploring a simple question: what
portion of the objects required to model a problem domain are
generatable by a program - such that the generated code is as good
as or even better than code crafted by humans?

The question stems from many years of looking at object models and
their limitations. For instance, one of the main problems one often
finds in production C++ code is a lack of a large number of useful
"facilities" that all objects on all domain models should have; for
instance, a simple way of dumping current state to a stream, like
/toString/ in Java. Developers tend to add facilities like these on a
haphazard sort of way, because it is laborious and not particularly
exciting functionality to work on; by the time the domain model has
matured, its too late to find time for these sort of fundamental
activities.

We further observed that a large portion of the objects required to
model a problem domain have fairly straightforward behaviours; in many
cases they are but glorified structs with a few trivial behaviours
bolted on such as serialisation and hashing. A lot of programmer time
is taken on generating getters, setters, serialisation code and so
on. This is also code that is easy to get sloppy with, because its so
repetitive.

Dogen aims to code generate all such code - and /only/ such code;
anything which is deemed non-trivial is expected to be done by
humans. For everything else, we aim to add support in Dogen.

*** Models and modeling

Programming is the art of refining abstractions. In general, the
programmer's job is to create a set of constructs that represent
entities in some problem domain or other; and to get those entities to
cooperate successfully in producing work that is defined as useful by
something or someone. Together, these entities form /a model/ because
they are representation inside of the computer of a subset of the
problem domain.

/Domain modeling/ is then the activity of finding a set of domain
types that describe the domain in question via their properties and
their relationships.

Many programmers don't dwell too much on the fact that they are
modeling - its just an implicit activity done as part of
coding. However, as we shall see, the modern way of looking at
programming puts a strong emphasis in understanding and introspecting
about this activity.

*** Code generation and meta-models

The concept of programs that generate programs is probably as old as
computer science itself: it certainly was a common feature in the days
of machine code and assembler code programming. These ideas were
incorporated in early languages such as LISP, where there was a
blurring of the lines between hand crafted source code and machine
generated source code. Sadly, these progressive thoughts faded into
the foreground as the C family of languages took front stage.

It's not as if code generation disappeared - it just went into
hiding. In fact, there are many widely used tools in the Open Source
ecosystem that generate code:

- [[https://developers.google.com/protocol-buffers/][Google Protocol Buffers]]
- [[http://www.codesynthesis.com/products/odb/][ODB]]: C++ Object-Relational Mapping (ORM)
- [[http://www.codesynthesis.com/products/xsde/][eXSD]]: XSD/e: XML for Light-Weight C++ Applications
- [[http://msdn.microsoft.com/en-us/library/windows/desktop/aa367300(v%3Dvs.85).aspx][MIDL]]: COM IDL compiler
- and many more.

Each of these tools are designed to do a specific task and to do it
well, hiding as much as possible of the code generation details from
the end user. We call these are special-purpose code generators -
although, as we shall see, in a sense all code generators are special
purpose. The code generated by these tools contains both the data
structures they require as well as hard-coded behaviour associated
with them: how to read and write them from raw storage (in the case of
Protocol Buffers), how to read and write them from the database (ODB),
and so on.

All code generators have an internal set of data structures that
represent the entities to generate - explicitly or implicitly. These
data structures are known as the /meta-model/. Meta-models are a class
of models that focus on describing models themselves. They allow code
to introspect and to think about code; to reflect. In this form, code
generation is simply the transformation of a model, described in one
such representation, into another representation - source code -
following the rules laid out by the grammar of a programming
language. The richer the meta-model, the more expressive the generated
code can be - and vice-versa. It is in this light that we call certain
classes of code generators /special purpose/, because they have
meta-models that are very focused, designed only for the task at
hand. Don't think of this as a disadvantage though: there is a price
to pay in complexity for every ounce of flexibility, so its best to
have simple code that does one thing and does it well.

Nevertheless, meta-models can be useful in a more general form when
designing software applications: they can allow one to reason about
the structure of the code. One of the most common meta-models in
existence is [[http://en.wikipedia.org/wiki/Unified_Modeling_Language][UML]]. UML is used widely in the industry and there are
many tools that can be used to generate source code from UML
diagrams. It is simultaneously ubiquitous - that is, available
everywhere - and complete - that is, as a meta-model, it defines a
extensive list of concepts for pretty much any aspect of
programming. Thus it is common for tools to take a UML representation
and use it to generate source code; as examples of Open Source tools
that can generate source code from a UML diagram see:

- [[http://dia2code.sourceforge.net/][dia2code]]
- [[http://umbrello.kde.org/][Umbrello]] (see [[http://docs.kde.org/development/en/kdesdk/umbrello/code-import-generation.html][this]] for code generation)

In a sense one, may think of these as /general purpose/ code
generators because they output code that is not tied up to any
specific purpose, other than to model the problem domain. Unlike the
special purpose tools, the generated code is very much skeleton code,
code that adds little in terms of behaviour. This is all as it should
be: the more specific your intent is, the more the code generator can
do for you and, conversely, the less specific your intent is, the less
helpful the code generator can be.

The astute reader would have already devised a simple solution to the
behaviour conundrum: nothing stops us from modeling the signatures of
methods in the meta-model - after all UML provides us with all the
required machinery - and then hand-craft an implementation for these
methods. Indeed there are code generators which permit such workflows;
they are known as /merging code generators/. The merging aspect comes
from the fact that the code generator must be able to distinguish
between the hand-crafted code and the machine generated code in order
to handle meta-model updates.

So these are three key themes for Dogen: special purpose code
generation, general purpose code generation and merging code
generation. But before we can proceed, we need to add one more actor
to the scene.

*** Domain Driven Design

One of the main problems facing software engineers working on large
systems is the need to clearly separate business rules from
scaffolding code. In many ways, this need originates from the long
forgotten days when the word /Application/ was coined: the use of
computer science /applied/ to a specific problem to provide an
automated solution to the set of people with the problem - the
/users/. During the process of development, users will provide all
sorts of insights into what it is they want solved, and these are
ultimately captured in code. Code will also be made up of reading and
writing records to a database, socket communication, reading and
writing to file and so on; the challenge then is to avoid obscuring
the former while dealing with the latter.

Many people have thought deeply about this dichotomy. Arguably, the
most significant advance was made by Eric Evans with his seminal book
[[http://www.amazon.co.uk/Domain-driven-Design-Tackling-Complexity-Software/dp/0321125215][Domain-Driven Design]]: Tackling Complexity in the Heart of
Software. Domain Driven Design (DDD) is a software engineering
methodology that places great emphasis on understanding the problem
domain and, coupled with Agile, it provides a great platform for
iterative improvements both to the understanding and to its expression
in code. DDD places great emphasis in defining a clear and concise
domain model - a set of classes and relationships that model the
insights provided by the users and domain experts in general. It also
explains the difference between the conceptual domain model and myriad
of representations: UML diagrams, specification documents, oral
conversations and, most importantly, source code.

*** Adding It All Together

The key idea behind Dogen is that all of the aspects we described up
til now are deeply interrelated. That is to say that we store deep
knowledge about the domain in meta-models, which tend to be
represented graphically - say in UML class diagrams; and we do so
because these representations provide a quick and yet expressive way
to communicate domain knowledge. But those very same documents are -
or can be made - sufficiently complete to be used as a basis for the
code generation of skeleton code by some general purpose code
generation tool. Furthermore, there are a large number of services
that are required of most domain models, and these can be thought of
as special purpose extensions to such a general purpose tool; and,
finally, that which cannot be code generated can be manually added and
merged in.

Lets return to the "basic services" required by all domain
models. What do we mean exactly? Well, ODB and the like already hinted
at some of the things one may wish to do with C++ objects - persist
them in a database - but there are other even more fundamental
requirements:

- the ability to support getters and setters, hashing, comparisons,
  assignment, move construction and many other fundamental behaviours;
- the ability to dump the current state of the object to a C++ stream
  in a format that is parsable by external tools (like say JSON);
- the ability to generate [[http://stackoverflow.com/questions/5140475/how-to-write-native-c-debugger-visualizers-in-gdb-totalview-for-complicated-t][debugger visualisers]];
- the ability to serialise and deserialise objects using a multitude
  of technologies such as [[http://download.oracle.com/otn_hosted_doc/coherence/353CPP/index.html][POF]], [[http://www.boost.org/doc/libs/1_55_0/libs/serialization/doc/index.html][Boost Serialisation]], [[https://github.com/hjiang/jsonxx][JSON]], [[http://libxmlplusplus.sourceforge.net/][XML]] and many
  others;
- the ability to generate objects populated with random data for
  testing;
- ...

And on and on. The more we looked, the more boilerplate code we
found - code that could easily be generated for the vast majority of
the cases. Of course, there are quite a few corner cases which are
just too hard to automate but they can easily be manually coded.

The picture that emerges from this [[http://en.wikipedia.org/wiki/Thought_experiment][gedankenexperiment]] is some kind of
"cyborg" coding - a type of programming where any and all aspects that
can be reduced to a set of rules inferable from the structure of the
domain model, are implemented as extensions of the code
generator. Dogen is an attempt to create such a tool. As we are C++
developers we started off by trying to implement the vision as a C++
tool; but the notions are general enough that they would apply to any
programming language.

** The Dogen Architecture

Almost all code in Dogen is implemented as Dogen domain models; that
is, we use Dogen to generate the vast majority of Dogen itself, and we
do so for several reasons:

- dog-fooding: using your own tool frequently is a great way of making
  sure the tool does what it is meant to do and does so in a workable,
  pragmatic manner.
- keeping our feet on the ground: if we have some crazy ideas and
  break Dogen, we can no longer develop Dogen. Thus Dogen must always
  be able to code-generate itself at all points in the development
  cycle, which forces one to think "extremely" incrementally.
- code faster and test our theoretical underpinnings: if our ideas
  around code generation are correct, Dogen should significantly
  speed-up development.

Dogen is made up of a large number of domain models. These fall into
two broad categories: /test models/ and /core models/. Test models are
models we created specifically to test some aspect of code
generation - such as say inheritance - and whose code is not used by
the main binary. The core models are what really makes up the
application and that is what is of interest for this chapter.

The core models are hooked together in a fashion similar to that of
the internals of a compiler, and so core models belong to one of three
groups: the front-end, the middle-end and the back-end. The front-end
group of models allows for different sources of domain information to
be plugged into Dogen. The middle-end model - as there is only one -
is where all the language neutral transformations take place; It can
be thought of as a bridge between domain modeling and code
generation. Finally, the back-end group of models are responsible for
expressing SML as code according to the grammar of a programming
language like C++.

Lets look at each of these in more detail.

*** The Front-end

When we started developing Dogen, we chose Dia as our main input
format. Dia is a simple yet very powerful tool for drawing structured
diagrams that focuses almost exclusively on diagram editing, and
leaves other use cases to external tools. To their credit, a number of
tools have sprung up around Dia, in no small part due to the
simplicity and stability of their XML file format. Some of these tools
generate code from Dia XML, others convert code into diagrams; we
aimed for Dogen to be another chain in that tooling ecosystem.

At the same time, Dogen has been developed from the start with the
intention to support multiple input formats. We knew that different
people would have different needs and for some Dia would not be
sufficient. So we imagined a pipeline that was made up with a pair of
models: one to model closely the input model and a /transformation/
model responsible for converting the input model into the middle-end

Dogen. Furthermore, all of Dogen's own models have been created and
are being maintained using this application, so it is very core to the
code generation experience.

There are two models responsible for implementing this use case:

- =dia=
- =dia_to_sml=

The =dia= model has a representation of the Dia XML types, and tries
to do so as faithfully as possible. It was created to avoid having a
direct dependency with Dia's code base. Since Dia XML changes very
infrequently and since we use such a small part of Dia's
functionality, this turned out to be a good decision.

**** Meta-data and tags

In certain cases we had the need to pass certain information to SML
for which there was no available equivalent in Dia. In some cases
these were just shortcomings of the application and could be solved by
patching it; in some other cases, it just made no sense at all to
convey this kind of information in Dia. To solve this problem in a
general manner, we created a set of special "instructions" that are
interpreted by Dogen. These instructions are passed in to Dogen via
UML Comments, with a special form:

: #DOGEN KEY=VALUE

All lines starting with the well-known prefix =#DOGEN= are considered
special instructions. They must follow the key-value-pair form defined
above.

Initially this was done to fix a couple of minor problems with Dia,
but this infrastructure has taken a life of its own, and its now used
through Dogen. Each sub-system takes responsibility of its own keys -
it defines them and validates to ensure the values for a key are
valid. In the remainder of this manual you will find sections with a
name similar to this one, where we will define the tags available for
that component and their semantics.

These keys within Dogen are known as /tags/ and they are part of the
meta-data processing sub-system.

Dia defines the following tags:

- =dia.comment=: Comment provided by user when dia does not allow for
  it.
- =dia.identity_attribute=: Attribute that provides this entity its
  identity.
- =is_final=: If true, the type cannot be inherited from.

*** The middle-end

We store the domain model internally as SML - a /meta-model/ largely
based on Domain Driven Design. A meta-model is simply a model whose
sole purpose is to describe other models. SML is designed to capture
all the details of the domain model that are required for code
generation. SML is not designed for anything else, so it is very terse
and not a particularly obvious model.

*** The back-ends

We need to express the meta-model as code. That is, we need to
generate a /representation/ of the different "parts" the domain type,
according to the rules of some well-known /grammar/: that is, it must
obey to a set of rules defined somewhere. Typical grammars are
programming languages such as C++ or SQL, but they can also be more
esoteric such as a Dia diagram; it uses the Dia XML grammar.

A /representation/ in this context is understood to be a physical
expression of a domain type - e.g. as zeroes and ones stored in a file
somewhere.

Representations have two related concepts: facets and aspects. These
are best explained by way of an example. The most fundamental facet is
the @e types facet. This is the class definition itself. A facet is
made up of @e aspects - for example in C++ there is a header file and
an implementation file. However, an aspect need not map directly to a
file - its perfectly possible to have more than one aspect in a file.

* PART II - PRACTICE

In this part we describe how to use Dogen, from very simple use cases
building up to more complex ones. We also explain how Dogen can be
integrated with a build system and other practical aspects of its
usage.

** Hello World Example

* PART III - SPECIFICATIONS

This part is made up of a set of specifications on different aspects
of Dogen, such as project structure, coding standards, and so on. The
objective is to create norms as we go along so that new developers
understand the reasons behind historical decisions. These norms are
not set in stone, of course. They are expected to change whenever the
rationale behind them no longer applies, or if there are better
options available - options that, for whatever reason, were not
considered originally.

Its important to notice that these specifications will never be
complete - in the sense that we will never cover all aspects of the
development of Dogen. There will always be things that will remain
unspecified. We shall try to at least cover those we consider more
important. But the document is organic, and will be constantly
evolving over time.

The final reason behind the creation of these standards - and their
inclusion in the main application manual - is that Dogen is a /code
generator/; that is, we are making a large number of decisions about
code that other people will use and in a very real sense, will be
stuck with. Due to this we think there is a need to explain to users
the reasons behind these choices.

** RFC 2119

The definitions in RFC 2119 apply to the text of this part. In
particular:

#+begin_quote
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
#+end_quote

** File and Directory Specification

In this section we specify aspects related to the layout of Dogen in
version control, folder structure, file names and so on. The words
"folder" and "directory" are used interchangeably.

*** Content type

1. All files added to the git repository should be in plain
   text. /Rationale/: most of the tools we use produce textual
   representations of their data files, and they should be preferred
   whenever possible. /Exceptions/: Bitmap images and other content
   type which has only binary representation is acceptable, but should
   be used only as last resort.

*** Naming

1. All files and folders in must have lower-case names. /Rationale/:
   avoid as much as possible case-sensitive issues in platforms where
   casing is not that well designed, such as Windows. /Exceptions:/
   Files or folders that follow well-known naming conventions take
   precedence, such as the GNU files (e.g. =README=, =INSTALL=, etc.)
   and cmake files (=CMakeLists.txt=).

2. The dot character =.= shall only be used to separate the file name
   from its extension (e.g. =file.txt=). /Rationale/: For some reason,
   dots in the middle of the name seems to confuse cmake and force
   rebuilding targets that are up-to-date.

3. Multi-word file and directory names must make use of the underscore
   character =_= to separate words (e.g.: =folder_name=).

4. In cases where there is a need to distinguish between several
   multi-word components in a file name, the dash character =-= must
   be used (e.g. =some_word-some_other_word.extension=).

5. Names must contain only lowercase ASCII letters (=a-z=), numbers
   (=0-9=), underscores (=_=), hyphens (=-=), and periods
   (=.=). Spaces must not be used. /Rationale/: Spaces and other
   special characters cause cause obscure issues, both in Windows and
   on UNIX. These problems are often very hard to root-cause.

6. The first and last character of a file name must not be a period
   (=.=). /Rationale/: POSIX has special rules for names beginning
   with a period. Windows prohibits names ending in a period (source:
   [[http://www.boost.org/development/requirements.html][boost]]). /Exceptions:/ Control files such as =.gitignore=.

7. The first character of names must not be a hyphen =-=. /Rationale/:
   It would be too confusing or ambiguous in certain contexts (source:
   [[http://www.boost.org/development/requirements.html][boost]]).

8. The maximum length of directory and file names is 31
   characters. /Rationale/: Long file names cause obscure issues.

9. The total path length must not exceed 207 characters. /Rationale/:
   Dictated by ISO 9660:1999 (source: [[http://www.boost.org/development/requirements.html][boost]]). In addition, many file
   systems don't really like paths longer than 255 characters, so its
   best to keep well below this limit.

10. Files have additional naming conventions that are dependent on the
    file type. See language specific policies for details.

*** Root folder

1. The top-level Dogen folder is known as the /root folder/. The
   remainder of this section deals with all the sub-folders of the
   root folder, as well as its key files. /Notes/: The root folder is
   the directory you create when you clone the project from git -
   i.e. it contains the =.git= folder. The folder should be named
   =dogen= (in lower-case).

2. The =build= folder contains all the scripts, configuration and
   utilities required to perform a build and to package it for
   end-user consumption. /Notes/: In general, =build= should contain
   either files used directly by the makefiles or files that get
   copied over to the build's output directory for further
   processing. Artefacts contained in the =build= directory are:
   extensions to cmake (both third party and our own), packaging
   scripts, ctest scripts for Continuous Integration, templates (code
   templates, configuration file templates, etc) and configuration for
   code-quality tools (valgrind, etc).

3. The =data= directory contains the data files used by Dogen at run
   time; these are expected to be part of an installation.

4. The =diagrams= directory contains all of the Dogen diagrams used to
   generate Dogen source code. /Notes/: At present it does not contain
   the test models. There is no good reason for this; a story has been
   added to the backlog to fix it.

5. The =doc= folder contains manually crafted documentation. /Notes/:
   The types of artefacts contained are manuals, illustrative diagrams
   (not for code generation purposes), project plans, screen-shots,
   etc.

6. The =images= folder contains all the graphical artefacts required
   by the project. /Notes/: Artefacts such as icons, logos etc should
   be housed here.

7. The =patches= directory contains ongoing work that is not yet ready
   to be committed. /Notes/: We use this directory as a poor-man's
   branch. The reason why is that we often use extremely short-lived
   branches (e.g. less than 2 days) where the code is not yet ready to
   be pushed, but we may require access to it from multiple
   machines. It seems a bit heavy-weight to create and destroy remote
   branches for this case, so instead we save work down as
   patches. This directory has very short-lived files.

8. The =projects= folder is a sub-folder of the root folder. It
   contains the Dogen source code, in any of the supported programming
   languages. The sub-folders of projects are named after each
   individual project. /Rationale/: This directory could have been
   called =src= for source code, but we reserved this for the
   directory of the implementation files in C++. /Notes/ The word
   "project" here is used in a rather vague sense, but it can be
   understood to mean anything which generates a shared library,
   static library or executable or any other such cohesive unit work -
   say a set of ruby scripts around a given theme (sanity). We freely
   mix projects which exist solely to test dogen with projects that
   provide end-user functionality.

9. The =sql= directory contains sql configuration files and other
   bits and pieces. /Notes/: we should either deprecate this directory
   or move it to projects.

10. The =test_data= directory contains all the files required to run
    the Dogen unit tests.

*** Output folder

1. The folder under which the build is performed is called the
   /output/ folder. /Notes/: This is the location where all artefacts
   generated by the build are placed, such as binaries, shared
   objects, automatically generated documentation, instantiated
   templates, etc.

2. In-source builds are not permitted, so the output folder must not
   be the same folder as the root folder. /Rationale/: In-source
   builds mix the source code with build artefacts and can result in
   very messy situations. /Notes/: This is enforced by the CMake
   files.

3. The output folder should be located at the same level as the root
   folder. /Notes/: This is a convention; the directory can be located
   anywhere else outside the root folder. The CMake files must work
   regardless of the location of the output directory.

4. The /output/ folder should be name =output=. /Rationale/: it can be
   named anything you like and there must not be any dependencies on
   the name of the output folder. However, to increase
   interoperability with other developers its preferable to give it a
   standard name.

5. Users can partition the output folder, creating directories for
   each supported platform, compiler, debug and release, etc. This is
   known as a /multi-target/ setup. /Rationale/ Keeping all the
   targets within a single folder means its easy to start from scratch
   by deleting the top-level folder.

6. When using multi-target, the output folder's sub-folders should be
   named after the [[http://wiki.debian.org/Multiarch/Tuples][GNU triplet]] - including the compiler
   version. /Rationale/ These directories are setup manually because
   target folders contain a full-blown cmake environment, independent
   from the others. Its not possible to generate this multi-targeted
   setup directly from the makefiles. Example multi-target folder
   structure:

#+begin_example
output/linux-amd64-gcc-4_6_1-debug
output/win32-x86-clang-3_0-release
#+end_example

7. Within a target folder - whether on single or multi-target setups -
   the key folder is =stage=; it contains all the binaries and
   documentation ready for packaging.

*** Third party content

1. The project's git repository is expected to only contain code owned
   by dogen; all the external dependencies must be installed by the
   user as a build prerequisite (see doc/BUILD for details).

2. In exceptional cases where the third party dependency is both small
   and not readily available in packaged form, it is acceptable to add
   it to the repository. This is the case with CMake extensions and
   with the boost portable serialisation library. Once these projects
   are packaged they shall be removed from the repository.

** C++ coding standards

In general, we follow the [[http://www.boost.org/development/requirements.html][Boost Library Requirements and Guidelines]]
document. In a few rare cases we may make choices that contradict it;
we shall try to explain the rationale best we can for those cases.

These coding standards are not to be understood as classic coding
standards such as [[http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml][Google's]]. We are just intending to capture the
rationale of some of the choices we have made in the past so that it
is not forgotten. Thus you may find the standards somewhat sparse, and
silent in areas where other standards are very verbose.

*** Physical Layout

The physical layout deals with files, directories, binaries and any
other artefacts that live in storage. We concern ourselves with only
those physical artefacts that are part of or directly related to the
C++ domain; the generic aspects of physical layout are dealt with in
[[file:project_structure.org][project structure]].

**** Files

***** Extensions

C++ source code can be stored in two types of files:

- *header files*: in kitanda they always have the extension =hpp=;
- *implementation files*: in kitanda they always have the extension
  =cpp=.

***** Names

The general conventions for file naming are specified under [[file:project_structure.org][project
structure]]. In terms of C++, files will be named according to the type
of its contents:

- files containing a /class/ shall be named after the class;
- files containing nothing but /namespace/ shall be named after the
  namespace (see below why this can happen);
- files containing one or more free-functions shall be named according
  to the /logical module/ that these functions constitute
  (e.g. =utility= or =pricing=, etc).

***** Makefiles

Each top-level folder under =src= has a [[file:infrastructure/cmake.org][cmake]] makefile. In addition,
each sub-folder of these top-level folders may or may not contain a
makefile, depending on whether or not it houses a component.

***** Folders

All C++ source code lives under =code/cpp=. Kitanda's c++ source code
is split into the usual src and include folders:

- *src*: all implementation files and headers not expected to
  be used by client code;
- *include*: public headers.

****** Include Folder

All consumers of the API - including kitanda's own components - are
expected to put the include folder in the compiler's include path,
resulting in =#includes= of the form:

#+begin_example
#include "kitanda/kitanda.hpp"
#+end_example

There are three folders under include:

- *boost*: endian related code required by eos;
- *eos*: support for portable binary boost serialisation;
- *kitanda*: kitanda's public API.

As explained in [[file:project_structure.org][project structure]], as a general rule, the repository
only contains code owned by kitanda; all the external dependencies
must be installed by the user as a prerequisite for building
kitanda. The exception to this rule is the eos library, as its source
code is scattered all over the Internet and cannot be easily put
together.

The kitanda folder in include is the top-level container of the
system's public API; each of its sub-folders represents a component
(e.g.: org, comms, etc). Most components have a set of commonly
recurring folders (folders with well-known folder names are in
quotes):

- *domain*: this houses the component's domain objects. Domain objects
  are struct-like types, data-structures with very minimal
  behaviour. @e domain is the default folder name, but its acceptable
  to name it something else if there is a naturally fitting name
  (e.g. protocol in the comms component).
- *streaming*: streaming operators for domain types. We only define
  the inserter operator, and it shall only be used logging or
  debugging purposes.
- *serialization*: boost serialisation support for the domain
  types. Please note the American spelling,
- *database*: database support for the domain types.
- *hash*: boost hash support for the domain types.
- *other folders*: any number of folders containing objects which act
  on the domain types and contain org logic.

Of course, not all components require all folders.

****** Source Folder

The src folder contains implementation code for kitanda's
components. Applications are always directly below src
(e.g. =src/luena=, =src/kuito=, etc.); libraries can be at the
top-level - when a component is implemented as one single library -
but they can also be sub-folders of the component. Its up to the
component to determine the approach. The binaries themselves reflect
the chosen structure:

- =libkitanda_org.so= (=kitanda_org.dll= in windows): one single
  library for the org component; or
- =libkitanda_org_streaming.so= (or =kitanda_org_streaming.dll= in
  windows): multiple libraries for each "sub-component" of the org
  component.

****** Specs

All components in the system have a set of codified behaviours: the
/component specification/. In addition, the system itself may have
additional constraints, which live in the /system specification/. In a
more general form: there are only two types of behaviour within the
system, specified or unspecified. Specified behaviour is that which is
covered by a spec; unspecified behaviour is that which is not.  With
regards to specs, components may chose to implement it in one of two
ways:

- by creating a sub-folder called /spec/; or
- by adding the spec classes directly to the main
  component folder.

The size of the spec should determine the approach.

FIXME: link to kevin henney pappers
http://skillsmatter.com/podcast/java-jee/kevlin-henney-rethinking-unit-testing-in-c-plus-plus

*** Language
**** Preamble

All C++ files shall start with a descriptive text stating copyright
information and a pointer to the file where the complete licence
document can be located. This section is called the /preamble/. The
contents of the preamble are as follows:

#+begin_example
/* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
 *
 * Copyright (C) 2012 Kitanda
 *
 * This file is distributed under the Kitanda Proprietary Software
 * Licence. See doc/LICENCE.TXT for details.
 *
 */
#+end_example

The preamble is available as the [[file:infrastructure/emacs.org][yas snippet]] =preamble=.

**** Header guards

All header files shall have header guards after the [[Preamble][preamble]]. The name
of the guard shall be composed of the following structure:
=NAMESPACE_FILENAME_EXTENSION=. Each of these fields has the following
meaning:

- *NAMESPACE*: List of namespaces contained on that file, in upper
  case, separated by an underscore. Namespaces with multi-word names
  also use underscores as separators. Example:
  =NAMESPACE_ONE_NAMESPACE_TWO=;
- *FILENAME*: Name of the file, in upper case, separated by
  underscores when multi-word. Example: =THIS_IS_A_FILENAME=;
- *EXTENSION*: Always set to =HPP=.

The traditional header guard check is followed by the Microsoft one as
it has been proved in the boost mailing list that #pragma once
produces a speed boost.

Example of a complete header guard:

#+begin_example
#ifndef KITANDA_KITANDA_HPP
#define KITANDA_KITANDA_HPP

#if defined(_MSC_VER) && (_MSC_VER >= 1200)
#pragma once
#endif

...

#endif
#+end_example

Header guards can be generated in emacs via the [[file:infrastructure/emacs.org][yas snippet]] =once=.

**** Tabs vs Spaces

All C++ source code shall be indented with spaces as this ensures the
same indentation regardless of editor. Tabs shall not be used.

*Note*: This is enforced in emacs via the both whitespace mode and the
settings in the [[Preamble][preamble]] of every file.

The boost [[http://www.boost.org/development/requirements.html][Tabs rationale]] states:

#+begin_quote
Tabs are banned because of the practical problems caused by tabs in
multi-developer projects like Boost, rather than any dislike in
principle. See mailing list archives. Problems include maintenance of
a single source file by programmers using tabs and programmers using
spaces, and the difficulty of enforcing a consistent tab policy other
than just "no tabs". Discussions concluded that Boost files should
either all use tabs, or all use spaces, and thus the decision to stick
with spaces.
#+end_quote

For a detailed analysis see [[http://www.jwz.org/doc/tabs-vs-spaces.html][Tabs versus Spaces: An Eternal Holy War]] by
jwz.

**** Column width

Source code must not exceed 80 columns. Continuations on subsequent
lines should be avoided - the line should read as a complete
statement. /Rationale/: the human brain is used to scan one line at a
time in rapid eye movements; making long lines or multi-line
statements short-circuits this process.

This also has the beneficial side-effect of making the code readable
even on text-mode consoles at 80x25, and allowing the editing/viewing
of two files on a high resolution monitor.

*Note*: This is enforced in emacs via the whitespace mode.

**** Function length

Functions should not exceed a screen full, measured at around 25 lines
of code. Functions become more and more difficult to understand as
they become larger.

**** Documentation

We use [[http://www.stack.nl/~dimitri/doxygen/][doxygen]] in our literate programming model. Most identifiers
should be documented, except in trivial cases. In particular:

- *namespace documentation*: Every namespace generating folder must
  have a namespace documentation header; this is a header file whose
  name matches the namespace and contains documentation explaining its
  purpose.

**** General Identifier naming

Identifier names in general follow the STL and boost conventions, with
a few kitanda peculiarities:

- *classes, structs, namespaces, functions and variables* are
  lower-cased and separated by underscores when multi-word;
  e.g. =this_is_a_class_name=. They should not be longer than four to
  five words at the extreme - with the exception of specification
  methods. These are always propositions and can be as long as
  necessary - provided the entire line does not exceed the maximum
  line size. FIXME point to specification section.
- *member variables*, in addition to the above, shall be post-fixed
  with an underscore, e.g. =member_variable_=. /Rationale/: underscore
  prefixes are reserved by standard C++.
- *concepts* are camel cased with a leading capital,
  e.g. ThisIsAConcept.

**** Indentation

In general, all source code must be indented using the /Stroupstrup/
style (FIXME add link to Wikipedia); there are a few kitanda
modifications, as follows.

*Namespaces* are not indented; the =namespace= keyword must always
start at column zero. Namespace closing brackets must always close all
the enclosing namespaces in a single line. Namespace closing brackets
are never followed by comments such as the name of the namespace being
closed. /Rationale/: all major editors tell you which namespace the
bracket is closing so this just adds more maintenance work for no
reason.

#+begin_example
namespace A {
namespace B {

} }
#+end_example

**** Literate programming

Almost all identifiers should have some form of doxygen comments;
however, we should try to avoid documenting too much directly in the
source code. Instead, org articles should be used for detailed
descriptions, specifications, etc.

Rules for comments:

1. The value of a comment is directly proportional to the distance
   between the comment and the code. Good comments stay as close as
   possible to the code they're referencing. As distance increases,
   the odds of developers making an edit without seeing the comment
   that goes with the code increases. The comment becomes misleading,
   out of date, or worse, incorrect. Distant comments are unreliable
   at best.

2. Comments with complex formatting cannot be trusted. Complex
   formatting is a pain to edit and a disincentive to maintenance. If
   it is difficult to edit a comment, it's very likely a developer has
   avoided or postponed synchronizing his work with the comments. I
   view complex comments with extreme skepticism.

3. Don't include redundant information in the comments. Why have a
   Revision History section-- isn't that what we use source control
   for? Besides the fact that this is totally redundant, the odds of a
   developer remembering, after every single edit, to update that
   comment section at the top of the procedure are.. very low.

4. The best kind of comments are the ones you don't need. The only
   "comments" guaranteed to be accurate 100% of the time-- and even
   that is debatable-- is the body of the code itself. Endeavor to
   write self-documenting code whenever possible. An occasional
   comment to illuminate or clarify is fine, but if you frequently
   write code full of "tricky parts" and reams of comments, maybe it's
   time to refactor.

5. [[http://www.codinghorror.com/blog/2006/12/code-tells-you-how-comments-tell-you-why.html][Code tells you how, comments tell you why]].

Source of 1-4: [[http://www.codinghorror.com/blog/2004/11/when-good-comments-go-bad.html][When Good Comments Go Bad]]

**** Operators

- Always on the same namespace as the class.

FIXME: clean this up.

How to decide if operator should be global or in a class?

My thoughts are this:

- Do you have two classes A & B that you want to make comparable with operator==?
- If it is just one, the choice is obvious to me: make it a member.
- But for two different classes, there is a symmetry between A==B &
  B==A, yet there isn't in the declaration of the fn.
- You need both "bool A::operator==(B const &) const" & "bool
  B::operator==(A const &) const". A solution is to have one as a
  member, the other as a free fn, calling the member.
- That means you have to have unsymmetric class declarations. Which is
  simply a matter of taste.
- If there are suitable getters already, then that is fine, one can
  have two free operator== fns, one as "bool operator==(A const &, B
  const &)" and the other for the commutative operation. Where you
  place the decls and the definitions is up to you.
- Probably best to distribute the decls across the two header files,
  according to the first parameter, forward declare the second, and
  place the definition in the equivalent, related source files.
- This nicely maintains symmetry, allows you to make the declaration
  of "bool operator==(A const &, B const &)" a friend of both classes,
  as can the decl "bool operator==(B const &, A const &)" be made a
  friend of both, avoid getters if they are ugly.

On operator==:

always check for =this= equality (e.g. self).

**** Swap function

- [[http://stackoverflow.com/questions/1998744/benefits-of-a-swap-function][why we should define it]].

**** Referential transparency

A function is referentially transparent when it is guaranteed to
return the same value every time it is called with a given set of
arguments. This is a great guarantee to be able to make about your
functions as it reduces the number of things you need to keep in your
head. Referentially transparent functions really can be thought of as
black boxes: always deterministic and predictable. But a function that
operates on mutable objects cannot be referentially transparent—it
cannot make any guarantees that future calls involving that same
object will result in the same value since that object could have a
different value at any time.

- [[http://technomancy.us/159][In which we plot an escape from the quagmire of equality]]

**** Make invalid states unrepresentable

In the functional world, programmers try to make it impossible to
express invalid states, such that the type system itself stops
programming errors. For example, say function =F= takes an enumeration
of type =E= as its first argument and a second argument which is only
valid for value =e= of the enumeration. Following the "make invalid
states undefined" principle, the second argument of =F= should not be
visible when the enumeration has values other than =e=.

In a more static world, this probably means falling down to MPL to
define the enum.

**** Constructors and Destructors

- [[http://www.developerfusion.com/article/133063/constructors-in-c11/][Constructors in C++11]]
- [[http://www.codesynthesis.com/~boris/blog/2012/04/04/when-provide-empty-destructor/][When to provide an empty destructor]]

1. The non-copyable idiom shall be implemented by deleting the
   required constructors:

#+begin_example
struct non_copyable {
   non_copyable & operator=(const non_copyable&) = delete;
   non_copyable(const non_copyable&) = delete;
};
#+end_example

2. The non-default constructible idiom shall be implemented by
   deleting the default constructor:

#+begin_example
   NonCopyable() = delete;
#+end_example

3. All regular constructors with a single parameter shall be marked as
   explicit. See [[http://www.parashift.com/c%2B%2B-faq-lite/ctors.html#faq-10.22][What is the purpose of the explicit keyword?]]

4. Move constructors and copy constructors shall never be marked as
   explicit. See [[http://stackoverflow.com/questions/6758717/explicit-move-constructor][Explicit move constructor?]]

5. Abstract classes shall be enforced to be abstract via the pure
   virtual destructor. See [[http://www-304.ibm.com/support/docview.wss?uid%3Dswg21165191][Creating a Pure Virtual Destructor for a
   C++ Abstract Class]].

#+begin_example
struct abstract {
   virtual ~abstract() noexcept = 0;
};

abstract::~abstract() { }
#+end_example

**** Enumerations

#+begin_quote
An enumeration is a user-defined type consisting of a set of named
constants called enumerators. Enumerators have values.
#+end_quote

1. All enumerations shall be defined as C++-11 strong enumerations
   with the correct underlying type.

  - [[http://www.codeguru.com/cpp/cpp/article.php/c19083/C-2011-Stronglytyped-Enums.htm][C++ 2011: Strongly-typed Enums]]
  - [[http://www.cprogramming.com/c%2B%2B11/c%2B%2B11-nullptr-strongly-typed-enum-class.html][Better types in C++11 - nullptr, enum classes (strongly typed enumerations) and cstdint]]
  - [[http://nic-gamedev.blogspot.co.uk/2012/04/c11-strongly-typed-enum.html][C++11: Strongly-typed enum]]
  - [[http://www.nullptr.me/2012/01/04/enum-classes/#.T7aeNvHgYhY][C++11: enum classes]]

2. The enumeration name shall always be plural.

#+begin_example
enum class node_types : unsigned int {
...
#+end_example

   /Rationale/: This is a very difficult decision to make, mainly
   because an enumeration identifier is actually doing two jobs: a)
   the declaration of a domain of valid values - as such its the name
   of the set containing those valid values, so it makes sense to be
   plural (e.g. =player_types= is the set of valid =player_type=); b)
   its also used as a type for variables that take a single value of
   that domain, in which case it should be singular.

   The declaration of the enum occurs only once, whilst its usage is
   everywhere so at first sight it seems that the singular usage
   should win. This is the approach taken by Microsoft for
   C#. However, another /very/ typical use case is to declare getters
   and setters for a value of a type of an enumeration. This is where
   we reach the limits of C++ compiler intelligence because if we have
   both an enumeration and a member variable declared with the same
   name, the compiler thinks that we are hiding one of the names. Thus
   we end up with all sorts of imaginative getter/setter names to get
   around this problem. Due to this, the final decision is to use a
   plural name for the enumeration so that the singular name can be
   reserved for getters, setters and variables of that type.

3. Doxygen documentation shall be used to document the enumeration
   itself and all the enumerators.

#+begin_example
/**
 * @brief A given set of XML data is modeled as a tree of
 * nodes. This enumeration specifies the different node types.
 */
enum class node_types : unsigned int {
    none = 0, ///< Read method has not yet been called
...
#+end_example

4. Inserters shall be provided for each enumeration, dumping a textual
   representation of a given instance of an enumeration. The textual
   representation shall take the form =ENUMERATION::ENUMERATOR=, e.g.:
   =node_types::none=.

#+begin_example
std::ostream& operator<<(std::ostream& stream, object_types value);
#+end_example

5. It is possible for a variable of an enumeration type to have a
   value which is not part of the enumeration's domain. The exception
   =invalid_enum_value= (in =kitanda::utility::exception::=) shall be
   thrown at the point of detection.

6. When a given value of an enumeration which is part of its domain is
   not valid in a given context, the exception =invalid_enum_value=
   shall be thrown.

7. A function to convert from an instance of the underlying type to an
   enumerator may be provided. The function name shall take the form
   =to_ENUM_NAME=, where =ENUM_NAME= takes the singular form (e.g. for
   =object_types= its =object_type=). The function shall have a single
   parameter called =value=, of the same type as the underlying type
   of the enumeration. The function shall be defined in the utility
   namespace.

#+begin_example
object_types to_object_type(unsigned int value);
#+end_example

8. A function to convert a string representation of a value in the
   enumeration's domain to an enumerator may be provided. The function
   name shall take the form =parse_ENUM_NAME=, where =ENUM_NAME= takes
   the singular form (e.g. for =object_types= its =object_type=). The
   function shall have a have a single parameter of type =std::string=
   named =value=.

   The function shall be defined in the utility namespace. The format
   of the input string is enumeration dependent and may not be related
   to the streaming of the enumeration. /Rationale/: the prefix
   =parse= is used to imply that the conversion is more elaborate than
   the underlying type conversion. This is used by Microsoft for C#.

#+begin_example
object_types parse_object_type(std::string value);
#+end_example

9. A function to convert from an instance of the enumeration to the
   underlying type shall not be provided. Users are expected to use
   the =static_cast= operator.

**** Classes

1. Class names shall not use a prefix or post-fix to indicate their
   "meta-type". For example =IInterface= or =base_class= are bad
   names. /Rationale/: this is just a variation of Hungarian
   notation. For a good explanation of why the =base= suffix is a bad
   idea see Cwalina's [[http://blogs.msdn.com/b/kcwalina/archive/2005/12/16/basesuffix.aspx][blog post]].

2. Virtual destructors must be implemented due to a problem with
   clang - e.g. we cannot rely on using =default=. The canonical form
   for the virtual destructor:

#+begin_example
    virtual ~CLASS_NAME() noexcept {}
#+end_example

  Note that there is no space between ={}=.

*** References

This section lists books, sites and other material that we have read,
perused or have been otherwise in contact with, and provide useful
information with regards to C++ standards.

**** Coding standards and guidelines

- [[http://www.amazon.co.uk/Coding-Standards-Rules-Guidelines-Practices/dp/0321113586/ref%3Dsr_1_1?s%3Dbooks&ie%3DUTF8&qid%3D1393685150&sr%3D1-1&keywords%3DC%252B%252B%2BCoding%2BStandards%2B%253A%2BRules%252C%2BGuidelines%252C%2Band%2BBest%2BPractices][C++ Coding Standards : Rules, Guidelines, and Best Practices]]: The
  book to read on coding standards, especially for C++. Legislate what
  needs to be legislated and leaves alone what should be left alone.
- [[http://www.boost.org/development/requirements.html][Boost coding standards]]: The standards and recommendations of the
  boost project, on the whole useful.
- [[http://www.amazon.co.uk/Large-Scale-C-Software-Design-APC/dp/0201633620][Large Scale Software Development With C++]]: This is a classic book
  that discusses many aspects which are normally not considered such
  as minimising compilation times, etc. Unfortunately it is dated on
  some parts, making the read a bit more difficult. We need to do a
  second parse of this book.
- [[http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml][Google C++ Style Guide]]: Good in some bits, although very bad in
  others (for example, bans exceptions).
- [[https://github.com/holtgrewe/linty][Linty]]: Style Checking with Python & libclang. Requires v3.0.
- [[http://www.parashift.com/c%2B%2B-faq-lite/][C++ FAQ]]: large resource on how to use the language
- [[http://www.doc.ic.ac.uk/lab/cplus/c%2B%2B.rules/][Programming in C++, Rules and Recommendations]]

**** C++ Reference

- [[http://en.cppreference.com/w/cpp][C++ Reference wiki]]

**** C++-11

- [[http://msdn.microsoft.com/en-us/library/hh279654%2528v%3Dvs.110%2529.aspx][Welcome Back to C++ (Modern C++)]]
- [[http://herbsutter.com/elements-of-modern-c-style/][Elements of Modern C++ Style]]
- [[http://www2.research.att.com/~bs/C%2B%2B0xFAQ.html][C++11 - the recently approved new ISO C++ standard]]

**** Videos and Talks

- [[http://vimeo.com/23975522][C++0x Lambda Functions]]

**** Useful Libraries

- [[http://boost-sandbox.sourceforge.net/libs/time_series/doc/html/time_series/user_s_guide.html#time_series.user_s_guide.series_containers][Boost Time Series]]: Library that didn't make it into boost but seems
  like a very good foundation for all the time series code. See also
  the [[http://lists.boost.org/boost-announce/2007/08/0142.php][list of problems with the library.]]
- [[http://www.craighenderson.co.uk/mapreduce/][Boost.MapReduce]]: [[http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/mapreduce-osdi04.pdf][Map reduce]] implementation for single process.
- [[http://blog.quibb.org/cppbench/][CPP Bench]]: benchmarking in C++.
- [[https://bitbucket.org/maghoff/jsonxx/wiki/Home][JSONXX]]: JSON library for C++.
- [[http://calvados.di.unipi.it/dokuwiki/doku.php?id%3Dffnamespace:about%20][FastFlow]]: a skeletal multi-core programming framework
- [[http://lwn.net/Articles/370307/#Comments][0MQ: A new approach to messaging]]
- [[http://www.globus.org/toolkit/about.html][Globulus Toolkit]]: grid toolkit for C++
- [[http://www.dre.vanderbilt.edu/~sutambe/files/LEESA/LEESA.pdf][LEESA]]: XML in C++
- [[http://www.codesynthesis.com/products/xsde/][XSD/e]]: Light-weight XML in C++ 
- [[https://github.com/consultomd/json_spirit][JSON Spirit]]
- [[https://github.com/d5][Node.Native]]: C++ Node.js implementation
- [[http://svn.boost.org/svn/boost/sandbox/monotonic/libs/monotonic/doc/html/index.html][Boost.Monotonic]]: Special purpose allocators. Discussed on mailing
  list [[http://lists.boost.org/Archives/boost/2012/03/191798.php][here]].
- [[http://code.google.com/p/ceres-solver/][Ceres Solver]]: Library for solving nonlinear least squares
- [[http://code.google.com/p/dtl-cpp/][DTL]]: The diff template library.

**** Other C++ articles

- [[http://altdevblogaday.com/2011/11/09/a-low-level-curriculum-for-c-and-c/][A Low Level Curriculum for C and C++]]
- [[http://altdevblogaday.com/2011/11/24/c-c-low-level-curriculum-part-2-data-types/][C / C++ Low Level Curriculum part 2: Data Types]]
- [[http://altdevblogaday.com/2011/12/14/c-c-low-level-curriculum-part-3-the-stack/][C / C++ Low Level Curriculum Part 3: The Stack]]
- [[http://smellegantcode.wordpress.com/tag/c11/][Linq in C++]] See also the fix [[http://boost.2283326.n4.nabble.com/range-cannot-use-lambda-predicate-in-adaptor-with-certain-algorithms-td3560157.html][here]].
- [[http://confluence.jetbrains.net/display/TW/Cpp%2BUnit%2BTest%2BReporting][Changing the output of Boost.Test]]
- [[http://software.intel.com/en-us/articles/extending-stl-for-games/][Extending STL for Games]]
- [[http://en.wikibooks.org/wiki/Optimizing_C%252B%252B][Optimising C++]]: free ebook
- [[http://google-opensource.blogspot.co.uk/2010/10/integrating-r-with-c-rcpp-rinside-and.html][Integrating R with C++: Rcpp, RInside, and RProtobuf]]
- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1476.html][Iterator facade and adaptor]]
- [[http://blog.rethinkdb.com/improving-a-large-c-project-with-coroutines][Improving a large C++ project with co-routines]]
- [[http://www.ibm.com/developerworks/aix/library/au-ctools1_boost/%20][Get to know the Boost unit test framework]]
- [[http://drdobbs.com/parallel/232400273][Welcome to the Parallel Jungle!]]
- [[http://www.radmangames.com/programming/how-futures-aid-task-based-multithreading][How futures aid task based multi-threading]]
- [[http://www.viva64.com/en/l/full/][Lessons on development of 64-bit C/C++ applications]]
- [[http://boost-spirit.com/home/2010/05/11/a-framework-for-rad-spirit/][A framework for RAD]]: boost spirit
- [[http://arma.sourceforge.net/][Armadillo: C++ linear algebra library]]
- [[http://nerds-central.blogspot.co.uk/2012/05/c11-future-chaining-for-easy-highly.html][C++11: Future Chaining For Easy, Highly Threaded Execution]]
- [[http://www.altdevblogaday.com/2012/04/26/functional-programming-in-c/][Functional Programming in C++]]

**** Videos

- [[http://www.corensic.com/Learn/Resources/ConcurrencyTutorialPartOne.aspx][C++ Concurrency Series]]: Part 1

* APPENDIX

** Appendix A - Related Work

This section is a bit of a general research bucket. It contains a set
of links to the C++ code generators we have found on our wanderings on
the internet, as well as other interesting projects in this space -
including those in other programming languages. It also contains books
and papers on the subject we have read, or intend to read.

- [[http://www.amazon.co.uk/Domain-Driven-Design-Tackling-Complexity-ebook/dp/B00794TAUG/ref%3Dsr_1_2?ie%3DUTF8&qid%3D1368380797&sr%3D8-2&keywords%3Dmodel%2Bdriven%2Bdesign][Domain-Driven Design: Tackling Complexity in the Heart of Software]]:
  The Eric Evans book from which we tried to steal most concepts in
  Dogen. A must read for any developer.
- [[http://www.amazon.co.uk/EMF-Eclipse-Modeling-Framework-ebook/dp/B0013TPYVW/ref%3Dsr_1_2?s%3Dbooks&ie%3DUTF8&qid%3D1368380262&sr%3D1-2&keywords%3DEclipse%2BModeling%2BFramework%2B%255BPaperback%255D][EMF: Eclipse Modeling Framework]]: The original EMF book. Useful read
  for anyone interested in code generation.
- [[http://www.scribd.com/doc/78264699/Model-Driven-Architecture-for-Reverse-Engineering-Technologies-Strategic-Directions-and-System-Evolution-Premier-Reference-Source][Model Driven Architecture for Reverse Engineering Technologies]]:
  Preview of a potentially interesting MDA book.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: This paper will be of
  interest if we decide to support multiple languages.
- [[http://www.vollmann.com/pubs/meta/meta/meta.html][Metaclasses and Reflection in C++]]: Some (early) ideas on
  implementing a MOP (Meta Object Protocol) in C++.
- [[https://code.google.com/a/eclipselabs.org/p/cppgenmodel/][cppgenmodel - A model driven C++ code generator]]: This seems more
  like a run time / reflection based generator.
- [[https://code.google.com/p/emf4cpp/][EMF4CPP - Eclipse Modeling Framework]]: C++ port of the EMF/eCore
  eclipse framework. As with Java it includes run time support. There
  is also [[http://apps.nabbel.es/dsdm2010/download_files/dsdm2010_senac.pdf][a paper]] on it.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: Describes a meta-model
  designed to model Java and C++.
- [[http://marofra.com/oldhomepage/MetaCPlusPlusDoc/metacplusplus-1.html][The Meta-C++ User Manual]]: Another early C++ meta-modeling
  tool. Contains interesting ideas around C++ meta-models.
- The Columbus C++ Schema: Useful tool for re-engineering large C++
  code bases. Contains a meta-model for C++. A number of papers have
  been written about it:
  - [[http://www.inf.u-szeged.hu/~beszedes/research/tech27_ferenc_r.pdf][Columbus – Reverse Engineering Tool and Schema for C++]]
  - [[http://journal.ub.tu-berlin.de/eceasst/article/download/10/19][Third Workshop on Software Evolution through Transformations]]:
    Embracing the Change
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_schema.ppt.pdf][Towards a Standard Schema for C/C++]]
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_columbus_schema_cpp.pdf][Data Exchange with the Columbus Schema for C++]]
- [[http://www.cpgf.org/][CPGF]]: An open source C++ library for reflection, script binding,
  serialisation and callbacks.
- [[http://www.artima.com/articles/dci_vision.html][DCI]]: The DCI Architecture: A New Vision of Object-Oriented
  Programming. Some fundamental insights on the nature of OO.
- [[http://www.ischo.com/xrtti/index.html][xrtti]]: Extending C++ with a richer reflection.
- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3883.html][Code checkers and generators]]: adding AngularJS-like capabilities to
  C++.
- [[http://stackoverflow.com/questions/355650/c-html-template-framework-templatizing-library-html-generator-library][Text Template libraries for C++]]: T4 like implementations for C++.
